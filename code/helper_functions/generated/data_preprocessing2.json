[
    {
        "function_name": "remove_outliers",
        "file_name": "data_cleaning.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "threshold": "float",
            "window_size": "int"
        },
        "objectives": [
            "For each column in the dataframe \"data\", apply a rolling standard deviation with a window size of \"window_size\".",
            "Create a mask for values that exceed \"threshold\" standard deviations from the rolling mean.",
            "Create a new dataframe by removing all rows where at least one value is masked.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def remove_outliers(data, threshold, window_size):\n    mask = pd.DataFrame(False, index=data.index, columns=data.columns)\n    \n    for col in data.columns:\n        roll_std = data[col].rolling(window_size).std()\n        roll_mean = data[col].rolling(window_size).mean()\n        mask[col] = np.abs(data[col] - roll_mean) > threshold * roll_std\n    \n    new_data = data[~mask.any(axis=1)]\n    \n    return new_data"
    },
    {
        "function_name": "calculate_weighted_mean",
        "file_name": "data_aggregation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "value_column": "str",
            "weight_column": "str"
        },
        "objectives": [
            "Calculate the weighted mean of the column \"value_column\" in the dataframe \"data\" using the weights in the column \"weight_column\".",
            "Create a new dataframe by grouping the data by the weighted mean and calculating the sum of the weights for each group.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_weighted_mean(data, value_column, weight_column):\n    weighted_mean = (data[value_column] * data[weight_column]).sum() / data[weight_column].sum()\n    data['group'] = np.where(data[value_column] <= weighted_mean, 'less_than_mean', 'greater_than_mean')\n    grouped_data = data.groupby('group')\n    \n    sum_of_weights = grouped_data[weight_column].sum().reset_index()\n    \n    return sum_of_weights"
    },
    {
        "function_name": "remove_correlated_columns",
        "file_name": "data_reduction.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "threshold": "float",
            "iteration_limit": "int"
        },
        "objectives": [
            "For each column in the dataframe \"data\", calculate the correlation coefficient with every other column.",
            "Create a mask for correlation coefficients that are greater than \"threshold\".",
            "Iterate through the columns and remove any columns that have a correlation coefficient greater than \"threshold\" with any other column.",
            "Repeat the iteration until no columns are removed or the iteration limit is reached.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_correlated_columns(data, threshold, iteration_limit):\n    iteration = 0\n    removed_columns = set()\n    \n    while iteration < iteration_limit:\n        correlation_matrix = data.corr()\n        mask = correlation_matrix.abs() > threshold\n        \n        columns_to_remove = set()\n        for col in data.columns:\n            if col in removed_columns:\n                continue\n            for other_col in data.columns:\n                if col != other_col and mask.loc[col, other_col]:\n                    columns_to_remove.add(col)\n                    break\n        \n        if not columns_to_remove:\n            break\n        \n        removed_columns.update(columns_to_remove)\n        data = data.drop(columns_to_remove, axis=1)\n        \n        iteration += 1\n    \n    return data"
    },
    {
        "function_name": "update_holiday_dates",
        "file_name": "date_manipulations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "holiday_list": "list"
        },
        "objectives": [
            "Identify the rows in the dataframe \"data\" where the date in column \"date_column\" is a holiday (present in the \"holiday_list\").",
            "Create a new column \"is_holiday\" and assign 1 if the date is a holiday, 0 otherwise.",
            "Shift the values in the column \"date_column\" by one day for the rows where \"is_holiday\" is 1.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def update_holiday_dates(data, date_column, holiday_list):\n    data[date_column] = pd.to_datetime(data[date_column])\n    data['is_holiday'] = data[date_column].isin(holiday_list).astype(int)\n    \n    mask = data['is_holiday'] == 1\n    data.loc[mask, date_column] = data.loc[mask, date_column] + pd.Timedelta(days=1)\n    \n    return data"
    },
    {
        "function_name": "fill_missing_dates",
        "file_name": "date_imputation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "start_date": "str",
            "end_date": "str",
            "frequency": "str"
        },
        "objectives": [
            "Create a new dataframe with a date range from \"start_date\" to \"end_date\" with a frequency of \"frequency\".",
            "Merge the new dataframe with the original dataframe \"data\" on the date column.",
            "Fill in missing values in the merged dataframe using linear interpolation.",
            "Return the merged dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def fill_missing_dates(data, start_date, end_date, frequency):\n    date_range = pd.date_range(start=start_date, end=end_date, freq=frequency)\n    new_data = pd.DataFrame(pd.Series(date_range), columns=['date'])\n    \n    merged_data = pd.merge(new_data, data, on='date', how='outer')\n    \n    merged_data.interpolate(method='linear', limit_direction='both', inplace=True)\n    \n    return merged_data"
    },
    {
        "function_name": "group_and_aggregate",
        "file_name": "data_aggregation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_columns": "list",
            "aggregate_columns": "list"
        },
        "objectives": [
            "Group the dataframe \"data\" by the columns specified in \"group_columns\".",
            "Calculate the sum of the columns specified in \"aggregate_columns\" for each group.",
            "Create a new column \"average\" and assign the average of the aggregated columns for each group.",
            "Return the grouped dataframe with the aggregated columns and the \"average\" column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_and_aggregate(data, group_columns, aggregate_columns):\n    grouped_data = data.groupby(group_columns)[aggregate_columns].sum().reset_index()\n    \n    grouped_data['average'] = grouped_data[aggregate_columns].mean(axis=1)\n    \n    return grouped_data"
    },
    {
        "function_name": "filter_and_threshold",
        "file_name": "correlation_filters.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "threshold": "float"
        },
        "objectives": [
            "Find the top 3 columns in the dataframe \"df\" that have the highest correlation with the last column of the dataframe (assuming the last column is the target variable).",
            "Create a new dataframe \"top_corr_df\" containing only these 3 columns and the last column.",
            "In \"top_corr_df\", apply a threshold to each column (except the last column) such that all values less than the threshold are replaced with the median of the column, and all values greater than the threshold are replaced with the mean of the column.",
            "Drop all rows in \"top_corr_df\" that have a null value in any of the first 3 columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def filter_and_threshold(df, threshold):\n    corr_matrix = df.corr()\n    top_corr_cols = corr_matrix.iloc[:-1, -1].sort_values(ascending=False).head(3).index\n    \n    top_corr_df = df[top_corr_cols.tolist() + [df.columns[-1]]]\n    \n    for col in top_corr_cols:\n        median = top_corr_df[col].median()\n        mean = top_corr_df[col].mean()\n        top_corr_df[col] = top_corr_df[col].apply(lambda x: median if x < threshold else (mean if x > threshold else x))\n    \n    top_corr_df.dropna(subset=top_corr_cols, inplace=True)\n    \n    return top_corr_df"
    },
    {
        "function_name": "scale_and_correlate",
        "file_name": "scaling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_name": "str",
            "threshold": "float"
        },
        "objectives": [
            "Find the minimum and maximum values of the column \"col_name\" in the dataframe \"df\".",
            "Create a new column \"scaled_col\" in \"df\" where each value is scaled to be between 0 and 1 using the minimum and maximum values.",
            "For each value in \"scaled_col\", apply a threshold such that all values less than the threshold are replaced with 0 and all values greater than the threshold are replaced with 1.",
            "Calculate the correlation between the original column \"col_name\" and the scaled column \"scaled_col\" and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def scale_and_correlate(df, col_name, threshold):\n    min_val = df[col_name].min()\n    max_val = df[col_name].max()\n    \n    df['scaled_col'] = (df[col_name] - min_val) / (max_val - min_val)\n    df['scaled_col'] = df['scaled_col'].apply(lambda x: 0 if x < threshold else (1 if x > threshold else x))\n    \n    correlation = df[col_name].corr(df['scaled_col'])\n    return correlation"
    },
    {
        "function_name": "remove_outlier_columns",
        "file_name": "outlier_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "k": "int",
            "threshold": "float"
        },
        "objectives": [
            "Find the columns in the dataframe \"df\" that have at least \"k\" number of unique values.",
            "Calculate the \"inter-quartile range\" for each of these columns.",
            "Identify columns that have an IQR greater than the \"threshold\" value, and rename these columns to \"outlier_col\".",
            "Create a new dataframe by keeping only the columns that were not renamed, and call it \"cleaned_df\".",
            "Return the \"outlier_col\" columns and \"cleaned_df\"."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_outlier_columns(df, k, threshold):\n\toutlier_cols = []\n\tcleaned_df = df.copy()\n\t\n\tfor col in df.columns:\n\t\tif df[col].nunique() >= k:\n\t\t\tiqr = df[col].quantile(0.75) - df[col].quantile(0.25)\n\t\t\tif iqr > threshold:\n\t\t\t\toutlier_cols.append(col)\n\t\t\t\tcleaned_df.rename(columns={col: 'outlier_col'}, inplace=True)\n\t\n\tcleaned_df = cleaned_df.loc[:, ~cleaned_df.columns.isin(outlier_cols)]\n\t\n\treturn outlier_cols, cleaned_df"
    },
    {
        "function_name": "remove_highly_correlated",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "max_correlation": "float"
        },
        "objectives": [
            "Calculate the correlation matrix for the dataframe \"data\".",
            "Identify pairs of columns that have a correlation greater than \"max_correlation\".",
            "Create a new dataframe by dropping one of the columns from each pair of highly correlated columns, and call it \"decorrelated_df\".",
            "Return the \"decorrelated_df\"."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def remove_highly_correlated(data, max_correlation):\n\tdecorrelated_df = data.copy()\n\tcorr_matrix = data.corr()\n\t\n\tfor i in range(len(corr_matrix)):\n\t\tfor j in range(i+1, len(corr_matrix)):\n\t\t\tif abs(corr_matrix.iloc[i, j]) > max_correlation:\n\t\t\t\tdecorrelated_df = decorrelated_df.drop(data.columns[j], axis=1)\n\t\n\treturn decorrelated_df"
    },
    {
        "function_name": "redact_sensitive_info",
        "file_name": "data_privacy.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "sensitive_cols": "list[str]"
        },
        "objectives": [
            "Identify the columns in the dataframe \"df\" that contain sensitive information.",
            "Create a new dataframe by replacing the values in the sensitive columns with the string \"REDACTED\".",
            "Return the \"redacted_df\"."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def redact_sensitive_info(df, sensitive_cols):\n\tredacted_df = df.copy()\n\t\n\tfor col in sensitive_cols:\n\t\tif col in df.columns:\n\t\t\tredacted_df[col] = 'REDACTED'\n\t\n\treturn redacted_df"
    },
    {
        "function_name": "join_dataframes",
        "file_name": "data_integration.py",
        "parameters": {
            "df1": "pandas.DataFrame",
            "df2": "pandas.DataFrame",
            "join_cols": "list[str]"
        },
        "objectives": [
            "Perform an outer join on the two dataframes \"df1\" and \"df2\" using the columns specified in \"join_cols\".",
            "Replace all the NaN values in the joined dataframe with the string \"UNKNOWN\".",
            "Return the joined dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def join_dataframes(df1, df2, join_cols):\n\tjoined_df = pd.merge(df1, df2, on=join_cols, how='outer')\n\tjoined_df.fillna('UNKNOWN', inplace=True)\n\t\n\treturn joined_df"
    },
    {
        "function_name": "merge_and_calculate_stats",
        "file_name": "statistical_analysis.py",
        "parameters": {
            "df1": "pandas.DataFrame",
            "df2": "pandas.DataFrame",
            "threshold": "float"
        },
        "objectives": [
            "Find the columns in both dataframes df1 and df2 that have a correlation coefficient greater than the specified threshold.",
            "Merge the two dataframes based on these columns and call it merged_df.",
            "Calculate the mean and standard deviation of each column in merged_df and store them in a list of tuples.",
            "Return the list of tuples and merged_df."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def merge_and_calculate_stats(df1, df2, threshold):\n    # Step 1: Find the columns in both dataframes that have a correlation coefficient greater than the threshold\n    correlated_cols = []\n    for col1 in df1.columns:\n        for col2 in df2.columns:\n            if np.corrcoef(df1[col1], df2[col2])[0, 1] > threshold:\n                correlated_cols.append((col1, col2))\n    \n    if not correlated_cols:\n        raise ValueError(\"No correlated columns found.\")\n    \n    # Step 2: Merge the two dataframes based on these columns\n    merged_dfs = []\n    for col_pair in correlated_cols:\n        merged_df = pd.merge(df1, df2, left_on=col_pair[0], right_on=col_pair[1])\n        merged_dfs.append(merged_df)\n    \n    # Step 3: Calculate the mean and standard deviation of each column in merged_df\n    stats_ls = []\n    for merged_df in merged_dfs:\n        stats = [(col, merged_df[col].mean(), merged_df[col].std()) for col in merged_df.columns]\n        stats_ls.append(stats)\n    \n    return stats_ls, merged_dfs"
    },
    {
        "function_name": "time_series_analysis",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "time_series": "pandas.Series",
            "window_size": "int"
        },
        "objectives": [
            "For each window of size 'window_size' in the time series, calculate the mean, median, and standard deviation.",
            "Create a new dataframe with these calculated values and the corresponding window start and end indices.",
            "Return the dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def time_series_analysis(time_series, window_size):\n    # Step 1: Calculate the mean, median, and standard deviation for each window\n    window_stats = []\n    for i in range(0, len(time_series), window_size):\n        window = time_series[i:i+window_size]\n        window_mean = window.mean()\n        window_median = window.median()\n        window_std = window.std()\n        window_stats.append((i, i+window_size, window_mean, window_median, window_std))\n    \n    # Step 2: Create a new dataframe with the calculated values and corresponding window indices\n    stats_df = pd.DataFrame(window_stats, columns=['start', 'end', 'mean', 'median', 'std'])\n    \n    return stats_df"
    },
    {
        "function_name": "value_above_threshold",
        "file_name": "threshold_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "threshold": "float"
        },
        "objectives": [
            "For each column in the dataframe data, find the values that are above the specified threshold.",
            "Create a new dataframe with these values and the corresponding column names and indices.",
            "Group the new dataframe by column name and calculate the mean and count of values above the threshold for each column.",
            "Return the grouped dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def value_above_threshold(data, threshold):\n    # Step 1: Find the values that are above the specified threshold for each column\n    above_threshold_values = []\n    for col in data.columns:\n        for i, value in enumerate(data[col]):\n            if value > threshold:\n                above_threshold_values.append((col, i, value))\n    \n    # Step 2: Create a new dataframe with the values above the threshold and corresponding column names and indices\n    above_threshold_df = pd.DataFrame(above_threshold_values, columns=['column', 'index', 'value'])\n    \n    # Step 3: Group the new dataframe by column name and calculate the mean and count of values above the threshold for each column\n    grouped_df = above_threshold_df.groupby('column')[['value']].agg(['mean', 'count'])\n    \n    return grouped_df"
    },
    {
        "function_name": "categorical_column_expansion",
        "file_name": "category_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categories": "list"
        },
        "objectives": [
            "Find the number of unique values in each column of the dataframe \"data\".",
            "Create a new dataframe \"category_counts\" with the number of unique values for each category in \"categories\".",
            "Iterate over each category and for each unique value in that category, create a new column in \"data\" with the count of rows where the category equals the unique value.",
            "Return \"category_counts\" and the updated dataframe \"data\"."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def categorical_column_expansion(data, categories):\n    category_counts = pd.DataFrame(index=categories, columns=[f'count_{col}' for col in categories])\n    \n    for i, category in enumerate(categories):\n        unique_values = data[category].unique()\n        category_counts.iloc[i] = [len(unique_values) for _ in range(len(categories))]\n        \n        for value in unique_values:\n            col_name = f'{category}_{value}'\n            data[col_name] = data.apply(lambda row: 1 if row[category] == value else 0, axis=1)\n    \n    return category_counts, data"
    },
    {
        "function_name": "weighted_group_sum",
        "file_name": "weighted_aggregation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "weight_column": "str",
            "group_column": "str"
        },
        "objectives": [
            "Calculate the weighted sum of all numeric columns in the dataframe \"data\", using the weights from \"weight_column\".",
            "Group the dataframe by the \"group_column\" and calculate the sum of each column within each group.",
            "Create a new column in the dataframe with the weighted sum of the grouped sums.",
            "Return the dataframe with the new column and the original weight column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def weighted_group_sum(data, weight_column, group_column):\n    # Calculate weighted sum of all numeric columns\n    weighted_sum = data.select_dtypes(include=[int, float]).apply(lambda x: (x * data[weight_column]).sum())\n    \n    # Group by group_column and calculate sum of each column within each group\n    group_sums = data.groupby(group_column).sum()\n    \n    # Create a new column with the weighted sum of the grouped sums\n    weighted_group_sum_column = group_sums.apply(lambda x: x.dot(weighted_sum), axis=1)\n    data[f'weighted_{group_column}_sum'] = weighted_group_sum_column\n    \n    return data"
    },
    {
        "function_name": "group_low_frequency_categories",
        "file_name": "categorical_data_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_cols": "list",
            "threshold": "float"
        },
        "objectives": [
            "For each categorical column in df, calculate the frequency of each category.",
            "Identify all the categories with frequency less than threshold.",
            "Group these categories into a single category called \"Others\" in each column.",
            "Return the modified dataframe and a dictionary with the original categories and their corresponding new categories."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_low_frequency_categories(df, cat_cols, threshold):\n    # Step 1: Calculate the frequency of each category in each column\n    freq_dict = {}\n    for col in cat_cols:\n        freq_dict[col] = df[col].value_counts()\n    \n    # Step 2: Identify all the categories with frequency less than threshold\n    low_freq_cats = {}\n    for col, freqs in freq_dict.items():\n        low_freq_cats[col] = [cat for cat, freq in freqs.items() if freq < threshold]\n    \n    # Step 3: Group these categories into a single category called \"Others\" in each column\n    new_cats = {}\n    for col, cats in low_freq_cats.items():\n        new_cats[col] = {}\n        for cat in cats:\n            new_cats[col][cat] = 'Others'\n        for cat in df[col].unique():\n            if cat not in cats:\n                new_cats[col][cat] = cat\n    \n    df.replace(new_cats, inplace=True)\n    \n    return df, new_cats"
    },
    {
        "function_name": "find_top_k_values",
        "file_name": "value_distribution_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "k": "int"
        },
        "objectives": [
            "Find the top k most common values in each column of df.",
            "Create a new dataframe with these top k most common values and their corresponding frequencies.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def find_top_k_values(df, k):\n    # Step 1: Find the top k most common values in each column\n    top_k_values = {}\n    for col in df.columns:\n        top_k_values[col] = df[col].value_counts().head(k)\n    \n    # Step 2: Create a new dataframe with these top k most common values and their corresponding frequencies\n    new_df = pd.DataFrame({col: top_k_values[col].index for col in df.columns})\n    new_df_freq = pd.DataFrame({col: top_k_values[col].values for col in df.columns})\n    \n    # Combine the dataframes\n    new_df = pd.concat([new_df, new_df_freq], axis=1)\n    \n    return new_df"
    },
    {
        "function_name": "filter_groupby_sum",
        "file_name": "groupby_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "groupby_cols": "list",
            "sum_cols": "list",
            "threshold": "int"
        },
        "objectives": [
            "Group the input DataFrame `df` by the columns specified in `groupby_cols`.",
            "Calculate the sum of the columns specified in `sum_cols` for each group.",
            "Filter out the groups that have a sum less than the specified `threshold`.",
            "Return the filtered DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_groupby_sum(df, groupby_cols, sum_cols, threshold):\n    # Step 1: Group the DataFrame by the specified columns\n    grouped_df = df.groupby(groupby_cols)\n    \n    # Step 2: Calculate the sum of the specified columns for each group\n    sum_df = grouped_df[sum_cols].sum().reset_index()\n    \n    # Step 3: Filter out the groups that have a sum less than the threshold\n    filtered_df = sum_df[(sum_df[sum_cols[0]] >= threshold)]\n    \n    # Step 4: Merge the filtered DataFrame with the original DataFrame\n    merged_df = pd.merge(df, filtered_df, on=groupby_cols)\n    \n    return merged_df"
    },
    {
        "function_name": "identify_outliers",
        "file_name": "window_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the rolling mean of the last column in the DataFrame `df` with a window size of `window_size`.",
            "Calculate the standard deviation of the rolling mean.",
            "Identify the rows where the standard deviation is greater than the specified `threshold`.",
            "Return the indices of these rows."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_outliers(df, window_size, threshold):\n    # Step 1: Calculate the rolling mean of the last column\n    rolling_mean = df.iloc[:, -1].rolling(window_size).mean()\n    \n    # Step 2: Calculate the standard deviation of the rolling mean\n    rolling_std = rolling_mean.rolling(window_size).std()\n    \n    # Step 3: Identify the rows where the standard deviation is greater than the threshold\n    outlier_indices = rolling_std[rolling_std > threshold].index\n    \n    return outlier_indices"
    },
    {
        "function_name": "filter_top_categories",
        "file_name": "category_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list",
            "top_n": "int"
        },
        "objectives": [
            "Identify the top `top_n` most frequent categories in each of the columns specified in `categorical_cols`.",
            "Create a new DataFrame with only the most frequent categories.",
            "Return the new DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_top_categories(df, categorical_cols, top_n):\n    # Step 1: Identify the top categories in each column\n    top_categories = {}\n    for col in categorical_cols:\n        top_categories[col] = df[col].value_counts().index[:top_n]\n    \n    # Step 2: Create a new DataFrame with only the most frequent categories\n    filtered_df = df.copy()\n    for col in categorical_cols:\n        filtered_df = filtered_df[filtered_df[col].isin(top_categories[col])]\n    \n    return filtered_df"
    },
    {
        "function_name": "mark_holidays",
        "file_name": "date_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "holiday_dates": "list"
        },
        "objectives": [
            "Identify the rows in the DataFrame `df` where the date in the column `date_col` is a holiday date specified in `holiday_dates`.",
            "Create a new column 'is_holiday' and mark these rows as True.",
            "Return the updated DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def mark_holidays(df, date_col, holiday_dates):\n    # Step 1: Convert the date column to datetime format\n    df[date_col] = pd.to_datetime(df[date_col])\n    \n    # Step 2: Create a new column 'is_holiday' and mark the holiday dates as True\n    df['is_holiday'] = df[date_col].isin(holiday_dates)\n    \n    return df"
    },
    {
        "function_name": "lemmatize_sentences",
        "file_name": "text_preprocessing.py",
        "parameters": {
            "text_data": "str",
            "max_length": "int",
            "min_length": "int"
        },
        "objectives": [
            "Split the given text data into sentences using regular expressions.",
            "Filter out the sentences that have a length less than min_length or greater than max_length.",
            "Lemmatize each word in the remaining sentences using the WordNet lemmatizer.",
            "Return a list of lists where each sublist contains the lemmatized words in a sentence."
        ],
        "import_lines": [
            "import re",
            "from nltk.stem import WordNetLemmatizer"
        ],
        "function_def": "def lemmatize_sentences(text_data, max_length, min_length):\n    # Step 1: Split the given text data into sentences using regular expressions\n    sentences = re.split(r'[.!?]', text_data)\n    \n    # Step 2: Filter out the sentences that have a length less than min_length or greater than max_length\n    filtered_sentences = [sentence for sentence in sentences if min_length <= len(sentence) <= max_length]\n    \n    # Step 3: Lemmatize each word in the remaining sentences using the WordNet lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentences = [[lemmatizer.lemmatize(word) for word in sentence.split()] for sentence in filtered_sentences]\n    \n    # Step 4: Return a list of lists where each sublist contains the lemmatized words in a sentence\n    return lemmatized_sentences"
    },
    {
        "function_name": "select_correlated_features",
        "file_name": "feature_selection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "corr_threshold": "float"
        },
        "objectives": [
            "Calculate the correlation between all pairs of numerical columns in the dataframe df.",
            "Identify the columns that have a correlation greater than corr_threshold with the target_col.",
            "Create a new dataframe that includes the target_col and the identified columns.",
            "Scale the identified columns using the StandardScaler."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def select_correlated_features(df, target_col, corr_threshold):\n    # Step 1: Calculate the correlation between all pairs of numerical columns in the dataframe df\n    corr_matrix = df.corr()\n    \n    # Step 2: Identify the columns that have a correlation greater than corr_threshold with the target_col\n    correlated_cols = [col for col in corr_matrix.columns if corr_matrix.loc[target_col, col] > corr_threshold and col != target_col]\n    \n    # Step 3: Create a new dataframe that includes the target_col and the identified columns\n    selected_df = df[[target_col] + correlated_cols]\n    \n    # Step 4: Scale the identified columns using the StandardScaler\n    scaler = StandardScaler()\n    selected_df[correlated_cols] = scaler.fit_transform(selected_df[correlated_cols])\n    \n    return selected_df"
    },
    {
        "function_name": "calculate_date_diffs",
        "file_name": "date_manipulations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "date_col": "str"
        },
        "objectives": [
            "Group the dataframe df by the id_col and date_col.",
            "Calculate the difference in days between consecutive dates for each group.",
            "Create a new column in df that contains the calculated differences.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_date_diffs(df, id_col, date_col):\n    # Step 1: Group the dataframe df by the id_col and date_col\n    df['date'] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[id_col, date_col], inplace=True)\n    \n    # Step 2: Calculate the difference in days between consecutive dates for each group\n    df['date_diff'] = df.groupby(id_col)['date'].diff().dt.days\n    \n    # Step 3: Create a new column in df that contains the calculated differences\n    df['date_diff'].fillna(0, inplace=True)\n    \n    # Step 4: Return the resulting dataframe\n    df['date_diff'] = df['date_diff'].astype(int)\n    return df"
    },
    {
        "function_name": "select_important_words",
        "file_name": "text_analysis.py",
        "parameters": {
            "text_data": "str",
            "max_features": "int"
        },
        "objectives": [
            "Split the given text data into words using regular expressions.",
            "Remove stop words from the word list.",
            "Apply TF-IDF transformation to the word list.",
            "Select the top max_features words based on their TF-IDF scores.",
            "Return a dictionary where the keys are the selected words and the values are their TF-IDF scores."
        ],
        "import_lines": [
            "import re",
            "from nltk.corpus import stopwords",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def select_important_words(text_data, max_features):\n    # Step 1: Split the given text data into words using regular expressions\n    words = re.findall(r'\\b\\w+\\b', text_data.lower())\n    \n    # Step 2: Remove stop words from the word list\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [word for word in words if word not in stop_words]\n    \n    # Step 3: Apply TF-IDF transformation to the word list\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform([' '.join(filtered_words)])\n    \n    # Step 4: Select the top max_features words based on their TF-IDF scores\n    feature_names = vectorizer.get_feature_names_out()\n    important_words = {feature_names[i]: tfidf.toarray()[0][i] for i in np.argsort(tfidf.toarray()[0])[-max_features:]}\n    \n    # Step 5: Return a dictionary where the keys are the selected words and the values are their TF-IDF scores\n    return important_words"
    },
    {
        "function_name": "get_unique_columns",
        "file_name": "column_filters.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "threshold": "float",
            "k": "int"
        },
        "objectives": [
            "Find all columns in the dataframe that have at least 'k' number of unique values.",
            "For each of these columns, calculate the ratio of unique values to total values.",
            "Find the columns that have this ratio greater than the 'threshold'.",
            "Return a new dataframe that includes only these columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def get_unique_columns(df, threshold, k):\n    # Step 1: Find all columns in the dataframe that have at least 'k' number of unique values\n    unique_cols = [col for col in df.columns if df[col].nunique() >= k]\n    \n    # Step 2: For each of these columns, calculate the ratio of unique values to total values\n    unique_ratios = {col: df[col].nunique() / len(df) for col in unique_cols}\n    \n    # Step 3: Find the columns that have this ratio greater than the 'threshold'\n    filtered_cols = [col for col, ratio in unique_ratios.items() if ratio > threshold]\n    \n    # Step 4: Return a new dataframe that includes only these columns\n    return df[filtered_cols]"
    },
    {
        "function_name": "get_relevant_keywords",
        "file_name": "text_operations.py",
        "parameters": {
            "text_data": "list",
            "keywords": "list",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the frequency of each keyword in the text data.",
            "Normalize the frequencies by the total number of words.",
            "Find the keywords that have a frequency greater than the 'threshold'.",
            "Return a list of these keywords."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def get_relevant_keywords(text_data, keywords, threshold):\n    # Step 1: Calculate the frequency of each keyword in the text data\n    word_counts = Counter(re.findall(r'\\b\\w+\\b', ' '.join(text_data)))\n    \n    # Step 2: Normalize the frequencies by the total number of words\n    total_words = sum(word_counts.values())\n    keyword_freqs = {keyword: word_counts[keyword] / total_words for keyword in keywords}\n    \n    # Step 3: Find the keywords that have a frequency greater than the 'threshold'\n    relevant_keywords = [keyword for keyword, freq in keyword_freqs.items() if freq > threshold]\n    \n    # Step 4: Return a list of these keywords\n    return relevant_keywords"
    },
    {
        "function_name": "extract_unique_keywords",
        "file_name": "text_analysis.py",
        "parameters": {
            "text_file": "str",
            "keyword": "str",
            "threshold": "int"
        },
        "objectives": [
            "Open the text file specified by \"text_file\" and read its content.",
            "Split the content into sentences.",
            "Count the occurrences of the given keyword in each sentence.",
            "Filter out the sentences that contain the keyword less than the specified threshold.",
            "Return a list of unique keywords (excluding the given keyword) that appear in the filtered sentences."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def extract_unique_keywords(text_file, keyword, threshold):\n\twith open(text_file, 'r') as file:\n\t\tcontent = file.read()\n\t\n\tsentences = re.split(r'[.!?]', content)\n\t\n\tsentence_dict = {}\n\tfor sentence in sentences:\n\t\tkeyword_count = sentence.lower().count(keyword.lower())\n\t\tif keyword_count >= threshold:\n\t\t\tunique_keywords = set(re.findall(r'\\b\\w+\\b', sentence.lower())) - {keyword.lower()}\n\t\t\tsentence_dict[sentence] = unique_keywords\n\t\n\tunique_keywords = set.union(*sentence_dict.values())\n\t\n\treturn list(unique_keywords)"
    },
    {
        "function_name": "group_by_categories",
        "file_name": "category_grouping.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "n": "int"
        },
        "objectives": [
            "Find the top n categories in each categorical column of the dataframe data.",
            "Create a new column 'category' for each categorical column, containing only the top n categories.",
            "Group the dataframe by each categorical column and calculate the mean of all numerical columns for each group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_by_categories(data, n):\n    categorical_cols = data.select_dtypes(include=[object]).columns\n    \n    for col in categorical_cols:\n        # Find top n categories\n        top_n = data[col].value_counts().index[:n]\n        \n        # Create new column with only top n categories\n        data[f'{col}_category'] = data[col].apply(lambda x: x if x in top_n else 'Others')\n        \n        # Group by new column and calculate mean of numerical columns\n        grouping_df = data.groupby(f'{col}_category')[data.select_dtypes(include=[np.number]).columns].mean()\n        \n        # Display the grouping result\n        print(f'Grouping by {col}:')\n        print(grouping_df)\n        \n    return data"
    },
    {
        "function_name": "remove_duplicates",
        "file_name": "similarity_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "k": "int"
        },
        "objectives": [
            "Calculate the cosine similarity between each pair of rows in the dataframe df.",
            "Identify the top k most similar pairs of rows based on the cosine similarity.",
            "Remove one row from each pair to avoid duplicates."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics.pairwise import cosine_similarity"
        ],
        "function_def": "def remove_duplicates(df, k):\n    # Calculate cosine similarity\n    similarity_matrix = cosine_similarity(df)\n    \n    # Get upper triangular matrix\n    upper_triangular = np.triu(similarity_matrix, k=1)\n    \n    # Identify top k most similar pairs\n    top_k_pairs = np.unravel_index(np.argsort(-upper_triangular, axis=None)[:k], upper_triangular.shape)\n    \n    # Remove one row from each pair\n    removed_rows = set(top_k_pairs[0])\n    \n    return df[~df.index.isin(removed_rows)]"
    },
    {
        "function_name": "impute_missing_values",
        "file_name": "missing_value_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "missing_percentage": "float"
        },
        "objectives": [
            "Calculate the percentage of missing values in each column of the dataframe df.",
            "Identify columns with a missing value percentage greater than missing_percentage.",
            "Impute missing values in the identified columns using the mean of the respective columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_missing_values(df, missing_percentage):\n    # Calculate percentage of missing values in each column\n    missing_percentage_series = df.isna().mean() * 100\n    \n    # Identify columns with missing value percentage greater than missing_percentage\n    imputation_cols = missing_percentage_series[missing_percentage_series > missing_percentage].index\n    \n    # Impute missing values in identified columns\n    for col in imputation_cols:\n        mean_value = df[col].mean()\n        df[col] = df[col].fillna(mean_value)\n    \n    return df"
    },
    {
        "function_name": "filter_unique_columns",
        "file_name": "data_filtering.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "threshold": "int"
        },
        "objectives": [
            "Find all columns in the dataframe \"data\" that have a unique count greater than \"threshold\".",
            "Create a new dataframe called \"unique_value_count\" where the index is the column name and the value is the unique count.",
            "Remove all rows from \"data\" where at least one of the columns has a unique count less than or equal to \"threshold\".",
            "Calculate the standard deviation of each column in the cleaned dataframe and add it to a list called \"std_dev_list\"."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_unique_columns(data, threshold):\n    unique_value_count = {}\n    \n    # Step 1: Find all columns in the dataframe \"data\" that have a unique count greater than \"threshold\"\n    for col in data.columns:\n        unique_count = data[col].nunique()\n        unique_value_count[col] = unique_count\n    \n    unique_cols = [col for col, count in unique_value_count.items() if count > threshold]\n    \n    # Step 2: Create a new dataframe called \"unique_value_count\" where the index is the column name and the value is the unique count\n    unique_value_count_df = pd.DataFrame(list(unique_value_count.items()), columns=['column', 'unique_count'])\n    \n    # Step 3: Remove all rows from \"data\" where at least one of the columns has a unique count less than or equal to \"threshold\"\n    cleaned_data = data[unique_cols].dropna()\n    \n    # Step 4: Calculate the standard deviation of each column in the cleaned dataframe and add it to a list called \"std_dev_list\"\n    std_dev_list = []\n    for col in cleaned_data.columns:\n        std_dev_list.append(cleaned_data[col].std())\n    \n    return unique_value_count_df, std_dev_list"
    },
    {
        "function_name": "split_dataframes",
        "file_name": "data_splitting.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "split_ratio": "float"
        },
        "objectives": [
            "Split the dataframe \"data\" into two parts based on the \"split_ratio\".",
            "Calculate the mean of each column in the first part and add it to a list called \"mean_list_1\".",
            "Calculate the mean of each column in the second part and add it to a list called \"mean_list_2\".",
            "Create a new dataframe called \"split_data\" where the first part is in the first \"split_ratio\" percentage rows and the second part is in the remaining rows."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_dataframes(data, split_ratio):\n    split_index = int(len(data) * split_ratio)\n    \n    # Step 1: Split the dataframe \"data\" into two parts based on the \"split_ratio\"\n    first_part = data.head(split_index)\n    second_part = data.tail(len(data) - split_index)\n    \n    mean_list_1 = []\n    # Step 2: Calculate the mean of each column in the first part and add it to a list called \"mean_list_1\"\n    for col in first_part.columns:\n        mean_list_1.append(first_part[col].mean())\n    \n    mean_list_2 = []\n    # Step 3: Calculate the mean of each column in the second part and add it to a list called \"mean_list_2\"\n    for col in second_part.columns:\n        mean_list_2.append(second_part[col].mean())\n    \n    # Step 4: Create a new dataframe called \"split_data\" where the first part is in the first \"split_ratio\" percentage rows and the second part is in the remaining rows\n    split_data = pd.concat([first_part, second_part])\n    \n    return mean_list_1, mean_list_2, split_data"
    },
    {
        "function_name": "find_common_words",
        "file_name": "code_parser.py",
        "parameters": {
            "func_file": "str",
            "class_file": "str"
        },
        "objectives": [
            "Open the text file specified by func_file, and create a list of functions names from function definitions.",
            "Open the text file specified by class_file, and create a list of class names from class definitions.",
            "Find common words in the list of function names and class names and count the frequency of each common word.",
            "Create a dictionary where the key is the common word and the value is the frequency."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def find_common_words(func_file, class_file):\n    # Step 1: Open the text file specified by func_file, and create a list of functions names from function definitions\n    with open(func_file, 'r') as file:\n        func_content = file.read()\n    func_names = re.findall(r'def\\s+(\\w+)\\s*\\(', func_content)\n    \n    # Step 2: Open the text file specified by class_file, and create a list of class names from class definitions\n    with open(class_file, 'r') as file:\n        class_content = file.read()\n    class_names = re.findall(r'class\\s+(\\w+)\\s*\\:', class_content)\n    \n    # Step 3: Find common words in the list of function names and class names and count the frequency of each common word\n    common_words = set(func_names) & set(class_names)\n    word_freq = {}\n    for word in common_words:\n        word_freq[word] = func_names.count(word) + class_names.count(word)\n    \n    return word_freq"
    },
    {
        "function_name": "count_ngrams",
        "file_name": "nlp_utils.py",
        "parameters": {
            "text": "str",
            "n": "int"
        },
        "objectives": [
            "Split the input text into n-grams, where each n-gram is a substring of length n.",
            "Count the frequency of each unique n-gram.",
            "Sort the n-grams by frequency in descending order.",
            "Return a dictionary where the keys are the n-grams and the values are their frequencies."
        ],
        "import_lines": [],
        "function_def": "def count_ngrams(text, n):\n    # Step 1: Split the input text into n-grams, where each n-gram is a substring of length n\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    \n    # Step 2: Count the frequency of each unique n-gram\n    freq_dict = {}\n    for ngram in ngrams:\n        if ngram in freq_dict:\n            freq_dict[ngram] += 1\n        else:\n            freq_dict[ngram] = 1\n    \n    # Step 3: Sort the n-grams by frequency in descending order\n    sorted_ngrams = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n    \n    # Step 4: Return a dictionary where the keys are the n-grams and the values are their frequencies\n    return dict(sorted_ngrams)"
    },
    {
        "function_name": "replace_with_log",
        "file_name": "array_utils.py",
        "parameters": {
            "arr": "numpy.ndarray",
            "threshold": "float"
        },
        "objectives": [
            "Find the indices of the elements in the array that are greater than the specified threshold.",
            "Create a new array that contains the same elements as the original array, but with the elements at the found indices replaced with their logarithm.",
            "Return the new array."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def replace_with_log(arr, threshold):\n    # Step 1: Find the indices of the elements in the array that are greater than the specified threshold\n    indices = np.where(arr > threshold)\n    \n    # Step 2: Create a new array that contains the same elements as the original array\n    new_arr = arr.copy()\n    \n    # Step 3: Replace the elements at the found indices with their logarithm\n    new_arr[indices] = np.log(new_arr[indices])\n    \n    return new_arr"
    },
    {
        "function_name": "group_by_day",
        "file_name": "datetime_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "time_column": "str"
        },
        "objectives": [
            "Convert the date and time columns to a single datetime column.",
            "Create a new column that contains the day of the week for each datetime.",
            "Group the dataframe by the day of the week and calculate the mean of the values in another specified column.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_by_day(df, date_column, time_column, value_column):\n    # Step 1: Convert the date and time columns to a single datetime column\n    df['datetime'] = pd.to_datetime(df[date_column] + ' ' + df[time_column])\n    \n    # Step 2: Create a new column that contains the day of the week for each datetime\n    df['day_of_week'] = df['datetime'].dt.day_name()\n    \n    # Step 3: Group the dataframe by the day of the week and calculate the mean of the values in the specified column\n    group_df = df.groupby('day_of_week')[value_column].mean().reset_index()\n    \n    return group_df"
    },
    {
        "function_name": "filter_categorical_columns",
        "file_name": "categorical_encoding.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list",
            "threshold": "int"
        },
        "objectives": [
            "One-hot encode the specified categorical columns in the dataframe.",
            "For each categorical column, calculate the frequency of each category and filter out the categories with a frequency below the specified threshold.",
            "Merge the filtered one-hot encoded columns back into the original dataframe.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_categorical_columns(data, categorical_columns, threshold):\n    # One-hot encode the categorical columns\n    encoded_data = pd.get_dummies(data, columns=categorical_columns)\n    \n    # Filter out the categories with a frequency below the threshold\n    for col in categorical_columns:\n        category_counts = data[col].value_counts()\n        filtered_categories = category_counts[category_counts >= threshold].index\n        encoded_data = encoded_data[[c for c in encoded_data.columns if c.startswith(col + '_') and c.split('_')[-1] in filtered_categories]]\n    \n    # Merge the filtered one-hot encoded columns back into the original dataframe\n    updated_data = pd.concat([data, encoded_data], axis=1)\n    \n    return updated_data"
    },
    {
        "function_name": "filter_correlation_pairs",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "correlation_threshold": "float"
        },
        "objectives": [
            "Calculate the correlation matrix of the dataframe.",
            "Filter out the correlation pairs with a correlation coefficient below the specified threshold.",
            "Create a new dataframe with the filtered correlation pairs and their corresponding coefficients.",
            "Return the dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def filter_correlation_pairs(data, correlation_threshold):\n    # Calculate the correlation matrix\n    corr_matrix = data.corr()\n    \n    # Filter out the correlation pairs with a correlation coefficient below the threshold\n    corr_pairs = []\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i+1, len(corr_matrix.columns)):\n            if np.abs(corr_matrix.iloc[i, j]) >= correlation_threshold:\n                corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n    \n    # Create a new dataframe with the filtered correlation pairs and coefficients\n    corr_pairs_df = pd.DataFrame(corr_pairs, columns=['col1', 'col2', 'correlation'])\n    \n    return corr_pairs_df"
    },
    {
        "function_name": "binning_analysis",
        "file_name": "binning_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "bin_column": "str",
            "target_column": "str",
            "bin_size": "int"
        },
        "objectives": [
            "Bin the values in the 'bin_column' into bins of size 'bin_size'.",
            "For each bin, calculate the mean and standard deviation of the 'target_column' values.",
            "Create a new dataframe with the bin values and corresponding mean and standard deviation.",
            "Return the dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def binning_analysis(data, bin_column, target_column, bin_size):\n    # Bin the values in the 'bin_column' into bins of size 'bin_size'\n    bins = np.arange(data[bin_column].min(), data[bin_column].max() + bin_size, bin_size)\n    data['bin'] = pd.cut(data[bin_column], bins=bins)\n    \n    # Calculate the mean and standard deviation of the 'target_column' values for each bin\n    bin_stats = data.groupby('bin')[target_column].agg(['mean', 'std'])\n    \n    return bin_stats"
    },
    {
        "function_name": "filter_groups_by_mean",
        "file_name": "data_filtering.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "top_n": "int",
            "threshold": "float"
        },
        "objectives": [
            "Group the dataframe \"data\" by the column \"group_column\".",
            "Calculate the mean of the values for each group.",
            "Identify the groups with a mean greater than \"threshold\" and sort them in descending order.",
            "Select the top \"top_n\" groups and create a new dataframe with only these groups.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_groups_by_mean(data, group_column, top_n, threshold):\n    # Step 1: Group the dataframe by the column \"group_column\" and calculate the mean of the values for each group\n    mean_values = data.groupby(group_column).mean().reset_index()\n    \n    # Step 2: Identify the groups with a mean greater than \"threshold\" and sort them in descending order\n    filtered_groups = mean_values[mean_values.iloc[:, 1] > threshold].sort_values(by=mean_values.columns[1], ascending=False)\n    \n    # Step 3: Select the top \"top_n\" groups\n    top_groups = filtered_groups.head(top_n)[group_column]\n    \n    # Step 4: Create a new dataframe with only the selected groups\n    new_df = data[data[group_column].isin(top_groups)]\n    \n    return new_df"
    },
    {
        "function_name": "extract_time_components",
        "file_name": "datetime_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "time_format": "str"
        },
        "objectives": [
            "Convert the date column to a datetime object with the specified time format.",
            "Extract the hour, minute, and second from the datetime object.",
            "Create new columns for hour, minute, and second and assign the extracted values.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def extract_time_components(df, date_column, time_format):\n    # Step 1: Convert the date column to a datetime object with the specified time format\n    df[date_column] = pd.to_datetime(df[date_column], format=time_format)\n    \n    # Step 2: Extract the hour, minute, and second from the datetime object\n    df['hour'] = df[date_column].dt.hour\n    df['minute'] = df[date_column].dt.minute\n    df['second'] = df[date_column].dt.second\n    \n    return df"
    },
    {
        "function_name": "cumulative_sum_exceeds_threshold",
        "file_name": "data_manipulation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Calculate the cumulative sum of the values in the target column.",
            "Identify the rows where the cumulative sum is greater than the threshold.",
            "Create a new column \"cumulative_exceeds_threshold\" and assign 1 if the cumulative sum is greater than the threshold, 0 otherwise.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def cumulative_sum_exceeds_threshold(data, target_column, threshold):\n    # Step 1: Calculate the cumulative sum of the values in the target column\n    cumulative_sum = data[target_column].cumsum()\n    \n    # Step 2: Identify the rows where the cumulative sum is greater than the threshold\n    mask = cumulative_sum > threshold\n    \n    # Step 3: Create a new column \"cumulative_exceeds_threshold\" and assign 1 if the cumulative sum is greater than the threshold, 0 otherwise\n    data['cumulative_exceeds_threshold'] = mask.astype(int)\n    \n    return data"
    },
    {
        "function_name": "calculate_correlation_with_onehot",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "One-hot encode the categorical column.",
            "Concatenate the one-hot encoded columns with the original dataframe.",
            "Calculate the correlation coefficient between the one-hot encoded columns and the target column.",
            "Return the correlation coefficients as a dictionary."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_correlation_with_onehot(df, categorical_column, target_column):\n    # Step 1: One-hot encode the categorical column\n    onehot_df = pd.get_dummies(df[categorical_column], drop_first=True)\n    \n    # Step 2: Concatenate the one-hot encoded columns with the original dataframe\n    new_df = pd.concat([df, onehot_df], axis=1)\n    \n    # Step 3: Calculate the correlation coefficient between the one-hot encoded columns and the target column\n    correlation_coefficients = new_df[onehot_df.columns].corrwith(new_df[target_column]).to_dict()\n    \n    return correlation_coefficients"
    },
    {
        "function_name": "select_informative_features",
        "file_name": "feature_selection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "target_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "One-hot encode the categorical column.",
            "Calculate the mutual information between the one-hot encoded columns and the target column.",
            "Filter out the one-hot encoded columns with mutual information less than the threshold.",
            "Return a dictionary where the keys are the selected one-hot encoded columns and the values are their mutual information scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import mutual_info_classif"
        ],
        "function_def": "def select_informative_features(df, categorical_column, target_column, threshold):\n    # Step 1: One-hot encode the categorical column\n    onehot_df = pd.get_dummies(df[categorical_column], drop_first=True)\n    \n    # Step 2: Calculate the mutual information between the one-hot encoded columns and the target column\n    mutual_info = mutual_info_classif(onehot_df, df[target_column])\n    \n    # Step 3: Filter out the one-hot encoded columns with mutual information less than the threshold\n    informative_features = onehot_df.columns[mutual_info >= threshold]\n    \n    # Step 4: Return a dictionary where the keys are the selected one-hot encoded columns and the values are their mutual information scores\n    return dict(zip(informative_features, mutual_info[mutual_info >= threshold]))"
    },
    {
        "function_name": "extract_date_components",
        "file_name": "date_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "timestamp_column": "str"
        },
        "objectives": [
            "Convert the timestamp_column to a datetime object.",
            "Extract the year, month, and day from the datetime object.",
            "Create new columns in the dataframe for the year, month, and day.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def extract_date_components(df, timestamp_column):\n    # Step 1: Convert the timestamp_column to a datetime object\n    df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n    \n    # Step 2: Extract the year, month, and day from the datetime object\n    df['year'] = df[timestamp_column].dt.year\n    df['month'] = df[timestamp_column].dt.month\n    df['day'] = df[timestamp_column].dt.day\n    \n    # Step 3: Create new columns in the dataframe for the year, month, and day\n    df['year'] = df['year'].astype(int)\n    df['month'] = df['month'].astype(int)\n    df['day'] = df['day'].astype(int)\n    \n    # Step 4: Return the resulting dataframe\n    return df"
    },
    {
        "function_name": "calculate_moving_average",
        "file_name": "window_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "feature_column": "str"
        },
        "objectives": [
            "Group the dataframe by the id_column.",
            "Calculate the moving average of the feature_column for each group.",
            "Create a new column in the dataframe with the moving average.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_moving_average(df, id_column, feature_column):\n    # Step 1: Group the dataframe by the id_column\n    df.sort_values(by=id_column, inplace=True)\n    \n    # Step 2: Calculate the moving average of the feature_column for each group\n    df['moving_average'] = df.groupby(id_column)[feature_column].transform(lambda x: x.rolling(window=3).mean())\n    \n    # Step 3: Create a new column in the dataframe with the moving average\n    df['moving_average'] = df['moving_average'].astype(float)\n    \n    # Step 4: Return the resulting dataframe\n    return df"
    },
    {
        "function_name": "tf_idf_scores",
        "file_name": "text_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "text_column": "str",
            "stop_words": "list"
        },
        "objectives": [
            "Tokenize the text data in the specified column and remove stop words.",
            "Calculate the term frequency-inverse document frequency (TF-IDF) of each word in the text data.",
            "Create a new column in the dataframe with the TF-IDF scores for each document.",
            "Return the dataframe with the new column and the TF-IDF vectorizer."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "import nltk",
            "from nltk.corpus import stopwords"
        ],
        "function_def": "def tf_idf_scores(data, text_column, stop_words):\n    # Tokenize the text data and remove stop words\n    data[text_column] = data[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n    \n    # Calculate the TF-IDF scores\n    vectorizer = TfidfVectorizer()\n    tf_idf_scores = vectorizer.fit_transform(data[text_column])\n    \n    # Create a new column with the TF-IDF scores\n    data['tf_idf_scores'] = vectorizer.transform(data[text_column]).toarray().tolist()\n    \n    return data, vectorizer"
    },
    {
        "function_name": "date_interval_aggregation",
        "file_name": "date_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "interval": "str"
        },
        "objectives": [
            "Convert the date column to datetime format.",
            "Create a new column with the date intervals (e.g., day, week, month).",
            "Group the dataframe by the date interval and calculate the sum of each numeric column within each group.",
            "Return the grouped dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def date_interval_aggregation(data, date_column, interval):\n    # Convert the date column to datetime format\n    data[date_column] = pd.to_datetime(data[date_column])\n    \n    # Create a new column with the date intervals\n    if interval == 'day':\n        data['date_interval'] = data[date_column].dt.date\n    elif interval == 'week':\n        data['date_interval'] = data[date_column].dt.to_period('W').dt.start_time\n    elif interval == 'month':\n        data['date_interval'] = data[date_column].dt.to_period('M').dt.start_time\n    \n    # Group by date interval and calculate the sum of each numeric column\n    numeric_cols = data.select_dtypes(include=[int, float]).columns\n    grouped_data = data.groupby('date_interval')[numeric_cols].sum()\n    \n    return grouped_data"
    },
    {
        "function_name": "kmeans_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "K": "int"
        },
        "objectives": [
            "Perform K-means clustering on the dataframe.",
            "Create a new column with the cluster labels.",
            "Return the dataframe with the cluster labels and the cluster centers."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def kmeans_clustering(data, K):\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=K)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Create a new column with the cluster labels\n    data['cluster_label'] = cluster_labels\n    \n    # Get the cluster centers\n    cluster_centers = kmeans.cluster_centers_\n    \n    return data, cluster_centers"
    },
    {
        "function_name": "check_correlation",
        "file_name": "correlation_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col1": "str",
            "col2": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the correlation coefficient between the two specified columns.",
            "Check if the absolute value of the correlation coefficient is greater than the specified threshold.",
            "Create a new column with the correlation coefficient.",
            "Return the updated DataFrame with the correlation coefficient column and whether the columns are highly correlated."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def check_correlation(df, col1, col2, threshold):\n    # Step 1: Calculate the correlation coefficient between the two specified columns\n    corr_coef = np.corrcoef(df[col1], df[col2])[0, 1]\n    \n    # Step 2: Check if the absolute value of the correlation coefficient is greater than the specified threshold\n    highly_correlated = abs(corr_coef) > threshold\n    \n    # Step 3: Create a new column with the correlation coefficient\n    df[f'corr_{col1}_{col2}'] = corr_coef\n    \n    return df, highly_correlated"
    },
    {
        "function_name": "reduce_categories",
        "file_name": "categorical_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_name": "str",
            "max_categories": "int"
        },
        "objectives": [
            "Identify the top N categories in the specified column based on frequency.",
            "Create a new column with the identified top categories.",
            "Map the top categories to a numerical value.",
            "Return the updated DataFrame with the top categories column and the mapped values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def reduce_categories(df, col_name, max_categories):\n    # Step 1: Identify the top N categories in the specified column based on frequency\n    top_categories = df[col_name].value_counts().head(max_categories).index\n    \n    # Step 2: Create a new column with the identified top categories\n    df[f'reduced_{col_name}'] = df[col_name].apply(lambda x: x if x in top_categories else 'Other')\n    \n    # Step 3: Map the top categories to a numerical value\n    category_map = {x: i for i, x in enumerate(top_categories)}\n    df[f'reduced_{col_name}_mapped'] = df[f'reduced_{col_name}'].map(category_map).fillna(max_categories)\n    \n    return df"
    },
    {
        "function_name": "detect_anomalies",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "timestamp_column": "str",
            "value_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Extract the timestamps from the dataframe data and convert them to datetime format.",
            "Calculate the rolling mean of the value column over the specified window size.",
            "Create a new column containing the rolling mean and append it to the dataframe.",
            "Identify the timestamps where the value exceeds the rolling mean by more than 2 standard deviations.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_anomalies(data, timestamp_column, value_column, window_size):\n    # Step 1: Extract the timestamps and convert them to datetime format\n    data[timestamp_column] = pd.to_datetime(data[timestamp_column])\n    \n    # Step 2: Calculate the rolling mean of the value column\n    data[f'rolling_mean_{value_column}'] = data[value_column].rolling(window_size).mean()\n    \n    # Step 3: Identify the timestamps where the value exceeds the rolling mean by more than 2 standard deviations\n    data['anomaly'] = np.where(np.abs(data[value_column] - data[f'rolling_mean_{value_column}']) > 2 * data[value_column].rolling(window_size).std(), 1, 0)\n    \n    return data"
    },
    {
        "function_name": "reduce_correlated_features",
        "file_name": "feature_reduction.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "correlation_threshold": "float"
        },
        "objectives": [
            "Calculate the correlation matrix of the dataframe.",
            "Identify highly correlated features with a correlation coefficient greater than the specified threshold.",
            "Remove one feature from each pair of highly correlated features.",
            "Return the reduced dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def reduce_correlated_features(data, correlation_threshold):\n    corr_matrix = data.corr()\n    corr_matrix = np.abs(corr_matrix)\n    \n    # Get upper triangular matrix, excluding diagonal\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    \n    # Find index of feature with correlation greater than threshold\n    to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n    \n    to_drop = list(set(to_drop))\n    \n    return data.drop(to_drop, axis=1)"
    },
    {
        "function_name": "bin_means",
        "file_name": "binning.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "bin_size": "int",
            "bin_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "Discretize the values in the specified bin_column into bins of a specified size.",
            "Calculate the mean of the target_column within each bin.",
            "Create a new column with the bin means.",
            "Return the dataframe with the new column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_means(data, bin_size, bin_column, target_column):\n    bins = np.arange(0, data[bin_column].max() + bin_size, bin_size)\n    data['bin'] = np.digitize(data[bin_column], bins)\n    \n    bin_means = data.groupby('bin')[target_column].mean().reset_index()\n    bin_means = bin_means.rename({target_column: f'{target_column}_bin_mean'}, axis=1)\n    \n    data = data.merge(bin_means, on='bin')\n    data = data.drop('bin', axis=1)\n    \n    return data"
    },
    {
        "function_name": "transform_variables",
        "file_name": "variable_transformation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "transformation_method": "str ('log', 'sqrt', or 'standardize')"
        },
        "objectives": [
            "Apply the specified transformation method to all numeric columns in the dataframe.",
            "Handle missing values by skipping them during transformation.",
            "Return the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def transform_variables(data, transformation_method):\n    numeric_data = data.select_dtypes(include=[int, float])\n    \n    if transformation_method == 'log':\n        transformed_data = numeric_data.apply(lambda x: np.log(x + 1), axis=0)\n    elif transformation_method == 'sqrt':\n        transformed_data = numeric_data.apply(lambda x: np.sqrt(x), axis=0)\n    elif transformation_method == 'standardize':\n        scaler = StandardScaler()\n        transformed_data = pd.DataFrame(scaler.fit_transform(numeric_data), columns=numeric_data.columns)\n    \n    return pd.concat([data.select_dtypes(exclude=[int, float]), transformed_data], axis=1)"
    },
    {
        "function_name": "detect_anomalies",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "anomaly_detection_method": "str ('zscore', 'modified_zscore', or 'iqr')",
            "threshold": "float"
        },
        "objectives": [
            "Detect anomalies in the data using the specified method.",
            "Identify the outliers based on the threshold value.",
            "Create a new column to indicate whether each row is an outlier.",
            "Return the dataframe with the outlier indication."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy import stats"
        ],
        "function_def": "def detect_anomalies(data, anomaly_detection_method, threshold):\n    anomaly_indicators = []\n    \n    for col in data.columns:\n        if anomaly_detection_method == 'zscore':\n            z_scores = np.abs(stats.zscore(data[col]))\n            is_anomaly = z_scores > threshold\n        elif anomaly_detection_method == 'modified_zscore':\n            median = np.median(data[col])\n            MAD = np.median(np.abs(data[col] - median))\n            modified_z_scores = np.abs((data[col] - median) / MAD)\n            is_anomaly = modified_z_scores > threshold\n        elif anomaly_detection_method == 'iqr':\n            Q1 = np.percentile(data[col], 25)\n            Q3 = np.percentile(data[col], 75)\n            IQR = Q3 - Q1\n            is_anomaly = np.logical_or(data[col] < (Q1 - threshold * IQR), data[col] > (Q3 + threshold * IQR))\n        \n        anomaly_indicators.append(is_anomaly)\n    \n    outlier_indication = np.any(np.array(anomaly_indicators), axis=0)\n    data['is_outlier'] = outlier_indication\n    \n    return data"
    },
    {
        "function_name": "fuzzy_match",
        "file_name": "data_matching.py",
        "parameters": {
            "df1": "pandas.DataFrame",
            "df2": "pandas.DataFrame",
            "matching_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Perform a fuzzy match between two dataframes using a matching column.",
            "Calculate the similarity score between the matching columns.",
            "Create a new column with the similarity score.",
            "Return the matched dataframe with the similarity score."
        ],
        "import_lines": [
            "import pandas as pd",
            "from fuzzywuzzy import fuzz"
        ],
        "function_def": "def fuzzy_match(df1, df2, matching_column, threshold):\n    df1['similarity_score'] = df1[matching_column].apply(lambda x: fuzz.ratio(x, df2[matching_column].iloc[0]))\n    df1 = df1[df1['similarity_score'] > threshold]\n    return df1"
    },
    {
        "function_name": "mark_outliers",
        "file_name": "outlier_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the mean and standard deviation of the target_column.",
            "Identify outliers in the target_column by checking if the values are more than threshold standard deviations away from the mean.",
            "Create a new column that marks the outliers with a value of 1 and non-outliers with a value of 0.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def mark_outliers(data, target_column, threshold):\n    mean = data[target_column].mean()\n    std_dev = data[target_column].std()\n    \n    data['outlier'] = np.where(np.abs(data[target_column] - mean) > threshold * std_dev, 1, 0)\n    \n    return data"
    },
    {
        "function_name": "select_top_groups",
        "file_name": "group_ranking.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "top_n": "int"
        },
        "objectives": [
            "Group the dataframe by the group_column and calculate the mean of the value_column within each group.",
            "Rank the groups by their mean values in descending order.",
            "Select the top_n groups with the highest mean values.",
            "Return the updated dataframe with only the top_n groups."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def select_top_groups(data, group_column, value_column, top_n):\n    group_means = data.groupby(group_column)[value_column].mean().reset_index()\n    \n    group_means = group_means.sort_values(value_column, ascending=False)\n    \n    top_groups = group_means.iloc[:top_n, :]\n    \n    return data[data[group_column].isin(top_groups[group_column])]"
    },
    {
        "function_name": "set_operation",
        "file_name": "set_operations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column1": "str",
            "column2": "str",
            "method": "str ('intersection', 'union', or 'difference')"
        },
        "objectives": [
            "Calculate the intersection, union, or difference of the two columns based on the method.",
            "Create a new column that contains the result of the set operation.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def set_operation(data, column1, column2, method):\n    if method == 'intersection':\n        result = data[column1] & data[column2]\n    elif method == 'union':\n        result = data[column1] | data[column2]\n    elif method == 'difference':\n        result = data[column1] & ~data[column2]\n    \n    data['set_result'] = result\n    \n    return data"
    },
    {
        "function_name": "rolling_mean",
        "file_name": "rolling_aggregations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Group the dataframe by the group_column and calculate the rolling mean of the value_column within each group.",
            "Calculate the rolling mean using the specified window_size.",
            "Create a new column that contains the rolling mean values.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_mean(data, group_column, value_column, window_size):\n    data['rolling_mean'] = data.groupby(group_column)[value_column].transform(lambda x: x.rolling(window_size).mean())\n    \n    return data"
    },
    {
        "function_name": "calculate_rolling_sum_and_cumulative_product",
        "file_name": "rolling_calculations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str",
            "factor_column": "str"
        },
        "objectives": [
            "Sort the dataframe by the id_column and date_column.",
            "Calculate the rolling sum of the factor_column for each id group.",
            "Create a new column 'rolling_sum' in the dataframe and assign the rolling sum values to it.",
            "Calculate the cumulative product of the 'rolling_sum' column for each id group and assign it to a new column 'cumulative_product'."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_rolling_sum_and_cumulative_product(df, id_column, date_column, factor_column):\n    df.sort_values(by=[id_column, date_column], inplace=True)\n    df['rolling_sum'] = df.groupby(id_column)[factor_column].transform(lambda x: x.rolling(3).sum())\n    df['cumulative_product'] = df.groupby(id_column)['rolling_sum'].transform(lambda x: x.cumprod())\n    return df"
    },
    {
        "function_name": "handle_missing_values_and_create_flag",
        "file_name": "missing_value_handling.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Find the percentage of missing values in each column of the dataframe.",
            "Identify the columns that have a percentage of missing values greater than the threshold.",
            "Remove the identified columns from the dataframe.",
            "Create a new column 'missing_value_flag' in the dataframe and assign 1 to rows where the target_column is missing, 0 otherwise."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def handle_missing_values_and_create_flag(data, target_column, threshold):\n    missing_value_percentages = data.isnull().mean()\n    columns_to_remove = missing_value_percentages[missing_value_percentages > threshold].index\n    data = data.drop(columns=columns_to_remove)\n    data['missing_value_flag'] = data[target_column].isnull().astype(int)\n    return data"
    },
    {
        "function_name": "resample_and_calculate_aggregations",
        "file_name": "time_series_calculations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "frequency": "str"
        },
        "objectives": [
            "Convert the time_column to a datetime format.",
            "Resample the dataframe by the specified frequency.",
            "Calculate the sum, mean, and count for each resampled group.",
            "Create a new dataframe with the resampled results."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_calculate_aggregations(data, time_column, frequency):\n    data[time_column] = pd.to_datetime(data[time_column])\n    data.set_index(time_column, inplace=True)\n    resampled_data = data.resample(frequency).agg(['sum', 'mean', 'count'])\n    return resampled_data"
    },
    {
        "function_name": "calculate_correlation_and_mutual_info",
        "file_name": "correlation_and_mutual_info.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "Calculate the correlation coefficient between the feature_column and the target_column.",
            "Create a new column 'correlation_flag' in the dataframe and assign 1 to rows where the correlation coefficient is greater than 0.5, 0 otherwise.",
            "Calculate the mutual information between the feature_column and the target_column.",
            "Create a new column 'mutual_info_flag' in the dataframe and assign 1 to rows where the mutual information is greater than 0.5, 0 otherwise."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics import mutual_info_score"
        ],
        "function_def": "def calculate_correlation_and_mutual_info(df, feature_column, target_column):\n    correlation_coefficient = df[feature_column].corr(df[target_column])\n    df['correlation_flag'] = (correlation_coefficient > 0.5).astype(int)\n    mutual_info = mutual_info_score(df[feature_column], df[target_column])\n    df['mutual_info_flag'] = (mutual_info > 0.5).astype(int)\n    return df"
    },
    {
        "function_name": "transform_categorical_and_numerical",
        "file_name": "feature_engineering.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "numerical_column": "str"
        },
        "objectives": [
            "One-hot encode the categorical column.",
            "Scale the numerical column using StandardScaler.",
            "Concatenate the one-hot encoded categorical column and the scaled numerical column.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def transform_categorical_and_numerical(data, categorical_column, numerical_column):\n    categorical_data = pd.get_dummies(data[categorical_column], drop_first=True)\n    numerical_data = data[numerical_column]\n    scaler = StandardScaler()\n    scaled_numerical_data = pd.DataFrame(scaler.fit_transform(numerical_data.values.reshape(-1, 1)), columns=[numerical_column])\n    \n    transformed_data = pd.concat([categorical_data, scaled_numerical_data], axis=1)\n    \n    return transformed_data"
    },
    {
        "function_name": "get_top_groups_by_sum",
        "file_name": "data_grouping.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "top_n": "int",
            "min_value": "float"
        },
        "objectives": [
            "Group the dataframe by the column \"group_column\" and calculate the sum of the values for each group.",
            "Filter out the groups with a sum less than the minimum value.",
            "Sort the remaining groups in descending order of their sum.",
            "Select the top \"top_n\" groups and return their group names."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def get_top_groups_by_sum(df, group_column, top_n, min_value):\n    summed_values = df.groupby(group_column).sum().reset_index()\n    filtered_groups = summed_values[summed_values.iloc[:, 1] > min_value].sort_values(by=summed_values.columns[1], ascending=False)\n    top_groups = filtered_groups.head(top_n)[group_column]\n    \n    return top_groups.values.tolist()"
    },
    {
        "function_name": "group_year_mean_std",
        "file_name": "groupby_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "groupby_cols": "list"
        },
        "objectives": [
            "Convert the date column to a datetime column and extract the year.",
            "Group the dataframe by the extracted year and specified columns.",
            "Calculate the mean and standard deviation of values in another specified column for each group.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_year_mean_std(df, date_column, groupby_cols, value_column):\n    # Step 1: Convert the date column to a datetime column and extract the year\n    df['year'] = pd.to_datetime(df[date_column]).dt.year\n    \n    # Step 2: Group the dataframe by the extracted year and specified columns\n    df['groupby_cols'] = df[groupby_cols].apply(tuple, axis=1)\n    \n    # Step 3: Calculate the mean and standard deviation of values in the specified column for each group\n    grouped_df = df.groupby(['year', 'groupby_cols'])[value_column].agg(['mean', 'std']).reset_index()\n    \n    # Step 4: Return the resulting dataframe\n    return grouped_df"
    },
    {
        "function_name": "calculate_correlation",
        "file_name": "encoding_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list",
            "target_column": "str"
        },
        "objectives": [
            "One-hot encode the specified categorical columns.",
            "Merge the encoded columns into the original dataframe.",
            "Calculate the correlation between each encoded column and the target column.",
            "Return a dataframe with the encoded column names and their corresponding correlation values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_correlation(df, categorical_cols, target_column):\n    # Step 1: One-hot encode the specified categorical columns\n    encoded_df = pd.get_dummies(df[categorical_cols])\n    \n    # Step 2: Merge the encoded columns into the original dataframe\n    merged_df = pd.concat([df, encoded_df], axis=1)\n    \n    # Step 3: Calculate the correlation between each encoded column and the target column\n    correlation_df = merged_df[encoded_df.columns.tolist() + [target_column]].corr()\n    \n    # Step 4: Return a dataframe with the encoded column names and their corresponding correlation values\n    return correlation_df[encoded_df.columns.tolist()].loc[target_column].to_frame('correlation')"
    },
    {
        "function_name": "roll_up_values",
        "file_name": "window_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int",
            "column_name": "str"
        },
        "objectives": [
            "Roll up the values in the specified column using the specified window size.",
            "Calculate the mean and standard deviation of the rolled-up values.",
            "Return a dataframe with the mean and standard deviation values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def roll_up_values(df, window_size, column_name):\n    # Step 1: Roll up the values in the specified column using the specified window size\n    rolled_up_df = df[column_name].rolling(window_size).aggregate(['mean', 'std'])\n    \n    # Step 2: Return a dataframe with the mean and standard deviation values\n    return rolled_up_df"
    },
    {
        "function_name": "encode_column",
        "file_name": "encoding_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "encoding_type": "str"
        },
        "objectives": [
            "Label encode or one-hot encode the specified column based on the encoding type.",
            "Return the encoded dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def encode_column(df, column_name, encoding_type):\n    if encoding_type == 'label_encoding':\n        # Step 1: Label encode the specified column\n        encoded_df = df.copy()\n        encoded_df[column_name] = LabelEncoder().fit_transform(df[column_name])\n    elif encoding_type == 'one_hot_encoding':\n        # Step 1: One-hot encode the specified column\n        encoded_df = pd.get_dummies(df, columns=[column_name])\n    else:\n        raise ValueError(\"Invalid encoding type\")\n    \n    # Step 2: Return the encoded dataframe\n    return encoded_df"
    },
    {
        "function_name": "resample_and_aggregate",
        "file_name": "temporal_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "timestamp_column": "str",
            "period": "str (e.g., 'D' for daily, 'W' for weekly, 'M' for monthly)"
        },
        "objectives": [
            "Convert the timestamp column to datetime format.",
            "Resample the data by the specified period.",
            "Calculate the mean and standard deviation of each column after resampling.",
            "Return the resampled data with the new columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_aggregate(data, timestamp_column, period):\n    # Step 1: Convert the timestamp column to datetime format\n    data[timestamp_column] = pd.to_datetime(data[timestamp_column])\n    \n    # Step 2: Resample the data by the specified period\n    resampled_data = data.resample(period, on=timestamp_column)\n    \n    # Step 3: Calculate the mean and standard deviation of each column after resampling\n    aggregated_data = resampled_data.agg(['mean', 'std'])\n    \n    return aggregated_data"
    },
    {
        "function_name": "quantile_binary",
        "file_name": "quantile_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_name": "str",
            "quantile": "float"
        },
        "objectives": [
            "Calculate the quantile of the specified column.",
            "Create a new column with a boolean value indicating whether each row's value is above or below the quantile.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def quantile_binary(data, column_name, quantile):\n    # Step 1: Calculate the quantile of the specified column\n    quantile_val = data[column_name].quantile(quantile)\n    \n    # Step 2: Create a new column with a boolean value indicating whether each row's value is above or below the quantile\n    data[f'{column_name}_above_quantile'] = data[column_name] > quantile_val\n    \n    return data"
    },
    {
        "function_name": "top_n_values",
        "file_name": "grouping_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "value_column": "str",
            "group_column": "str",
            "top_n": "int"
        },
        "objectives": [
            "Group the dataframe by the specified group column.",
            "Calculate the top N values for the value column within each group.",
            "Return a new dataframe with the top N values for each group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def top_n_values(df, value_column, group_column, top_n):\n    # Step 1: Group the dataframe by the specified group column\n    grouped_df = df.groupby(group_column)\n    \n    # Step 2: Calculate the top N values for the value column within each group\n    top_n_values = grouped_df[value_column].nlargest(top_n)\n    \n    return top_n_values"
    },
    {
        "function_name": "histogram_normalization",
        "file_name": "histogram_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "num_bins": "int"
        },
        "objectives": [
            "Divide the values in the target_column into num_bins bins and calculate the frequency of each bin.",
            "Create a new column \"bin_labels\" and assign it the bin labels for each value in the target_column.",
            "Normalize the frequencies by dividing by the total number of rows in the dataframe.",
            "Return the updated dataframe with the bin_labels and normalized frequencies."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def histogram_normalization(df, target_column, num_bins):\n    bins = np.linspace(df[target_column].min(), df[target_column].max(), num_bins + 1)\n    df['bin_labels'] = np.digitize(df[target_column], bins)\n    frequencies = df['bin_labels'].value_counts(normalize=True)\n    df['normalized_frequencies'] = df['bin_labels'].apply(lambda x: frequencies[x])\n    return df"
    },
    {
        "function_name": "one_hot_encoding",
        "file_name": "encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list[str]"
        },
        "objectives": [
            "One-hot encode the categorical columns using pd.get_dummies.",
            "Create a new dataframe with the one-hot encoded columns and the remaining columns from the original dataframe.",
            "Remove any duplicate rows in the new dataframe.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def one_hot_encoding(df, categorical_columns):\n    one_hot_df = pd.get_dummies(df[categorical_columns], drop_first=True)\n    new_df = pd.concat([df.drop(categorical_columns, axis=1), one_hot_df], axis=1)\n    new_df = new_df.drop_duplicates()\n    return new_df"
    },
    {
        "function_name": "event_transition",
        "file_name": "event_transition.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "timestamp_column": "str",
            "event_column": "str"
        },
        "objectives": [
            "Sort the dataframe by the id_column and timestamp_column.",
            "Create a new column \"event_prev\" and assign it the previous event for each id.",
            "Create a new column \"event_next\" and assign it the next event for each id.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def event_transition(data, id_column, timestamp_column, event_column):\n    sorted_data = data.sort_values([id_column, timestamp_column])\n    sorted_data['event_prev'] = sorted_data.groupby(id_column)[event_column].shift(1)\n    sorted_data['event_next'] = sorted_data.groupby(id_column)[event_column].shift(-1)\n    return sorted_data"
    },
    {
        "function_name": "find_similar_values",
        "file_name": "similarity_measures.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "For each row in the dataframe, calculate the number of values that are within a specific threshold of the target value.",
            "Create a new column with these counts.",
            "Rank the rows based on these counts in descending order.",
            "Return the top 10% of the rows with the highest counts."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def find_similar_values(data, target_column, threshold):\n    # Calculate the number of values that are within a specific threshold of the target value for each row\n    data['similar_count'] = data.apply(lambda row: np.sum(np.abs(row.drop(target_column) - row[target_column]) < threshold), axis=1)\n    \n    # Rank the rows based on these counts in descending order\n    data['rank'] = data['similar_count'].rank(method='dense', ascending=False)\n    \n    # Return the top 10% of the rows with the highest counts\n    return data[data['rank'] <= 0.1 * len(data)]"
    },
    {
        "function_name": "reduce_noise",
        "file_name": "data_smoothing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "For each group in the dataframe, calculate the rolling mean and standard deviation of the values in the value column.",
            "Create new columns with these rolling values.",
            "Remove any rows where the rolling standard deviation is greater than the rolling mean.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def reduce_noise(data, group_column, value_column, window_size):\n    # Calculate the rolling mean and standard deviation for each group\n    data['rolling_mean'] = data.groupby(group_column)[value_column].transform(lambda x: x.rolling(window_size).mean())\n    data['rolling_std'] = data.groupby(group_column)[value_column].transform(lambda x: x.rolling(window_size).std())\n    \n    # Remove any rows where the rolling standard deviation is greater than the rolling mean\n    data = data[data['rolling_std'] <= data['rolling_mean']]\n    \n    return data"
    },
    {
        "function_name": "fuzzy_merge",
        "file_name": "merge_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "left_column": "str",
            "right_column": "str",
            "merge_type": "str"
        },
        "objectives": [
            "Perform a fuzzy merge between the left and right columns.",
            "Calculate the similarity score for each pair of values.",
            "Select the pairs with a similarity score above a specified threshold.",
            "Return a new dataframe containing the merged values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from fuzzywuzzy import fuzz"
        ],
        "function_def": "def fuzzy_merge(data, left_column, right_column, merge_type):\n    # Calculate the similarity score for each pair of values\n    similarity_scores = []\n    for left_value in data[left_column]:\n        for right_value in data[right_column]:\n            similarity_score = fuzz.ratio(left_value, right_value)\n            similarity_scores.append((left_value, right_value, similarity_score))\n    \n    # Select the pairs with a similarity score above a specified threshold\n    threshold = 80\n    merged_pairs = [pair for pair in similarity_scores if pair[2] >= threshold]\n    \n    # Create a new dataframe with the merged values\n    result = pd.DataFrame(merged_pairs, columns=['left', 'right', 'similarity_score'])\n    \n    return result"
    },
    {
        "function_name": "feature_selection",
        "file_name": "feature_selection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "method": "str"
        },
        "objectives": [
            "Perform feature selection using the specified method.",
            "Select the top k features with the highest importance scores.",
            "Return a new dataframe containing the selected features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import SelectKBest, mutual_info_classif",
            "from sklearn.ensemble import RandomForestClassifier"
        ],
        "function_def": "def feature_selection(data, target_column, method):\n    # Perform feature selection using the specified method\n    if method == 'mutual_info':\n        selector = SelectKBest(mutual_info_classif, k=10)\n    elif method == 'random_forest':\n        selector = RandomForestClassifier(n_estimators=100)\n        selector.fit(data.drop(target_column, axis=1), data[target_column])\n        importance_scores = selector.feature_importances_\n        selector = SelectKBest(lambda X, y: importance_scores, k=10)\n    \n    # Select the top k features with the highest importance scores\n    selected_features = selector.fit_transform(data.drop(target_column, axis=1), data[target_column])\n    \n    # Create a new dataframe with the selected features\n    result = pd.DataFrame(selected_features, columns=data.drop(target_column, axis=1).columns[:10])\n    \n    return result"
    },
    {
        "function_name": "moving_average",
        "file_name": "signal_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Sort the dataframe by the id column.",
            "Calculate the moving average of the value column for each id using the specified window size.",
            "Identify the rows where the moving average is greater than the overall mean of the value column.",
            "Create a new column with the moving average and the identified rows.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def moving_average(data, id_column, value_column, window_size):\n    data = data.sort_values(id_column)\n    moving_avg = data.groupby(id_column)[value_column].transform(lambda x: x.rolling(window_size).mean())\n    moving_avg.values[np.isnan(moving_avg.values)] = 0\n    data['moving_average'] = moving_avg\n    data['is_high'] = moving_avg > data[value_column].mean()\n    return data"
    },
    {
        "function_name": "topic_modeling",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "text_column": "str",
            "num_topics": "int"
        },
        "objectives": [
            "Perform natural language processing (NLP) to extract topics from the text data using Non-Negative Matrix Factorization (NMF).",
            "Identify the top num_topics topics.",
            "Create a new column with the topic distribution for each row.",
            "Return the updated dataframe and the topic data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.decomposition import NMF",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def topic_modeling(data, text_column, num_topics):\n    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n    tfidf_data = vectorizer.fit_transform(data[text_column])\n    nmf = NMF(n_components=num_topics)\n    topic_distribution = nmf.fit_transform(tfidf_data)\n    topic_data = pd.DataFrame(topic_distribution, columns=[f'topic_{i}' for i in range(num_topics)])\n    data['topic_distribution'] = topic_data.apply(lambda x: ', '.join([f'{col}: {val}' for col, val in zip(topic_data.columns, x) if val > 0]), axis=1)\n    return data, topic_data"
    },
    {
        "function_name": "pairwise_similarity",
        "file_name": "neighborhood_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "distance_threshold": "float",
            "k": "int"
        },
        "objectives": [
            "Calculate the Euclidean distance between each pair of rows in the dataframe.",
            "Identify pairs of rows with a distance less than the distance_threshold.",
            "For each identified pair, calculate the similarity between the two rows based on the number of shared k-nearest neighbors.",
            "Return a new dataframe with the pair-wise similarities."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.spatial.distance import pdist, squareform"
        ],
        "function_def": "def pairwise_similarity(df, distance_threshold, k):\n    # Calculate Euclidean distance between each pair of rows in the dataframe\n    distance_matrix = squareform(pdist(df.select_dtypes(include=[int, float])))\n    \n    # Identify pairs of rows with a distance less than the distance_threshold\n    nearby_pairs = [(i, j) for i in range(len(distance_matrix)) for j in range(i+1, len(distance_matrix[i])) if distance_matrix[i, j] < distance_threshold]\n    \n    # For each identified pair, calculate the similarity between the two rows based on the number of shared k-nearest neighbors\n    similarities = []\n    for pair in nearby_pairs:\n        i, j = pair\n        neighbors_i = distance_matrix[i].argsort()[:k+1]\n        neighbors_j = distance_matrix[j].argsort()[:k+1]\n        shared_neighbors = len(set(neighbors_i) & set(neighbors_j))\n        similarity = shared_neighbors / k\n        similarities.append((i, j, similarity))\n    \n    return pd.DataFrame(similarities, columns=['row_i', 'row_j', 'similarity'])"
    },
    {
        "function_name": "binning_and_labeling",
        "file_name": "numerical_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_column": "str",
            "bins": "int"
        },
        "objectives": [
            "Calculate the median of the numerical column.",
            "Create bins based on the median.",
            "Assign a label to each bin.",
            "Create a new column with the assigned labels."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def binning_and_labeling(df, numerical_column, bins):\n    # Step 1: Calculate the median of the numerical column\n    median = df[numerical_column].median()\n    \n    # Step 2: Create bins based on the median\n    df['bin'] = np.where(df[numerical_column] < median, 'Low', 'High')\n    \n    # Step 3: Assign a label to each bin\n    labels = {label: i for i, label in enumerate(df['bin'].unique())}\n    \n    # Step 4: Create a new column with the assigned labels\n    df['bin_label'] = df['bin'].map(labels)\n    \n    return df"
    },
    {
        "function_name": "temporal_resampling",
        "file_name": "temporal_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "frequency": "str ('D', 'W', 'M', 'Q', 'Y')"
        },
        "objectives": [
            "Extract the hour of the day from the time column.",
            "Create a new column with the extracted hour.",
            "Resample the dataframe to the specified frequency.",
            "Calculate the sum of a specific column for each resampled group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def temporal_resampling(df, time_column, frequency):\n    # Step 1: Extract the hour of the day from the time column\n    df[time_column] = pd.to_datetime(df[time_column])\n    df['hour'] = df[time_column].dt.hour\n    \n    # Step 2: Create a new column with the extracted hour\n    df = df\n    \n    # Step 3: Resample the dataframe to the specified frequency\n    df_resampled = df.resample(frequency, on=time_column)\n    \n    # Step 4: Calculate the sum of a specific column for each resampled group\n    aggregated_df = df_resampled['value'].sum().reset_index()\n    \n    return aggregated_df"
    },
    {
        "function_name": "normalize_and_correlate",
        "file_name": "normalization.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "method": "str",
            "k": "int"
        },
        "objectives": [
            "Perform normalization on the numerical columns of the dataframe using the specified method.",
            "Calculate the correlation matrix for the normalized dataframe.",
            "Identify pairs of columns that have a correlation greater than a certain threshold, which is calculated as the 90th percentile of the correlation coefficients.",
            "Select the top k features using the calculated correlation matrix."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
        ],
        "function_def": "def normalize_and_correlate(data, method, k):\n    # Perform normalization on numerical columns\n    if method == 'min_max':\n        scaler = MinMaxScaler()\n    elif method == 'standard':\n        scaler = StandardScaler()\n    \n    normalized_data = data.copy()\n    for col in data.select_dtypes(include=[np.number]).columns:\n        normalized_data[[col]] = scaler.fit_transform(data[[col]])\n    \n    # Calculate correlation matrix\n    corr_matrix = normalized_data.corr()\n    \n    # Identify pairs of columns that have a correlation greater than a certain threshold\n    threshold = np.percentile(corr_matrix.values.flatten(), 90)\n    correlated_cols = []\n    for i in range(len(corr_matrix)):\n        for j in range(i+1, len(corr_matrix)):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                correlated_cols.append((corr_matrix.columns[i], corr_matrix.columns[j]))\n    \n    # Select the top k features using the calculated correlation matrix\n    feature_importances = np.abs(corr_matrix).mean(axis=0)\n    top_features = feature_importances.nlargest(k).index\n    \n    return correlated_cols, top_features"
    },
    {
        "function_name": "label_rare_groups",
        "file_name": "data_labeling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "top_n": "int",
            "threshold": "float"
        },
        "objectives": [
            "Group the dataframe by the categorical column and calculate the count of each group.",
            "Identify the top \"top_n\" groups with the highest count.",
            "Filter the dataframe to include only rows from the top \"top_n\" groups.",
            "Create a new column with a binary label (0 or 1) for each row, indicating whether the row is from a group with a count above \"threshold\".",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def label_rare_groups(df, categorical_column, top_n, threshold):\n    # Step 1: Group the dataframe by the categorical column and calculate the count of each group\n    group_counts = df[categorical_column].value_counts()\n    \n    # Step 2: Identify the top \"top_n\" groups with the highest count\n    top_groups = group_counts.nlargest(top_n).index\n    \n    # Step 3: Filter the dataframe to include only rows from the top \"top_n\" groups\n    df = df[df[categorical_column].isin(top_groups)]\n    \n    # Step 4: Create a new column with a binary label (0 or 1) for each row, indicating whether the row is from a group with a count above \"threshold\"\n    df['label'] = df[categorical_column].map(group_counts).apply(lambda x: 1 if x > threshold else 0)\n    \n    return df"
    },
    {
        "function_name": "simple_regression",
        "file_name": "modeling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "feature_columns": "list[str]",
            "default_value": "float"
        },
        "objectives": [
            "Calculate the correlation between each feature column and the target column.",
            "Create a new column with the predicted values using a linear regression model.",
            "Replace missing values in the predicted column with the \"default_value\".",
            "Calculate the mean squared error between the predicted values and the actual values.",
            "Return the updated dataframe and the mean squared error."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.metrics import mean_squared_error"
        ],
        "function_def": "def simple_regression(df, target_column, feature_columns, default_value):\n    # Step 1: Calculate the correlation between each feature column and the target column\n    correlations = df[feature_columns + [target_column]].corr()\n    \n    # Step 2: Create a new column with the predicted values using a linear regression model\n    model = LinearRegression()\n    model.fit(df[feature_columns], df[target_column])\n    df['predicted'] = model.predict(df[feature_columns])\n    \n    # Step 3: Replace missing values in the predicted column with the \"default_value\"\n    df['predicted'] = df['predicted'].fillna(default_value)\n    \n    # Step 4: Calculate the mean squared error between the predicted values and the actual values\n    mse = mean_squared_error(df[target_column], df['predicted'])\n    \n    return df, mse"
    },
    {
        "function_name": "split_and_filter_strings",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "min_length": "int",
            "max_length": "int"
        },
        "objectives": [
            "Split the strings in the specified column into a list of substrings.",
            "Filter out substrings that are too short (less than \"min_length\") or too long (more than \"max_length\").",
            "Create a new column with the filtered list of substrings.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_and_filter_strings(df, column_name, min_length, max_length):\n    # Step 1: Split the strings in the specified column into a list of substrings\n    df[column_name] = df[column_name].apply(lambda x: x.split())\n    \n    # Step 2: Filter out substrings that are too short (less than \"min_length\") or too long (more than \"max_length\")\n    df[column_name] = df[column_name].apply(lambda x: [s for s in x if len(s) >= min_length and len(s) <= max_length])\n    \n    return df"
    },
    {
        "function_name": "group_by_time_threshold",
        "file_name": "time_series_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "timestamp_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Convert the timestamp column to datetime format.",
            "Sort the DataFrame by the timestamp column.",
            "Identify groups of consecutive rows with the same id that have a time difference greater than the specified threshold.",
            "Create a new column indicating whether each row is the start of a new group."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def group_by_time_threshold(df, id_column, timestamp_column, threshold):\n    # Step 1: Convert the timestamp column to datetime format\n    df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n    \n    # Step 2: Sort the DataFrame by the timestamp column\n    df = df.sort_values(by=timestamp_column)\n    \n    # Step 3: Identify groups of consecutive rows with the same id that have a time difference greater than the specified threshold\n    df['time_diff'] = df.groupby(id_column)[timestamp_column].diff().dt.total_seconds()\n    df['new_group'] = ((df['time_diff'] > threshold) | (df['time_diff'].isna())) & (df['time_diff'].notna())\n    \n    return df"
    },
    {
        "function_name": "filter_by_centrality",
        "file_name": "graph_operations.py",
        "parameters": {
            "graph": "networkx.Graph",
            "node_attribute": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the centrality measure of each node in the graph.",
            "Identify nodes with a centrality measure greater than the specified threshold.",
            "Create a subgraph containing only the nodes with high centrality.",
            "Return the subgraph."
        ],
        "import_lines": [
            "import networkx as nx"
        ],
        "function_def": "def filter_by_centrality(graph, node_attribute, threshold):\n    # Step 1: Calculate the centrality measure of each node in the graph\n    centrality = nx.degree_centrality(graph)\n    \n    # Step 2: Identify nodes with a centrality measure greater than the specified threshold\n    high_centrality_nodes = [node for node, score in centrality.items() if score > threshold]\n    \n    # Step 3: Create a subgraph containing only the nodes with high centrality\n    subgraph = graph.subgraph(high_centrality_nodes)\n    \n    return subgraph"
    },
    {
        "function_name": "handle_high_cardinality",
        "file_name": "categorical_handling.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "cardinality_threshold": "int"
        },
        "objectives": [
            "Identify the categorical column in the dataframe with high cardinality (i.e., the number of unique values exceeds the threshold).",
            "Impute missing values in the identified categorical column with the most frequent value.",
            "Group the dataframe by the imputed categorical column and calculate the count and proportion of each group.",
            "Return the dataframe with the imputed categorical column and the group counts and proportions."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def handle_high_cardinality(data, categorical_column, cardinality_threshold):\n    # Check if the categorical column has high cardinality\n    if data[categorical_column].nunique() > cardinality_threshold:\n        # Impute missing values with the most frequent value\n        data[categorical_column] = data[categorical_column].fillna(data[categorical_column].value_counts().index[0])\n        \n        # Group by the imputed categorical column and calculate count and proportion\n        group_counts = data[categorical_column].value_counts()\n        group_proportions = group_counts / len(data)\n        data[f'{categorical_column}_count'] = data[categorical_column].map(group_counts)\n        data[f'{categorical_column}_proportion'] = data[categorical_column].map(group_proportions)\n    \n    return data"
    },
    {
        "function_name": "pairwise_diff_outlier_detection",
        "file_name": "outlier_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_1": "str",
            "column_2": "str",
            "max_diff": "float"
        },
        "objectives": [
            "Calculate the pairwise differences between the values in the two specified columns.",
            "Identify the pairs with differences greater than the maximum allowed difference.",
            "Mark the pairs with large differences as outliers.",
            "Return the dataframe with the pairwise differences and the outlier indication."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def pairwise_diff_outlier_detection(data, column_1, column_2, max_diff):\n    # Calculate pairwise differences between the values in the two columns\n    pairwise_diffs = np.abs(data[column_1].values[:, None] - data[column_2].values[None, :])\n    \n    # Identify pairs with differences greater than the maximum allowed difference\n    large_diff_pairs = pairwise_diffs > max_diff\n    \n    # Mark pairs with large differences as outliers\n    outlier_indication = np.any(large_diff_pairs, axis=0)\n    data['outlier'] = outlier_indication\n    \n    return data"
    },
    {
        "function_name": "pca_split",
        "file_name": "pca_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "label_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Split the dataframe into two subsets based on the specified label column and threshold.",
            "Apply Principal Component Analysis (PCA) to the two subsets separately.",
            "Return the resulting dataframes with the top k principal components."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.decomposition import PCA",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def pca_split(df, label_column, threshold):\n    # Split the dataframe into two subsets based on the specified label column and threshold\n    subset1 = df[df[label_column] < threshold]\n    subset2 = df[df[label_column] >= threshold]\n    \n    # Apply PCA to the two subsets separately\n    pca1 = PCA(n_components=0.95)\n    pca2 = PCA(n_components=0.95)\n    principal_components1 = pca1.fit_transform(StandardScaler().fit_transform(subset1.drop(label_column, axis=1)))\n    principal_components2 = pca2.fit_transform(StandardScaler().fit_transform(subset2.drop(label_column, axis=1)))\n    \n    # Create new dataframes with the top k principal components\n    df1 = pd.DataFrame(principal_components1, columns=[f'PC{i+1}' for i in range(principal_components1.shape[1])])\n    df2 = pd.DataFrame(principal_components2, columns=[f'PC{i+1}' for i in range(principal_components2.shape[1])])\n    df1[label_column] = subset1[label_column]\n    df2[label_column] = subset2[label_column]\n    \n    return df1, df2"
    },
    {
        "function_name": "crossover_strategy",
        "file_name": "technical_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "horizon": "int"
        },
        "objectives": [
            "Create a rolling window of the specified horizon size.",
            "Calculate the short-term exponential moving average (EMA) and long-term EMA for each row within the window.",
            "Create a new column in the dataframe that contains the result of the crossover strategy.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def crossover_strategy(df, date_column, horizon):\n    # Create a rolling window of the specified horizon size\n    df['short_ema'] = df['Close'].ewm(span=12, adjust=False).mean()\n    df['long_ema'] = df['Close'].ewm(span=26, adjust=False).mean()\n    \n    # Create a new column in the dataframe that contains the result of the crossover strategy\n    df['crossover_signal'] = 0\n    df.loc[df['short_ema'] > df['long_ema'], 'crossover_signal'] = 1\n    \n    return df"
    },
    {
        "function_name": "robust_regression",
        "file_name": "robust_regression.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "method": "str (Lawson-Hanson or Sparrow)"
        },
        "objectives": [
            "Perform iterative re-weighted least squares (IRLS) to fit a robust regression model.",
            "Use the provided method to calculate the influence matrix for the model.",
            "Calculate the variance-covariance matrix of the regression coefficients.",
            "Return the estimated regression coefficients, variance-covariance matrix, and influence matrix."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.linalg import LinAlgError"
        ],
        "function_def": "def robust_regression(df, target_col, method):\n    # Perform IRLS to fit the robust regression model\n    X = df.drop(target_col, axis=1)\n    y = df[target_col]\n    w = np.ones_like(y)\n    for _ in range(5):  # 5 iterations for simplicity\n        X_w = np.diag(np.sqrt(w))\n        try:\n            beta = np.linalg.pinv(X_w @ X) @ X_w @ y\n        except LinAlgError:\n            beta = np.linalg.lstsq(X_w @ X, X_w @ y, rcond=None)[0]\n        residuals = y - X @ beta\n        s = 1.4826 * np.median(np.abs(residuals))\n        w = (1 / (residuals / s)) ** 2\n    \n    # Calculate the influence matrix\n    if method == 'Lawson-Hanson':\n        influence_matrix = np.linalg.inv(X.T @ np.diag(w**2) @ X)\n    elif method == 'Sparrow':\n        influence_matrix = (X.T @ np.diag(w**2) @ X) ** -1\n    else:\n        raise ValueError('Invalid method')\n    \n    # Calculate the variance-covariance matrix of the regression coefficients\n    vcov_matrix = influence_matrix / (X.shape[0] - X.shape[1] - 1)\n    \n    return beta, vcov_matrix, influence_matrix"
    },
    {
        "function_name": "gradient_boosting_regression",
        "file_name": "gradient_boosting.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "n_trees": "int",
            "max_depth": "int"
        },
        "objectives": [
            "Fit a gradient boosting regressor model to the data.",
            "Use the model to predict the target variable for the training data.",
            "Calculate the feature importance of each feature in the model.",
            "Return the feature importance and predicted values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.ensemble import GradientBoostingRegressor",
            "from sklearn.model_selection import train_test_split"
        ],
        "function_def": "def gradient_boosting_regression(df, target_col, n_trees, max_depth):\n    # Split the data into training and testing sets\n    X = df.drop(target_col, axis=1)\n    y = df[target_col]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Fit a gradient boosting regressor model to the training data\n    model = GradientBoostingRegressor(n_estimators=n_trees, max_depth=max_depth, random_state=42)\n    model.fit(X_train, y_train)\n    \n    # Use the model to predict the target variable for the training data\n    y_pred = model.predict(X_train)\n    \n    # Calculate the feature importance of each feature in the model\n    feature_importance = model.feature_importances_\n    \n    return feature_importance, y_pred"
    },
    {
        "function_name": "correlation_sum",
        "file_name": "correlation_sum.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols": "list of str",
            "correlation_threshold": "float"
        },
        "objectives": [
            "Calculate the pairwise correlation between each pair of columns in the list.",
            "Identify the pairs of columns with correlation greater than the specified threshold.",
            "Create a new column for each pair of highly correlated columns by summing the values of the two columns.",
            "Return the resulting data with the new columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def correlation_sum(df, cols, correlation_threshold):\n    # Calculate the pairwise correlation between each pair of columns in the list\n    correlation_matrix = df[cols].corr()\n    \n    # Identify the pairs of columns with correlation greater than the specified threshold\n    highly_correlated_pairs = [(i, j) for i in range(correlation_matrix.shape[0]) for j in range(i) if correlation_matrix.iloc[i, j] > correlation_threshold]\n    \n    # Create a new column for each pair of highly correlated columns by summing the values of the two columns\n    new_cols = {}\n    for i, j in highly_correlated_pairs:\n        col1 = cols[i]\n        col2 = cols[j]\n        new_col = df[col1] + df[col2]\n        new_cols[f\"{col1}_{col2}_sum\"] = new_col\n    \n    # Return the resulting data with the new columns\n    resulting_data = pd.concat([df, pd.DataFrame(new_cols)], axis=1)\n    return resulting_data"
    },
    {
        "function_name": "handle_missing_values",
        "file_name": "data_imputation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "missing_value_threshold": "float",
            "normalization_method": "str"
        },
        "objectives": [
            "Identify the columns in the dataframe that have missing values and replace them based on the 'missing_value_threshold'.",
            "Drop any columns that have a missing value percentage above the threshold.",
            "Normalize the data using the specified normalization method ('min-max' or 'standardization')."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
        ],
        "function_def": "def handle_missing_values(data, missing_value_threshold, normalization_method):\n    # Step 1: Identify the columns in the dataframe that have missing values and replace them based on the 'missing_value_threshold'\n    for col in data.columns:\n        missing_value_percentage = data[col].isnull().mean()\n        if missing_value_percentage > missing_value_threshold:\n            data.drop(col, axis=1, inplace=True)\n        else:\n            data[col].fillna(data[col].mean(), inplace=True)\n    \n    # Step 2: Normalize the data using the specified normalization method ('min-max' or 'standardization')\n    if normalization_method == 'min-max':\n        scaler = MinMaxScaler()\n        datanormalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    elif normalization_method == 'standardization':\n        scaler = StandardScaler()\n        datanormalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    \n    return datanormalized"
    },
    {
        "function_name": "extract_relevant_text_features",
        "file_name": "text_features.py",
        "parameters": {
            "text_data": "list of str",
            "ngram_range": "tuple of int",
            "top_n_features": "int"
        },
        "objectives": [
            "Extract n-grams (sequences of n words) from the text data based on the 'ngram_range'.",
            "Calculate the TF-IDF scores for the extracted n-grams.",
            "Select the top 'top_n_features' n-grams with the highest TF-IDF scores."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def extract_relevant_text_features(text_data, ngram_range, top_n_features):\n    # Step 1: Extract n-grams (sequences of n words) from the text data based on the 'ngram_range'\n    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n    tfidf = vectorizer.fit_transform(text_data)\n    \n    # Step 2: Calculate the TF-IDF scores for the extracted n-grams\n    tfidf_scores = np.array(tfidf.sum(axis=0)).ravel()\n    \n    # Step 3: Select the top 'top_n_features' n-grams with the highest TF-IDF scores\n    top_features = np.argsort(tfidf_scores)[::-1][:top_n_features]\n    top_features_scores = tfidf_scores[top_features]\n    \n    return dict(zip(vectorizer.get_feature_names_out()[top_features], top_features_scores))"
    },
    {
        "function_name": "linear_regression",
        "file_name": "linear_regression.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "model": "str ('lasso', 'ridge', or 'elastic_net')"
        },
        "objectives": [
            "One-hot encode categorical variables in the dataframe.",
            "Scale the data using StandardScaler.",
            "Train a linear regression model based on the specified model type.",
            "Return the trained model and the predicted values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
            "from sklearn.linear_model import Lasso, Ridge, ElasticNet"
        ],
        "function_def": "def linear_regression(data, target_column, model):\n    # One-hot encode categorical variables\n    categorical_cols = data.select_dtypes(include=['object']).columns\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    encoded_data = pd.get_dummies(data, columns=categorical_cols)\n    \n    # Scale the data using StandardScaler\n    scaler = StandardScaler()\n    scaled_data = pd.DataFrame(scaler.fit_transform(encoded_data), columns=encoded_data.columns)\n    \n    # Train a linear regression model based on the specified model type\n    if model == 'lasso':\n        model_instance = Lasso()\n    elif model == 'ridge':\n        model_instance = Ridge()\n    elif model == 'elastic_net':\n        model_instance = ElasticNet()\n    \n    model_instance.fit(scaled_data.drop(target_column, axis=1), scaled_data[target_column])\n    \n    # Return the trained model and the predicted values\n    predictions = model_instance.predict(scaled_data.drop(target_column, axis=1))\n    \n    return model_instance, predictions"
    },
    {
        "function_name": "cluster_categorical_data",
        "file_name": "clustering_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "n_clusters": "int"
        },
        "objectives": [
            "One-hot encode the categorical columns.",
            "Perform k-means clustering on the encoded data.",
            "Assign a cluster label to each row in the original data.",
            "Return the clustered data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def cluster_categorical_data(df, categorical_columns, n_clusters):\n    encoder = OneHotEncoder()\n    encoded_data = encoder.fit_transform(df[categorical_columns])\n    \n    kmeans = KMeans(n_clusters=n_clusters)\n    cluster_labels = kmeans.fit_predict(encoded_data)\n    \n    df['cluster_label'] = cluster_labels\n    \n    # Return the clustered data\n    return df"
    },
    {
        "function_name": "group_by_date",
        "file_name": "temporal_aggregations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "grouping_columns": "list",
            "aggregation_columns": "list"
        },
        "objectives": [
            "Convert the date column to datetime format and extract the year and month.",
            "Group the dataframe by the year, month, and the specified grouping columns.",
            "Calculate the mean and standard deviation of the specified aggregation columns for each group.",
            "Return the grouped dataframe with the aggregated values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_by_date(data, date_column, grouping_columns, aggregation_columns):\n    # Convert the date column to datetime format and extract the year and month\n    data[date_column] = pd.to_datetime(data[date_column])\n    data['year'] = data[date_column].dt.year\n    data['month'] = data[date_column].dt.month\n    \n    # Group the dataframe by the year, month, and the specified grouping columns\n    grouped_data = data.groupby(['year', 'month'] + grouping_columns)\n    \n    # Calculate the mean and standard deviation of the specified aggregation columns for each group\n    aggregated_data = grouped_data[aggregation_columns].agg(['mean', 'std'])\n    \n    return aggregated_data"
    },
    {
        "function_name": "iterative_regression",
        "file_name": "regression_models.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "predictor_columns": "list",
            "num_iterations": "int"
        },
        "objectives": [
            "Split the data into training and testing sets.",
            "Implement a simple iterative regression algorithm to predict the target column.",
            "Repeat the regression for num_iterations iterations and track the mean squared error (MSE) for each iteration.",
            "Return the final predicted values and the MSE for each iteration."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.model_selection import train_test_split"
        ],
        "function_def": "def iterative_regression(data, target_column, predictor_columns, num_iterations):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(data[predictor_columns], data[target_column], test_size=0.2, random_state=42)\n    \n    # Initialize the predicted values and MSE for each iteration\n    predicted_values = np.zeros((len(y_test), num_iterations))\n    mse_values = np.zeros(num_iterations)\n    \n    # Repeat the regression for num_iterations iterations\n    for i in range(num_iterations):\n        # Implement a simple iterative regression algorithm\n        weights = np.random.rand(len(predictor_columns))\n        predictions = np.dot(X_test, weights)\n        \n        # Calculate the MSE for each iteration\n        mse_values[i] = np.mean((predictions - y_test) ** 2)\n        \n        # Update the predicted values for each iteration\n        predicted_values[:, i] = predictions\n    \n    return predicted_values, mse_values"
    },
    {
        "function_name": "rolling_volatility",
        "file_name": "volatility_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_name": "str",
            "window_size": "int"
        },
        "objectives": [
            "Calculate the rolling standard deviation of the specified column.",
            "Calculate the rolling mean of the specified column.",
            "Create a new column with a boolean value indicating whether each row's value is above or below the rolling mean plus one standard deviation.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_volatility(data, column_name, window_size):\n    # Calculate the rolling standard deviation of the specified column\n    data[f'{column_name}_std'] = data[column_name].rolling(window_size).std()\n    \n    # Calculate the rolling mean of the specified column\n    data[f'{column_name}_mean'] = data[column_name].rolling(window_size).mean()\n    \n    # Create a new column with a boolean value indicating whether each row's value is above or below the rolling mean plus one standard deviation\n    data[f'{column_name}_above_volatility'] = data[column_name] > (data[f'{column_name}_mean'] + data[f'{column_name}_std'])\n    \n    return data"
    },
    {
        "function_name": "date_standardization",
        "file_name": "date_manipulations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "time_zone": "str"
        },
        "objectives": [
            "Convert the date column to a standardized format.",
            "Extract the hour, day, month, and year from the standardized date column.",
            "Create new columns for each extracted feature.",
            "Convert the date column to the specified time zone."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def date_standardization(data, date_column, time_zone):\n    # Convert the date column to a standardized format\n    data[date_column] = pd.to_datetime(data[date_column])\n    \n    # Extract the hour, day, month, and year from the standardized date column\n    data['hour'] = data[date_column].dt.hour\n    data['day'] = data[date_column].dt.day\n    data['month'] = data[date_column].dt.month\n    data['year'] = data[date_column].dt.year\n    \n    # Convert the date column to the specified time zone\n    data[date_column] = data[date_column].dt.tz_localize(time_zone)\n    \n    return data"
    },
    {
        "function_name": "merge_normalization_encoding",
        "file_name": "normalization.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numeric_cols": "list[str]",
            "categorical_cols": "list[str]"
        },
        "objectives": [
            "Perform normalization on the numerical columns using Min-Max Scaler.",
            "Perform one-hot encoding on the categorical columns.",
            "Merge the normalized numerical columns and the encoded categorical columns.",
            "Return the merged dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler",
            "from sklearn.preprocessing import OneHotEncoder"
        ],
        "function_def": "def merge_normalization_encoding(data, numeric_cols, categorical_cols):\n    # Perform normalization on numerical columns\n    scaler = MinMaxScaler()\n    normalized_data = data[numeric_cols]\n    for col in numeric_cols:\n        normalized_data[col] = scaler.fit_transform(normalized_data[col].values.reshape(-1, 1))\n    \n    # Perform one-hot encoding on categorical columns\n    encoder = OneHotEncoder()\n    encoded_data = encoder.fit_transform(data[categorical_cols])\n    \n    # Merge the normalized numerical columns and the encoded categorical columns\n    merged_data = pd.concat([normalized_data.reset_index(drop=True), pd.DataFrame(encoded_data.toarray())], axis=1)\n    \n    return merged_data"
    },
    {
        "function_name": "pca_reconstruction",
        "file_name": "dimensionality_reduction.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "k": "int"
        },
        "objectives": [
            "Perform Principal Component Analysis (PCA) on the dataframe to reduce dimensionality.",
            "Select the top k principal components.",
            "Reconstruct the original dataframe using the selected principal components.",
            "Calculate the reconstruction error."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def pca_reconstruction(data, k):\n    # Perform Principal Component Analysis (PCA) to reduce dimensionality\n    pca = PCA(n_components=k)\n    pca_data = pca.fit_transform(data)\n    \n    # Reconstruct the original dataframe using the selected principal components\n    reconstructed_data = pca.inverse_transform(pca_data)\n    \n    # Calculate the reconstruction error\n    error = ((data.values - reconstructed_data) ** 2).mean()\n    \n    return reconstructed_data, error"
    },
    {
        "function_name": "sentiment_analysis_filter",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "text_column": "str",
            "n_grams": "int",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the sentiment score of the text in the text_column using a machine learning model.",
            "Create a new column with the sentiment scores.",
            "Extract the most frequently occurring n-grams in the text_column.",
            "Filter out the rows with a sentiment score greater than the threshold and create a new dataframe with the remaining rows and the extracted n-grams."
        ],
        "import_lines": [
            "from nltk.sentiment.vader import SentimentIntensityAnalyzer",
            "from nltk.util import ngrams",
            "from collections import Counter"
        ],
        "function_def": "def sentiment_analysis_filter(data, text_column, n_grams, threshold):\n    # Calculate the sentiment score of the text in the text_column using a machine learning model\n    sia = SentimentIntensityAnalyzer()\n    data['sentiment_score'] = data[text_column].apply(lambda x: sia.polarity_scores(x)['compound'])\n    \n    # Extract the most frequently occurring n-grams in the text_column\n    n_grams_list = []\n    for text in data[text_column]:\n        text_n_grams = ngrams(text.split(), n_grams)\n        n_grams_list.extend(text_n_grams)\n    top_n_grams = Counter(n_grams_list).most_common(10)\n    \n    # Filter out the rows with a sentiment score greater than the threshold\n    filtered_data = data[data['sentiment_score'] <= threshold].copy()\n    \n    # Create a new dataframe with the remaining rows and the extracted n-grams\n    result_data = pd.DataFrame(top_n_grams, columns=['n_gram', 'frequency'])\n    filtered_data['n_grams'] = filtered_data[text_column].apply(lambda x: ', '.join([gram for gram in top_n_grams if gram[0] in x.split()]))\n    \n    return filtered_data, result_data"
    },
    {
        "function_name": "group_by_ranking",
        "file_name": "data_summary.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "aggregation_metric": "str",
            "top_n": "int"
        },
        "objectives": [
            "Group the dataframe by the group_column and calculate the sum, mean, and count for each group.",
            "Rank the groups based on the sums in descending order.",
            "Create a new dataframe containing the top_n groups and the grouped results."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_by_ranking(data, group_column, aggregation_metric, top_n):\n    # Group by the group_column and calculate the sum, mean, and count for each group\n    group_data = data.groupby(group_column)[aggregation_metric].agg(['sum', 'mean', 'count'])\n    \n    # Rank the groups based on the sums in descending order\n    ranked_groups = group_data['sum'].nlargest(top_n)\n    \n    # Create a new dataframe containing the top_n groups and the grouped results\n    top_grouped_data = pd.DataFrame(group_data.loc[ranked_groups.index])\n    \n    return top_grouped_data"
    },
    {
        "function_name": "category_ranking",
        "file_name": "category_insights.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "category_column": "str"
        },
        "objectives": [
            "Calculate the frequency of each category in the category_column.",
            "Create a new column \"category_rank\" and assign a rank to each category based on its frequency.",
            "Create a new column \"considered\" and assign a boolean value indicating whether each category's frequency is more than 1% of the total number of rows.",
            "Return the updated dataframe with the category_rank and considered columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def category_ranking(data, category_column):\n    category_freq = data[category_column].value_counts()\n    data['category_rank'] = data[category_column].map(category_freq.rank(method='dense', ascending=False))\n    threshold = len(data) * 0.01\n    data['considered'] = data[category_column].map(lambda x: True if category_freq[x] > threshold else False)\n    \n    return data"
    },
    {
        "function_name": "top_n_ranking",
        "file_name": "sorting_utils.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "sorting_columns": "list",
            "top_n": "int"
        },
        "objectives": [
            "Sort the dataframe by the specified sorting_columns in descending order.",
            "Create a new column \"rank\" and assign a rank to each row based on its position in the sorted dataframe.",
            "Return the top_n rows with the highest rank (i.e., the top_n rows in the sorted dataframe)."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def top_n_ranking(data, sorting_columns, top_n):\n    sorted_data = data.sort_values(sorting_columns, ascending=[False]*len(sorting_columns))\n    sorted_data['rank'] = sorted_data.index + 1\n    return sorted_data.nlargest(top_n, 'rank')"
    },
    {
        "function_name": "categorize_data",
        "file_name": "categorical_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "top_n": "int"
        },
        "objectives": [
            "Calculate the frequency of each category in the categorical_column.",
            "Identify the top N categories with the highest frequency.",
            "Create a new column that marks the top N categories with a value of 1 and other categories with a value of 0.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def categorize_data(data, categorical_column, top_n):\n    # Step 1: Calculate the frequency of each category in the categorical_column\n    category_frequency = data[categorical_column].value_counts()\n    \n    # Step 2: Identify the top N categories with the highest frequency\n    top_categories = category_frequency.nlargest(top_n).index\n    \n    # Step 3: Create a new column that marks the top N categories with a value of 1 and other categories with a value of 0\n    data['top_category'] = np.where(data[categorical_column].isin(top_categories), 1, 0)\n    \n    return data"
    },
    {
        "function_name": "sequence_analysis",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "sequence_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Shift the sequence_column by the specified window_size to create a new column.",
            "Calculate the difference between the original column and the shifted column.",
            "Create a new dataframe that includes the original column, the shifted column, and the difference column.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def sequence_analysis(data, sequence_column, window_size):\n    # Step 1: Shift the sequence_column by the specified window_size to create a new column\n    data['shifted_sequence'] = data[sequence_column].shift(window_size)\n    \n    # Step 2: Calculate the difference between the original column and the shifted column\n    data['sequence_diff'] = data[sequence_column] - data['shifted_sequence']\n    \n    # Step 3: Create a new dataframe that includes the original column, the shifted column, and the difference column\n    sequence_data = data[[sequence_column, 'shifted_sequence', 'sequence_diff']]\n    \n    return sequence_data"
    },
    {
        "function_name": "identify_data_gaps",
        "file_name": "temporal_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "max_gap": "int"
        },
        "objectives": [
            "Convert the timestamp column to datetime format and sort the dataframe by the id_column and timestamp_column.",
            "Calculate the time difference between consecutive rows for each id in the id_column.",
            "Identify the gaps in the data where the time difference is greater than the specified max_gap.",
            "Create a new dataframe with the rows where the time difference is greater than the max_gap and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_data_gaps(data, id_column, timestamp_column, max_gap):\n    data[timestamp_column] = pd.to_datetime(data[timestamp_column])\n    data.sort_values([id_column, timestamp_column], inplace=True)\n    \n    data['time_diff'] = data.groupby(id_column)[timestamp_column].diff()\n    data['time_diff'] = data['time_diff'].dt.total_seconds()\n    \n    data['gap_indicator'] = data['time_diff'] > max_gap\n    \n    result_data = data[data['gap_indicator'] == True][[id_column, timestamp_column, 'time_diff']].copy()\n    result_data.rename(columns={'time_diff': 'gap_size'}, inplace=True)\n    \n    return result_data"
    },
    {
        "function_name": "generate_alerts",
        "file_name": "data_conversion.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "condition_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the exponential moving average of the values in the condition_column.",
            "Identify the rows where the exponential moving average is greater than the specified threshold.",
            "Create a new column 'alert' and assign True to the rows where the exponential moving average is greater than the threshold.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def generate_alerts(data, condition_column, threshold, alpha=0.5):\n    data['ema'] = data[condition_column].ewm(alpha=alpha, adjust=False).mean()\n    \n    data['alert'] = data['ema'] > threshold\n    \n    return data"
    },
    {
        "function_name": "handle_missing_values_in_windows",
        "file_name": "missing_value_imputation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "window_size": "int",
            "min_observations": "int"
        },
        "objectives": [
            "Calculate the rolling mean and standard deviation of the data with the specified \"window_size\".",
            "Identify windows with less than \"min_observations\" non-null values.",
            "Replace the values in these windows with the median of the surrounding windows.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def handle_missing_values_in_windows(data, window_size, min_observations):\n    # Step 1: Calculate the rolling mean and standard deviation of the data\n    rolling_mean = data.rolling(window_size).mean()\n    rolling_std = data.rolling(window_size).std()\n    \n    # Step 2: Identify windows with less than \"min_observations\" non-null values\n    valid_observations = data.rolling(window_size).count()\n    invalid_windows = valid_observations < min_observations\n    \n    # Step 3: Replace the values in these windows with the median of the surrounding windows\n    modified_data = data.copy()\n    for i in range(len(data)):\n        if invalid_windows.iloc[i]:\n            start_idx = max(0, i - window_size)\n            end_idx = min(len(data), i + window_size)\n            surrounding_median = data.iloc[start_idx:i].median()\n            if pd.notna(surrounding_median):\n                modified_data.iloc[i] = surrounding_median\n    \n    return modified_data"
    },
    {
        "function_name": "filter_rare_categories",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Perform a one-hot encoding of the categorical column.",
            "Apply a threshold to the one-hot encoded columns to filter out rare categories.",
            "Use the filtered one-hot encoded columns as new features in the dataframe.",
            "Return the resulting dataframe with the new features."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_rare_categories(df, categorical_column, threshold):\n    # Perform a one-hot encoding of the categorical column\n    onehot_df = pd.get_dummies(df[categorical_column], drop_first=True)\n    \n    # Apply a threshold to the one-hot encoded columns to filter out rare categories\n    filtered_columns = onehot_df.columns[onehot_df.sum() > threshold]\n    filtered_onehot_df = onehot_df[filtered_columns]\n    \n    # Use the filtered one-hot encoded columns as new features in the dataframe\n    result = pd.concat([df, filtered_onehot_df], axis=1)\n    \n    return result"
    },
    {
        "function_name": "winsorize_values",
        "file_name": "data_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "min_threshold": "float",
            "max_threshold": "float"
        },
        "objectives": [
            "Apply the winsorization technique to the specified column to limit the extreme values.",
            "Clip the values below the min_threshold to the min_threshold value.",
            "Clip the values above the max_threshold to the max_threshold value.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def winsorize_values(df, column_name, min_threshold, max_threshold):\n    # Apply the winsorization technique\n    df[column_name] = df[column_name].clip(lower=min_threshold, upper=max_threshold)\n    \n    return df"
    },
    {
        "function_name": "remove_multicollinearity",
        "file_name": "feature_selection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_names": "list of str",
            "correlation_threshold": "float"
        },
        "objectives": [
            "Calculate the correlation matrix for the specified columns.",
            "Identify the pairs of columns with correlation coefficient greater than the correlation_threshold.",
            "Remove one column from each pair to avoid multicollinearity.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def remove_multicollinearity(data, column_names, correlation_threshold):\n    # Calculate the correlation matrix\n    correlation_matrix = data[column_names].corr()\n    \n    # Identify the pairs of columns with correlation coefficient greater than the correlation_threshold\n    correlation_pairs = []\n    for i in range(len(column_names)):\n        for j in range(i+1, len(column_names)):\n            if correlation_matrix.iloc[i, j] > correlation_threshold:\n                correlation_pairs.append((column_names[i], column_names[j]))\n    \n    # Remove one column from each pair to avoid multicollinearity\n    columns_to_remove = []\n    for pair in correlation_pairs:\n        if pair[0] not in columns_to_remove:\n            columns_to_remove.append(pair[0])\n    \n    data = data.drop(columns=columns_to_remove)\n    \n    return data"
    },
    {
        "function_name": "train_decision_tree",
        "file_name": "classification_models.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "continuous_columns": "list of str",
            "target_column": "str"
        },
        "objectives": [
            "One-hot encode the categorical columns.",
            "Scale the continuous columns using the StandardScaler.",
            "Concatenate the one-hot encoded categorical data and the scaled continuous data.",
            "Train a simple DecisionTreeClassifier on the concatenated data to predict the target column.",
            "Return the trained model and the predicted probabilities."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder, StandardScaler",
            "from sklearn.tree import DecisionTreeClassifier"
        ],
        "function_def": "def train_decision_tree(data, categorical_columns, continuous_columns, target_column):\n    # One-hot encode the categorical columns\n    encoder = OneHotEncoder()\n    encoded_data = pd.DataFrame(encoder.fit_transform(data[categorical_columns]).toarray())\n    \n    # Scale the continuous columns\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data[continuous_columns])\n    \n    # Concatenate the one-hot encoded categorical data and the scaled continuous data\n    concatenated_data = pd.concat([encoded_data, pd.DataFrame(scaled_data)], axis=1)\n    \n    # Train a simple DecisionTreeClassifier on the concatenated data\n    model = DecisionTreeClassifier()\n    model.fit(concatenated_data, data[target_column])\n    \n    # Return the trained model and the predicted probabilities\n    predicted_probabilities = model.predict_proba(concatenated_data)\n    \n    return model, predicted_probabilities"
    },
    {
        "function_name": "preprocess_date_data",
        "file_name": "date_preprocessing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Sort the dataframe by the date_column.",
            "Calculate the day of the week for each date and add it as a new column 'day_of_week'.",
            "Identify and remove rows where the date is missing or invalid.",
            "Merge the dataframe with a holiday calendar dataframe to indicate whether each date is a holiday."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from pandas.tseries.holiday import USFederalHolidayCalendar"
        ],
        "function_def": "def preprocess_date_data(data, date_column, threshold):\n    # Sort the dataframe by the date_column\n    data.sort_values(by=date_column, inplace=True)\n    \n    # Calculate the day of the week for each date and add it as a new column 'day_of_week'\n    data['day_of_week'] = pd.to_datetime(data[date_column]).dt.day_name()\n    \n    # Identify and remove rows where the date is missing or invalid\n    data.dropna(subset=[date_column], inplace=True)\n    data = data[pd.to_datetime(data[date_column]).notnull()]\n    \n    # Merge the dataframe with a holiday calendar dataframe to indicate whether each date is a holiday\n    cal = USFederalHolidayCalendar()\n    holidays = cal.holidays(start='2000-01-01', end='2022-12-31')\n    data['is_holiday'] = data[date_column].isin(holidays).astype(int)\n    \n    return data"
    },
    {
        "function_name": "calculate_percentile_rank_and_score_change",
        "file_name": "scoring.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "score_column": "str"
        },
        "objectives": [
            "Calculate the percentile rank of each score in the score_column for each id group.",
            "Add the percentile rank as a new column 'percentile_rank'.",
            "Identify and remove rows where the score is missing or invalid.",
            "Calculate the percentage change in score from the previous row for each id group and add it as a new column 'score_change'."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def calculate_percentile_rank_and_score_change(df, id_column, score_column):\n    # Calculate the percentile rank of each score in the score_column for each id group\n    df['percentile_rank'] = df.groupby(id_column)[score_column].rank(method='min', pct=True)\n    \n    # Add the percentile rank as a new column 'percentile_rank'\n    df['percentile_rank'] = df['percentile_rank'] * 100\n    \n    # Identify and remove rows where the score is missing or invalid\n    df.dropna(subset=[score_column], inplace=True)\n    df = df[pd.to_numeric(df[score_column], errors='coerce').notnull()]\n    \n    # Calculate the percentage change in score from the previous row for each id group and add it as a new column 'score_change'\n    df['score_change'] = df.groupby(id_column)[score_column].pct_change() * 100\n    \n    return df"
    },
    {
        "function_name": "calculate_ewma_and_std_dev",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "time_series_data": "pandas.Series",
            "window_size": "int"
        },
        "objectives": [
            "Calculate the exponentially weighted moving average (EWMA) of the time series data using the specified window size.",
            "Calculate the standard deviation of the time series data using the specified window size.",
            "Identify and remove outliers from the time series data based on the EWMA and standard deviation.",
            "Add the EWMA and standard deviation as new columns 'ewma' and 'std_dev'."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def calculate_ewma_and_std_dev(time_series_data, window_size):\n    # Calculate the exponentially weighted moving average (EWMA) of the time series data using the specified window size\n    ewma = time_series_data.ewm(span=window_size, adjust=False).mean()\n    \n    # Calculate the standard deviation of the time series data using the specified window size\n    std_dev = time_series_data.rolling(window_size).std()\n    \n    # Identify and remove outliers from the time series data based on the EWMA and standard deviation\n    z_scores = np.abs((time_series_data - ewma) / std_dev)\n    filtered_data = time_series_data[z_scores < 3]\n    \n    # Add the EWMA and standard deviation as new columns 'ewma' and 'std_dev'\n    filtered_data['ewma'] = ewma\n    filtered_data['std_dev'] = std_dev\n    \n    return filtered_data"
    },
    {
        "function_name": "add_time_features",
        "file_name": "datetime_utils.py",
        "parameters": {
            "dataframe": "pandas.DataFrame",
            "datetime_column": "str",
            "holiday_column": "str"
        },
        "objectives": [
            "Convert the datetime column in the dataframe to a datetime format and extract the day of the week and the month.",
            "Calculate the average value of each day of the week and each month in the dataframe, excluding holidays.",
            "Create new columns with these average values and add them to the dataframe.",
            "Remove any rows where the datetime is a holiday."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def add_time_features(dataframe, datetime_column, holiday_column):\n    # Convert datetime_column to datetime format\n    dataframe[datetime_column] = pd.to_datetime(dataframe[datetime_column])\n    \n    # Extract day of the week and month\n    dataframe['day_of_week'] = dataframe[datetime_column].dt.dayofweek\n    dataframe['month'] = dataframe[datetime_column].dt.month\n    \n    # Calculate average value of each day of the week and each month, excluding holidays\n    average_daily_values = dataframe[dataframe[holiday_column]==False].groupby('day_of_week').mean().add_suffix('_avg')\n    average_monthly_values = dataframe[dataframe[holiday_column]==False].groupby('month').mean().add_suffix('_avg')\n    \n    # Create new columns with these average values and add them to the dataframe\n    dataframe = dataframe.merge(average_daily_values, left_on='day_of_week', right_index=True)\n    dataframe = dataframe.merge(average_monthly_values, left_on='month', right_index=True)\n    \n    # Remove any rows where the datetime is a holiday\n    dataframe = dataframe[dataframe[holiday_column]==False]\n    \n    return dataframe"
    },
    {
        "function_name": "flag_outliers",
        "file_name": "outlier_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "num_points": "int",
            "threshold": "float"
        },
        "objectives": [
            "Randomly select a subset of the data with a specified number of points.",
            "Calculate the density of each point in the subset based on the k-nearest neighbors.",
            "Flag points as outliers if their density is below the specified threshold.",
            "Return the updated DataFrame with the outlier flags."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.neighbors import NearestNeighbors"
        ],
        "function_def": "def flag_outliers(data, num_points, threshold):\n    # Randomly select a subset of the data with a specified number of points\n    subset = data.sample(n=num_points)\n    \n    # Calculate the density of each point in the subset based on the k-nearest neighbors\n    neigh = NearestNeighbors(n_neighbors=10)\n    neigh.fit(subset.values)\n    distances, _ = neigh.kneighbors(subset.values)\n    densities = 1 / distances[:, -1]\n    \n    # Flag points as outliers if their density is below the specified threshold\n    subset['outlier'] = densities < threshold\n    \n    # Return the updated DataFrame with the outlier flags\n    return pd.merge(data, subset[['outlier']], how='left', left_index=True, right_index=True)"
    },
    {
        "function_name": "partition_aggregation",
        "file_name": "data_partitioning.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "aggregation_function": "str",
            "num_partitions": "int"
        },
        "objectives": [
            "Partition the data into a specified number of partitions based on the id column.",
            "Apply a specified aggregation function to the value column within each partition.",
            "Return the aggregated values and the partition boundaries."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def partition_aggregation(data, id_column, value_column, aggregation_function, num_partitions):\n    # Partition the data into a specified number of partitions based on the id column\n    data['partition'] = pd.qcut(data[id_column], q=num_partitions, labels=False)\n    \n    # Apply a specified aggregation function to the value column within each partition\n    aggregated_values = data.groupby('partition')[value_column].agg(aggregation_function)\n    \n    # Return the aggregated values and the partition boundaries\n    partition_boundaries = np.quantile(data[id_column], [i / num_partitions for i in range(num_partitions + 1)])\n    return aggregated_values, partition_boundaries"
    },
    {
        "function_name": "weighted_mean_regression",
        "file_name": "regression_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "weight_column": "str",
            "target_column": "str",
            "max_iterations": "int"
        },
        "objectives": [
            "Calculate the weighted mean of the `target_column` in the dataframe `df` using the `weight_column`.",
            "Update the weights based on the mean squared error between the predicted and actual values.",
            "Repeat the process for a maximum of `max_iterations` iterations or until convergence.",
            "Return the final weights and the predicted values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def weighted_mean_regression(df, weight_column, target_column, max_iterations):\n    weights = df[weight_column]\n    predicted_values = (df[target_column] * weights).sum() / weights.sum()\n    for _ in range(max_iterations):\n        errors = df[target_column] - predicted_values\n        weights = weights * (1 - errors ** 2)\n        weights = weights / weights.sum()\n        predicted_values = (df[target_column] * weights).sum() / weights.sum()\n        if np.allclose(errors, 0):\n            break\n    return weights, predicted_values"
    },
    {
        "function_name": "rolling_quantile_averages",
        "file_name": "window_operations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_name": "str",
            "window_size": "int",
            "quantile": "float"
        },
        "objectives": [
            "Calculate the rolling quantile of the `column_name` in the dataframe `data` with a window size of `window_size`.",
            "Identify the rows where the values are within the `quantile` of the rolling quantile.",
            "Calculate the average value of the identified rows for each window.",
            "Return a new dataframe with the average values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_quantile_averages(data, column_name, window_size, quantile):\n    rolling_quantile = data[column_name].rolling(window_size).quantile(quantile)\n    within_quantile = data[data[column_name] <= rolling_quantile]\n    averages = within_quantile[column_name].rolling(window_size).mean()\n    return averages"
    },
    {
        "function_name": "normalize_transaction_data",
        "file_name": "transaction_analysis.py",
        "parameters": {
            "transaction_data": "pandas.DataFrame",
            "product_column": "str",
            "time_column": "str"
        },
        "objectives": [
            "Convert the transaction data to a pivot table with the product as the index and time as the columns.",
            "Fill missing values with the mean of each product across all time.",
            "Normalize the data by subtracting the mean and dividing by the standard deviation for each product.",
            "Return the normalized data."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def normalize_transaction_data(transaction_data, product_column, time_column):\n    # Step 1: Convert the transaction data to a pivot table with the product as the index and time as the columns\n    pivot_data = transaction_data.pivot(index=product_column, columns=time_column, values='quantity')\n    \n    # Step 2: Fill missing values with the mean of each product across all time\n    pivot_data.fillna(pivot_data.mean(axis=1), inplace=True)\n    \n    # Step 3: Normalize the data by subtracting the mean and dividing by the standard deviation for each product\n    normalized_data = (pivot_data - pivot_data.mean(axis=1)) / pivot_data.std(axis=1)\n    \n    return normalized_data"
    },
    {
        "function_name": "identify_inactive_users",
        "file_name": "user_engagement.py",
        "parameters": {
            "user_data": "pandas.DataFrame",
            "event_column": "str",
            "time_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Calculate the time difference between consecutive events for each user.",
            "Identify users who have a time difference greater than the threshold.",
            "Create a new column indicating whether each user is inactive.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def identify_inactive_users(user_data, event_column, time_column, threshold):\n    # Step 1: Calculate the time difference between consecutive events for each user\n    user_data['time_diff'] = user_data.groupby('user_id')[time_column].diff()\n    \n    # Step 2: Identify users who have a time difference greater than the threshold\n    inactive_users = user_data[user_data['time_diff'] > threshold]['user_id'].unique()\n    \n    # Step 3: Create a new column indicating whether each user is inactive\n    user_data['is_inactive'] = np.where(user_data['user_id'].isin(inactive_users), True, False)\n    \n    return user_data"
    },
    {
        "function_name": "perform_topic_modeling",
        "file_name": "text_analysis.py",
        "parameters": {
            "text_data": "pandas.Series",
            "topic_column": "str"
        },
        "objectives": [
            "Perform topic modeling using Latent Dirichlet Allocation (LDA) on the text data.",
            "Create a new column indicating the assigned topic for each document.",
            "Return the updated dataframe with the topic assignments."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.decomposition import LatentDirichletAllocation",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "import numpy as np"
        ],
        "function_def": "def perform_topic_modeling(text_data, topic_column):\n    # Step 1: Perform topic modeling using Latent Dirichlet Allocation (LDA) on the text data\n    tfidf_vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.7)\n    tfidf_data = tfidf_vectorizer.fit_transform(text_data)\n    lda_model = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf_data)\n    \n    # Step 2: Create a new column indicating the assigned topic for each document\n    topic_assignments = np.argmax(lda_model.transform(tfidf_data), axis=1)\n    \n    # Step 3: Return the updated dataframe with the topic assignments\n    text_data['topic_assignment'] = topic_assignments\n    \n    return text_data"
    },
    {
        "function_name": "analyze_column_difference",
        "file_name": "data_statistics.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column1": "str",
            "column2": "str",
            "threshold": "int"
        },
        "objectives": [
            "Calculate the correlation between column1 and column2.",
            "Create a new column that represents the difference between column1 and column2.",
            "Calculate the mean and standard deviation of the new column.",
            "Identify the rows where the difference is greater than the threshold plus the standard deviation.",
            "Return the corresponding data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def analyze_column_difference(data, column1, column2, threshold):\n    # Step 1: Calculate the correlation between column1 and column2\n    correlation = data[column1].corr(data[column2])\n    \n    # Step 2: Create a new column that represents the difference between column1 and column2\n    data['difference'] = data[column1] - data[column2]\n    \n    # Step 3: Calculate the mean and standard deviation of the new column\n    mean_difference = data['difference'].mean()\n    std_difference = data['difference'].std()\n    \n    # Step 4: Identify the rows where the difference is greater than the threshold plus the standard deviation\n    filtered_data = data[data['difference'] > threshold + std_difference]\n    \n    # Step 5: Return the corresponding data\n    return filtered_data"
    },
    {
        "function_name": "detect_anomalies",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Sort the data by the id_column and value_column in ascending order.",
            "Calculate the moving average of the value_column over a window of size window_size.",
            "Identify the rows where the moving average is greater than the value_column.",
            "Create a new column 'anomaly' and mark these rows as True, otherwise False.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_anomalies(data, id_column, value_column, window_size):\n    data = data.sort_values([id_column, value_column])\n    \n    data['moving_average'] = data[value_column].rolling(window_size).mean()\n    \n    data['anomaly'] = (data['moving_average'] > data[value_column])\n    \n    return data"
    },
    {
        "function_name": "extract_keywords",
        "file_name": "text_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "keyword_list": "list of str",
            "sentiment_threshold": "float"
        },
        "objectives": [
            "Perform sentiment analysis on the text_column.",
            "Identify the rows where the sentiment score is greater than the sentiment_threshold.",
            "Filter the text data to include only the rows where the sentiment score is greater than the sentiment_threshold.",
            "Extract the keywords from the filtered text data.",
            "Return a dictionary where the keys are the extracted keywords and the values are their frequencies."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.sentiment import SentimentIntensityAnalyzer",
            "from collections import Counter",
            "import re"
        ],
        "function_def": "def extract_keywords(df, text_column, keyword_list, sentiment_threshold):\n    sia = SentimentIntensityAnalyzer()\n    \n    df['sentiment_score'] = df[text_column].apply(lambda x: sia.polarity_scores(x)['compound'])\n    \n    filtered_df = df[df['sentiment_score'] > sentiment_threshold]\n    \n    text_data = filtered_df[text_column].tolist()\n    \n    text_data = ' '.join(text_data)\n    \n    keywords = re.findall(r'\\b\\w+\\b', text_data.lower())\n    \n    keyword_freq = Counter(keywords)\n    \n    return dict(keyword_freq)"
    },
    {
        "function_name": "detect_daily_average_breaches",
        "file_name": "data_breaches.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str",
            "target_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Group the input DataFrame `df` by the `id_column` and calculate the daily rolling average of the `target_column` values.",
            "Identify the rows where the rolling average exceeds the specified `threshold` for each group.",
            "Create a new column 'flag' and assign True to the rows where the rolling average exceeds the threshold.",
            "Return the resulting DataFrame with the 'flag' column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_daily_average_breaches(df, id_column, date_column, target_column, threshold):\n    df[date_column] = pd.to_datetime(df[date_column])\n    df.set_index(date_column, inplace=True)\n    \n    df['rolling_avg'] = df.groupby(id_column)[target_column].transform(lambda x: x.rolling('1D').mean())\n    df['flag'] = df.groupby(id_column)['rolling_avg'].transform(lambda x: x > threshold)\n    \n    return df.reset_index()"
    },
    {
        "function_name": "identify_high_value_customers",
        "file_name": "customer_insights.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "transaction_column": "str",
            "customer_column": "str",
            "date_column": "str",
            "top_n_customers": "int",
            "threshold": "float"
        },
        "objectives": [
            "Group the input DataFrame `df` by the `date_column` and the `customer_column` and calculate the total transaction value for each customer on each date.",
            "Identify the top `top_n_customers` customers with the highest transaction value for each date.",
            "Calculate the cumulative sum of the transaction values for each customer over time.",
            "Filter out customers with a cumulative sum that exceeds the specified `threshold`.",
            "Return the resulting DataFrame with the filtered customer data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_high_value_customers(df, transaction_column, customer_column, date_column, top_n_customers, threshold):\n    df[date_column] = pd.to_datetime(df[date_column])\n    \n    daily_transactions = df.groupby([date_column, customer_column])[transaction_column].sum().reset_index()\n    \n    top_customers = daily_transactions.groupby(date_column)[transaction_column].nlargest(top_n_customers).reset_index(drop=True)\n    \n    cumulative_transactions = daily_transactions.groupby(customer_column)[transaction_column].cumsum().reset_index()\n    \n    filtered_transactions = cumulative_transactions[cumulative_transactions[transaction_column] < threshold]\n    \n    return filtered_transactions"
    },
    {
        "function_name": "analyze_sentiment_distribution",
        "file_name": "sentiment_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "comment_column": "str",
            "sentiment_column": "str",
            "topic_column": "str",
            "sentiment_threshold": "float"
        },
        "objectives": [
            "Group the input DataFrame `df` by the `topic_column` and calculate the average sentiment for each topic.",
            "Identify the topics with an average sentiment score that exceeds the specified `sentiment_threshold`.",
            "Filter out rows where the sentiment does not exceed the `sentiment_threshold`.",
            "Group the remaining rows by the `topic_column` and calculate the sentiment distribution for each topic.",
            "Return the resulting DataFrame with the topic sentiment distribution."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def analyze_sentiment_distribution(df, comment_column, sentiment_column, topic_column, sentiment_threshold):\n    topic_sentiments = df.groupby(topic_column)[sentiment_column].mean().reset_index()\n    \n    top_topics = topic_sentiments[topic_sentiments[sentiment_column] > sentiment_threshold]\n    \n    top_topic_df = df[df[topic_column].isin(top_topics[topic_column])]\n    \n    sentiment_distributions = top_topic_df.groupby(topic_column)[sentiment_column].value_counts(normalize=True).reset_index()\n    \n    return sentiment_distributions"
    },
    {
        "function_name": "merge_date_ranges",
        "file_name": "date_range_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "start_date_column": "str",
            "end_date_column": "str",
            "max_gap": "int"
        },
        "objectives": [
            "Identify the rows with overlapping date ranges (i.e., start_date <= end_date).",
            "Group the rows by the id column and calculate the total duration of each group.",
            "Check for gaps in the date ranges within each group and calculate the total gap duration.",
            "If the total gap duration exceeds the specified max_gap, merge the date ranges in the group.",
            "Return the updated dataframe with the merged date ranges."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_date_ranges(df, id_column, start_date_column, end_date_column, max_gap):\n    # Step 1: Group the rows by the id column\n    groups = df.groupby(id_column)\n    \n    # Step 2: Calculate the total duration of each group\n    total_durations = groups.apply(lambda x: (x[end_date_column] - x[start_date_column]).sum())\n    \n    # Step 3: Check for gaps in the date ranges\n    gaps = []\n    for name, group in groups:\n        sorted_group = group.sort_values(start_date_column)\n        for i in range(1, len(sorted_group)):\n            gap = (sorted_group.iloc[i][start_date_column] - sorted_group.iloc[i - 1][end_date_column]).days\n            if gap > 0:\n                gaps.append((name, gap))\n    \n    # Step 4: Merge the date ranges if the total gap duration exceeds the max_gap\n    for name, gap in gaps:\n        if gap > max_gap:\n            group = groups.get_group(name)\n            merged_start_date = group[start_date_column].min()\n            merged_end_date = group[end_date_column].max()\n            df.loc[df[id_column] == name, [start_date_column, end_date_column]] = [merged_start_date, merged_end_date]\n    \n    return df"
    },
    {
        "function_name": "grouped_rolling_std",
        "file_name": "grouped_rollings.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "grouping_columns": "list of str",
            "aggregation_columns": "list of str",
            "threshold": "float",
            "window_size": "int"
        },
        "objectives": [
            "Group the data by the specified columns and apply aggregation functions to specified columns.",
            "Calculate the rolling standard deviation of the aggregated values using the specified window size.",
            "Identify the groups where the standard deviation is greater than the specified threshold.",
            "Return the resulting data with an additional column indicating whether the group meets the threshold condition."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def grouped_rolling_std(df, grouping_columns, aggregation_columns, threshold, window_size):\n    # Group the data by the specified columns and apply aggregation functions to specified columns\n    aggregated_df = df.groupby(grouping_columns)[aggregation_columns].mean().reset_index()\n    \n    # Calculate the rolling standard deviation of the aggregated values using the specified window size\n    aggregated_df['rolling_std'] = aggregated_df.groupby(grouping_columns)[aggregation_columns].transform(lambda x: x.rolling(window_size).std())\n    \n    # Identify the groups where the standard deviation is greater than the specified threshold\n    aggregated_df['meets_threshold'] = aggregated_df['rolling_std'] > threshold\n    \n    return aggregated_df"
    },
    {
        "function_name": "calculate_similarity",
        "file_name": "similarity_calculation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "feature_columns": "list of str",
            "threshold": "float",
            "top_n": "int"
        },
        "objectives": [
            "Calculate the similarity between rows using the cosine similarity metric.",
            "Identify the top N rows with the highest similarity for each row.",
            "Return the resulting data with the top N similar rows."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics.pairwise import cosine_similarity"
        ],
        "function_def": "def calculate_similarity(df, id_column, feature_columns, threshold, top_n):\n    # Calculate the similarity between rows using the cosine similarity metric\n    similarity_matrix = cosine_similarity(df[feature_columns], df[feature_columns])\n    \n    # Identify the top N rows with the highest similarity for each row\n    top_similar_rows = []\n    for i in range(len(similarity_matrix)):\n        top_n_rows = np.argsort(similarity_matrix[i])[-top_n:]\n        top_similar_rows.append([(df.iloc[j][id_column], similarity_matrix[i, j]) for j in top_n_rows if i != j and similarity_matrix[i, j] >= threshold])\n    \n    # Return the resulting data with the top N similar rows\n    return pd.DataFrame({'id': df[id_column], 'top_similar_rows': top_similar_rows})"
    },
    {
        "function_name": "train_text_classifier",
        "file_name": "text_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "target_column": "str",
            "max_features": "int"
        },
        "objectives": [
            "Convert the text data in the specified column to a numerical representation using TF-IDF.",
            "Select the top features with the highest importance based on the TF-IDF representation.",
            "Train a random forest classifier on the selected features and the target column.",
            "Return the trained classifier and the feature importance."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.feature_selection import SelectFromModel"
        ],
        "function_def": "def train_text_classifier(df, text_column, target_column, max_features):\n    # Convert the text data to a numerical representation using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=max_features)\n    X = vectorizer.fit_transform(df[text_column])\n    \n    # Train a random forest classifier on the TF-IDF representation and the target column\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, df[target_column])\n    \n    # Select the top features with the highest importance based on the TF-IDF representation\n    selector = SelectFromModel(clf, threshold=0.01)\n    selector.fit(X, df[target_column])\n    \n    # Return the trained classifier and the feature importance\n    return clf, selector.get_support()"
    },
    {
        "function_name": "decompose_time_series",
        "file_name": "time_series_decomposition.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "value_column": "str",
            "seasonal_period": "int"
        },
        "objectives": [
            "Convert the date column to a datetime format.",
            "Extract the seasonal component from the time series data using the specified period.",
            "Calculate the trend component using a moving average.",
            "Create a new column containing the residual component.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def decompose_time_series(df, date_column, value_column, seasonal_period):\n    # Convert the date column to a datetime format\n    df[date_column] = pd.to_datetime(df[date_column])\n    \n    # Extract the seasonal component from the time series data\n    seasonal_component = df[value_column].rolling(seasonal_period).mean()\n    \n    # Calculate the trend component using a moving average\n    trend_component = df[value_column].rolling(seasonal_period * 2).mean()\n    \n    # Create a new column containing the residual component\n    df['residual'] = df[value_column] - seasonal_component - trend_component\n    \n    return df"
    },
    {
        "function_name": "detect_and_remove_outliers",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "anomaly_detection_method": "str ('z-score', 'IQR')"
        },
        "objectives": [
            "Convert the specified time column to a datetime format.",
            "Normalize the numeric columns by removing outliers using the specified anomaly detection method.",
            "If the anomaly detection method is 'z-score', use the z-score method to detect and remove outliers.",
            "If the anomaly detection method is 'IQR', use the Interquartile Range (IQR) method to detect and remove outliers.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def detect_and_remove_outliers(df, time_column, anomaly_detection_method):\n    df[time_column] = pd.to_datetime(df[time_column])\n    numeric_df = df.select_dtypes(include=[int, float])\n    \n    if anomaly_detection_method == 'z-score':\n        z_scores = np.abs((numeric_df - numeric_df.mean()) / numeric_df.std())\n        mask = z_scores > 3\n        numeric_df[z_scores > 3] = np.nan\n    elif anomaly_detection_method == 'IQR':\n        Q1 = numeric_df.quantile(0.25)\n        Q3 = numeric_df.quantile(0.75)\n        IQR = Q3 - Q1\n        mask = ((numeric_df < (Q1 - 1.5 * IQR)) | (numeric_df > (Q3 + 1.5 * IQR))).astype(bool)\n        numeric_df[mask] = np.nan\n    \n    df[numeric_df.columns] = numeric_df\n    \n    return df"
    },
    {
        "function_name": "preprocess_data",
        "file_name": "data_preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numeric_columns": "list",
            "categorical_columns": "list"
        },
        "objectives": [
            "One-hot encode the categorical columns.",
            "Scale the numeric columns using StandardScaler.",
            "Combine the encoded categorical columns and scaled numeric columns.",
            "Return the preprocessed dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
        ],
        "function_def": "def preprocess_data(df, numeric_columns, categorical_columns):\n    # One-hot encode the categorical columns\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    encoded_data = encoder.fit_transform(df[categorical_columns])\n    encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out())\n    \n    # Scale the numeric columns using StandardScaler\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df[numeric_columns])\n    scaled_df = pd.DataFrame(scaled_data, columns=numeric_columns)\n    \n    # Combine the encoded categorical columns and scaled numeric columns\n    preprocessed_df = pd.concat([encoded_df, scaled_df], axis=1)\n    \n    return preprocessed_df"
    },
    {
        "function_name": "resample_data",
        "file_name": "datetime_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "interval": "str"
        },
        "objectives": [
            "Convert the date column to a datetime object.",
            "Resample the dataframe by the specified interval.",
            "Calculate the mean, median, and standard deviation for each resampled group.",
            "Return the aggregated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_data(df, date_column, interval):\n    # Convert the date column to a datetime object\n    df[date_column] = pd.to_datetime(df[date_column])\n    \n    # Resample the dataframe by the specified interval\n    resampled_df = df.resample(interval, on=date_column)\n    \n    # Calculate the mean, median, and standard deviation for each resampled group\n    aggregated_df = resampled_df.agg(['mean', 'median', 'std'])\n    \n    return aggregated_df"
    },
    {
        "function_name": "get_correlated_features",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the correlation between the feature_column and all other columns.",
            "Filter the correlated columns based on the specified threshold.",
            "Return the filtered dataframe with the feature_column and correlated columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def get_correlated_features(df, feature_column, threshold):\n    # Calculate the correlation between the feature_column and all other columns\n    correlation_df = df.corr()\n    \n    # Filter the correlated columns based on the specified threshold\n    correlated_columns = correlation_df[feature_column].abs()[lambda x: x > threshold].index\n    \n    # Return the filtered dataframe with the feature_column and correlated columns\n    filtered_df = df[[feature_column] + list(correlated_columns)]\n    \n    return filtered_df"
    },
    {
        "function_name": "scale_normalize_identify",
        "file_name": "data_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "scaling_column": "str",
            "normalization_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Standard scale the values in the scaling_column using the StandardScaler from scikit-learn.",
            "Normalize the values in the normalization_column using the MinMaxScaler from scikit-learn.",
            "Identify rows where the scaled value is greater than the threshold and the normalized value is less than the threshold.",
            "Return the DataFrame with a new column indicating whether each row meets the specified conditions."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
        ],
        "function_def": "def scale_normalize_identify(df, scaling_column, normalization_column, threshold):\n    # Step 1: Standard scale the values in the scaling_column\n    scaler = StandardScaler()\n    df['scaled_value'] = scaler.fit_transform(df[[scaling_column]])\n    \n    # Step 2: Normalize the values in the normalization_column\n    normalizer = MinMaxScaler()\n    df['normalized_value'] = normalizer.fit_transform(df[[normalization_column]])\n    \n    # Step 3: Identify rows where the scaled value is greater than the threshold and the normalized value is less than the threshold\n    df['meets_conditions'] = (df['scaled_value'] > threshold) & (df['normalized_value'] < threshold)\n    \n    return df"
    },
    {
        "function_name": "filter_word_freq",
        "file_name": "text_analysis.py",
        "parameters": {
            "text_data": "list",
            "stop_words": "list",
            "min_freq": "int"
        },
        "objectives": [
            "Remove the stop words from each text in the `text_data`.",
            "Calculate the frequency of each word in the text data.",
            "Filter out the words with frequency less than the specified `min_freq`.",
            "Return the resulting word frequencies."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def filter_word_freq(text_data, stop_words, min_freq):\n    # Remove the stop words from each text\n    cleaned_text = [' '.join([word for word in text.split() if word not in stop_words]) for text in text_data]\n    \n    # Calculate the frequency of each word in the text data\n    word_freq = Counter(' '.join(cleaned_text).split())\n    \n    # Filter out the words with frequency less than the min_freq\n    filtered_word_freq = {word: freq for word, freq in word_freq.items() if freq >= min_freq}\n    \n    return filtered_word_freq"
    },
    {
        "function_name": "remove_low_correlation_cols",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "max_correlation": "float"
        },
        "objectives": [
            "Calculate the correlation between the `target_col` and all other columns in the dataframe.",
            "Identify the columns with correlation less than the specified `max_correlation`.",
            "Remove these columns from the dataframe.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_low_correlation_cols(df, target_col, max_correlation):\n    # Calculate the correlation between the target_col and all other columns\n    correlation = df.corrwith(df[target_col])\n    \n    # Identify the columns with correlation less than the max_correlation\n    low_correlation_cols = correlation[(correlation < max_correlation) & (correlation != 1)].index\n    \n    # Remove these columns from the dataframe\n    result_df = df.drop(low_correlation_cols, axis=1)\n    \n    return result_df"
    },
    {
        "function_name": "row_similarity",
        "file_name": "data_similarity.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "feature_columns": "list of str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the pairwise cosine similarity between each pair of rows in the dataframe.",
            "Calculate the average similarity for each row.",
            "Identify the rows with an average similarity greater than the threshold.",
            "Create a new dataframe with the ids of the similar rows."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics.pairwise import cosine_similarity"
        ],
        "function_def": "def row_similarity(data, id_column, feature_columns, threshold):\n    # Step 1: Calculate the pairwise cosine similarity between each pair of rows in the dataframe\n    similarity_matrix = cosine_similarity(data[feature_columns])\n    \n    # Step 2: Calculate the average similarity for each row\n    average_similarity = similarity_matrix.mean(axis=1)\n    \n    # Step 3: Identify the rows with an average similarity greater than the threshold\n    similar_rows = data[id_column][average_similarity > threshold]\n    \n    # Step 4: Create a new dataframe with the ids of the similar rows\n    result_data = pd.DataFrame(similar_rows).reset_index(drop=True)\n    \n    return result_data"
    },
    {
        "function_name": "discretize_and_encode",
        "file_name": "encoding_operations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "continuous_column": "str",
            "categorical_column": "str",
            "bins": "int"
        },
        "objectives": [
            "Discretize the continuous column into bins using the specified number of bins.",
            "Create a new column with the discretized values.",
            "Perform one-hot encoding on the categorical column.",
            "Merge the discretized column with the one-hot encoded categorical column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import KBinsDiscretizer",
            "from sklearn.preprocessing import OneHotEncoder"
        ],
        "function_def": "def discretize_and_encode(data, continuous_column, categorical_column, bins):\n    # Discretize the continuous column\n    discretizer = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='kmeans')\n    discretized_values = discretizer.fit_transform(data[[continuous_column]])\n    data[f'{continuous_column}_discretized'] = discretized_values\n    \n    # One-hot encode the categorical column\n    encoder = OneHotEncoder()\n    encoded_values = encoder.fit_transform(data[[categorical_column]])\n    encoded_columns = [f'{categorical_column}_{i}' for i in range(encoded_values.shape[1])]\n    encoded_df = pd.DataFrame(encoded_values.toarray(), columns=encoded_columns)\n    \n    # Merge the discretized column with the one-hot encoded categorical column\n    result = pd.concat([data[f'{continuous_column}_discretized'], encoded_df], axis=1)\n    \n    return result"
    },
    {
        "function_name": "text_vectorization",
        "file_name": "text_processing.py",
        "parameters": {
            "text_data": "list of str",
            "max_features": "int",
            "min_df": "float"
        },
        "objectives": [
            "Tokenize the input text data into words.",
            "Remove stopwords from the tokenized words.",
            "Fit a TF-IDF vectorizer to the tokenized words and transform them into numerical vectors.",
            "Select the top max_features words with the highest TF-IDF scores.",
            "Return a dictionary where the keys are the selected words and the values are their TF-IDF scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from nltk.corpus import stopwords",
            "import nltk"
        ],
        "function_def": "def text_vectorization(text_data, max_features, min_df):\n    # Tokenize the input text data into words\n    tokenized_words = [word.split() for word in text_data]\n    \n    # Remove stopwords from the tokenized words\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [[word for word in words if word not in stop_words] for words in tokenized_words]\n    \n    # Fit a TF-IDF vectorizer to the tokenized words and transform them into numerical vectors\n    vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df)\n    tfidf_vectors = vectorizer.fit_transform([' '.join(words) for words in filtered_words])\n    \n    # Select the top max_features words with the highest TF-IDF scores\n    feature_names = vectorizer.get_feature_names_out()\n    tfidf_scores = tfidf_vectors.toarray().sum(axis=0)\n    selected_features = {feature_names[i]: score for i, score in enumerate(tfidf_scores) if score > 0}\n    \n    return selected_features"
    },
    {
        "function_name": "anomaly_detection",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "series_data": "pandas.Series",
            "window_size": "int",
            "threshold": "float"
        },
        "objectives": [
            "Apply a rolling window of size window_size to the input series data.",
            "Calculate the standard deviation of the values in each window.",
            "Filter out values that are more than threshold standard deviations away from the mean.",
            "Return the filtered series."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def anomaly_detection(series_data, window_size, threshold):\n    # Apply a rolling window of size window_size to the input series data\n    rolling_window = series_data.rolling(window_size)\n    \n    # Calculate the standard deviation of the values in each window\n    std_dev = rolling_window.std()\n    \n    # Filter out values that are more than threshold standard deviations away from the mean\n    mean = rolling_window.mean()\n    filtered_values = series_data[(series_data - mean).abs() <= threshold * std_dev]\n    \n    return filtered_values"
    },
    {
        "function_name": "ngram_sentiment_analysis",
        "file_name": "nlp_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "sentiment_column": "str",
            "n_grams": "int"
        },
        "objectives": [
            "Perform sentiment analysis on the text data in the specified column.",
            "Extract n-grams from the text data.",
            "Calculate the sentiment score for each n-gram.",
            "Create a new dataframe with the n-grams and corresponding sentiment scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from nltk.sentiment.vader import SentimentIntensityAnalyzer",
            "from nltk.util import ngrams"
        ],
        "function_def": "def ngram_sentiment_analysis(df, text_column, sentiment_column, n_grams):\n    # Initialize the sentiment intensity analyzer\n    sia = SentimentIntensityAnalyzer()\n    \n    # Perform sentiment analysis on the text data\n    df[sentiment_column] = df[text_column].apply(lambda x: sia.polarity_scores(x)['compound'])\n    \n    # Extract n-grams from the text data\n    ngram_df = df[text_column].apply(lambda x: list(ngrams(x.split(), n_grams)))\n    \n    # Calculate the sentiment score for each n-gram\n    ngram_sentiments = ngram_df.apply(lambda x: [sia.polarity_scores(' '.join(gram))['compound'] for gram in x])\n    \n    # Create a new dataframe with the n-grams and corresponding sentiment scores\n    ngram_df = pd.DataFrame({'ngrams': ngram_df.tolist(), 'sentiments': ngram_sentiments.tolist()})\n    \n    return ngram_df"
    },
    {
        "function_name": "cluster_groupby_stats",
        "file_name": "clustering_operations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "groupby_column": "str",
            "target_column": "str",
            "n_clusters": "int"
        },
        "objectives": [
            "Group the data by the specified column.",
            "Calculate the mean and standard deviation of the target column for each group.",
            "Perform K-Means clustering on the mean and standard deviation values.",
            "Create a new dataframe with the cluster labels and corresponding statistics."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def cluster_groupby_stats(data, groupby_column, target_column, n_clusters):\n    # Group the data by the specified column\n    grouped_data = data.groupby(groupby_column)\n    \n    # Calculate the mean and standard deviation of the target column for each group\n    mean_data = grouped_data[target_column].mean()\n    std_data = grouped_data[target_column].std()\n    \n    # Perform K-Means clustering on the mean and standard deviation values\n    kmeans = KMeans(n_clusters=n_clusters)\n    cluster_labels = kmeans.fit_predict(pd.DataFrame({'mean': mean_data, 'std': std_data}))\n    \n    # Create a new dataframe with the cluster labels and corresponding statistics\n    cluster_df = pd.DataFrame({'cluster_label': cluster_labels, 'mean': mean_data, 'std': std_data})\n    \n    return cluster_df"
    },
    {
        "function_name": "convert_date",
        "file_name": "datetime_utils.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "format": "str"
        },
        "objectives": [
            "Convert the date column to a specified format.",
            "Extract the year, month, and day from the date column.",
            "Create new columns for the year, month, and day.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def convert_date(data, date_column, format):\n    # Convert the date column to a specified format\n    data[date_column] = pd.to_datetime(data[date_column], format=format)\n    \n    # Extract the year, month, and day from the date column\n    data['year'] = data[date_column].dt.year\n    data['month'] = data[date_column].dt.month\n    data['day'] = data[date_column].dt.day\n    \n    return data"
    },
    {
        "function_name": "remove_null_columns",
        "file_name": "data_quality.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "null_threshold": "float"
        },
        "objectives": [
            "Calculate the percentage of null values in each column.",
            "Identify the columns with null percentage greater than the specified threshold.",
            "Remove these columns from the dataframe.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_null_columns(data, null_threshold):\n    # Calculate the percentage of null values in each column\n    null_percentage = data.isnull().mean()\n    \n    # Identify the columns with null percentage greater than the specified threshold\n    null_columns = null_percentage[null_percentage > null_threshold].index\n    \n    # Remove these columns from the dataframe\n    updated_data = data.drop(null_columns, axis=1)\n    \n    return updated_data"
    },
    {
        "function_name": "identify_influential_nodes",
        "file_name": "graph_analysis.py",
        "parameters": {
            "graph_data": "list",
            "node_column": "str",
            "edge_column": "str"
        },
        "objectives": [
            "Create a graph from the graph_data.",
            "Calculate the degree centrality for each node in the graph.",
            "Identify the nodes with a degree centrality greater than the mean degree centrality.",
            "Return the subgraph containing the identified nodes."
        ],
        "import_lines": [
            "import networkx as nx"
        ],
        "function_def": "def identify_influential_nodes(graph_data, node_column, edge_column):\n    # Create a graph from the graph_data\n    graph = nx.Graph()\n    graph.add_edges_from(graph_data)\n    \n    # Calculate the degree centrality for each node in the graph\n    degree_centralities = nx.degree_centrality(graph)\n    \n    # Identify the nodes with a degree centrality greater than the mean degree centrality\n    mean_degree_centrality = np.mean(list(degree_centralities.values()))\n    influential_nodes = [node for node, centrality in degree_centralities.items() if centrality > mean_degree_centrality]\n    \n    # Return the subgraph containing the identified nodes\n    return graph.subgraph(influential_nodes)"
    },
    {
        "function_name": "identify_outlier_groups",
        "file_name": "cluster_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "cluster_column": "str",
            "feature_column": "str"
        },
        "objectives": [
            "Group the dataframe by the cluster_column.",
            "Calculate the mean and standard deviation of the feature_column for each group.",
            "Identify the groups with a mean feature_column value greater than 1 standard deviation away from the overall mean.",
            "Return the updated dataframe with the identified groups."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_outlier_groups(data, cluster_column, feature_column):\n    # Group the dataframe by the cluster_column\n    group_means = data.groupby(cluster_column)[feature_column].mean()\n    group_stds = data.groupby(cluster_column)[feature_column].std()\n    \n    # Calculate the mean and standard deviation of the feature_column for each group\n    overall_mean = data[feature_column].mean()\n    overall_std = data[feature_column].std()\n    \n    # Identify the groups with a mean feature_column value greater than 1 standard deviation away from the overall mean\n    outlier_groups = group_means[(np.abs(group_means - overall_mean) > overall_std)].index\n    \n    # Return the updated dataframe with the identified groups\n    data['outlier'] = np.where(data[cluster_column].isin(outlier_groups), 1, 0)\n    \n    return data"
    },
    {
        "function_name": "hierarchical_clustering",
        "file_name": "clustering_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "clustering_column": "str",
            "n_clusters": "int",
            "linkage_method": "str"
        },
        "objectives": [
            "Perform hierarchical clustering on the specified column using the specified linkage method.",
            "Identify the optimal number of clusters using the silhouette score.",
            "Create a new column that contains the cluster labels and add it to the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import AgglomerativeClustering",
            "from sklearn.metrics import silhouette_score"
        ],
        "function_def": "def hierarchical_clustering(data, clustering_column, n_clusters, linkage_method):\n    # Perform hierarchical clustering on the specified column using the specified linkage method\n    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage_method)\n    cluster_labels = clustering.fit_predict(data[[clustering_column]])\n    \n    # Identify the optimal number of clusters using the silhouette score\n    silhouette = silhouette_score(data[[clustering_column]], cluster_labels)\n    \n    # Create a new column that contains the cluster labels and add it to the original dataframe\n    data['cluster_labels'] = cluster_labels\n    \n    return data"
    },
    {
        "function_name": "identify_first_events_and_time_diff",
        "file_name": "event_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str",
            "event_column": "str"
        },
        "objectives": [
            "Convert the date column to datetime format and sort the data by date.",
            "Identify the first occurrence of each event for each id.",
            "Create a new column 'first_event_date' with the first occurrence dates.",
            "Calculate the time difference between the first event date and the current date for each id."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def identify_first_events_and_time_diff(data, id_column, date_column, event_column):\n    data[date_column] = pd.to_datetime(data[date_column])\n    data.sort_values(by=date_column, inplace=True)\n    \n    first_events = data.loc[data.groupby([id_column, event_column])[date_column].idxmin()]\n    data = data.merge(first_events[[id_column, event_column, date_column]], on=[id_column, event_column], how='left', suffixes=('', '_first'))\n    data['time_diff'] = (data[date_column] - data[date_column + '_first']).dt.total_seconds()\n    \n    return data"
    },
    {
        "function_name": "calculate_exp_moving_average_and_identify_outliers",
        "file_name": "outlier_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Calculate the exponential moving average of the value column for each window of size 'window_size'.",
            "Create a new column 'exp_moving_average' with the exponential moving average values.",
            "Calculate the cumulative sum of the absolute differences between the original values and the exponential moving averages.",
            "Identify the rows where the cumulative sum exceeds twice the standard deviation of the cumulative sum."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def calculate_exp_moving_average_and_identify_outliers(data, id_column, value_column, window_size):\n    data['exp_moving_average'] = data.groupby(id_column)[value_column].transform(lambda x: x.ewm(span=window_size, adjust=False).mean())\n    data['cumulative_sum'] = (data[value_column] - data['exp_moving_average']).abs().cumsum()\n    \n    std_dev = data['cumulative_sum'].std()\n    data['outlier'] = data['cumulative_sum'] > 2 * std_dev\n    \n    return data"
    },
    {
        "function_name": "community_detection",
        "file_name": "graph_analysis.py",
        "parameters": {
            "graph_data": "dict",
            "edge_threshold": "float",
            "community_threshold": "float"
        },
        "objectives": [
            "Create a graph from the input graph data.",
            "Remove the edges with weights less than the specified edge threshold.",
            "Identify the communities in the graph using the Louvain community detection algorithm.",
            "Filter out the communities with a size less than the specified community threshold.",
            "Return the community assignments for each node."
        ],
        "import_lines": [
            "import networkx as nx",
            "from networkx.algorithms import community",
            "import numpy as np"
        ],
        "function_def": "def community_detection(graph_data, edge_threshold, community_threshold):\n    # Create a graph from the input graph data\n    graph = nx.Graph()\n    for node, edges in graph_data.items():\n        for edge, weight in edges.items():\n            if weight > edge_threshold:\n                graph.add_edge(node, edge, weight=weight)\n    \n    # Identify the communities in the graph using the Louvain community detection algorithm\n    communities = community.asyn_lpa_communities(graph, weight='weight')\n    \n    # Filter out the communities with a size less than the specified community threshold\n    filtered_communities = [ comunidad for comunidad in communities if len(comunidad) > community_threshold]\n    \n    # Get the community assignments for each node\n    community_assignments = {}\n    for i, comunidad in enumerate(filtered_communities):\n        for node in comunidad:\n            community_assignments[node] = i\n    \n    return community_assignments"
    },
    {
        "function_name": "identify_high_value_customers",
        "file_name": "customer_segmentation.py",
        "parameters": {
            "customer_data": "pandas.DataFrame",
            "transaction_column": "str",
            "spend_threshold": "float",
            "time_window": "str (in the format 'X days' or 'X months' or 'X years')"
        },
        "objectives": [
            "Calculate the total spend for each customer within the specified time window.",
            "Identify customers who have a total spend greater than or equal to the spend threshold.",
            "Create a new column indicating whether each customer is a high-value customer.",
            "Return the updated dataframe with the high-value customers."
        ],
        "import_lines": [
            "import pandas as pd",
            "from dateutil.relativedelta import relativedelta"
        ],
        "function_def": "def identify_high_value_customers(customer_data, transaction_column, spend_threshold, time_window):\n    # Calculate the time window in days\n    if 'day' in time_window:\n        time_window_days = int(time_window.split(' ')[0])\n    elif 'month' in time_window:\n        time_window_days = int(time_window.split(' ')[0]) * 30\n    elif 'year' in time_window:\n        time_window_days = int(time_window.split(' ')[0]) * 365\n    \n    # Calculate the total spend for each customer within the specified time window\n    customer_data['date'] = pd.to_datetime(customer_data['date'])\n    max_date = customer_data['date'].max()\n    min_date = max_date - relativedelta(days=time_window_days)\n    customer_data = customer_data[(customer_data['date'] >= min_date) & (customer_data['date'] <= max_date)]\n    total_spend = customer_data.groupby('customer_id')[transaction_column].sum().reset_index()\n    \n    # Identify customers who have a total spend greater than or equal to the spend threshold\n    high_value_customers = total_spend[total_spend[transaction_column] >= spend_threshold]['customer_id'].tolist()\n    \n    # Create a new column indicating whether each customer is a high-value customer\n    customer_data['is_high_value'] = np.where(customer_data['customer_id'].isin(high_value_customers), True, False)\n    \n    return customer_data"
    },
    {
        "function_name": "analyze_promotion_effectiveness",
        "file_name": "sales_analysis.py",
        "parameters": {
            "product_data": "pandas.DataFrame",
            "sales_column": "str",
            "promo_column": "str",
            "start_date": "str",
            "end_date": "str"
        },
        "objectives": [
            "Calculate the sales for each product before and after the promotion period.",
            "Calculate the percentage increase in sales for each product during the promotion period.",
            "Identify products with a percentage increase greater than 20%.",
            "Return the dataframe with the products and their corresponding percentage increase."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def analyze_promotion_effectiveness(product_data, sales_column, promo_column, start_date, end_date):\n    # Convert date columns to datetime format\n    product_data['date'] = pd.to_datetime(product_data['date'])\n    start_date = pd.to_datetime(start_date)\n    end_date = pd.to_datetime(end_date)\n    \n    # Filter data for the promotion period\n    promo_data = product_data[(product_data['date'] >= start_date) & (product_data['date'] <= end_date)]\n    non_promo_data = product_data[(product_data['date'] < start_date) | (product_data['date'] > end_date)]\n    \n    # Calculate sales for each product before and after the promotion period\n    promo_sales = promo_data.groupby('product_id')[sales_column].sum().reset_index()\n    non_promo_sales = non_promo_data.groupby('product_id')[sales_column].sum().reset_index()\n    \n    # Calculate the percentage increase in sales for each product during the promotion period\n    promo_sales['percentage_increase'] = ((promo_sales[sales_column] - non_promo_sales[sales_column]) / non_promo_sales[sales_column]) * 100\n    \n    # Identify products with a percentage increase greater than 20%\n    effective_promos = promo_sales[promo_sales['percentage_increase'] > 20][['product_id', 'percentage_increase']]\n    \n    return effective_promos"
    },
    {
        "function_name": "identify_high_paying_departments",
        "file_name": "employee_insights.py",
        "parameters": {
            "employee_data": "pandas.DataFrame",
            "salary_column": "str",
            "department_column": "str",
            "company_column": "str"
        },
        "objectives": [
            "Calculate the average salary for each department and company.",
            "Identify the department and company with the highest average salary.",
            "Create a new column indicating whether each employee is in the department and company with the highest average salary.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_high_paying_departments(employee_data, salary_column, department_column, company_column):\n    # Calculate the average salary for each department and company\n    avg_salary = employee_data.groupby([department_column, company_column])[salary_column].mean().reset_index()\n    \n    # Identify the department and company with the highest average salary\n    max_salary = avg_salary[salary_column].max()\n    max_salary_dept = avg_salary[avg_salary[salary_column] == max_salary][department_column].tolist()[0]\n    max_salary_company = avg_salary[avg_salary[salary_column] == max_salary][company_column].tolist()[0]\n    \n    # Create a new column indicating whether each employee is in the department and company with the highest average salary\n    employee_data['is_high_paying_dept'] = np.where((employee_data[department_column] == max_salary_dept) & (employee_data[company_column] == max_salary_company), True, False)\n    \n    return employee_data"
    },
    {
        "function_name": "identify_high_scoring_students",
        "file_name": "student_performance.py",
        "parameters": {
            "student_data": "pandas.DataFrame",
            "score_column": "str",
            "subject_column": "str",
            "grade_column": "str"
        },
        "objectives": [
            "Calculate the average score for each subject and grade.",
            "Identify the subject and grade with the highest average score.",
            "Create a new column indicating whether each student is in the subject and grade with the highest average score.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_high_scoring_students(student_data, score_column, subject_column, grade_column):\n    # Calculate the average score for each subject and grade\n    avg_score = student_data.groupby([subject_column, grade_column])[score_column].mean().reset_index()\n    \n    # Identify the subject and grade with the highest average score\n    max_score = avg_score[score_column].max()\n    max_score_subject = avg_score[avg_score[score_column] == max_score][subject_column].tolist()[0]\n    max_score_grade = avg_score[avg_score[score_column] == max_score][grade_column].tolist()[0]\n    \n    # Create a new column indicating whether each student is in the subject and grade with the highest average score\n    student_data['is_high_scoring'] = np.where((student_data[subject_column] == max_score_subject) & (student_data[grade_column] == max_score_grade), True, False)\n    \n    return student_data"
    },
    {
        "function_name": "predict_values_excluding_holidays",
        "file_name": "holiday_adjusted_predictions.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "value_column": "str",
            "holiday_list": "list of str"
        },
        "objectives": [
            "Convert the date column to a datetime column and extract the day of the week.",
            "Mark the rows where the date is a holiday as True, otherwise False.",
            "Calculate the rolling average of the value column over a window of size 7 days, excluding holidays.",
            "Create a new column 'predicted_value' and assign the rolling average values to it, where the date is not a holiday."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def predict_values_excluding_holidays(data, date_column, value_column, holiday_list):\n    data[date_column] = pd.to_datetime(data[date_column])\n    data['day_of_week'] = data[date_column].dt.dayofweek\n    data['is_holiday'] = data[date_column].isin(holiday_list)\n    data['predicted_value'] = data[value_column].rolling(window=7, center=True, min_periods=1).mean()\n    data.loc[data['is_holiday'], 'predicted_value'] = None\n    return data"
    },
    {
        "function_name": "variation_analysis",
        "file_name": "variation_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Perform a variation analysis to detect anomalies in the value_column for each group.",
            "Identify the groups where the variation is greater than the threshold.",
            "Return a dataframe with the groups and variation values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def variation_analysis(df, group_column, value_column, threshold):\n    # Perform variation analysis\n    variation_df = df.groupby(group_column)[value_column].agg(lambda x: np.var(x))\n    \n    # Identify groups where variation is greater than threshold\n    anomaly_groups = variation_df[variation_df > threshold]\n    \n    # Return dataframe with results\n    results_df = pd.DataFrame({'group': anomaly_groups.index, 'variation': anomaly_groups.values})\n    \n    return results_df"
    },
    {
        "function_name": "calculate_roc_auc",
        "file_name": "evaluation_metrics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "positive_class": "str"
        },
        "objectives": [
            "Calculate the ROC-AUC score for each column in the dataframe with respect to the target_column.",
            "Create a new dataframe with the column names and their corresponding ROC-AUC scores.",
            "Sort the dataframe by the ROC-AUC scores in descending order.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics import roc_auc_score"
        ],
        "function_def": "def calculate_roc_auc(df, target_column, positive_class):\n    df = df.copy()\n    df[target_column] = df[target_column].apply(lambda x: 1 if x == positive_class else 0)\n    roc_auc_scores = {}\n    for column in df.columns:\n        if column != target_column:\n            try:\n                roc_auc = roc_auc_score(df[target_column], df[column])\n                roc_auc_scores[column] = roc_auc\n            except ValueError:\n                continue\n    roc_auc_df = pd.DataFrame(list(roc_auc_scores.items()), columns=['columns', 'roc_auc'])\n    sorted_roc_auc_df = roc_auc_df.sort_values(by='roc_auc', ascending=False)\n    return sorted_roc_auc_df"
    },
    {
        "function_name": "evaluate_model_performance",
        "file_name": "model_evaluation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_variable": "str",
            "fold": "int"
        },
        "objectives": [
            "Perform K-fold cross-validation on the data.",
            "Train a logistic regression model on each fold.",
            "Evaluate the performance of each model using the area under the receiver operating characteristic curve (AUC-ROC).",
            "Return the average AUC-ROC value across all folds."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import KFold",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.metrics import roc_auc_score",
            "import numpy as np"
        ],
        "function_def": "def evaluate_model_performance(data, target_variable, fold):\n    # Perform K-fold cross-validation on the data\n    kf = KFold(n_splits=fold, shuffle=True)\n    \n    # Train a logistic regression model on each fold\n    auc_values = []\n    for train_index, val_index in kf.split(data):\n        X_train, X_val = data.drop(target_variable, axis=1).iloc[train_index], data.drop(target_variable, axis=1).iloc[val_index]\n        y_train, y_val = data[target_variable].iloc[train_index], data[target_variable].iloc[val_index]\n        \n        lr_model = LogisticRegression()\n        lr_model.fit(X_train, y_train)\n        \n        # Evaluate the performance of each model using the area under the receiver operating characteristic curve (AUC-ROC)\n        y_pred = lr_model.predict_proba(X_val)[:, 1]\n        auc_values.append(roc_auc_score(y_val, y_pred))\n    \n    # Return the average AUC-ROC value across all folds\n    return np.mean(auc_values)"
    },
    {
        "function_name": "kolmogorov_smirnov_test",
        "file_name": "statistical_tests.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "continuous_column": "str",
            "categorical_column": "str"
        },
        "objectives": [
            "Apply the Kolmogorov-Smirnov test to the continuous_column and the categorical_column to calculate the p-value for each category.",
            "Create a new column 'ks_stat' and assign the KS statistic for each category.",
            "Create a new column 'ks_pvalue' and assign the p-value for each category.",
            "Apply the Bonferroni correction to the p-values based on the number of categories."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy import stats"
        ],
        "function_def": "def kolmogorov_smirnov_test(data, continuous_column, categorical_column):\n    categories = data[categorical_column].unique()\n    data['ks_stat'] = None\n    data['ks_pvalue'] = None\n    \n    for category in categories:\n        category_data = data[data[categorical_column] == category]\n        ks_stat, ks_pvalue = stats.ks_2samp(category_data[continuous_column], data[continuous_column])\n        data.loc[data[categorical_column] == category, 'ks_stat'] = ks_stat\n        data.loc[data[categorical_column] == category, 'ks_pvalue'] = ks_pvalue\n    \n    data['ks_pvalue.adj'] = data['ks_pvalue'] * len(categories)\n    \n    return data"
    },
    {
        "function_name": "time_difference_analysis",
        "file_name": "time_series_insights.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "group_column": "str"
        },
        "objectives": [
            "Convert the time column to datetime format.",
            "Create a new column 'time_diff' and assign the time difference between consecutive rows within each group.",
            "Calculate the average time difference within each group.",
            "Create a new column 'avg_time_diff' and assign the average time difference for each group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def time_difference_analysis(data, time_column, group_column):\n    data[time_column] = pd.to_datetime(data[time_column])\n    data = data.sort_values(by=[group_column, time_column])\n    \n    data['time_diff'] = data.groupby(group_column)[time_column].diff()\n    \n    avg_time_diff = data.groupby(group_column)['time_diff'].mean().reset_index()\n    avg_time_diff.columns = [group_column, 'avg_time_diff']\n    \n    data = pd.merge(data, avg_time_diff, on=group_column, how='left')\n    \n    return data"
    },
    {
        "function_name": "train_model",
        "file_name": "machine_learning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "model_type": "str"
        },
        "objectives": [
            "Split the data into training and testing sets.",
            "Train a machine learning model (either Logistic Regression or Decision Tree) on the training data to predict the target variable.",
            "Evaluate the performance of the model using metrics such as accuracy, precision, and recall.",
            "Return the predicted values along with the performance metrics."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.metrics import accuracy_score, precision_score, recall_score",
            "import numpy as np"
        ],
        "function_def": "def train_model(df, target_column, model_type):\n    # Split the data into training and testing sets\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Train a machine learning model\n    if model_type == \"LogisticRegression\":\n        model = LogisticRegression()\n    elif model_type == \"DecisionTree\":\n        model = DecisionTreeClassifier()\n    else:\n        raise ValueError(\"Invalid model type\")\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the testing data\n    y_pred = model.predict(X_test)\n    \n    # Evaluate the performance of the model\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    \n    return y_pred, accuracy, precision, recall"
    },
    {
        "function_name": "encode_categorical_variables",
        "file_name": "encoding.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "category_column": "str",
            "encoding_method": "str (either 'label' or 'onehot')"
        },
        "objectives": [
            "Check if the encoding_method is 'label' or 'onehot'.",
            "If 'label', encode the category_column using label encoding.",
            "If 'onehot', encode the category_column using onehot encoding.",
            "Return the encoded data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder",
            "from sklearn.preprocessing import OneHotEncoder"
        ],
        "function_def": "def encode_categorical_variables(data, category_column, encoding_method):\n    if encoding_method == 'label':\n        # Encode the category_column using label encoding\n        encoder = LabelEncoder()\n        data[category_column] = encoder.fit_transform(data[category_column])\n    elif encoding_method == 'onehot':\n        # Encode the category_column using onehot encoding\n        encoder = OneHotEncoder()\n        encoded_data = encoder.fit_transform(data[[category_column]])\n        encoded_df = pd.DataFrame(encoded_data.toarray())\n        data = pd.concat([data, encoded_df], axis=1)\n    else:\n        raise ValueError(\"Invalid encoding method\")\n    \n    return data"
    },
    {
        "function_name": "scale_numerical_variables",
        "file_name": "scaling.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "scaling_method": "str (either 'standard' or 'minmax')"
        },
        "objectives": [
            "Check if the scaling_method is 'standard' or 'minmax'.",
            "If 'standard', scale the data using standardization.",
            "If 'minmax', scale the data using min-max scaling.",
            "Return the scaled data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def scale_numerical_variables(data, scaling_method):\n    if scaling_method == 'standard':\n        # Scale the data using standardization\n        scaler = StandardScaler()\n        scaled_data = scaler.fit_transform(data)\n    elif scaling_method == 'minmax':\n        # Scale the data using min-max scaling\n        scaler = MinMaxScaler()\n        scaled_data = scaler.fit_transform(data)\n    else:\n        raise ValueError(\"Invalid scaling method\")\n    \n    scaled_df = pd.DataFrame(scaled_data)\n    return scaled_df"
    },
    {
        "function_name": "handle_missing_values",
        "file_name": "missing_value_handling.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "handling_method": "str (either 'mean', 'median', or 'mode')"
        },
        "objectives": [
            "Check if the handling_method is 'mean', 'median', or 'mode'.",
            "If 'mean', replace missing values with the mean of each column.",
            "If 'median', replace missing values with the median of each column.",
            "If 'mode', replace missing values with the mode of each column.",
            "Return the data with missing values handled."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def handle_missing_values(data, handling_method):\n    if handling_method == 'mean':\n        # Replace missing values with the mean of each column\n        data = data.fillna(data.mean())\n    elif handling_method == 'median':\n        # Replace missing values with the median of each column\n        data = data.fillna(data.median())\n    elif handling_method == 'mode':\n        # Replace missing values with the mode of each column\n        data = data.fillna(data.mode().iloc[0])\n    else:\n        raise ValueError(\"Invalid handling method\")\n    \n    return data"
    },
    {
        "function_name": "event_rate_analysis",
        "file_name": "event_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "time_column": "str",
            "event_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Sort the dataframe by the id_column and time_column.",
            "Create a new column \"event_count\" and assign it the number of events for each id within a window of specified size.",
            "Identify the rows where the event count is greater than the average event count for each id.",
            "Create a new column \"event_rate\" and assign it the rate of events for each id."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def event_rate_analysis(data, id_column, time_column, event_column, window_size):\n    sorted_data = data.sort_values([id_column, time_column])\n    event_count = sorted_data.groupby(id_column)[event_column].transform(lambda x: x.rolling(window_size).count())\n    avg_event_count = sorted_data.groupby(id_column)[event_column].transform(lambda x: x.rolling(window_size).count().mean())\n    event_rate = sorted_data.groupby(id_column)[event_column].transform(lambda x: x.rolling(window_size).count() / window_size)\n    \n    sorted_data['event_count'] = event_count\n    sorted_data['is_high_event_count'] = event_count > avg_event_count\n    sorted_data['event_rate'] = event_rate\n    \n    return sorted_data"
    },
    {
        "function_name": "score_classification",
        "file_name": "score_classification.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "score_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the percentile rank of each score in the score_column for each id group.",
            "Identify the rows where the score is above the specified threshold.",
            "Create a new column \"score_category\" and assign it a category based on the score percentile rank.",
            "Calculate the percentage of rows in each score category."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def score_classification(df, id_column, score_column, threshold):\n    percentile_rank = df.groupby(id_column)[score_column].rank(method='min', pct=True) * 100\n    df['percentile_rank'] = percentile_rank\n    \n    above_threshold = np.where(df[score_column] > threshold, 1, 0)\n    df['above_threshold'] = above_threshold.astype(int)\n    \n    score_category = np.select([percentile_rank < 20, (percentile_rank >= 20) & (percentile_rank < 50), percentile_rank >= 80], ['low', 'medium', 'high'])\n    df['score_category'] = score_category\n    \n    score_percentage = df['score_category'].value_counts(normalize=True) * 100\n    df['score_percentage'] = df['score_category'].map(score_percentage)\n    \n    return df"
    },
    {
        "function_name": "moving_averages",
        "file_name": "grouping_operations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Group the dataframe by the id_column.",
            "Calculate the moving average of the value_column within each group using the specified window size.",
            "Create a new column with the moving averages.",
            "Merge the moving averages with the original dataframe.",
            "Return the merged dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def moving_averages(data, id_column, value_column, window_size):\n    # Step 1: Group the dataframe by the id_column\n    grouped_data = data.groupby(id_column)\n    \n    # Step 2: Calculate the moving average of the value_column within each group\n    moving_averages = grouped_data[value_column].rolling(window_size).mean()\n    \n    # Step 3: Create a new column with the moving averages\n    moving_averages = moving_averages.reset_index(level=1, drop=True)\n    \n    # Step 4: Merge the moving averages with the original dataframe\n    merged_data = pd.merge(data, moving_averages, left_index=True, right_index=True)\n    \n    return merged_data"
    },
    {
        "function_name": "evaluate_random_forest",
        "file_name": "evaluation_metrics.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "continuous_columns": "list of str",
            "target_column": "str",
            "test_size": "float"
        },
        "objectives": [
            "One-hot encode the categorical columns.",
            "Scale the continuous columns using the MinMaxScaler.",
            "Split the data into training and testing sets using StratifiedShuffleSplit.",
            "Train a simple RandomForestClassifier on the training data to predict the target column.",
            "Evaluate the model using the testing data and return the accuracy score."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler",
            "from sklearn.model_selection import StratifiedShuffleSplit",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.metrics import accuracy_score"
        ],
        "function_def": "def evaluate_random_forest(data, categorical_columns, continuous_columns, target_column, test_size):\n    # One-hot encode the categorical columns\n    encoder = OneHotEncoder()\n    encoded_data = pd.DataFrame(encoder.fit_transform(data[categorical_columns]).toarray())\n    \n    # Scale the continuous columns\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data[continuous_columns])\n    \n    # Concatenate the one-hot encoded categorical data and the scaled continuous data\n    concatenated_data = pd.concat([encoded_data, pd.DataFrame(scaled_data)], axis=1)\n    \n    # Split the data into training and testing sets\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n    for train_index, test_index in sss.split(concatenated_data, data[target_column]):\n        train_data, test_data = concatenated_data.iloc[train_index], concatenated_data.iloc[test_index]\n        train_labels, test_labels = data[target_column].iloc[train_index], data[target_column].iloc[test_index]\n    \n    # Train a simple RandomForestClassifier on the training data\n    model = RandomForestClassifier()\n    model.fit(train_data, train_labels)\n    \n    # Evaluate the model using the testing data\n    predictions = model.predict(test_data)\n    accuracy = accuracy_score(test_labels, predictions)\n    \n    return accuracy"
    },
    {
        "function_name": "tf_idf_transform",
        "file_name": "text_preprocessing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "text_column": "str",
            "max_features": "int"
        },
        "objectives": [
            "Convert the text column to lowercase.",
            "Remove special characters and punctuation from the text column.",
            "Use TF-IDF to transform the text column into a matrix of features.",
            "Select the top max_features words by TF-IDF score and create a new matrix with only these features."
        ],
        "import_lines": [
            "import pandas as pd",
            "import re",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def tf_idf_transform(data, text_column, max_features):\n    # Convert the text column to lowercase\n    data[text_column] = data[text_column].apply(lambda x: x.lower())\n    \n    # Remove special characters and punctuation from the text column\n    data[text_column] = data[text_column].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n    \n    # Use TF-IDF to transform the text column into a matrix of features\n    vectorizer = TfidfVectorizer(max_features=max_features)\n    tf_idf_data = vectorizer.fit_transform(data[text_column])\n    \n    return tf_idf_data"
    },
    {
        "function_name": "cluster_strong_edges",
        "file_name": "graph_analysis.py",
        "parameters": {
            "graph": "networkx.Graph",
            "edge_attribute": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the strength of each edge in the graph based on the specified attribute.",
            "Identify the edges with a strength greater than the specified threshold.",
            "Create a new graph containing only the strong edges.",
            "Calculate the clusters in the new graph using a community detection algorithm."
        ],
        "import_lines": [
            "import networkx as nx",
            "from networkx.algorithms import community"
        ],
        "function_def": "def cluster_strong_edges(graph, edge_attribute, threshold):\n    # Calculate the strength of each edge in the graph\n    edge_strength = {edge: graph.get_edge_data(edge[0], edge[1])[edge_attribute] for edge in graph.edges}\n    \n    # Identify the edges with a strength greater than the specified threshold\n    strong_edges = [edge for edge, strength in edge_strength.items() if strength > threshold]\n    \n    # Create a new graph containing only the strong edges\n    strong_graph = graph.edge_subgraph(strong_edges)\n    \n    # Calculate the clusters in the new graph\n    clusters = community.greedy_modularity_communities(strong_graph)\n    \n    return clusters"
    },
    {
        "function_name": "entity_frequencies",
        "file_name": "entity_recognition.py",
        "parameters": {
            "text": "str",
            "entities": "list"
        },
        "objectives": [
            "Tokenize the input text into individual words and phrases.",
            "For each entity, calculate the number of occurrences in the text.",
            "Determine the most frequent entity.",
            "Return a dictionary with the entity frequencies and the most frequent entity."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def entity_frequencies(text, entities):\n    # Step 1: Tokenize the input text into individual words and phrases\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Step 2: For each entity, calculate the number of occurrences in the text\n    frequencies = Counter(token for token in tokens if token in entities)\n    \n    # Step 3: Determine the most frequent entity\n    most_frequent_entity = frequencies.most_common(1)[0][0]\n    \n    # Step 4: Return a dictionary with the entity frequencies and the most frequent entity\n    return dict(frequencies), most_frequent_entity"
    },
    {
        "function_name": "preprocess_text",
        "file_name": "text_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "text_column": "str",
            "min_token_length": "int",
            "max_token_length": "int"
        },
        "objectives": [
            "Preprocess the text data in the specified column by converting it to lowercase and removing special characters.",
            "Tokenize the text data into individual words or tokens.",
            "Filter out tokens that are shorter than the minimum token length or longer than the maximum token length.",
            "Return a new dataframe with the preprocessed and filtered text data."
        ],
        "import_lines": [
            "import pandas as pd",
            "import re"
        ],
        "function_def": "def preprocess_text(data, text_column, min_token_length, max_token_length):\n    # Preprocess text data by converting to lowercase and removing special characters\n    data[text_column] = data[text_column].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x).lower())\n    \n    # Tokenize text data into individual words or tokens\n    data[text_column] = data[text_column].apply(lambda x: x.split())\n    \n    # Filter out tokens that are shorter than the minimum token length or longer than the maximum token length\n    data[text_column] = data[text_column].apply(lambda x: [token for token in x if min_token_length <= len(token) <= max_token_length])\n    \n    return data"
    },
    {
        "function_name": "kernel_density_estimation",
        "file_name": "density_estimation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "kernel_type": "str",
            "bandwidth": "float"
        },
        "objectives": [
            "Perform kernel density estimation on the data using the specified kernel type and bandwidth.",
            "Calculate the density values for each data point.",
            "Return a new dataframe with the density values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import gaussian_kde",
            "from sklearn.kernel_approximation import RBFSampler"
        ],
        "function_def": "def kernel_density_estimation(data, kernel_type, bandwidth):\n    # Perform kernel density estimation on the data\n    if kernel_type == 'gaussian':\n        kde = gaussian_kde(data.values.T, bw_method=bandwidth)\n    elif kernel_type == 'rbf':\n        rbf_sampler = RBFSampler(gamma=bandwidth)\n        kde = rbf_sampler.fit_transform(data.values)\n    \n    # Calculate the density values for each data point\n    density_values = kde.evaluate(data.values.T)\n    \n    # Return a new dataframe with the density values\n    result = pd.DataFrame(density_values, columns=['density'])\n    \n    return result"
    },
    {
        "function_name": "remove_outliers",
        "file_name": "data_cleaning.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Normalize the data in the specified column using the Min-Max Scaler.",
            "Identify data points with normalized values greater than the specified threshold.",
            "Remove rows that correspond to the identified data points.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def remove_outliers(data, feature_column, threshold):\n    # Normalize the data in the specified column using the Min-Max Scaler\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data[[feature_column]])\n    \n    # Identify data points with normalized values greater than the specified threshold\n    outliers = normalized_data[:, 0] > threshold\n    \n    # Remove rows that correspond to the identified data points\n    data = data[~outliers]\n    \n    return data"
    },
    {
        "function_name": "add_max_returns",
        "file_name": "risk_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "threshold": "float",
            "window_size": "int"
        },
        "objectives": [
            "Convert the date column to datetime type and set it as the index of the dataframe.",
            "Calculate the daily returns of each column in the dataframe.",
            "Identify the columns with a standard deviation of returns above the specified threshold.",
            "Create a new column that contains the rolling window's maximum value for each identified column, and add it to the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_max_returns(df, date_column, threshold, window_size):\n    # Convert the date column to datetime type and set it as the index of the dataframe\n    df[date_column] = pd.to_datetime(df[date_column])\n    df.set_index(date_column, inplace=True)\n    \n    # Calculate the daily returns of each column in the dataframe\n    returns = df.pct_change()\n    \n    # Identify the columns with a standard deviation of returns above the specified threshold\n    volatile_columns = returns.std().gt(threshold)\n    \n    # Create a new column that contains the rolling window's maximum value for each identified column\n    for col in volatile_columns[volatile_columns].index:\n        df[f'max_{col}'] = df[col].rolling(window_size).max()\n    \n    return df"
    },
    {
        "function_name": "evaluate_assignment_completion",
        "file_name": "assignment_evaluation.py",
        "parameters": {
            "student_data": "pandas.DataFrame",
            "assignment_column": "str",
            "score_column": "str",
            "max_attempts": "int"
        },
        "objectives": [
            "Calculate the total score for each assignment across all attempts.",
            "Identify the assignments where the total score is greater than or equal to the maximum possible score minus the maximum attempts.",
            "Create a new column to indicate whether each assignment is complete.",
            "Calculate the percentage of complete assignments for each student."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def evaluate_assignment_completion(student_data, assignment_column, score_column, max_attempts):\n    total_score = student_data.groupby(assignment_column)[score_column].sum().reset_index()\n    \n    max_possible_score = max_attempts * total_score[score_column].max()\n    complete_assignments = total_score[total_score[score_column] >= max_possible_score - max_attempts]\n    \n    student_data['is_complete'] = np.where(student_data[assignment_column].isin(complete_assignments[assignment_column]), True, False)\n    \n    completion_percentage = student_data.groupby('student_id')['is_complete'].mean() * 100\n    \n    return student_data"
    },
    {
        "function_name": "detect_consecutive_values",
        "file_name": "sequence_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_name": "str",
            "threshold": "int"
        },
        "objectives": [
            "Calculate the length of the longest consecutive sequence of a single value in the specified column.",
            "Identify the rows where the length of the sequence is greater than the specified threshold.",
            "Create a new column with the length of the sequence and the identified rows.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_consecutive_values(data, column_name, threshold):\n    data['value'] = data[column_name]\n    data['group'] = (data['value'] != data['value'].shift()).cumsum()\n    data['sequence_length'] = data.groupby('group')['value'].transform('count')\n    data['is_long_sequence'] = data['sequence_length'] > threshold\n    return data"
    },
    {
        "function_name": "high_degree_shortest_paths",
        "file_name": "graph_analysis.py",
        "parameters": {
            "graph": "networkx.Graph",
            "weight_attribute": "str"
        },
        "objectives": [
            "Calculate the weighted degree of each node in the graph.",
            "Identify the nodes with a weighted degree greater than the average weighted degree of all nodes.",
            "Create a new graph containing only the nodes with a high weighted degree.",
            "Calculate the shortest path between each pair of nodes in the new graph.",
            "Return the shortest paths."
        ],
        "import_lines": [
            "import networkx as nx",
            "import numpy as np"
        ],
        "function_def": "def high_degree_shortest_paths(graph, weight_attribute):\n    # Step 1: Calculate the weighted degree of each node in the graph\n    weighted_degree = {node: sum([graph.get_edge_data(node, neighbor)[weight_attribute] for neighbor in graph.neighbors(node)]) for node in graph.nodes}\n    \n    # Step 2: Identify the nodes with a weighted degree greater than the average weighted degree of all nodes\n    avg_weighted_degree = np.mean(list(weighted_degree.values()))\n    high_degree_nodes = [node for node, degree in weighted_degree.items() if degree > avg_weighted_degree]\n    \n    # Step 3: Create a new graph containing only the nodes with a high weighted degree\n    high_degree_graph = graph.subgraph(high_degree_nodes)\n    \n    # Step 4: Calculate the shortest path between each pair of nodes in the new graph\n    shortest_paths = {}\n    for source in high_degree_graph.nodes:\n        for target in high_degree_graph.nodes:\n            if source != target:\n                shortest_paths[(source, target)] = nx.shortest_path(high_degree_graph, source, target, weight=weight_attribute)\n    \n    return shortest_paths"
    },
    {
        "function_name": "calculate_weighted_average",
        "file_name": "groupby_utils.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Calculate the frequency of each group in the specified column.",
            "Identify groups with frequency greater than the specified threshold.",
            "Calculate the weighted average of values in a specified column for each identified group.",
            "Sort the groups by their weighted averages in descending order.",
            "Return a dataframe with the group names, their frequencies, and their weighted averages."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_weighted_average(data, group_column, threshold):\n    # Step 1: Calculate the frequency of each group in the specified column\n    frequency_df = data[group_column].value_counts().to_frame('frequency')\n    \n    # Step 2: Identify groups with frequency greater than the specified threshold\n    identified_groups = frequency_df[frequency_df['frequency'] > threshold].index\n    \n    # Step 3: Calculate the weighted average of values in a specified column for each identified group\n    weighted_average_df = data[data[group_column].isin(identified_groups)].groupby(group_column)['value'].apply(lambda x: (x * x.count()).sum() / x.count())\n    \n    # Step 4: Sort the groups by their weighted averages in descending order\n    sorted_weighted_average_df = weighted_average_df.reset_index().sort_values(by='value', ascending=False)\n    \n    # Step 5: Return a dataframe with the group names, their frequencies, and their weighted averages\n    result_df = pd.merge(sorted_weighted_average_df, frequency_df, left_on=group_column, right_index=True)\n    return result_df"
    },
    {
        "function_name": "kmeans_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "X": "numpy.ndarray",
            "y": "numpy.ndarray",
            "k": "int"
        },
        "objectives": [
            "Perform k-means clustering on the input data X.",
            "Calculate the silhouette score for each cluster.",
            "Calculate the calinski-harabasz index for the entire clustering.",
            "Return the cluster labels, silhouette scores, and the calinski-harabasz index."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.cluster import KMeans",
            "from sklearn.metrics import silhouette_score, calinski_harabasz_score"
        ],
        "function_def": "def kmeans_clustering(X, y, k):\n    # Step 1: Perform k-means clustering on the input data X\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    cluster_labels = kmeans.fit_predict(X)\n    \n    # Step 2: Calculate the silhouette score for each cluster\n    silhouette_scores = silhouette_score(X, cluster_labels)\n    \n    # Step 3: Calculate the calinski-harabasz index for the entire clustering\n    calinski_harabasz_index = calinski_harabasz_score(X, cluster_labels)\n    \n    # Step 4: Return the cluster labels, silhouette scores, and the calinski-harabasz index\n    return cluster_labels, silhouette_scores, calinski_harabasz_index"
    },
    {
        "function_name": "encode_categorical_with_threshold",
        "file_name": "categorical_encoding.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_cols": "list[str]",
            "threshold": "float",
            "max_categories": "int"
        },
        "objectives": [
            "One-hot encode the categorical columns.",
            "Filter out categories with a frequency below the specified threshold.",
            "Group categories with a frequency above the threshold but not among the top 'max_categories' into a new category called 'Others'.",
            "Merge the encoded categorical columns with the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder"
        ],
        "function_def": "def encode_categorical_with_threshold(data, categorical_cols, threshold, max_categories):\n    # One-hot encode the categorical columns\n    encoded_data_list = []\n    for col in categorical_cols:\n        encoder = OneHotEncoder(sparse_output=False)\n        encoded_data = pd.DataFrame(encoder.fit_transform(data[[col]]).astype(int), columns=encoder.get_feature_names_out([col]))\n        \n        # Filter out categories with a frequency below the specified threshold\n        category_counts = data[col].value_counts(normalize=True)\n        valid_categories = category_counts[category_counts > threshold].index.tolist()\n        encoded_data = encoded_data[[f'{col}_{cat}' for cat in valid_categories]]\n        \n        # Group categories with a frequency above the threshold but not among the top 'max_categories' into a new category called 'Others'\n        top_categories = category_counts.nlargest(max_categories).index.tolist()\n        other_categories = [f'{col}_{cat}' for cat in valid_categories if cat not in top_categories]\n        encoded_data[' Others'] = encoded_data[other_categories].sum(axis=1)\n        \n        encoded_data_list.append(encoded_data)\n    \n    # Merge the encoded categorical columns with the original dataframe\n    encoded_data = pd.concat(encoded_data_list, axis=1)\n    merged_data = pd.concat([data, encoded_data], axis=1)\n    \n    return merged_data"
    },
    {
        "function_name": "interpolate_and_roll",
        "file_name": "time_series_interpolation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "value_column": "str",
            "min_periods": "int"
        },
        "objectives": [
            "Convert the time column to a datetime column.",
            "Interpolate missing values in the value column using linear interpolation.",
            "Calculate the rolling average of the interpolated values over a window of size 'min_periods'.",
            "Return the rolling average values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def interpolate_and_roll(data, time_column, value_column, min_periods):\n    # Convert the time column to a datetime column\n    data[time_column] = pd.to_datetime(data[time_column])\n    \n    # Interpolate missing values in the value column using linear interpolation\n    data[value_column] = data[value_column].interpolate(method='linear', limit_direction='both')\n    \n    # Calculate the rolling average of the interpolated values over a window of size 'min_periods'\n    rolling_average = data[value_column].rolling(min_periods, min_periods=min_periods).mean()\n    \n    return rolling_average"
    },
    {
        "function_name": "feature_transformation",
        "file_name": "feature_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_column": "str",
            "target_column": "str",
            "transformation_type": "str (e.g., 'log', 'sqrt', etc.)"
        },
        "objectives": [
            "Apply the specified transformation to the feature column.",
            "Calculate the correlation between the transformed feature column and the target column.",
            "Identify the optimal transformation type that results in the highest correlation.",
            "Return the updated dataframe with the transformed feature column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def feature_transformation(df, feature_column, target_column, transformation_type):\n    # Apply the specified transformation to the feature column\n    if transformation_type == 'log':\n        transformed_feature = np.log(df[feature_column])\n    elif transformation_type == 'sqrt':\n        transformed_feature = np.sqrt(df[feature_column])\n    else:\n        raise ValueError(\"Unsupported transformation type\")\n    \n    # Calculate the correlation between the transformed feature column and the target column\n    correlation = np.corrcoef(transformed_feature, df[target_column])[0, 1]\n    \n    # Identify the optimal transformation type that results in the highest correlation\n    if correlation > 0.5:  # For demonstration purposes\n        df[feature_column] = transformed_feature\n    \n    return df"
    },
    {
        "function_name": "remove_consecutive_rows",
        "file_name": "data_cleansing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "timestamp_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Sort the dataframe by the timestamp_column and id_column.",
            "Calculate the time difference between consecutive rows for each id.",
            "Identify and remove rows where the time difference is less than the threshold.",
            "Create a new column 'time_diff' with the calculated time differences."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_consecutive_rows(data, id_column, timestamp_column, threshold):\n    # Sort the dataframe by the timestamp_column and id_column\n    data.sort_values(by=[id_column, timestamp_column], inplace=True)\n    \n    # Calculate the time difference between consecutive rows for each id\n    data['time_diff'] = data.groupby(id_column)[timestamp_column].diff().dt.total_seconds()\n    \n    # Identify and remove rows where the time difference is less than the threshold\n    data = data[data['time_diff'] >= threshold]\n    \n    return data"
    },
    {
        "function_name": "remove_excessive_values",
        "file_name": "data_filtering.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "feature_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Sort the dataframe by the id_column and feature_column.",
            "Calculate the cumulative sum of the feature_column for each id.",
            "Identify and remove rows where the cumulative sum exceeds the threshold.",
            "Create a new column 'cumulative_sum' with the calculated cumulative sums."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_excessive_values(data, id_column, feature_column, threshold):\n    # Sort the dataframe by the id_column and feature_column\n    data.sort_values(by=[id_column, feature_column], inplace=True)\n    \n    # Calculate the cumulative sum of the feature_column for each id\n    data['cumulative_sum'] = data.groupby(id_column)[feature_column].cumsum()\n    \n    # Identify and remove rows where the cumulative sum exceeds the threshold\n    data = data[data['cumulative_sum'] <= threshold]\n    \n    return data"
    },
    {
        "function_name": "hourly_condition_count",
        "file_name": "temporal_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "condition_column": "str"
        },
        "objectives": [
            "Convert the time column to a datetime column and extract the hour.",
            "Group the dataframe by the hour and a specified condition.",
            "Calculate the count of rows for each group.",
            "Create a new dataframe with the hour, condition, and count.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def hourly_condition_count(df, time_column, condition_column):\n    # Convert the time column to a datetime column and extract the hour\n    df['hour'] = pd.to_datetime(df[time_column]).dt.hour\n    \n    # Group the dataframe by the hour and a specified condition\n    grouped_df = df.groupby([pd.Grouper(key='hour'), condition_column]).size().reset_index(name='count')\n    \n    # Calculate the count of rows for each group\n    count_df = grouped_df.pivot(index='hour', columns=condition_column, values='count').fillna(0)\n    \n    # Create a new dataframe with the hour, condition, and count\n    melted_df = pd.melt(count_df.reset_index(), id_vars='hour', var_name='condition', value_name='count')\n    \n    # Return the resulting dataframe\n    return melted_df"
    },
    {
        "function_name": "train_and_evaluate_model",
        "file_name": "model_evaluation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "model_type": "str"
        },
        "objectives": [
            "Split the dataframe into training and testing sets.",
            "Train a machine learning model of the specified type on the training set to predict the target_column.",
            "Evaluate the model's performance on the testing set using metrics such as mean squared error (MSE), mean absolute error (MAE), R-squared.",
            "Create a new dataframe containing the performance metrics and their corresponding values.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.ensemble import RandomForestRegressor"
        ],
        "function_def": "def train_and_evaluate_model(df, target_column, model_type):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    if model_type == 'linear':\n        model = LinearRegression()\n    elif model_type == 'random_forest':\n        model = RandomForestRegressor()\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    mse = mean_squared_error(y_test, y_pred)\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    results_df = pd.DataFrame({'metric': ['MSE', 'MAE', 'R2'], 'value': [mse, mae, r2]})\n    return results_df"
    },
    {
        "function_name": "imputation",
        "file_name": "data_imputation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_name": "str",
            "method": "str"
        },
        "objectives": [
            "Impute missing values in the specified column using the median method if the method is 'median'.",
            "Impute missing values in the specified column using the mean method if the method is 'mean'.",
            "Impute missing values in the specified column using the most frequent value method if the method is 'most_frequent'.",
            "Return the updated dataframe with the imputed values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def imputation(data, column_name, method):\n    if method == 'median':\n        # Impute missing values using the median method\n        data[column_name] = data[column_name].fillna(data[column_name].median())\n    elif method == 'mean':\n        # Impute missing values using the mean method\n        data[column_name] = data[column_name].fillna(data[column_name].mean())\n    elif method == 'most_frequent':\n        # Impute missing values using the most frequent value method\n        most_frequent_value = data[column_name].value_counts().index[0]\n        data[column_name] = data[column_name].fillna(most_frequent_value)\n    return data"
    },
    {
        "function_name": "extract_topics",
        "file_name": "topic_modeling.py",
        "parameters": {
            "text_data": "list of str",
            "n_topics": "int",
            "max_iter": "int"
        },
        "objectives": [
            "Preprocess the text data by converting to lowercase, removing punctuation, and tokenizing.",
            "Create a TF-IDF matrix from the preprocessed text data.",
            "Perform Non-Negative Matrix Factorization (NMF) on the TF-IDF matrix to extract topics.",
            "Return the top n words for each topic and their corresponding weights."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.decomposition import NMF",
            "import re"
        ],
        "function_def": "def extract_topics(text_data, n_topics, max_iter):\n    # Preprocess the text data\n    preprocessed_text = [' '.join(re.findall(r'\\w+', text.lower())) for text in text_data]\n    \n    # Create a TF-IDF matrix from the preprocessed text data\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(preprocessed_text)\n    \n    # Perform NMF on the TF-IDF matrix to extract topics\n    nmf = NMF(n_components=n_topics, max_iter=max_iter)\n    topic_matrix = nmf.fit_transform(tfidf_matrix)\n    \n    # Get the top n words for each topic and their corresponding weights\n    feature_names = vectorizer.get_feature_names_out()\n    topic_words = {}\n    for topic in range(n_topics):\n        topic_weights = topic_matrix[:, topic]\n        top_words = np.argsort(topic_weights)[::-1][:10]\n        topic_words[topic] = {feature_names[word]: topic_weights[word] for word in top_words}\n    \n    return topic_words"
    },
    {
        "function_name": "cumulative_sum_analysis",
        "file_name": "cumulative_sum_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str"
        },
        "objectives": [
            "Group the data by the specified id column and calculate the cumulative sum of the value column.",
            "Identify the ids that have a cumulative sum above a certain threshold (e.g., 100).",
            "Create a new column to indicate whether each id has a high cumulative sum or not.",
            "Group the data by this new column and calculate the average value of the value column for each group.",
            "Return the updated dataframe with the new column and the average values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def cumulative_sum_analysis(df, id_column, value_column):\n    # Group the data by the specified id column and calculate the cumulative sum of the value column\n    df['cumulative_sum'] = df.groupby(id_column)[value_column].cumsum()\n    \n    # Identify the ids that have a cumulative sum above a certain threshold (e.g., 100)\n    df['high_cumulative_sum'] = np.where(df['cumulative_sum'] > 100, 1, 0)\n    \n    # Group the data by this new column and calculate the average value of the value column for each group\n    avg_values = df.groupby('high_cumulative_sum')[value_column].mean()\n    \n    # Return the updated dataframe with the new column and the average values\n    df['avg_value'] = df['high_cumulative_sum'].map(avg_values)\n    \n    return df"
    },
    {
        "function_name": "handle_outliers",
        "file_name": "outlier_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "outliers_threshold": "float",
            "outlier_removal_method": "str"
        },
        "objectives": [
            "Identify and remove outliers from the data based on the specified threshold and outlier removal method ('z-score' or 'IQR').",
            "Replace outliers with the median of the respective column.",
            "Perform a log transformation on the cleaned data."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "import math"
        ],
        "function_def": "def handle_outliers(data, outliers_threshold, outlier_removal_method):\n    # Step 1: Identify and remove outliers from the data \n    for col in data.columns:\n        if outlier_removal_method == 'z-score':\n            z_scores = np.abs((data[col] - data[col].mean()) / data[col].std())\n            outliers = z_scores[z_scores > outliers_threshold].index\n            data.loc[outliers, col] = np.nan\n        elif outlier_removal_method == 'IQR':\n            Q1, Q3 = data[col].quantile([0.25, 0.75])\n            IQR = Q3 - Q1\n            lower_bound = Q1 - (1.5 * IQR)\n            upper_bound = Q3 + (1.5 * IQR)\n            outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)].index\n            data.loc[outliers, col] = np.nan\n    \n    # Step 2: Replace outliers with the median of the respective column\n    for col in data.columns:\n        median = data[col].median()\n        data[col].fillna(median, inplace=True)\n    \n    # Step 3: Perform a log transformation on the cleaned data\n    for col in data.columns:\n        if data[col].min() > 0:\n            data[col] = np.log(data[col])\n    \n    return data"
    },
    {
        "function_name": "hierarchical_sampling",
        "file_name": "sampling_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "hierarchical_cols": "list of str",
            "hierarchy_depth": "int"
        },
        "objectives": [
            "Perform hierarchical sampling on the specified columns in the dataframe to reduce dimensionality.",
            "Ensure that each sample has the same hierarchy depth."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def hierarchical_sampling(df, hierarchical_cols, hierarchy_depth):\n    samples = []\n    for index, row in df.iterrows():\n        sample = []\n        for col in hierarchical_cols:\n            values = row[col]\n            depth = hierarchy_depth\n            while depth > 0:\n                sample.append(values[:depth])\n                depth -= 1\n        samples.append(sample)\n    \n    return samples"
    },
    {
        "function_name": "k_means_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_column": "str",
            "target_column": "str",
            "max_clusters": "int"
        },
        "objectives": [
            "Apply k-means clustering to the feature column.",
            "Evaluate the clustering performance using the silhouette score.",
            "Select the optimal number of clusters based on the silhouette score.",
            "Create a new dataframe with the cluster assignments and return it."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans",
            "from sklearn.metrics import silhouette_score"
        ],
        "function_def": "def k_means_clustering(df, feature_column, target_column, max_clusters):\n    # Apply k-means clustering to the feature column\n    clusters_list = []\n    for n_clusters in range(2, max_clusters + 1):\n        kmeans = KMeans(n_clusters=n_clusters)\n        cluster_labels = kmeans.fit_predict(df[feature_column].values.reshape(-1, 1))\n        clusters_list.append(cluster_labels)\n    \n    # Evaluate the clustering performance using the silhouette score\n    silhouette_scores = []\n    for cluster_labels in clusters_list:\n        score = silhouette_score(df[feature_column].values.reshape(-1, 1), cluster_labels)\n        silhouette_scores.append(score)\n    \n    # Select the optimal number of clusters based on the silhouette score\n    optimal_n_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n    \n    # Create a new dataframe with the cluster assignments\n    cluster_df = pd.DataFrame({'cluster': clusters_list[optimal_n_clusters - 2]})\n    \n    return cluster_df"
    },
    {
        "function_name": "kfold_cross_validation",
        "file_name": "cross_validation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "k": "int"
        },
        "objectives": [
            "Perform k-fold cross-validation on the dataframe to evaluate the performance of a random forest classifier.",
            "Calculate the precision, recall, and F1 score for each fold.",
            "Calculate the average precision, recall, and F1 score across all folds.",
            "Return the average precision, recall, and F1 score, and the feature importance of the model."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import KFold",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.metrics import precision_score, recall_score, f1_score"
        ],
        "function_def": "def kfold_cross_validation(data, target_column, k):\n    # Split the data into features and target\n    X = data.drop(target_column, axis=1)\n    y = data[target_column]\n    \n    # Initialize lists to store the precision, recall, and F1 score for each fold\n    precision_scores = []\n    recall_scores = []\n    f1_scores = []\n    \n    # Initialize a list to store the feature importance of each model\n    feature_importances = []\n    \n    # Perform k-fold cross-validation\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    for train_index, test_index in kf.split(X):\n        # Split the data into training and testing sets\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        # Train a random forest classifier on the training data\n        model = RandomForestClassifier(random_state=42)\n        model.fit(X_train, y_train)\n        \n        # Predict the target variable for the testing data\n        y_pred = model.predict(X_test)\n        \n        # Calculate the precision, recall, and F1 score for this fold\n        precision = precision_score(y_test, y_pred)\n        recall = recall_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred)\n        \n        # Append the precision, recall, and F1 score to the lists\n        precision_scores.append(precision)\n        recall_scores.append(recall)\n        f1_scores.append(f1)\n        \n        # Append the feature importance to the list\n        feature_importances.append(model.feature_importances_)\n    \n    # Calculate the average precision, recall, and F1 score across all folds\n    average_precision = sum(precision_scores) / len(precision_scores)\n    average_recall = sum(recall_scores) / len(recall_scores)\n    average_f1 = sum(f1_scores) / len(f1_scores)\n    \n    # Calculate the average feature importance across all models\n    average_feature_importance = sum(feature_importances) / len(feature_importances)\n    \n    return average_precision, average_recall, average_f1, average_feature_importance"
    },
    {
        "function_name": "identify_dense_regions",
        "file_name": "data_inspection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "threshold": "float",
            "min_rows": "int"
        },
        "objectives": [
            "Identify dense regions in the dataframe based on the specified threshold.",
            "A dense region is defined as a set of rows where the number of non-null values exceeds the threshold.",
            "Return a list of row indices that belong to a dense region, and a list of row indices that do not belong to a dense region."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_dense_regions(data, threshold, min_rows):\n    # Calculate the number of non-null values for each row\n    null_counts = data.isnull().sum(axis=1)\n    \n    # Identify dense regions\n    dense_region_indices = []\n    sparse_region_indices = []\n    start_index = 0\n    for end_index in range(len(data)):\n        if end_index == len(data) - 1 or null_counts[end_index + 1] / len(data.columns) > threshold:\n            if end_index - start_index + 1 >= min_rows:\n                dense_region_indices.extend(range(start_index, end_index + 1))\n            else:\n                sparse_region_indices.extend(range(start_index, end_index + 1))\n            start_index = end_index + 1\n    \n    return dense_region_indices, sparse_region_indices"
    },
    {
        "function_name": "filter_groups",
        "file_name": "grouping_utils.py",
        "parameters": {
            "dataframe": "pandas.DataFrame",
            "group_column": "str",
            "target_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Group the dataframe by the specified column and calculate the sum of the target column for each group.",
            "Filter out groups with a sum below the specified threshold.",
            "Create a new column in the original dataframe indicating whether each row belongs to a filtered group or not.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_groups(dataframe, group_column, target_column, threshold):\n    # Group the dataframe and calculate the sum of the target column for each group\n    group_sums = dataframe.groupby(group_column)[target_column].sum()\n    \n    # Filter out groups with a sum below the threshold\n    filtered_groups = group_sums[group_sums >= threshold].index\n    \n    # Create a new column indicating whether each row belongs to a filtered group or not\n    dataframe['is_filtered'] = dataframe[group_column].isin(filtered_groups)\n    \n    return dataframe"
    },
    {
        "function_name": "get_similar_documents",
        "file_name": "text_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "min_df": "int",
            "max_df": "float",
            "top_n": "int"
        },
        "objectives": [
            "Preprocess the text data in the specified column by removing stop words and punctuation.",
            "Convert the text data into a matrix of TF-IDF features.",
            "Calculate the similarity between all pairs of documents using the cosine similarity metric.",
            "Return the top_n most similar documents for each document."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity"
        ],
        "function_def": "def get_similar_documents(df, text_column, min_df, max_df, top_n):\n    # Preprocess the text data in the specified column by removing stop words and punctuation\n    vectorizer = TfidfVectorizer(stop_words='english', min_df=min_df, max_df=max_df)\n    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n    \n    # Calculate the similarity between all pairs of documents using the cosine similarity metric\n    similarity_matrix = cosine_similarity(tfidf_matrix)\n    \n    # Return the top_n most similar documents for each document\n    similar_documents = []\n    for i in range(len(similarity_matrix)):\n        similar_docs = sorted(enumerate(similarity_matrix[i]), key=lambda x: x[1], reverse=True)[1:top_n+1]\n        similar_documents.append([x[0] for x in similar_docs])\n    \n    return similar_documents"
    },
    {
        "function_name": "volatile_columns",
        "file_name": "risk_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Convert the date column to datetime type and set it as the index of the dataframe.",
            "Calculate the daily returns of each column in the dataframe.",
            "Identify the columns with a standard deviation of returns above the specified threshold.",
            "Create a new dataframe with the identified columns and corresponding standard deviation values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def volatile_columns(df, datetime_column, threshold):\n    # Convert the date column to datetime type and set it as the index of the dataframe\n    df[datetime_column] = pd.to_datetime(df[datetime_column])\n    df.set_index(datetime_column, inplace=True)\n    \n    # Calculate the daily returns of each column in the dataframe\n    returns = df.pct_change()\n    \n    # Identify the columns with a standard deviation of returns above the specified threshold\n    volatile_columns = returns.std().gt(threshold)\n    \n    # Create a new dataframe with the identified columns and corresponding standard deviation values\n    volatile_df = pd.DataFrame({'column': volatile_columns[volatile_columns].index, 'std_dev': returns.std()[volatile_columns].values})\n    \n    return volatile_df"
    },
    {
        "function_name": "sorted_group_aggregation",
        "file_name": "sorted_groupby_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "aggregation_column": "str",
            "aggregation_method": "str"
        },
        "objectives": [
            "Sort the dataframe based on the specified id column.",
            "Apply a groupby operation on the sorted dataframe.",
            "Aggregate the specified column based on the grouping operation using the specified aggregation method ('sum', 'mean', or 'count').",
            "Return the aggregated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def sorted_group_aggregation(df, id_column, aggregation_column, aggregation_method):\n    # Sort the dataframe based on the id column\n    sorted_df = df.sort_values(by=id_column)\n    \n    # Apply a groupby operation on the sorted dataframe\n    grouped_df = sorted_df.groupby(id_column)\n    \n    # Aggregate the specified column using the aggregation method\n    if aggregation_method == 'sum':\n        aggregated_df = grouped_df[aggregation_column].sum().reset_index()\n    elif aggregation_method == 'mean':\n        aggregated_df = grouped_df[aggregation_column].mean().reset_index()\n    elif aggregation_method == 'count':\n        aggregated_df = grouped_df[aggregation_column].count().reset_index()\n    else:\n        raise ValueError(\"Invalid aggregation method.\")\n    \n    return aggregated_df"
    },
    {
        "function_name": "levenshtein_distance_analysis",
        "file_name": "string_distance.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "max_distance": "int"
        },
        "objectives": [
            "Sort the data by the specified id column.",
            "Calculate the Levenshtein distance between consecutive rows.",
            "Identify pairs of rows with Levenshtein distance less than or equal to the max_distance.",
            "Return a new dataframe with the identified pairs and their corresponding Levenshtein distances."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.metrics import edit_distance"
        ],
        "function_def": "def levenshtein_distance_analysis(data, id_column, max_distance):\n    # Sort the data by the specified id column\n    data = data.sort_values(id_column)\n    \n    # Calculate the Levenshtein distance between consecutive rows\n    distances = []\n    for i in range(len(data) - 1):\n        distance = edit_distance(str(data.iloc[i][id_column]), str(data.iloc[i + 1][id_column]))\n        distances.append((data.iloc[i][id_column], data.iloc[i + 1][id_column], distance))\n    \n    # Identify pairs of rows with Levenshtein distance less than or equal to the max_distance\n    distances = [distance for distance in distances if distance[2] <= max_distance]\n    \n    # Return a new dataframe with the identified pairs and their corresponding Levenshtein distances\n    result = pd.DataFrame(distances, columns=['id1', 'id2', 'distance'])\n    return result"
    },
    {
        "function_name": "permutation_test",
        "file_name": "hypothesis_testing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "event_column": "str",
            "non_event_column": "str",
            "significance_level": "float"
        },
        "objectives": [
            "Perform a permutation test to calculate the p-value of the observed difference between the mean values of two columns.",
            "Determine if the observed difference is statistically significant based on the specified significance level.",
            "Return a new dataframe with the results of the permutation test, including the p-value and the conclusion about statistical significance."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import ttest_1samp",
            "import numpy as np"
        ],
        "function_def": "def permutation_test(data, event_column, non_event_column, significance_level):\n    # Calculate the observed difference between the mean values of two columns\n    observed_difference = np.mean(data[event_column]) - np.mean(data[non_event_column])\n    \n    # Perform a permutation test to calculate the p-value\n    p_value = 0\n    num_permutations = 10000\n    for _ in range(num_permutations):\n        # Permute the data by shuffling the rows\n        data_permuted = data.sample(frac=1)\n        \n        # Calculate the permuted difference\n        permuted_difference = np.mean(data_permuted[event_column]) - np.mean(data_permuted[non_event_column])\n        \n        # Update the p-value\n        if permuted_difference > observed_difference:\n            p_value += 1\n    \n    p_value /= num_permutations\n    \n    # Determine if the observed difference is statistically significant based on the specified significance level\n    statistically_significant = p_value < significance_level\n    \n    # Return a new dataframe with the results of the permutation test\n    result = pd.DataFrame(index=[0], columns=['p_value', 'statistically_significant'])\n    result['p_value'] = p_value\n    result['statistically_significant'] = statistically_significant\n    \n    return result"
    },
    {
        "function_name": "category_target_prediction",
        "file_name": "category_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "category_column": "str",
            "target_column": "str",
            "max_categories": "int"
        },
        "objectives": [
            "Identify the top categories in the category column based on the frequency of occurrence.",
            "Reduce the number of categories to the specified max_categories by grouping the least frequent categories into a single category named 'Others'.",
            "Calculate the target mean for each category.",
            "Create a new column with the predicted target value based on the category."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def category_target_prediction(data, category_column, target_column, max_categories):\n    # Identify the top categories in the category column based on the frequency of occurrence\n    category_counts = data[category_column].value_counts()\n    top_categories = category_counts.index[:max_categories]\n    \n    # Reduce the number of categories to the specified max_categories\n    data[category_column] = data[category_column].apply(lambda x: x if x in top_categories else 'Others')\n    \n    # Calculate the target mean for each category\n    category_target_means = data.groupby(category_column)[target_column].mean()\n    \n    # Create a new column with the predicted target value based on the category\n    data['predicted_target'] = data[category_column].map(category_target_means)\n    \n    return data"
    },
    {
        "function_name": "identify_correlated_categories",
        "file_name": "category_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_column": "str",
            "num_column": "str",
            "top_n": "int"
        },
        "objectives": [
            "Convert the categorical column to numerical values using LabelEncoder.",
            "Calculate the correlation between the numerical column and the encoded categorical column.",
            "Identify the top 'top_n' most correlated categories.",
            "Return a new DataFrame with the top 'top_n' categories and their corresponding correlation values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def identify_correlated_categories(df, cat_column, num_column, top_n):\n    # Convert the categorical column to numerical values using LabelEncoder\n    le = LabelEncoder()\n    df['encoded_cat'] = le.fit_transform(df[cat_column])\n    \n    # Calculate the correlation between the numerical column and the encoded categorical column\n    correlation = df[num_column].corr(df['encoded_cat'])\n    \n    # Identify the top 'top_n' most correlated categories\n    top_categories = df[cat_column].value_counts().head(top_n)\n    top_correlations = [correlation for cat in top_categories.index]\n    \n    # Return a new DataFrame with the top 'top_n' categories and their corresponding correlation values\n    result_df = pd.DataFrame({'category': top_categories.index, 'correlation': top_correlations})\n    return result_df"
    },
    {
        "function_name": "calculate_yoy_growth",
        "file_name": "revenue_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "revenue_column": "str"
        },
        "objectives": [
            "Convert the date column to a datetime column.",
            "Calculate the year-over-year (YoY) revenue growth.",
            "Create a new DataFrame with the YoY revenue growth values.",
            "Return the new DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_yoy_growth(data, date_column, revenue_column):\n    # Convert the date column to a datetime column\n    data[date_column] = pd.to_datetime(data[date_column])\n    \n    # Calculate the year-over-year (YoY) revenue growth\n    data['year'] = data[date_column].dt.year\n    data['prev_year_revenue'] = data.groupby('year')[revenue_column].shift(1)\n    data['yoy_growth'] = (data[revenue_column] - data['prev_year_revenue']) / data['prev_year_revenue']\n    \n    # Create a new DataFrame with the YoY revenue growth values\n    result_df = data[['year', 'yoy_growth']]\n    \n    return result_df"
    },
    {
        "function_name": "classify_data",
        "file_name": "classification.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_columns": "list[str]",
            "target_column": "str",
            "classification_method": "str ('logistic_regression', 'decision_tree', etc.)"
        },
        "objectives": [
            "Split the data into training and testing sets.",
            "Train a classification model using the specified method.",
            "Evaluate the model performance using metrics such as accuracy, precision, and recall.",
            "Create a new column containing the predicted values.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.metrics import accuracy_score, precision_score, recall_score"
        ],
        "function_def": "def classify_data(df, feature_columns, target_column, classification_method):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df[feature_columns], df[target_column], test_size=0.2, random_state=42)\n    \n    # Train a classification model using the specified method\n    if classification_method == 'logistic_regression':\n        model = LogisticRegression()\n    elif classification_method == 'decision_tree':\n        model = DecisionTreeClassifier()\n    \n    model.fit(X_train, y_train)\n    \n    # Evaluate the model performance using metrics such as accuracy, precision, and recall\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    \n    # Create a new column containing the predicted values\n    df['predicted_value'] = model.predict(df[feature_columns])\n    \n    return df"
    },
    {
        "function_name": "find_local_maxima",
        "file_name": "signal_processing.py",
        "parameters": {
            "arr": "numpy.ndarray",
            "window_size": "int"
        },
        "objectives": [
            "Calculate the moving average of the array elements within a specified window size.",
            "Identify the local maxima in the moving average array.",
            "Create a new array containing the indices of the local maxima.",
            "Return the new array."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def find_local_maxima(arr, window_size):\n    # Calculate the moving average of the array elements\n    moving_avg = np.convolve(arr, np.ones(window_size) / window_size, mode='valid')\n    \n    # Identify the local maxima in the moving average array\n    local_maxima_indices = np.where((np.diff(np.sign(np.diff(moving_avg))) < 0))[0] + 1\n    \n    # Create a new array containing the indices of the local maxima\n    maxima_indices = np.array([i + window_size // 2 for i in local_maxima_indices])\n    \n    return maxima_indices"
    },
    {
        "function_name": "calculate_peak_distances",
        "file_name": "signal_processing.py",
        "parameters": {
            "arr": "numpy.ndarray",
            "threshold": "float"
        },
        "objectives": [
            "Identify the peaks in the array that are greater than the specified threshold.",
            "Create a new array containing the indices of the peaks.",
            "Calculate the distance between each pair of peaks.",
            "Return the distances."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def calculate_peak_distances(arr, threshold):\n    # Identify the peaks in the array that are greater than the specified threshold\n    peak_indices = np.where(arr > threshold)[0]\n    \n    # Create a new array containing the indices of the peaks\n    peak_array = np.array(peak_indices)\n    \n    # Calculate the distance between each pair of peaks\n    peak_distances = np.diff(peak_array)\n    \n    return peak_distances"
    },
    {
        "function_name": "preprocess_missing_values",
        "file_name": "data_cleansing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "drop_threshold": "float",
            "fill_strategy": "str"
        },
        "objectives": [
            "Calculate the percentage of missing values in each column.",
            "Drop columns with more than the specified percentage of missing values.",
            "Fill the remaining missing values using the specified strategy (mean, median, mode).",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def preprocess_missing_values(df, drop_threshold, fill_strategy):\n    # Calculate the percentage of missing values in each column\n    missing_values_percentage = df.isna().mean() * 100\n    \n    # Drop columns with more than the specified percentage of missing values\n    df = df.dropna(axis=1, thresh=df.shape[0] * (1 - drop_threshold/100))\n    \n    # Fill the remaining missing values using the specified strategy\n    if fill_strategy == 'mean':\n        df = df.fillna(df.mean())\n    elif fill_strategy == 'median':\n        df = df.fillna(df.median())\n    elif fill_strategy == 'mode':\n        for col in df.columns:\n            df[col] = df[col].fillna(df[col].mode()[0])\n    \n    return df"
    },
    {
        "function_name": "categorize_low_cardinality_columns",
        "file_name": "data_cleansing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Group the dataframe by the specified column.",
            "For each group, calculate the number of unique values in each column.",
            "If the number of unique values in a column is less than the specified threshold, convert the column to categorical type.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def categorize_low_cardinality_columns(df, group_col, threshold):\n    # Group the dataframe by the specified column\n    grouped_df = df.groupby(group_col)\n    \n    # For each group, calculate the number of unique values in each column\n    for group_name, group in grouped_df:\n        for col in df.columns:\n            if col != group_col:\n                unique_values = group[col].nunique()\n                if unique_values < threshold:\n                    df.loc[df[group_col] == group_name, col] = df.loc[df[group_col] == group_name, col].astype('category')\n    \n    return df"
    },
    {
        "function_name": "clip_outliers",
        "file_name": "data_cleansing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "max_value": "int",
            "min_value": "int"
        },
        "objectives": [
            "For each numerical column, calculate the number of values above the maximum and below the minimum.",
            "Clip the values above the maximum and below the minimum to the maximum and minimum, respectively.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def clip_outliers(df, max_value, min_value):\n    # For each numerical column, calculate the number of values above the maximum and below the minimum\n    for col in df.select_dtypes(include=[int, float]).columns:\n        df.loc[df[col] > max_value, col] = max_value\n        df.loc[df[col] < min_value, col] = min_value\n    \n    return df"
    },
    {
        "function_name": "high_return_days",
        "file_name": "risk_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "datetime_column": "str",
            "column_name": "str",
            "threshold": "float"
        },
        "objectives": [
            "Convert the date column to datetime type and set it as the index of the dataframe.",
            "Calculate the daily returns of the column.",
            "Identify the days where the return is above the specified threshold.",
            "Create a new dataframe with the identified days and corresponding return values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def high_return_days(data, datetime_column, column_name, threshold):\n    data[datetime_column] = pd.to_datetime(data[datetime_column])\n    data.set_index(datetime_column, inplace=True)\n    daily_returns = data[column_name].pct_change()\n    high_return_days = daily_returns > threshold\n    result = pd.DataFrame({'day': high_return_days[high_return_days].index, 'return': daily_returns[high_return_days].values})\n    return result"
    },
    {
        "function_name": "high_value_groups",
        "file_name": "group_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Group the dataframe by the id column and calculate the mean of the value column.",
            "Identify the groups where the mean value is above the specified threshold.",
            "Create a new dataframe with the identified groups and corresponding mean values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def high_value_groups(data, id_column, value_column, threshold):\n    mean_values = data.groupby(id_column)[value_column].mean()\n    high_value_groups = mean_values > threshold\n    result = pd.DataFrame({'group': high_value_groups[high_value_groups].index, 'mean_value': mean_values[high_value_groups].values})\n    return result"
    },
    {
        "function_name": "discretization_analysis",
        "file_name": "discretization.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_name": "str",
            "bins": "int",
            "quantile": "float"
        },
        "objectives": [
            "Discretize the specified column into bins based on the quantile.",
            "Create a new column with the discretized values.",
            "Calculate the information gain of the discretized column.",
            "Determine if the discretization improves the information gain."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics import mutual_info_score"
        ],
        "function_def": "def discretization_analysis(data, column_name, bins, quantile):\n    # Step 1: Discretize the specified column into bins based on the quantile\n    discretized_column = pd.qcut(data[column_name], q=bins, duplicates='drop')\n    \n    # Step 2: Create a new column with the discretized values\n    data[f'{column_name}_discretized'] = discretized_column\n    \n    # Step 3: Calculate the information gain of the discretized column\n    information_gain = mutual_info_score(data[column_name], data[f'{column_name}_discretized'])\n    \n    # Step 4: Determine if the discretization improves the information gain\n    improved_information_gain = information_gain > mutual_info_score(data[column_name], data[column_name])\n    \n    return data, information_gain, improved_information_gain"
    },
    {
        "function_name": "remove_na_rows",
        "file_name": "data_cleansing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "max_na_count": "int",
            "na_column": "str"
        },
        "objectives": [
            "Identify the rows with a NaN value in the specified column.",
            "Count the number of NaN values in each row.",
            "Remove the rows with a NaN count greater than the specified threshold.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_na_rows(df, max_na_count, na_column):\n    # Identify the rows with a NaN value in the specified column\n    na_rows = df[na_column].isna()\n    \n    # Count the number of NaN values in each row\n    na_count = df.isna().sum(axis=1)\n    \n    # Remove the rows with a NaN count greater than the specified threshold\n    updated_df = df[(~na_rows) | (na_count <= max_na_count)]\n    \n    return updated_df"
    },
    {
        "function_name": "calculate_time_diff",
        "file_name": "temporal_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "timestamp_column": "str",
            "activity_column": "str"
        },
        "objectives": [
            "Convert the timestamp column to a datetime column and sort the dataframe by the id column and timestamp column.",
            "Create a new column that calculates the time difference between consecutive activities for each id.",
            "Fill missing time differences with the median time difference for each id.",
            "Return the sorted dataframe with the new column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_time_diff(data, id_column, timestamp_column, activity_column):\n    # Convert the timestamp column to a datetime column and sort the dataframe by the id column and timestamp column\n    data[timestamp_column] = pd.to_datetime(data[timestamp_column])\n    data = data.sort_values(by=[id_column, timestamp_column])\n    \n    # Create a new column that calculates the time difference between consecutive activities for each id\n    data['time_diff'] = data.groupby(id_column)[timestamp_column].diff()\n    \n    # Fill missing time differences with the median time difference for each id\n    median_time_diff = data.groupby(id_column)['time_diff'].median()\n    data['time_diff'] = data.apply(lambda row: median_time_diff[row[id_column]] if pd.isnull(row['time_diff']) else row['time_diff'], axis=1)\n    \n    return data"
    },
    {
        "function_name": "cumulative_sum_percentage",
        "file_name": "date_range_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "min_date": "str",
            "max_date": "str",
            "target_column": "str"
        },
        "objectives": [
            "Filter the data to include only rows where the date_column is within the range [min_date, max_date].",
            "Calculate the cumulative sum of the target_column over the filtered data.",
            "Create a new column with the percentage of the cumulative sum for each row.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def cumulative_sum_percentage(data, date_column, min_date, max_date, target_column):\n    # Filter the data to include only rows where the date_column is within the range [min_date, max_date]\n    data[date_column] = pd.to_datetime(data[date_column])\n    filtered_data = data[(data[date_column] >= min_date) & (data[date_column] <= max_date)]\n    \n    # Calculate the cumulative sum of the target_column over the filtered data\n    cumulative_sum = filtered_data[target_column].cumsum()\n    \n    # Create a new column with the percentage of the cumulative sum for each row\n    filtered_data['percentage'] = (cumulative_sum / cumulative_sum.max()) * 100\n    \n    return filtered_data"
    },
    {
        "function_name": "rolling_average_filter",
        "file_name": "time_series_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "value_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Convert the time_column to a datetime column.",
            "Calculate the rolling average of the value_column over a window of size window_size.",
            "Create a new column with the rolling average for each row.",
            "Identify rows where the rolling average is above the mean of the value_column.",
            "Return the updated dataframe with only the rows where the rolling average is above the mean of the value_column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_average_filter(data, time_column, value_column, window_size):\n    # Convert the time_column to a datetime column\n    data[time_column] = pd.to_datetime(data[time_column])\n    \n    # Calculate the rolling average of the value_column over a window of size window_size\n    rolling_average = data[value_column].rolling(window_size, min_periods=1).mean()\n    \n    # Create a new column with the rolling average for each row\n    data['rolling_average'] = rolling_average\n    \n    # Identify rows where the rolling average is above the mean of the value_column\n    filtered_data = data[data['rolling_average'] > data[value_column].mean()]\n    \n    return filtered_data"
    },
    {
        "function_name": "calculate_probability",
        "file_name": "probability_calculation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "top_n": "int",
            "threshold": "float"
        },
        "objectives": [
            "Group the dataframe by the categorical column and calculate the count of each group.",
            "Identify the top \"top_n\" groups with the highest count.",
            "Calculate the cumulative distribution of the counts for the top \"top_n\" groups.",
            "Create a new column 'probability' with the cumulative probability of each row, given the categorical column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_probability(data, categorical_column, top_n, threshold):\n    # Group the dataframe by the categorical column and calculate the count of each group\n    group_counts = data[categorical_column].value_counts()\n    \n    # Identify the top \"top_n\" groups with the highest count\n    top_groups = group_counts.nlargest(top_n).index\n    \n    # Calculate the cumulative distribution of the counts for the top \"top_n\" groups\n    cumulative_counts = group_counts[top_groups].cumsum()\n    \n    # Create a new column 'probability' with the cumulative probability of each row, given the categorical column\n    data['probability'] = data[categorical_column].map(cumulative_counts / cumulative_counts.max())\n    \n    return data"
    },
    {
        "function_name": "identify_time_gaps",
        "file_name": "time_gap_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "timestamp_column": "str",
            "max_gap": "int"
        },
        "objectives": [
            "Convert the timestamp column to datetime format and sort the dataframe by the id_column and timestamp_column.",
            "Calculate the time difference between consecutive rows for each id in the id_column.",
            "Identify the rows where the time difference is greater than the specified max_gap.",
            "Create a new dataframe with the rows where the time difference is greater than the max_gap and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_time_gaps(data, id_column, timestamp_column, max_gap):\n    data[timestamp_column] = pd.to_datetime(data[timestamp_column])\n    data.sort_values([id_column, timestamp_column], inplace=True)\n    \n    data['time_diff'] = data.groupby(id_column)[timestamp_column].diff()\n    data['time_diff'] = data['time_diff'].dt.total_seconds()\n    \n    data['gap_indicator'] = data['time_diff'] > max_gap\n    \n    result_data = data[data['gap_indicator'] == True][[id_column, timestamp_column, 'time_diff']].copy()\n    result_data.rename(columns={'time_diff': 'gap_size'}, inplace=True)\n    \n    return result_data"
    },
    {
        "function_name": "transformation_and_count",
        "file_name": "data_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "transformation_type": "str",
            "columns": "list"
        },
        "objectives": [
            "Apply a specified type of transformation to the specified columns.",
            "Scale the transformed values using MinMaxScaler from the sklearn library.",
            "Group the data by each transformed column and calculate the count of unique values for each group."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler",
            "import numpy as np"
        ],
        "function_def": "def transformation_and_count(df, transformation_type, columns):\n    transformed_df = pd.DataFrame()\n    for col in columns:\n        if transformation_type == 'log':\n            transformed_df[col] = df[col].apply(np.log)\n        elif transformation_type == 'sqrt':\n            transformed_df[col] = df[col].apply(np.sqrt)\n        \n    scaler = MinMaxScaler()\n    scaled_df = pd.DataFrame(scaler.fit_transform(transformed_df), columns=columns)\n    \n    group_counts = []\n    for col in columns:\n        group_counts.append(scaled_df.groupby(col).size().values.sum())\n    \n    return df.assign(count=group_counts[0])"
    },
    {
        "function_name": "split_hierarchical_column",
        "file_name": "hierarchical_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "hierarchical_column": "str",
            "separation_char": "str"
        },
        "objectives": [
            "Split the hierarchical column into separate columns based on the separation character.",
            "Create a new DataFrame with the split columns.",
            "Return the new DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_hierarchical_column(df, hierarchical_column, separation_char):\n    # Step 1: Split the hierarchical column into separate columns based on the separation character\n    split_columns = df[hierarchical_column].str.split(separation_char, expand=True)\n    \n    # Step 2: Create a new DataFrame with the split columns\n    split_df = pd.DataFrame(split_columns)\n    \n    return split_df"
    },
    {
        "function_name": "correlation_analysis_outlier_handling",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_name": "str",
            "max_outlier_ratio": "float",
            "k": "int"
        },
        "objectives": [
            "Identify the top k correlations between the specified column and other numerical columns in the dataframe.",
            "Mark the data points with values in the specified column that are more than 2 standard deviations away from the mean as outliers.",
            "Calculate the ratio of outliers in the specified column.",
            "If the outlier ratio is greater than the maximum allowed ratio, remove the outliers and re-calculate the top k correlations."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def correlation_analysis_outlier_handling(df, col_name, max_outlier_ratio, k):\n    # Identify top k correlations between the specified column and other numerical columns\n    corr_matrix = df.corr(numeric_only=True)\n    top_k_corr = corr_matrix[col_name].abs().sort_values(ascending=False).head(k+1)[1:]\n    \n    # Mark data points with values in the specified column that are more than 2 standard deviations away from the mean as outliers\n    mean = df[col_name].mean()\n    std_dev = df[col_name].std()\n    outliers = np.abs(df[col_name] - mean) > 2 * std_dev\n    \n    # Calculate the ratio of outliers in the specified column\n    outlier_ratio = outliers.sum() / len(df)\n    \n    # If the outlier ratio is greater than the maximum allowed ratio, remove the outliers and re-calculate the top k correlations\n    if outlier_ratio > max_outlier_ratio:\n        df = df[~outliers]\n        corr_matrix = df.corr(numeric_only=True)\n        top_k_corr = corr_matrix[col_name].abs().sort_values(ascending=False).head(k+1)[1:]\n    \n    return top_k_corr"
    },
    {
        "function_name": "select_high_lift_rows",
        "file_name": "gain_lift_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "liftmetric": "str",
            "minimum_lift": "float"
        },
        "objectives": [
            "Calculate the gain and lift charts for the target column.",
            "Select the top rows that have a lift greater than the minimum lift.",
            "Return the selected rows and the lift chart."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics import roc_auc_score"
        ],
        "function_def": "def select_high_lift_rows(data, target_column, liftmetric, minimum_lift):\n    # Step 1: Calculate the gain and lift charts for the target column\n    data['prediction'] = data[target_column].apply(lambda x: 1 if x > data[target_column].mean() else 0)\n    auc = roc_auc_score(data[target_column], data['prediction'])\n    gain_chart = data.groupby('prediction')[target_column].sum().reset_index()\n    lift_chart = gain_chart[gain_chart[target_column] > 0]\n    \n    # Step 2: Select the top rows that have a lift greater than the minimum lift\n    selected_rows = data.loc[data[target_column] > lift_chart[lift_chart['prediction'] > 0].iloc[0][target_column]]\n    \n    return selected_rows, lift_chart"
    },
    {
        "function_name": "select_rows_after_outlier_removal",
        "file_name": "data_cleaning.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "top_n": "int",
            "maximum_outliers": "int"
        },
        "objectives": [
            "Group the data by the group column and calculate the sum for each group.",
            "Select the top groups with the highest sum values.",
            "Within each of the top groups, remove outliers that are more than 3 standard deviations away from the mean.",
            "Return the selected rows."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def select_rows_after_outlier_removal(data, group_column, value_column, top_n, maximum_outliers):\n    # Step 1: Group the data by the group column and calculate the sum for each group\n    group_sums = data.groupby(group_column)[value_column].sum().reset_index()\n    \n    # Step 2: Select the top groups with the highest sum values\n    top_groups = group_sums.nlargest(top_n, value_column)\n    \n    # Step 3: Within each of the top groups, remove outliers that are more than 3 standard deviations away from the mean\n    selected_rows = pd.concat([data.loc[data[group_column] == group, :].loc[abs(data[value_column] - data[value_column].mean()) <= (3 * data[value_column].std())] for group in top_groups[group_column]])\n    \n    return selected_rows"
    },
    {
        "function_name": "identify_threshold_exceedance_points",
        "file_name": "threshold_exceedance.py",
        "parameters": {
            "time_series_data": "pandas.Series",
            "window_size": "int",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the cumulative sum of the time series data.",
            "Identify the points where the cumulative sum exceeds the specified threshold.",
            "Create a new dataframe with the points that meet the specified condition."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_threshold_exceedance_points(time_series_data, window_size, threshold):\n    # Calculate the cumulative sum of the time series data\n    cumulative_sum = time_series_data.cumsum()\n    \n    # Identify the points where the cumulative sum exceeds the specified threshold\n    exceedance_points = cumulative_sum[cumulative_sum > threshold].index\n    \n    # Create a new dataframe with the points that meet the specified condition\n    exceedance_df = pd.DataFrame({'points': exceedance_points})\n    \n    return exceedance_df"
    },
    {
        "function_name": "cluster_analysis",
        "file_name": "cluster_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "continuous_cols": "list[str]",
            "categorical_cols": "list[str]"
        },
        "objectives": [
            "Apply Gaussian mixture modeling to the continuous variables.",
            "Identify and separate the clusters.",
            "Calculate the information gain for each variable."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.mixture import GaussianMixture",
            "from sklearn.metrics import mutual_info_score"
        ],
        "function_def": "def cluster_analysis(data, continuous_cols, categorical_cols):\n    # Apply Gaussian mixture modeling to the continuous variables\n    gmm_model = GaussianMixture(n_components=5)\n    cluster_labels = gmm_model.fit_predict(data[continuous_cols])\n    \n    # Identify and separate the clusters\n    cluster_data = data.copy()\n    cluster_data['cluster'] = cluster_labels\n    \n    # Calculate the information gain for each variable\n    info_gain = []\n    for col in categorical_cols:\n        info_gain.append(mutual_info_score(cluster_data['cluster'], cluster_data[col]))\n    \n    return cluster_data, info_gain"
    },
    {
        "function_name": "preprocess_datetime",
        "file_name": "datetime_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "time_column": "str"
        },
        "objectives": [
            "Combine the date and time columns into a single datetime column.",
            "Convert the datetime column to a uniform timezone (UTC).",
            "Extract the day of the week, month, and year from the datetime column and create new columns for each."
        ],
        "import_lines": [
            "import pandas as pd",
            "import pytz"
        ],
        "function_def": "def preprocess_datetime(data, date_column, time_column):\n    # Combine the date and time columns into a single datetime column\n    data['datetime'] = pd.to_datetime(data[date_column] + ' ' + data[time_column])\n    \n    # Convert the datetime column to a uniform timezone (UTC)\n    data['datetime'] = data['datetime'].dt.tz_localize(pytz.timezone('US/Pacific')).dt.tz_convert(pytz.utc)\n    \n    # Extract the day of the week, month, and year from the datetime column and create new columns for each\n    data['day_of_week'] = data['datetime'].dt.day_name()\n    data['month'] = data['datetime'].dt.month\n    data['year'] = data['datetime'].dt.year\n    \n    return data"
    },
    {
        "function_name": "align_temporal_data",
        "file_name": "time_series_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "alignment_freq": "str"
        },
        "objectives": [
            "Convert the date column to a datetime format.",
            "Align the temporal data by resampling it to the specified frequency (e.g., daily, weekly, monthly).",
            "Forward fill missing values in the resampled data.",
            "Return the aligned DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def align_temporal_data(data, date_column, alignment_freq):\n    data[date_column] = pd.to_datetime(data[date_column])\n    data.set_index(date_column, inplace=True)\n    resampled_data = data.resample(alignment_freq).mean()\n    resampled_data = resampled_data.interpolate(method='ffill')\n    return resampled_data"
    },
    {
        "function_name": "entropy_based_decision_tree",
        "file_name": "decision_tree_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "confidence_level": "float",
            "max_depth": "int"
        },
        "objectives": [
            "Calculate the entropy of each column in the dataframe.",
            "Identify the most informative columns based on the entropy values and the given confidence level.",
            "Create a decision tree with the most informative columns as features and a maximum depth of max_depth.",
            "Return the decision tree and the most informative columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.model_selection import train_test_split",
            "from scipy.stats import entropy"
        ],
        "function_def": "def entropy_based_decision_tree(data, confidence_level, max_depth):\n    # Calculate entropy of each column\n    entropy_values = {}\n    for col in data.columns:\n        _, counts = np.unique(data[col], return_counts=True)\n        entropy_values[col] = entropy(counts / len(data), base=2)\n    \n    # Identify most informative columns\n    most_informative_cols = [col for col, entropy_value in entropy_values.items() if entropy_value > confidence_level]\n    \n    # Create decision tree\n    X = data[most_informative_cols]\n    y = data[\"target\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    return clf, most_informative_cols"
    },
    {
        "function_name": "k_means_clustering",
        "file_name": "clustering_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "num_clusters": "int",
            "max_iterations": "int"
        },
        "objectives": [
            "Implement the K-Means clustering algorithm to group similar data points.",
            "Initialize centroids randomly and update them iteratively.",
            "Repeat the process for a maximum of max_iterations iterations or until convergence.",
            "Return the cluster labels and the final centroids."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def k_means_clustering(data, num_clusters, max_iterations):\n    # Initialize centroids randomly\n    centroids = data.sample(n=num_clusters, random_state=42).values\n    \n    for _ in range(max_iterations):\n        # Assign cluster labels\n        distances = np.sqrt(((data.values - centroids[:, np.newaxis])**2).sum(axis=2))\n        cluster_labels = np.argmin(distances, axis=0)\n        \n        # Update centroids\n        new_centroids = np.array([data.values[cluster_labels == i].mean(axis=0) for i in range(num_clusters)])\n        \n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    return cluster_labels, centroids"
    },
    {
        "function_name": "correlation_based_regression",
        "file_name": "regression_models.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column1": "str",
            "column2": "str",
            "correlation_threshold": "float"
        },
        "objectives": [
            "Calculate the correlation between column1 and column2.",
            "If the correlation is above the correlation_threshold, calculate the linear regression between column1 and column2.",
            "If the correlation is below the correlation_threshold, calculate the polynomial regression of degree 2 between column1 and column2.",
            "Return the coefficients of the regression."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.preprocessing import PolynomialFeatures",
            "from sklearn.pipeline import make_pipeline"
        ],
        "function_def": "def correlation_based_regression(data, column1, column2, correlation_threshold):\n    # Calculate the correlation between column1 and column2\n    correlation = data[column1].corr(data[column2])\n    \n    # If the correlation is above the correlation_threshold, calculate the linear regression\n    if correlation > correlation_threshold:\n        model = LinearRegression()\n        model.fit(data[[column1]], data[column2])\n        coefficients = model.coef_\n    \n    # If the correlation is below the correlation_threshold, calculate the polynomial regression of degree 2\n    else:\n        model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n        model.fit(data[[column1]], data[column2])\n        coefficients = model.named_steps['linearregression'].coef_\n    \n    return coefficients"
    },
    {
        "function_name": "set_operations",
        "file_name": "set_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column1": "str",
            "column2": "str",
            "similarity_threshold": "float"
        },
        "objectives": [
            "Calculate the Jaccard similarity between column1 and column2.",
            "If the Jaccard similarity is above the similarity_threshold, calculate the intersection of the two columns.",
            "If the Jaccard similarity is below the similarity_threshold, calculate the union of the two columns.",
            "Return the resulting values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def set_operations(data, column1, column2, similarity_threshold):\n    # Calculate the Jaccard similarity between column1 and column2\n    jaccard_similarity = len(set(data[column1]) & set(data[column2])) / len(set(data[column1]) | set(data[column2]))\n    \n    # If the Jaccard similarity is above the similarity_threshold, calculate the intersection of the two columns\n    if jaccard_similarity > similarity_threshold:\n        result = list(set(data[column1]) & set(data[column2]))\n    \n    # If the Jaccard similarity is below the similarity_threshold, calculate the union of the two columns\n    else:\n        result = list(set(data[column1]) | set(data[column2]))\n    \n    return result"
    },
    {
        "function_name": "preprocessing_pipeline",
        "file_name": "preprocessing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numerical_columns": "list[str]",
            "categorical_columns": "list[str]"
        },
        "objectives": [
            "Standardize the numerical columns using the StandardScaler from scikit-learn.",
            "One-hot encode the categorical columns using pd.get_dummies.",
            "Concatenate the standardized numerical columns and the one-hot encoded categorical columns.",
            "Remove any duplicate rows in the resulting dataframe.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def preprocessing_pipeline(data, numerical_columns, categorical_columns):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data[numerical_columns])\n    scaled_df = pd.DataFrame(scaled_data, columns=numerical_columns)\n    one_hot_df = pd.get_dummies(data[categorical_columns], drop_first=True)\n    preprocessed_df = pd.concat([scaled_df, one_hot_df], axis=1)\n    preprocessed_df = preprocessed_df.drop_duplicates()\n    return preprocessed_df"
    },
    {
        "function_name": "identify_increasing_trends",
        "file_name": "trend_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "value_column": "str",
            "threshold": "float",
            "window_size": "int"
        },
        "objectives": [
            "Create a new column \"moving_avg\" that contains the moving average of the value column.",
            "Identify the rows where the moving average is greater than the threshold.",
            "Create a new dataframe with the identified rows and their corresponding moving averages.",
            "Add a new column \"trend\" that contains the trend of the moving average (e.g., 'increasing' or 'decreasing')."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_increasing_trends(data, value_column, threshold, window_size):\n    data['moving_avg'] = data[value_column].rolling(window=window_size).mean()\n    \n    increasing_trends = data[data['moving_avg'] > threshold]\n    increasing_trends['trend'] = 'increasing'\n    \n    decreasing_trends = data[data['moving_avg'] <= threshold]\n    decreasing_trends['trend'] = 'decreasing'\n    \n    result = pd.concat([increasing_trends, decreasing_trends])\n    return result"
    },
    {
        "function_name": "identify_sensor_anomalies",
        "file_name": "sensor_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "sensor_column": "str",
            "anomaly_threshold": "float"
        },
        "objectives": [
            "Group the dataframe by the id column.",
            "Calculate the mean and standard deviation of the sensor values for each group.",
            "Identify the rows where the sensor value is greater than the anomaly_threshold standard deviations away from the mean.",
            "Create a new dataframe with the identified rows and their corresponding sensor values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_sensor_anomalies(data, id_column, sensor_column, anomaly_threshold):\n    group_means = data.groupby(id_column)[sensor_column].mean()\n    group_stds = data.groupby(id_column)[sensor_column].std()\n    \n    data['mean'] = data[id_column].map(group_means)\n    data['std'] = data[id_column].map(group_stds)\n    \n    data['z_score'] = (data[sensor_column] - data['mean']) / data['std']\n    \n    anomalies = data[data['z_score'] > anomaly_threshold]\n    anomalies = anomalies[[id_column, sensor_column, 'z_score']]\n    \n    return anomalies"
    },
    {
        "function_name": "categorical_encoding",
        "file_name": "encoding_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns_to_encode": "list",
            "encoding_scheme": "str ('onehot', 'label', or 'ordinal')"
        },
        "objectives": [
            "Check if the specified columns exist in the dataframe.",
            "Apply the specified encoding scheme to the columns.",
            "If the encoding scheme is 'onehot', create new columns for each category.",
            "If the encoding scheme is 'label', assign a unique integer to each category.",
            "If the encoding scheme is 'ordinal', assign a unique integer to each category based on the category order."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def categorical_encoding(df, columns_to_encode, encoding_scheme):\n    for col in columns_to_encode:\n        if col not in df.columns:\n            raise ValueError(f\"The column '{col}' does not exist in the dataframe.\")\n        \n        if encoding_scheme == 'onehot':\n            df = pd.get_dummies(df, columns=[col])\n        elif encoding_scheme == 'label':\n            df[col] = df[col].astype('category')\n            df[col] = df[col].cat.codes\n        elif encoding_scheme == 'ordinal':\n            categories = df[col].unique()\n            category_map = {cat: i for i, cat in enumerate(categories)}\n            df[col] = df[col].map(category_map)\n        else:\n            raise ValueError(\"Invalid encoding scheme.\")\n    \n    return df"
    },
    {
        "function_name": "rolling_window_aggregation",
        "file_name": "rolling_window_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "rolling_window_size": "int",
            "aggregation_column": "str",
            "aggregation_method": "str ('mean', 'median', or 'sum')"
        },
        "objectives": [
            "Apply a rolling window operation to the dataframe.",
            "Calculate the specified aggregation method for the specified column within the rolling window."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_window_aggregation(data, rolling_window_size, aggregation_column, aggregation_method):\n    if aggregation_method == 'mean':\n        aggregated_data = data[aggregation_column].rolling(rolling_window_size).mean()\n    elif aggregation_method == 'median':\n        aggregated_data = data[aggregation_column].rolling(rolling_window_size).median()\n    elif aggregation_method == 'sum':\n        aggregated_data = data[aggregation_column].rolling(rolling_window_size).sum()\n    else:\n        raise ValueError(\"Invalid aggregation method.\")\n    \n    return aggregated_data"
    },
    {
        "function_name": "missing_value_imputation",
        "file_name": "imputation_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "imputation_column": "str",
            "imputation_method": "str ('mean', 'median', or 'mode')",
            "imputation_strategy": "str ('iterative' or 'non_iterative')"
        },
        "objectives": [
            "Check if there are missing values in the specified column.",
            "Apply the specified imputation method to fill in the missing values.",
            "If the imputation strategy is 'iterative', iteratively impute the missing values until convergence."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def missing_value_imputation(df, imputation_column, imputation_method, imputation_strategy):\n    if imputation_method == 'mean':\n        fill_value = df[imputation_column].mean()\n    elif imputation_method == 'median':\n        fill_value = df[imputation_column].median()\n    elif imputation_method == 'mode':\n        fill_value = df[imputation_column].mode().iloc[0]\n    else:\n        raise ValueError(\"Invalid imputation method.\")\n    \n    if imputation_strategy == 'iterative':\n        prev_fill_value = np.nan\n        while fill_value != prev_fill_value:\n            prev_fill_value = fill_value\n            df[imputation_column] = df[imputation_column].fillna(fill_value)\n            if imputation_method == 'mean':\n                fill_value = df[imputation_column].mean()\n            elif imputation_method == 'median':\n                fill_value = df[imputation_column].median()\n            elif imputation_method == 'mode':\n                fill_value = df[imputation_column].mode().iloc[0]\n    elif imputation_strategy == 'non_iterative':\n        df[imputation_column] = df[imputation_column].fillna(fill_value)\n    \n    return df"
    },
    {
        "function_name": "feature_scaling",
        "file_name": "scaling_utils.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "scaling_columns": "list",
            "scaling_method": "str ('standardization' or 'normalization')",
            "scaling_range": "tuple (min, max)"
        },
        "objectives": [
            "Apply the specified scaling method to the specified columns.",
            "If the scaling method is 'standardization', standardize the columns to have zero mean and unit variance.",
            "If the scaling method is 'normalization', normalize the columns to have values within the specified range."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def feature_scaling(data, scaling_columns, scaling_method, scaling_range):\n    for col in scaling_columns:\n        if scaling_method == 'standardization':\n            data[col] = (data[col] - data[col].mean()) / data[col].std()\n        elif scaling_method == 'normalization':\n            data[col] = (data[col] - data[col].min()) / (data[col].max() - data[col].min())\n            data[col] = data[col] * (scaling_range[1] - scaling_range[0]) + scaling_range[0]\n        else:\n            raise ValueError(\"Invalid scaling method.\")\n    \n    return data"
    },
    {
        "function_name": "discretize_percentiles",
        "file_name": "data_discretization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "num_buckets": "int"
        },
        "objectives": [
            "Group the dataframe by the id_column and calculate the cumulative distribution of the value_column.",
            "Create a new column 'percentile' with the percentile rank of each row's value within its group.",
            "Split the percentile values into num_buckets equal-width buckets.",
            "Create a new column 'bucket' with the bucket number for each row."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def discretize_percentiles(df, id_column, value_column, num_buckets):\n    df['percentile'] = df.groupby(id_column)[value_column].transform(lambda x: x.rank(pct=True))\n    \n    bucket_boundaries = np.linspace(0, 1, num_buckets + 1)\n    df['bucket'] = pd.cut(df['percentile'], bins=bucket_boundaries, labels=range(num_buckets), include_lowest=True)\n    \n    return df"
    },
    {
        "function_name": "smoothed_sharpe_ratio",
        "file_name": "risk_metrics.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_column": "str",
            "target_column": "str",
            "smoothing_param": "float"
        },
        "objectives": [
            "Create a new column 'smoothed' with the smoothed values of the feature_column using the target_column as weights.",
            "Implement the smoothing using the exponential smoothing formula with the specified smoothing_param.",
            "Calculate the Sharpe ratio of the smoothed values.",
            "Create a new column 'sharpe_ratio' with the Sharpe ratio values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def smoothed_sharpe_ratio(data, feature_column, target_column, smoothing_param):\n    data['smoothed'] = data[feature_column].ewm(alpha=1 - smoothing_param, adjust=False).mean()\n    data['sharpe_ratio'] = data['smoothed'].ewm(alpha=1 - smoothing_param, adjust=False).mean() / data['smoothed'].ewm(alpha=1 - smoothing_param, adjust=False, min_periods=1).std()\n    \n    return data"
    },
    {
        "function_name": "cluster_activities",
        "file_name": "activity_clustering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "activity_column": "str",
            "num_clusters": "int"
        },
        "objectives": [
            "Convert the time_column to datetime type and extract the hour of the day.",
            "Group the dataframe by the activity_column and calculate the average hourly activity counts.",
            "Use K-means clustering to group the activities into num_clusters clusters based on their hourly patterns.",
            "Create a new column 'cluster' with the cluster label for each activity."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans",
            "import numpy as np"
        ],
        "function_def": "def cluster_activities(df, time_column, activity_column, num_clusters):\n    df[time_column] = pd.to_datetime(df[time_column])\n    df['hour'] = df[time_column].dt.hour\n    \n    hourly_counts = df.pivot_table(index=activity_column, columns='hour', aggfunc='size', fill_value=0)\n    \n    kmeans = KMeans(n_clusters=num_clusters, init='k-means++')\n    cluster_labels = kmeans.fit_predict(hourly_counts)\n    \n    df['cluster'] = df[activity_column].map(lambda x: cluster_labels[hourly_counts.index.get_loc(x)])\n    \n    return df"
    },
    {
        "function_name": "predicted_probabilities",
        "file_name": "classification_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "threshold": "float",
            "n_estimators": "int"
        },
        "objectives": [
            "Split the data into training and testing sets.",
            "Train a random forest classifier on the training set to predict the target_column.",
            "Identify the rows in the testing set where the predicted probability is greater than the threshold.",
            "Create a new dataframe with the identified rows and their corresponding predicted probabilities."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.ensemble import RandomForestClassifier"
        ],
        "function_def": "def predicted_probabilities(data, target_column, threshold, n_estimators):\n    # Split the data into training and testing sets\n    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n    \n    # Train a random forest classifier on the training set to predict the target_column\n    X_train = train_data.drop(target_column, axis=1)\n    y_train = train_data[target_column]\n    classifier = RandomForestClassifier(n_estimators=n_estimators)\n    classifier.fit(X_train, y_train)\n    \n    # Identify the rows in the testing set where the predicted probability is greater than the threshold\n    X_test = test_data.drop(target_column, axis=1)\n    predicted_probabilities = classifier.predict_proba(X_test)[:, 1]\n    identified_rows = test_data[predicted_probabilities > threshold]\n    \n    # Create a new dataframe with the identified rows and their corresponding predicted probabilities\n    identified_rows['predicted_probability'] = predicted_probabilities[predicted_probabilities > threshold]\n    \n    return identified_rows"
    },
    {
        "function_name": "stratified_sampling",
        "file_name": "sampling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "population_column": "str",
            "sample_column": "str",
            "sample_size": "int"
        },
        "objectives": [
            "Perform stratified sampling to identify representative samples from the population.",
            "Calculate the weight of each sample based on the population and sample size.",
            "Create a new column with the weight for each sample.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split"
        ],
        "function_def": "def stratified_sampling(df, population_column, sample_column, sample_size):\n    # Perform stratified sampling to identify representative samples from the population\n    samples = train_test_split(df, train_size=sample_size, stratify=df[population_column], random_state=42)\n    \n    # Calculate the weight of each sample based on the population and sample size\n    weights = []\n    for _, sample in enumerate(samples):\n        weight = len(df) / sample_size\n        weights.extend([weight] * len(sample))\n    \n    # Create a new column with the weight for each sample\n    df[f'{sample_column}_weight'] = weights\n    \n    # Return the updated dataframe\n    return df"
    },
    {
        "function_name": "mutual_info_selector",
        "file_name": "information_theory.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "target_column": "str",
            "correlation_threshold": "float",
            "top_n": "int"
        },
        "objectives": [
            "Calculate the mutual information between the categorical column and the target column.",
            "Select the top N categorical variables based on their mutual information with the target column.",
            "Create a new dataframe with the selected categorical variables and the target column.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import mutual_info_classif"
        ],
        "function_def": "def mutual_info_selector(df, categorical_column, target_column, correlation_threshold, top_n):\n    mi = mutual_info_classif(df[categorical_column], df[target_column])\n    mi_df = pd.DataFrame({'feature': df[categorical_column].columns, 'mi': mi})\n    top_n_features = mi_df.nlargest(top_n, 'mi')['feature']\n    \n    new_df = pd.concat([df[top_n_features], df[target_column]], axis=1)\n    \n    return new_df"
    },
    {
        "function_name": "haversine_distance_filter",
        "file_name": "geospatial_operations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "latitude_column": "str",
            "longitude_column": "str",
            "radius_km": "float",
            "coordinates": "tuple"
        },
        "objectives": [
            "Convert the latitude and longitude columns to radians.",
            "Calculate the distance between each point and the specified coordinates using the Haversine formula.",
            "Filter the data to only include rows where the distance is within the specified radius.",
            "Return the filtered dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def haversine_distance_filter(data, latitude_column, longitude_column, radius_km, coordinates):\n    lat1, lon1 = coordinates\n    data['latitude_rad'] = np.radians(data[latitude_column])\n    data['longitude_rad'] = np.radians(data[longitude_column])\n    \n    dlat = data['latitude_rad'] - np.radians(lat1)\n    dlon = data['longitude_rad'] - np.radians(lon1)\n    \n    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1)) * np.cos(data['latitude_rad']) * np.sin(dlon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    \n    distance_km = 6371 * c  # Earth's radius in km\n    \n    filtered_data = data[distance_km <= radius_km]\n    \n    return filtered_data"
    },
    {
        "function_name": "identify_nearby_locations",
        "file_name": "geospatial_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "latitude_column": "str",
            "longitude_column": "str",
            "radius": "float",
            "center Coordinates": "tuple"
        },
        "objectives": [
            "Calculate the distance between each location and the center coordinates using the Haversine formula.",
            "Identify locations that are within the specified radius from the center coordinates.",
            "Create a new column indicating whether the location is within the radius or not."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def identify_nearby_locations(data, latitude_column, longitude_column, radius, center_coordinates):\n    center_latitude, center_longitude = center_coordinates\n    \n    data['distance'] = np.arccos(np.sin(np.radians(data[latitude_column])) * np.sin(np.radians(center_latitude)) +\n                                np.cos(np.radians(data[latitude_column])) * np.cos(np.radians(center_latitude)) *\n                                np.cos(np.radians(data[longitude_column]) - np.radians(center_longitude))) * 6371\n    \n    data['is_nearby'] = data['distance'] <= radius\n    \n    return data"
    },
    {
        "function_name": "identify_sessions_with_time_gaps",
        "file_name": "user_session_analysis.py",
        "parameters": {
            "logs": "pandas.DataFrame",
            "user_column": "str",
            "session_column": "str",
            "timestamp_column": "str",
            "time_threshold": "int"
        },
        "objectives": [
            "Sort the logs by the user column, session column, and timestamp column.",
            "Calculate the time difference between each log and the previous log for each user and session.",
            "Identify sessions with a time difference greater than the time threshold.",
            "Create a new column indicating whether the session has a time gap or not."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_sessions_with_time_gaps(logs, user_column, session_column, timestamp_column, time_threshold):\n    logs[timestamp_column] = pd.to_datetime(logs[timestamp_column])\n    \n    sorted_logs = logs.sort_values([user_column, session_column, timestamp_column])\n    \n    sorted_logs['time_diff'] = sorted_logs.groupby([user_column, session_column])[timestamp_column].diff().dt.total_seconds()\n    \n    sorted_logs['has_time_gap'] = sorted_logs['time_diff'] > time_threshold\n    \n    return sorted_logs"
    },
    {
        "function_name": "anomaly_detection",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "anomaly_threshold": "float",
            "k": "int"
        },
        "objectives": [
            "Calculate the local density of each data point in the dataframe.",
            "Identify data points with a local density less than the anomaly_threshold.",
            "For each identified data point, calculate the maximum distance to its k-nearest neighbors.",
            "Return a new dataframe with the data points and their corresponding anomaly scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.spatial import distance"
        ],
        "function_def": "def anomaly_detection(df, anomaly_threshold, k):\n    # Calculate local density\n    local_density = []\n    for i in range(len(df)):\n        distances = [distance.euclidean(df.iloc[i], df.iloc[j]) for j in range(len(df)) if i != j]\n        density = 1 / (np.sum(distances) / len(distances))\n        local_density.append(density)\n    \n    # Identify data points with low local density\n    anomalies = df.iloc[np.where(np.array(local_density) < anomaly_threshold)]\n    \n    # Calculate anomaly scores\n    scores = []\n    for index, row in anomalies.iterrows():\n        distances = np.array([distance.euclidean(row, df.iloc[j]) for j in range(len(df)) if index != j])\n        k_distances = np.sort(distances)[:k]\n        score = np.max(k_distances)\n        scores.append(score)\n    \n    # Create a new dataframe with anomaly scores\n    anomaly_df = pd.DataFrame({'anomaly_score': scores}, index=anomalies.index)\n    \n    return pd.concat([anomalies, anomaly_df], axis=1)"
    },
    {
        "function_name": "apply_moving_average_filter",
        "file_name": "filtering.py",
        "parameters": {
            "arr": "numpy.ndarray",
            "window_size": "int"
        },
        "objectives": [
            "Apply a moving average filter to the input array using the specified window size.",
            "Calculate the standard deviation of each window.",
            "Calculate the ratio of the moving average to the standard deviation.",
            "Return the filtered array and the ratio values."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def apply_moving_average_filter(arr, window_size):\n    # Apply a moving average filter to the input array\n    filtered_arr = np.convolve(arr, np.ones(window_size) / window_size, mode='same')\n    \n    # Calculate the standard deviation of each window\n    std_dev_values = np.std(np.lib.stride_tricks.sliding_window_view(arr, window_size), axis=1)\n    \n    # Calculate the ratio of the moving average to the standard deviation\n    ratio_values = filtered_arr[window_size // 2: -window_size // 2 + 1] / std_dev_values\n    \n    return filtered_arr, ratio_values"
    },
    {
        "function_name": "top_scorers",
        "file_name": "scoring.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "score_column": "str",
            "threshold": "float",
            "top_n": "int"
        },
        "objectives": [
            "Calculate the cumulative sum of scores for each id.",
            "Identify the top N ids with the highest cumulative score above the threshold.",
            "Create a new column indicating whether each id is in the top N.",
            "Return the updated dataframe with the top N ids."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def top_scorers(df, id_column, score_column, threshold, top_n):\n    # Calculate the cumulative sum of scores for each id\n    df['cumulative_score'] = df.groupby(id_column)[score_column].cumsum()\n    \n    # Identify the top N ids with the highest cumulative score above the threshold\n    top_ids = df[df['cumulative_score'] > threshold].groupby(id_column)['cumulative_score'].max().sort_values(ascending=False).head(top_n).index\n    \n    # Create a new column indicating whether each id is in the top N\n    df['is_top_scorer'] = np.where(df[id_column].isin(top_ids), True, False)\n    \n    return df"
    },
    {
        "function_name": "time_series_resampling",
        "file_name": "time_series_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "frequency": "str"
        },
        "objectives": [
            "Resample the data in the specified column to the specified frequency (e.g., hourly, daily, weekly).",
            "Aggregate the resampled data using the mean aggregation function.",
            "Fill missing values using interpolation.",
            "Return the resampled data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def time_series_resampling(df, time_column, frequency):\n    df[time_column] = pd.to_datetime(df[time_column])\n    df.set_index(time_column, inplace=True)\n    resampled_data = df.resample(frequency).mean()\n    resampled_data.interpolate(method='linear', inplace=True)\n    return resampled_data"
    },
    {
        "function_name": "evaluate_model",
        "file_name": "model_evaluation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "prediction_column": "str",
            "target_column": "str",
            "probability_threshold": "float"
        },
        "objectives": [
            "Calculate the probability of each class in the prediction column.",
            "Identify the rows where the predicted probability is greater than the specified threshold.",
            "Evaluate the performance of the model using metrics such as accuracy, precision, recall, and F1-score.",
            "Return the evaluation metrics."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
        ],
        "function_def": "def evaluate_model(df, prediction_column, target_column, probability_threshold):\n    # Calculate the probability of each class in the prediction column\n    predicted_probabilities = df[prediction_column].apply(lambda x: x.split(',')[:-1]).apply(lambda x: [float(i) for i in x])\n    \n    # Identify the rows where the predicted probability is greater than the specified threshold\n    predicted_classes = predicted_probabilities.apply(lambda x: np.argmax(x))\n    \n    # Evaluate the performance of the model using metrics such as accuracy, precision, recall, and F1-score\n    accuracy = accuracy_score(df[target_column], predicted_classes)\n    precision = precision_score(df[target_column], predicted_classes, average='weighted')\n    recall = recall_score(df[target_column], predicted_classes, average='weighted')\n    f1 = f1_score(df[target_column], predicted_classes, average='weighted')\n    \n    return accuracy, precision, recall, f1"
    },
    {
        "function_name": "merge_and_compare",
        "file_name": "data_merging.py",
        "parameters": {
            "df1": "pandas.DataFrame",
            "df2": "pandas.DataFrame",
            "common_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Merge the two dataframes based on a common column.",
            "Calculate the similarity between the rows of the two dataframes using a specified threshold.",
            "Create a new column that marks similar rows with a value of 1 and dissimilar rows with a value of 0.",
            "Return the merged dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_and_compare(df1, df2, common_column, threshold):\n    # Step 1: Merge the two dataframes based on a common column\n    merged_df = pd.merge(df1, df2, on=common_column)\n    \n    # Step 2: Calculate the similarity between the rows of the two dataframes using a specified threshold\n    merged_df['similarity'] = merged_df.apply(lambda x: 1 if abs(x['value_x'] - x['value_y']) <= threshold else 0, axis=1)\n    \n    return merged_df"
    },
    {
        "function_name": "analyze_customer_purchases",
        "file_name": "customer_insights.py",
        "parameters": {
            "transaction_data": "pandas.DataFrame",
            "customer_id_column": "str",
            "transaction_date_column": "str",
            "item_column": "str",
            "quantity_column": "str",
            "revenue_column": "str",
            "threshold_revenue": "float"
        },
        "objectives": [
            "Identify the top 10% of customers with the highest total revenue.",
            "For each customer, calculate the average quantity purchased per item.",
            "Determine the most frequently purchased item for each customer.",
            "Return a dataframe containing the top customers, their average quantity purchased per item, and their most frequently purchased item."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def analyze_customer_purchases(transaction_data, customer_id_column, transaction_date_column, item_column, quantity_column, revenue_column, threshold_revenue):\n    # Calculate the total revenue for each customer\n    total_revenue = transaction_data.groupby(customer_id_column)[revenue_column].sum().reset_index()\n    \n    # Identify the top 10% of customers with the highest total revenue\n    top_customers = total_revenue[total_revenue[revenue_column] > total_revenue[revenue_column].quantile(0.9)]\n    \n    # Filter the transaction data to only include top customers\n    top_customer_transactions = transaction_data[transaction_data[customer_id_column].isin(top_customers[customer_id_column])]\n    \n    # Calculate the average quantity purchased per item for each customer\n    average_quantity = top_customer_transactions.groupby([customer_id_column, item_column])[quantity_column].mean().reset_index()\n    \n    # Determine the most frequently purchased item for each customer\n    most_frequent_item = top_customer_transactions.groupby(customer_id_column)[item_column].apply(lambda x: x.value_counts().index[0]).reset_index()\n    \n    # Merge the results\n    result = pd.merge(top_customers, average_quantity, on=customer_id_column)\n    result = pd.merge(result, most_frequent_item, on=customer_id_column)\n    \n    return result"
    },
    {
        "function_name": "create_time_feature",
        "file_name": "feature_engineering.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str",
            "target_variable": "str"
        },
        "objectives": [
            "Handle missing values in the date_column by filling them with the median date of the respective id.",
            "Convert the date_column to a datetime format.",
            "Calculate the time difference in days between the current date and the median date for each id.",
            "Use the calculated time differences to create a new feature that can be used to predict the target_variable."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def create_time_feature(data, id_column, date_column, target_variable):\n    # Handle missing values in the date_column by filling them with the median date of the respective id\n    data[date_column] = pd.to_datetime(data[date_column])\n    data['median_date'] = data.groupby(id_column)[date_column].transform('median')\n    data[date_column] = data[date_column].fillna(data['median_date'])\n    \n    # Convert the date_column to a datetime format\n    data[date_column] = pd.to_datetime(data[date_column])\n    \n    # Calculate the time difference in days between the current date and the median date for each id\n    data['time_diff'] = (data[date_column] - data['median_date']).dt.days\n    \n    # Use the calculated time differences to create a new feature that can be used to predict the target_variable\n    scaler = StandardScaler()\n    data['time_feature'] = scaler.fit_transform(data[['time_diff']])\n    \n    return data"
    },
    {
        "function_name": "mark_identical_rows",
        "file_name": "similarity_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_1": "str",
            "column_2": "str"
        },
        "objectives": [
            "Calculate the Jaccard similarity between the two specified columns.",
            "Create a new column with the Jaccard similarities.",
            "Identify the rows with a Jaccard similarity of 1 (i.e., the two columns have the same values).",
            "Mark these rows as identical in a new column.",
            "Return the updated dataframe with the identical rows marked."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def mark_identical_rows(df, column_1, column_2):\n    # Calculate the Jaccard similarity between the two specified columns\n    df['jaccard_similarity'] = df.apply(lambda x: len(set(x[column_1]) & set(x[column_2])) / len(set(x[column_1]) | set(x[column_2])), axis=1)\n    \n    # Identify the rows with a Jaccard similarity of 1 (i.e., the two columns have the same values)\n    identical_rows = df['jaccard_similarity'] == 1\n    \n    # Mark these rows as identical in a new column\n    df['identical'] = identical_rows\n    \n    return df"
    },
    {
        "function_name": "bin_continuous_column",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "continuous_column": "str",
            "n_bins": "int",
            "log_transformation": "bool"
        },
        "objectives": [
            "Apply a log transformation to the specified continuous column if the transformation is requested.",
            "Calculate the bin edges for the continuous column using the specified number of bins.",
            "Assign each row to a bin based on its value in the continuous column.",
            "Create a new column with the corresponding bin labels."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_continuous_column(df, continuous_column, n_bins, log_transformation):\n    # Apply a log transformation to the specified continuous column if the transformation is requested\n    if log_transformation:\n        df[continuous_column] = np.log1p(df[continuous_column])\n    \n    # Calculate the bin edges for the continuous column using the specified number of bins\n    bin_edges = np.histogram_bin_edges(df[continuous_column], bins=n_bins)\n    \n    # Assign each row to a bin based on its value in the continuous column\n    df['bin_label'] = np.digitize(df[continuous_column], bin_edges)\n    \n    return df"
    },
    {
        "function_name": "lda_topic_modeling",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "num_topics": "int"
        },
        "objectives": [
            "Apply a term-frequency inverse document frequency (TF-IDF) transformation to the specified text column.",
            "Perform Latent Dirichlet Allocation (LDA) topic modeling on the TF-IDF transformed data.",
            "Create a new column with the corresponding topic labels."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.decomposition import LatentDirichletAllocation"
        ],
        "function_def": "def lda_topic_modeling(df, text_column, num_topics):\n    # Apply a TF-IDF transformation to the specified text column\n    vectorizer = TfidfVectorizer()\n    tfidf_data = vectorizer.fit_transform(df[text_column])\n    \n    # Perform LDA topic modeling on the TF-IDF transformed data\n    lda_model = LatentDirichletAllocation(n_components=num_topics)\n    topic_labels = lda_model.fit_transform(tfidf_data)\n    \n    # Create a new column with the corresponding topic labels\n    df['topic_label'] = np.argmax(topic_labels, axis=1)\n    \n    return df"
    },
    {
        "function_name": "calculate_mutual_information",
        "file_name": "feature_selection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column1": "str",
            "column2": "str",
            "column3": "str"
        },
        "objectives": [
            "Calculate the pairwise mutual information between the specified columns.",
            "Create a new dataframe with the mutual information values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics import mutual_info_score"
        ],
        "function_def": "def calculate_mutual_information(df, column1, column2, column3):\n    # Calculate the pairwise mutual information between the specified columns\n    mutual_info_12 = mutual_info_score(df[column1], df[column2])\n    mutual_info_13 = mutual_info_score(df[column1], df[column3])\n    mutual_info_23 = mutual_info_score(df[column2], df[column3])\n    \n    # Create a new dataframe with the mutual information values\n    mutual_info_df = pd.DataFrame({'Pair': ['(1, 2)', '(1, 3)', '(2, 3)'], 'Mutual Information': [mutual_info_12, mutual_info_13, mutual_info_23]})\n    \n    return mutual_info_df"
    },
    {
        "function_name": "holiday_impact_analysis",
        "file_name": "seasonality_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "group_column": "str",
            "value_column": "str",
            "holiday_column": "str"
        },
        "objectives": [
            "Convert the date column to datetime type and set it as the index of the dataframe.",
            "Group the dataframe by the group_column and calculate the mean of the value_column within each group.",
            "Identify the rows where the date is a holiday and the mean value is greater than the overall mean value.",
            "Return the resulting dataframe with the identified rows and a new column indicating whether the date is a holiday."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def holiday_impact_analysis(data, date_column, group_column, value_column, holiday_column):\n    # Convert the date column to datetime type and set it as the index of the dataframe\n    data[date_column] = pd.to_datetime(data[date_column])\n    data.set_index(date_column, inplace=True)\n    \n    # Group the dataframe by the group_column and calculate the mean of the value_column within each group\n    group_means = data.groupby(group_column)[value_column].mean()\n    \n    # Calculate the overall mean value\n    overall_mean = data[value_column].mean()\n    \n    # Identify the rows where the date is a holiday and the mean value is greater than the overall mean value\n    data['is_holiday'] = data[holiday_column]\n    data['group_mean'] = data[group_column].map(group_means)\n    data['is_impacted'] = (data['is_holiday']) & (data['group_mean'] > overall_mean)\n    \n    return data"
    },
    {
        "function_name": "cardinality_onehot_encode",
        "file_name": "preprocessing_utils.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "cardinality_threshold": "int"
        },
        "objectives": [
            "Identify categorical columns with cardinality above the specified threshold.",
            "Perform one-hot encoding on the identified columns.",
            "Drop any columns that are not identified.",
            "Return the encoded dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def cardinality_onehot_encode(data, categorical_columns, cardinality_threshold):\n    # Identify categorical columns with cardinality above the threshold\n    high_cardinality_columns = [col for col in categorical_columns if data[col].nunique() > cardinality_threshold]\n    \n    # Perform one-hot encoding on the identified columns\n    encoded_data = data.copy()\n    for col in high_cardinality_columns:\n        encoded_data = pd.concat([encoded_data, pd.get_dummies(encoded_data[col], drop_first=True)], axis=1)\n    \n    # Drop any columns that are not identified\n    low_cardinality_columns = [col for col in data.columns if col not in high_cardinality_columns and col not in [f'{col}_{cat}' for col in high_cardinality_columns for cat in data[col].unique()]]\n    encoded_data = encoded_data.drop(low_cardinality_columns, axis=1)\n    \n    return encoded_data"
    },
    {
        "function_name": "calculate_tfidf",
        "file_name": "nlp_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "term_document_frequency": "dict",
            "inverse_document_frequency": "dict"
        },
        "objectives": [
            "Calculate the term-frequency inverse document frequency (TF-IDF) for each term in each document.",
            "Create a new column with the TF-IDF scores.",
            "Return the dataframe with the TF-IDF scores."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_tfidf(df, term_document_frequency, inverse_document_frequency):\n    tfidf_scores = []\n    for index, row in df.iterrows():\n        document = row['document']\n        tfidf_score = 0\n        for term in document:\n            if term in term_document_frequency and term in inverse_document_frequency:\n                tf = term_document_frequency[term]\n                idf = inverse_document_frequency[term]\n                tfidf_score += tf * idf\n        tfidf_scores.append(tfidf_score)\n    \n    df['tfidf_score'] = tfidf_scores\n    \n    return df"
    },
    {
        "function_name": "track_medication_adherence",
        "file_name": "patient_care.py",
        "parameters": {
            "patient_data": "pandas.DataFrame",
            "medication_column": "str",
            "dosage_column": "str",
            "start_date_column": "str",
            "end_date_column": "str",
            "max_days_gap": "int"
        },
        "objectives": [
            "Flag patients who have gaps in their medication treatment exceeding the maximum allowed gap.",
            "Calculate the total cumulative dosage for each patient.",
            "Identify patients who have exceeded the maximum cumulative dosage threshold.",
            "Return the updated patient data with flags for treatment gaps and excessive dosage."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def track_medication_adherence(patient_data, medication_column, dosage_column, start_date_column, end_date_column, max_days_gap):\n    # Flag patients with gaps in treatment\n    patient_data['gap_in_treatment'] = False\n    patient_data = patient_data.sort_values([medication_column, start_date_column])\n    patient_data['prev_end_date'] = patient_data.groupby(medication_column)[end_date_column].shift(1)\n    patient_data['treatment_gap'] = (patient_data[start_date_column] - patient_data['prev_end_date']).dt.days\n    patient_data.loc[patient_data['treatment_gap'] > max_days_gap, 'gap_in_treatment'] = True\n    \n    # Calculate total cumulative dosage for each patient\n    patient_data['cumulative_dosage'] = patient_data.groupby(medication_column)[dosage_column].transform(pd.Series.cumsum)\n    \n    # Identify patients who have exceeded maximum cumulative dosage threshold\n    max_cumulative_dosage = patient_data[dosage_column].max() * len(patient_data)\n    patient_data['excessive_dosage'] = patient_data['cumulative_dosage'] > max_cumulative_dosage\n    \n    return patient_data"
    },
    {
        "function_name": "identify_anomalies",
        "file_name": "sensor_analysis.py",
        "parameters": {
            "sensor_data": "pandas.DataFrame",
            "value_column": "str",
            "time_column": "str",
            "threshold": "float",
            "window_size": "int"
        },
        "objectives": [
            "Detect anomalies in sensor readings based on the specified threshold.",
            "Calculate the rolling average of sensor readings.",
            "Identify periods where the rolling average exceeds the threshold.",
            "Return the updated sensor data with anomaly flags and rolling averages."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_anomalies(sensor_data, value_column, time_column, threshold, window_size):\n    # Detect anomalies in sensor readings\n    sensor_data['anomaly'] = False\n    sensor_data.loc[sensor_data[value_column].abs() > threshold, 'anomaly'] = True\n    \n    # Calculate rolling average of sensor readings\n    sensor_data['rolling_average'] = sensor_data[value_column].rolling(window_size).mean()\n    \n    # Identify periods where rolling average exceeds threshold\n    sensor_data['rolling_average_exceeded'] = sensor_data['rolling_average'] > threshold\n    \n    return sensor_data"
    },
    {
        "function_name": "select_diverse_features",
        "file_name": "population_analysis.py",
        "parameters": {
            "population_data": "pandas.DataFrame",
            "id_column": "str",
            "feature_column": "str",
            "k": "int"
        },
        "objectives": [
            "Select the top k features that contribute most to the diversity of the population.",
            "Calculate the entropy of the feature distribution for each feature.",
            "Rank features by their entropy values.",
            "Return the selected features and their corresponding entropy values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import entropy"
        ],
        "function_def": "def select_diverse_features(population_data, id_column, feature_column, k):\n    # Calculate entropy of feature distribution for each feature\n    feature_entropy = []\n    for feature in population_data[feature_column].unique():\n        feature_distribution = population_data[population_data[feature_column] == feature][id_column].value_counts(normalize=True)\n        feature_entropy.append((feature, entropy(feature_distribution)))\n    \n    # Rank features by their entropy values\n    feature_entropy.sort(key=lambda x: x[1], reverse=True)\n    \n    # Select top k features\n    selected_features = [feature for feature, _ in feature_entropy[:k]]\n    selected_entropy = [entropy for _, entropy in feature_entropy[:k]]\n    \n    return selected_features, selected_entropy"
    },
    {
        "function_name": "calculate_betweenness_centrality",
        "file_name": "network_analysis.py",
        "parameters": {
            "network_data": "pandas.DataFrame",
            "source_column": "str",
            "target_column": "str",
            "weight_column": "str"
        },
        "objectives": [
            "Calculate the betweenness centrality of each node in the network.",
            "Identify the top 3 nodes with the highest betweenness centrality.",
            "Return the updated network data with betweenness centrality values and top nodes."
        ],
        "import_lines": [
            "import pandas as pd",
            "from networkx import betweenness_centrality, from_pandas_edgelist"
        ],
        "function_def": "def calculate_betweenness_centrality(network_data, source_column, target_column, weight_column):\n    # Create a graph from the network data\n    G = from_pandas_edgelist(network_data, source=source_column, target=target_column, edge_attr=weight_column)\n    \n    # Calculate betweenness centrality of each node\n    betweenness_centrality_values = betweenness_centrality(G, weight=weight_column)\n    \n    # Identify top 3 nodes with highest betweenness centrality\n    top_nodes = sorted(betweenness_centrality_values, key=betweenness_centrality_values.get, reverse=True)[:3]\n    \n    # Update network data with betweenness centrality values and top nodes\n    network_data['betweenness_centrality'] = network_data[source_column].map(betweenness_centrality_values)\n    network_data['is_top_node'] = network_data[source_column].isin(top_nodes)\n    \n    return network_data"
    },
    {
        "function_name": "split_into_partitions",
        "file_name": "partitioning.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "partition_column": "str",
            "max_partitions": "int",
            "partition_method": "str"
        },
        "objectives": [
            "Split the dataframe into partitions based on the partition column and the specified partition method.",
            "Ensure that each partition does not exceed the specified max_partitions.",
            "Create a new column that indicates the partition ID for each row.",
            "Return the partitioned dataframe and the partition labels."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def split_into_partitions(data, partition_column, max_partitions, partition_method):\n    # Split the dataframe into partitions based on the partition column and the specified partition method\n    if partition_method == 'uniform':\n        bins = np.linspace(data[partition_column].min(), data[partition_column].max(), max_partitions+1)\n        partition_ids = np.digitize(data[partition_column], bins)\n    elif partition_method == 'quantile':\n        quantiles = np.linspace(0, 1, max_partitions+1)\n        partition_ids = np.digitize(data[partition_column], data[partition_column].quantile(quantiles))\n    else:\n        raise ValueError(\"Invalid partition method\")\n    \n    # Create a new column that indicates the partition ID for each row\n    data['partition_id'] = partition_ids\n    \n    return data, partition_ids"
    },
    {
        "function_name": "identify_high_variance_ids",
        "file_name": "variance_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "min_samples": "int",
            "max_samples": "int"
        },
        "objectives": [
            "Calculate the rolling standard deviation of the value column for each id using a window size that is randomly selected between min_samples and max_samples.",
            "Identify the ids where the rolling standard deviation is greater than the mean rolling standard deviation.",
            "Create a new column indicating whether each id has high variance."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def identify_high_variance_ids(data, id_column, value_column, min_samples, max_samples):\n    # Calculate the rolling standard deviation for each id\n    np.random.seed(0)  # For reproducibility\n    window_sizes = np.random.randint(min_samples, max_samples, size=len(data))\n    rolling_std_dev = []\n    for _, window_size in enumerate(window_sizes):\n        std_dev = data.groupby(id_column)[value_column].transform(lambda x: x.rolling(window_size).std())\n        rolling_std_dev.append(std_dev)\n    rolling_std_dev = pd.concat(rolling_std_dev, axis=1).mean(axis=1)\n    \n    # Identify the ids where the rolling standard deviation is greater than the mean rolling standard deviation\n    mean_rolling_std_dev = rolling_std_dev.mean()\n    data['has_high_variance'] = np.where(rolling_std_dev > mean_rolling_std_dev, True, False)\n    \n    return data"
    },
    {
        "function_name": "z_score_calculation",
        "file_name": "statistical_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str"
        },
        "objectives": [
            "Group the dataframe by the group column and calculate the mean and standard deviation of the value column.",
            "Create a new column with the z-scores for each group.",
            "Identify the groups with z-scores above a certain threshold (e.g., 2).",
            "Return the resulting dataframe with the groups and their corresponding z-scores."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def z_score_calculation(df, group_column, value_column):\n    # Group the dataframe by the group column and calculate the mean and standard deviation of the value column\n    group_stats = df.groupby(group_column)[value_column].agg(['mean', 'std'])\n    \n    # Create a new column with the z-scores for each group\n    df['z_score'] = (df[value_column] - group_stats['mean']) / group_stats['std']\n    \n    # Identify the groups with z-scores above a certain threshold (e.g., 2)\n    threshold = 2\n    filtered_groups = df[group_column][df['z_score'] > threshold]\n    \n    return df"
    },
    {
        "function_name": "std_dev_minima",
        "file_name": "signal_processing.py",
        "parameters": {
            "arr": "numpy.ndarray",
            "window_size": "int",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the moving standard deviation of the array elements within a specified window size.",
            "Identify the local minima in the moving standard deviation array.",
            "Create a new array containing the indices of the local minima.",
            "Replace all values in the original array that are below the threshold with the mean of the array."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def std_dev_minima(arr, window_size, threshold):\n    # Calculate the moving standard deviation of the array elements\n    moving_std = np.array([np.std(arr[i:i+window_size]) for i in range(len(arr)-window_size+1)])\n    \n    # Identify the local minima in the moving standard deviation array\n    local_minima_indices = np.where((np.diff(np.sign(np.diff(moving_std))) > 0))[0] + 1\n    \n    # Create a new array containing the indices of the local minima\n    minima_indices = np.array([i + window_size // 2 for i in local_minima_indices])\n    \n    # Replace all values in the original array that are below the threshold with the mean of the array\n    arr = np.where(arr < threshold, np.mean(arr), arr)\n    \n    return minima_indices, arr"
    },
    {
        "function_name": "group_by_categorical_columns",
        "file_name": "group_by_stats.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list[str]",
            "numerical_columns": "list[str]",
            "aggregation_functions": "dict[str, str]"
        },
        "objectives": [
            "Group the dataframe by the specified categorical columns.",
            "Calculate summary statistics (mean, median, mode, count) for the numerical columns using the specified aggregation functions."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_by_categorical_columns(df, categorical_columns, numerical_columns, aggregation_functions):\n    # Group the dataframe by the specified categorical columns\n    grouped_df = df.groupby(categorical_columns)\n    \n    # Calculate summary statistics for the numerical columns using the specified aggregation functions\n    summary_stats = grouped_df[numerical_columns].agg(aggregation_functions)\n    \n    return summary_stats"
    },
    {
        "function_name": "prepare_clustering_data",
        "file_name": "clustering_prep.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "k": "int"
        },
        "objectives": [
            "Apply Principal Component Analysis (PCA) to reduce the dimensionality of the dataframe to k principal components.",
            "Scale the resulting dataframe using StandardScaler."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.decomposition import PCA",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def prepare_clustering_data(df, k):\n    # Apply PCA to reduce the dimensionality of the dataframe to k principal components\n    pca = PCA(n_components=k)\n    pca_df = pca.fit_transform(df)\n    \n    # Scale the resulting dataframe using StandardScaler\n    scaler = StandardScaler()\n    scaled_df = scaler.fit_transform(pca_df)\n    \n    return pd.DataFrame(scaled_df, columns=[f'PC{i+1}' for i in range(k)])"
    },
    {
        "function_name": "calculate_mode_proportion",
        "file_name": "mode_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Group the dataframe by the id_column and calculate the mode of the categorical_column for each group.",
            "Identify the groups with a mode that occurs more than the specified threshold.",
            "Calculate the proportion of the mode in each identified group.",
            "Create a new column 'mode_proportion' in the dataframe with the calculated proportions."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_mode_proportion(data, id_column, categorical_column, threshold):\n    grouped_data = data.groupby(id_column)\n    mode_values = grouped_data[categorical_column].apply(lambda x: x.mode().iloc[0])\n    mode_counts = grouped_data[categorical_column].apply(lambda x: x.value_counts().iloc[0])\n    filtered_groups = mode_values[mode_counts > threshold].index\n    data['mode_proportion'] = data[id_column].apply(lambda x: mode_counts[x] / len(data[data[id_column] == x]) if x in filtered_groups else 0)\n    return data"
    },
    {
        "function_name": "calculate_event_frequency",
        "file_name": "event_frequency.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "event_column": "str",
            "time_window": "int"
        },
        "objectives": [
            "Convert the time column to datetime format and sort the data by time.",
            "Identify the events that occur within the specified time window for each event type.",
            "Calculate the frequency of each event type within the time window.",
            "Create a new column 'event_frequency' in the dataframe with the calculated frequencies."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_event_frequency(data, time_column, event_column, time_window):\n    data[time_column] = pd.to_datetime(data[time_column])\n    data.sort_values(by=time_column, inplace=True)\n    data['time_diff'] = data[time_column].diff().dt.total_seconds()\n    event_groups = data.groupby(event_column)\n    event_counts = event_groups['time_diff'].apply(lambda x: ((x <= time_window) & (x > 0)).sum())\n    data['event_frequency'] = data[event_column].apply(lambda x: event_counts[x])\n    return data"
    },
    {
        "function_name": "predict_and_analyze_residuals",
        "file_name": "residual_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "feature_columns": "list"
        },
        "objectives": [
            "Create a new column that contains the predicted target values using a linear regression model trained on the feature columns.",
            "Calculate the residuals between the actual and predicted target values.",
            "Create a new column that contains the absolute errors between the actual and predicted target values.",
            "Identify the rows where the absolute error exceeds the interquartile range of the residuals."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import LinearRegression"
        ],
        "function_def": "def predict_and_analyze_residuals(data, target_column, feature_columns):\n    # Create a new column that contains the predicted target values using a linear regression model trained on the feature columns\n    model = LinearRegression()\n    model.fit(data[feature_columns], data[target_column])\n    predicted_values = model.predict(data[feature_columns])\n    data['predicted_values'] = predicted_values\n    \n    # Calculate the residuals between the actual and predicted target values\n    residuals = data[target_column] - data['predicted_values']\n    \n    # Create a new column that contains the absolute errors between the actual and predicted target values\n    abs_errors = abs(residuals)\n    \n    # Identify the rows where the absolute error exceeds the interquartile range of the residuals\n    iqr = abs_errors.quantile(0.75) - abs_errors.quantile(0.25)\n    exceeding_rows = (abs_errors > iqr).values\n    \n    return exceeding_rows"
    },
    {
        "function_name": "predict_class",
        "file_name": "classification.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "feature_columns": "list[str]",
            "threshold": "float"
        },
        "objectives": [
            "Scale the feature columns using StandardScaler.",
            "Train a random forest classifier on the scaled data.",
            "Use the trained model to predict probabilities for the target column.",
            "Create a new column 'predicted_class' with class labels based on the threshold."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def predict_class(df, target_column, feature_columns, threshold):\n    # Scale the feature columns using StandardScaler\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(df[feature_columns])\n    \n    # Train a random forest classifier on the scaled data\n    model = RandomForestClassifier()\n    model.fit(scaled_features, df[target_column])\n    \n    # Use the trained model to predict probabilities for the target column\n    probabilities = model.predict_proba(scaled_features)[:, 1]\n    \n    # Create a new column 'predicted_class' with class labels based on the threshold\n    df['predicted_class'] = (probabilities >= threshold).astype(int)\n    \n    return df"
    },
    {
        "function_name": "calculate_z_scores",
        "file_name": "group_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "Group the data by the specified column.",
            "Calculate the mean and standard deviation of the target column for each group.",
            "Create a new column 'z_score' with the z-scores for each value in the target column.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_z_scores(df, group_column, target_column):\n    # Group the data by the specified column\n    group_stats = df.groupby(group_column)[target_column].agg(['mean', 'std'])\n    \n    # Calculate the z-scores for each value in the target column\n    df['z_score'] = (df[target_column] - group_stats['mean'].loc[df[group_column]].values) / group_stats['std'].loc[df[group_column]].values\n    \n    return df"
    },
    {
        "function_name": "iterative_feature_elimination",
        "file_name": "feature_elimination.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "method": "str",
            "num_iterations": "int"
        },
        "objectives": [
            "Perform iterative feature elimination using the specified method (correlation or mutual information).",
            "Eliminate the least important feature at each iteration and store the eliminated features in a list.",
            "Return the list of eliminated features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import mutual_info_classif",
            "from sklearn.feature_selection import SelectKBest",
            "from sklearn.feature_selection import f_classif"
        ],
        "function_def": "def iterative_feature_elimination(data, target_column, method, num_iterations):\n    # Initialize a list to store the eliminated features\n    eliminated_features = []\n    \n    # Perform iterative feature elimination\n    for _ in range(num_iterations):\n        if method == 'correlation':\n            correlations = data.corrwith(data[target_column]).abs()\n            correlations[target_column] = 0\n            least_important_feature = correlations.idxmin()\n        elif method == 'mutual information':\n            mutual_info = mutual_info_classif(data.drop(target_column, axis=1), data[target_column])\n            least_important_feature = data.drop(target_column, axis=1).columns[np.argmin(mutual_info)]\n        \n        eliminated_features.append(least_important_feature)\n        data.drop(least_important_feature, axis=1, inplace=True)\n    \n    return eliminated_features"
    },
    {
        "function_name": "bin_and_encode",
        "file_name": "data_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "bin_count": "int",
            "category_name": "str"
        },
        "objectives": [
            "Bin the specified column into the specified number of bins.",
            "Calculate the count of rows in each bin.",
            "Create a new column with the binned values.",
            "One-hot encode the binned column and add the encoded columns to the dataframe.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_and_encode(df, column_name, bin_count, category_name):\n    # Bin the specified column into the specified number of bins\n    bins = np.linspace(df[column_name].min(), df[column_name].max(), bin_count)\n    df[category_name] = np.digitize(df[column_name], bins)\n    \n    # Calculate the count of rows in each bin\n    bin_counts = df[category_name].value_counts()\n    \n    # One-hot encode the binned column and add the encoded columns to the dataframe\n    encoded_columns = pd.get_dummies(df[category_name], prefix=category_name)\n    df = pd.concat([df, encoded_columns], axis=1)\n    \n    return df"
    },
    {
        "function_name": "detect_outlier_groups",
        "file_name": "data_quality_check.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Group the dataframe by the group_column.",
            "Calculate the value_column median and interquartile range (IQR) for each group.",
            "Identify the groups with a median value_column value greater than 1.5 IQR away from the overall median.",
            "Create a new column 'outlier_group' and assign True to the rows belonging to the identified groups."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def detect_outlier_groups(df, group_column, value_column, threshold):\n    # Group the dataframe by the group_column\n    group_medians = df.groupby(group_column)[value_column].median()\n    group_iqr = df.groupby(group_column)[value_column].quantile(0.75) - df.groupby(group_column)[value_column].quantile(0.25)\n    \n    # Calculate the value_column median and interquartile range (IQR) for each group\n    overall_median = df[value_column].median()\n    overall_iqr = df[value_column].quantile(0.75) - df[value_column].quantile(0.25)\n    \n    # Identify the groups with a median value_column value greater than 1.5 IQR away from the overall median\n    outlier_groups = group_medians[(np.abs(group_medians - overall_median) > 1.5 * overall_iqr)].index\n    \n    # Create a new column 'outlier_group' and assign True to the rows belonging to the identified groups\n    df['outlier_group'] = np.where(df[group_column].isin(outlier_groups), True, False)\n    \n    return df"
    },
    {
        "function_name": "feature_importance_pruning",
        "file_name": "classification_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "feature_columns": "list",
            "max_depth": "int"
        },
        "objectives": [
            "Train a DecisionTreeClassifier on the data to predict the target_column.",
            "Calculate the feature importances based on the DecisionTreeClassifier.",
            "Prune the DecisionTreeClassifier to a specified max_depth.",
            "Identify the feature_columns with the highest importance scores after pruning."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.tree import DecisionTreeClassifier"
        ],
        "function_def": "def feature_importance_pruning(data, target_column, feature_columns, max_depth):\n    # Train a DecisionTreeClassifier on the data to predict the target_column\n    clf = DecisionTreeClassifier(random_state=42)\n    clf.fit(data[feature_columns], data[target_column])\n    \n    # Calculate the feature importances based on the DecisionTreeClassifier\n    feature_importances = clf.feature_importances_\n    \n    # Prune the DecisionTreeClassifier to a specified max_depth\n    clf_pruned = DecisionTreeClassifier(random_state=42, max_depth=max_depth)\n    clf_pruned.fit(data[feature_columns], data[target_column])\n    \n    # Identify the feature_columns with the highest importance scores after pruning\n    feature_importances_pruned = clf_pruned.feature_importances_\n    feature_importance_df = pd.DataFrame({'feature': feature_columns, 'importance': feature_importances_pruned})\n    feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n    \n    return feature_importance_df"
    },
    {
        "function_name": "group_by_std",
        "file_name": "group_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Group the dataframe by the group_column.",
            "Calculate the standard deviation of each group.",
            "Identify groups with a standard deviation above the threshold.",
            "Create a new column 'group_id' with a unique id for each group."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def group_by_std(df, group_column, threshold):\n    # Group the dataframe by the group_column\n    groups = df.groupby(group_column)\n    \n    # Calculate the standard deviation of each group\n    std_dev = groups.std()\n    \n    # Identify groups with a standard deviation above the threshold\n    valid_groups = std_dev[std_dev > threshold].index\n    \n    # Create a new column 'group_id' with a unique id for each group\n    df['group_id'] = df[group_column].apply(lambda x: x if x in valid_groups else np.nan)\n    \n    return df"
    },
    {
        "function_name": "split_frequency",
        "file_name": "text_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str"
        },
        "objectives": [
            "Split the string in the specified column by comma.",
            "Create new columns for each split string.",
            "Calculate the frequency of each unique string."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_frequency(df, column_name):\n    # Split the string in the specified column by comma\n    df_split = df[column_name].str.split(',', expand=True)\n    \n    # Create new columns for each split string\n    df = pd.concat([df, df_split], axis=1)\n    \n    # Calculate the frequency of each unique string\n    frequency = df_split.stack().value_counts()\n    \n    return df, frequency"
    },
    {
        "function_name": "rank_groups",
        "file_name": "group_ranking.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str"
        },
        "objectives": [
            "Group the dataframe by id_column and calculate the cumulative sum of value_column.",
            "Identify the group with the maximum cumulative sum.",
            "Create a new column 'rank' with the ranking of each group based on the cumulative sum."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rank_groups(df, id_column, value_column):\n    # Group the dataframe by id_column and calculate the cumulative sum of value_column\n    df['cumulative_sum'] = df.groupby(id_column)[value_column].cumsum()\n    \n    # Identify the group with the maximum cumulative sum\n    max_cumulative_sum = df['cumulative_sum'].max()\n    max_group = df[df['cumulative_sum'] == max_cumulative_sum][id_column].iloc[0]\n    \n    # Create a new column 'rank' with the ranking of each group based on the cumulative sum\n    df['rank'] = df[id_column].apply(lambda x: 1 if x == max_group else 2)\n    \n    return df"
    },
    {
        "function_name": "consecutive_row_analysis",
        "file_name": "row_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the average value of the specified column for each group of consecutive rows.",
            "Identify the groups with an average value greater than the specified threshold.",
            "Create a new column that indicates whether each row belongs to a group with an average value greater than the threshold.",
            "Return the resulting dataframe with the new column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def consecutive_row_analysis(df, column_name, threshold):\n    # Calculate the average value of the specified column for each group of consecutive rows\n    df['avg_value'] = df[column_name].rolling(window=10, min_periods=1).mean()\n    \n    # Identify the groups with an average value greater than the specified threshold\n    df['above_threshold'] = df['avg_value'] > threshold\n    \n    # Create a new column that indicates whether each row belongs to a group with an average value greater than the threshold\n    df['in_above_threshold_group'] = df['above_threshold'].rolling(window=10, min_periods=1).max()\n    \n    return df"
    },
    {
        "function_name": "calculate_age",
        "file_name": "date_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "interval": "str ('years', 'months', 'days')"
        },
        "objectives": [
            "Convert the date column to datetime format.",
            "Calculate the age of the records in the specified interval.",
            "Create a new column with the calculated age.",
            "Bucket the age into categories based on the specified interval and create a new column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_age(df, date_column, interval):\n    # Step 1: Convert the date column to datetime format\n    df[date_column] = pd.to_datetime(df[date_column])\n    \n    # Step 2: Calculate the age of the records in the specified interval\n    if interval == 'years':\n        age = (pd.Timestamp('today') - df[date_column]).dt.days / 365.25\n    elif interval == 'months':\n        age = (pd.Timestamp('today') - df[date_column]).dt.days / 30.44\n    elif interval == 'days':\n        age = (pd.Timestamp('today') - df[date_column]).dt.days\n    \n    # Step 3: Create a new column with the calculated age\n    df['age'] = age\n    \n    # Step 4: Bucket the age into categories based on the specified interval\n    if interval == 'years':\n        df['age_category'] = pd.cut(df['age'], bins=[0, 10, 20, 30, 40, 50, 60, 100], labels=['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61+'])\n    elif interval == 'months':\n        df['age_category'] = pd.cut(df['age'], bins=[0, 6, 12, 24, 36, 48, 60, 120], labels=['0-6', '7-12', '13-24', '25-36', '37-48', '49-60', '61+'])\n    elif interval == 'days':\n        df['age_category'] = pd.cut(df['age'], bins=[0, 30, 60, 90, 120, 150, 180, 365], labels=['0-30', '31-60', '61-90', '91-120', '121-150', '151-180', '181+'])\n    \n    return df"
    },
    {
        "function_name": "calculate_lagged_values_and_differences",
        "file_name": "time_series.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "lag": "int"
        },
        "objectives": [
            "Calculate the lagged values of the target column.",
            "Create a new column with the lagged values.",
            "Calculate the first-order differences of the target column.",
            "Create a new column with the first-order differences."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_lagged_values_and_differences(df, target_column, lag):\n    # Step 1: Calculate the lagged values of the target column\n    df['lagged_value'] = df[target_column].shift(lag)\n    \n    # Step 2: Calculate the first-order differences of the target column\n    df['first_order_difference'] = df[target_column] - df[target_column].shift(1)\n    \n    return df"
    },
    {
        "function_name": "mark_top_categories",
        "file_name": "categorical_encoding.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_cols": "list",
            "threshold": "int"
        },
        "objectives": [
            "Identify categorical columns in the data with more than 'threshold' unique values.",
            "Create a new column for each identified categorical column with a prefix 'is_'.",
            "In each new column, mark the top 'threshold' categories as 'True' based on their frequency.",
            "Return the updated DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def mark_top_categories(data, categorical_cols, threshold):\n    for col in categorical_cols:\n        if data[col].nunique() > threshold:\n            top_cats = data[col].value_counts().head(threshold).index\n            data[f'is_{col}'] = data[col].isin(top_cats)\n    return data"
    },
    {
        "function_name": "suppress_outliers",
        "file_name": "matrix_operations.py",
        "parameters": {
            "matrix": "numpy.ndarray",
            "threshold": "float"
        },
        "objectives": [
            "Identify the top 'threshold' percent of values in the matrix.",
            "Replace the identified values with their 90th percentile.",
            "Return the updated matrix."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def suppress_outliers(matrix, threshold):\n    percentile = np.percentile(matrix, threshold * 100)\n    matrix[matrix > percentile] = np.percentile(matrix, 90)\n    return matrix"
    },
    {
        "function_name": "impute_missing_values",
        "file_name": "imputation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "n_neighbors": "int",
            "feature_cols": "list"
        },
        "objectives": [
            "Scale the data using the StandardScaler.",
            "Perform k-Nearest Neighbors (k-NN) imputation for missing values in the feature_cols columns.",
            "Return the updated DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.impute import KNNImputer"
        ],
        "function_def": "def impute_missing_values(data, n_neighbors, feature_cols):\n    scaler = StandardScaler()\n    data[feature_cols] = scaler.fit_transform(data[feature_cols])\n    imputer = KNNImputer(n_neighbors=n_neighbors)\n    data[feature_cols] = imputer.fit_transform(data[feature_cols])\n    return data"
    },
    {
        "function_name": "lasso_regularization",
        "file_name": "regularization.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_column": "str",
            "target_column": "str",
            "alpha": "float"
        },
        "objectives": [
            "Perform LASSO regularization on the feature column to select the most important features.",
            "Calculate the coefficient of each feature.",
            "Create a new column with the predicted target value based on the selected features.",
            "Return the updated dataframe with the predicted target values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import Lasso"
        ],
        "function_def": "def lasso_regularization(data, feature_column, target_column, alpha):\n    # Perform LASSO regularization on the feature column to select the most important features\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(data[feature_column], data[target_column])\n    coefficients = lasso.coef_\n    \n    # Calculate the coefficient of each feature\n    feature_importances = pd.DataFrame({'feature': feature_column, 'coefficient': coefficients})\n    feature_importances = feature_importances.sort_values('coefficient', ascending=False)\n    \n    # Create a new column with the predicted target value based on the selected features\n    data['predicted_target'] = lasso.predict(data[feature_column])\n    \n    return data"
    },
    {
        "function_name": "vectorize_text",
        "file_name": "text_preprocessing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "text_column": "str",
            "stopwords": "list",
            "ngram_range": "tuple",
            "max_features": "int"
        },
        "objectives": [
            "Tokenize the text column and remove stopwords.",
            "Vectorize the text data using TF-IDF with n-grams.",
            "Select the top features based on the TF-IDF scores.",
            "Return the vectorized data and the top features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from nltk.corpus import stopwords"
        ],
        "function_def": "def vectorize_text(data, text_column, stopwords, ngram_range, max_features):\n    # Tokenize the text column and remove stopwords\n    data[text_column] = data[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n    \n    # Vectorize the text data using TF-IDF with n-grams\n    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n    vectorized_data = vectorizer.fit_transform(data[text_column])\n    \n    # Select the top features based on the TF-IDF scores\n    top_features = vectorizer.get_feature_names_out()\n    \n    return vectorized_data, top_features"
    },
    {
        "function_name": "one_hot_encode",
        "file_name": "categorical_data_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list",
            "target_column": "str"
        },
        "objectives": [
            "One-hot encode the categorical columns.",
            "Calculate the mutual information between the encoded columns and the target column.",
            "Select the top features based on the mutual information scores.",
            "Return the encoded data and the top features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import mutual_info_classif"
        ],
        "function_def": "def one_hot_encode(data, categorical_columns, target_column):\n    # One-hot encode the categorical columns\n    encoded_data = pd.get_dummies(data, columns=categorical_columns)\n    \n    # Calculate the mutual information between the encoded columns and the target column\n    mutual_info = mutual_info_classif(encoded_data, data[target_column])\n    \n    # Select the top features based on the mutual information scores\n    top_features = encoded_data.columns[mutual_info.argsort()[-len(categorical_columns):]]\n    \n    return encoded_data, top_features"
    },
    {
        "function_name": "clustering",
        "file_name": "unsupervised_learning.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_column": "str",
            "dist_metric": "str (euclidean or cosine)"
        },
        "objectives": [
            "Perform K-Means clustering with K=5 to group the data points based on the specified feature column.",
            "Calculate the silhouette coefficient for each cluster to evaluate the clustering quality.",
            "Identify the cluster with the highest silhouette coefficient and create a new column with the assigned cluster labels.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans",
            "from sklearn.metrics import silhouette_score"
        ],
        "function_def": "def clustering(data, feature_column, dist_metric):\n    # Perform K-Means clustering\n    if dist_metric == 'euclidean':\n        kmeans = KMeans(n_clusters=5)\n    elif dist_metric == 'cosine':\n        kmeans = KMeans(n_clusters=5, init='k-means++', metric='cosine')\n    else:\n        raise ValueError(\"Invalid distance metric. Please choose 'euclidean' or 'cosine'.\")\n    cluster_labels = kmeans.fit_predict(data[[feature_column]])\n    \n    # Calculate the silhouette coefficient for each cluster\n    silhouette = silhouette_score(data[[feature_column]], cluster_labels)\n    \n    # Identify the cluster with the highest silhouette coefficient\n    best_cluster = np.argmax(silhouette)\n    \n    # Create a new column with the assigned cluster labels\n    data['cluster_label'] = cluster_labels\n    \n    return data"
    },
    {
        "function_name": "customer_purchase_analysis",
        "file_name": "customer_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "time_column": "str",
            "product_column": "str",
            "k": "int"
        },
        "objectives": [
            "Filter the data to include only rows where the product is in the top k most frequently purchased products.",
            "Calculate the average time interval between purchases for each customer.",
            "Group the data by customer and calculate the total amount spent by each customer.",
            "Return a dataframe with the customer id, average time interval, and total amount spent."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def customer_purchase_analysis(data, id_column, time_column, product_column, k):\n    # Filter the data to include only rows where the product is in the top k most frequently purchased products\n    top_products = data[product_column].value_counts().head(k).index\n    filtered_data = data[data[product_column].isin(top_products)]\n    \n    # Calculate the average time interval between purchases for each customer\n    filtered_data[time_column] = pd.to_datetime(filtered_data[time_column])\n    filtered_data.sort_values(by=[id_column, time_column], inplace=True)\n    filtered_data['time_interval'] = filtered_data.groupby(id_column)[time_column].diff()\n    average_time_interval = filtered_data.groupby(id_column)['time_interval'].mean().reset_index()\n    \n    # Group the data by customer and calculate the total amount spent by each customer\n    total_spent = filtered_data.groupby(id_column)['amount'].sum().reset_index()\n    \n    # Return a dataframe with the customer id, average time interval, and total amount spent\n    result = pd.merge(average_time_interval, total_spent, on=id_column)\n    result = result[[id_column, 'time_interval', 'amount']]\n    result.rename(columns={'amount': 'total_spent'}, inplace=True)\n    return result"
    },
    {
        "function_name": "percentage_change_analysis",
        "file_name": "percentage_change.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str"
        },
        "objectives": [
            "Calculate the cumulative sum of the value column for each id.",
            "Calculate the percentage change of the cumulative sum for each id.",
            "Group the data by id and calculate the average percentage change for each id.",
            "Return a dataframe with the id and average percentage change."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def percentage_change_analysis(data, id_column, value_column):\n    # Calculate the cumulative sum of the value column for each id\n    data['cumulative_sum'] = data.groupby(id_column)[value_column].cumsum()\n    \n    # Calculate the percentage change of the cumulative sum for each id\n    data['percentage_change'] = data.groupby(id_column)['cumulative_sum'].pct_change()\n    \n    # Group the data by id and calculate the average percentage change for each id\n    average_percentage_change = data.groupby(id_column)['percentage_change'].mean().reset_index()\n    \n    return average_percentage_change"
    },
    {
        "function_name": "decision_tree_regressor",
        "file_name": "modeling.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_columns": "list[str]",
            "target_column": "str",
            "max_tree_depth": "int"
        },
        "objectives": [
            "Split the data into training and testing sets (e.g., 80% for training).",
            "Train a decision tree regressor on the training set to predict the target column.",
            "Prune the decision tree to a specified maximum depth.",
            "Use the pruned decision tree to predict the target column on the testing set.",
            "Return the predicted values and the feature importances."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.tree import DecisionTreeRegressor"
        ],
        "function_def": "def decision_tree_regressor(data, feature_columns, target_column, max_tree_depth):\n    # Step 1: Split the data into training and testing sets\n    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n    \n    # Step 2: Train a decision tree regressor on the training set\n    tree = DecisionTreeRegressor(max_depth=max_tree_depth, random_state=42)\n    tree.fit(train_data[feature_columns], train_data[target_column])\n    \n    # Step 3: Predict the target column on the testing set\n    predictions = tree.predict(test_data[feature_columns])\n    \n    # Step 4: Get the feature importances\n    importances = tree.feature_importances_\n    \n    return predictions, importances"
    },
    {
        "function_name": "rolling_sum_with_resampling",
        "file_name": "rolling_resampling.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "id_column": "str",
            "min_observations": "int",
            "frequency": "str"
        },
        "objectives": [
            "Group the dataframe by the id_column and date_column.",
            "Calculate the rolling sum of each group's values until min_observations is reached.",
            "Resample the dataframe to the specified frequency and fill missing values using interpolation.",
            "Calculate the rolling sum of each group's resampled values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_sum_with_resampling(data, date_column, id_column, min_observations, frequency):\n    data[date_column] = pd.to_datetime(data[date_column])\n    data.set_index(date_column, inplace=True)\n    data['rolling_sum'] = data.groupby(id_column).transform(lambda x: x.rolling(min_observations, min_periods=1).sum())\n    resampled_data = data.resample(frequency).interpolate(method='linear')\n    resampled_data['resampled_rolling_sum'] = resampled_data.groupby(id_column)['rolling_sum'].transform(lambda x: x.rolling(3).sum())\n    return resampled_data"
    },
    {
        "function_name": "ewma_and_ewstd",
        "file_name": "exponential_smoothing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "feature_column": "str",
            "alpha": "float"
        },
        "objectives": [
            "Calculate the exponentially weighted moving average of the feature_column for each id group.",
            "Calculate the exponentially weighted moving standard deviation of the feature_column for each id group.",
            "Return the calculated averages and standard deviations."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def ewma_and_ewstd(df, id_column, feature_column, alpha):\n    df['ewma'] = df.groupby(id_column)[feature_column].transform(lambda x: x.ewm(alpha=alpha, adjust=False).mean())\n    df['ewstd'] = df.groupby(id_column)[feature_column].transform(lambda x: x.ewm(alpha=alpha, adjust=False).std())\n    return df[['ewma', 'ewstd']]"
    },
    {
        "function_name": "discretize_numerical_column",
        "file_name": "data_transformation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numerical_column": "str",
            "bins": "int"
        },
        "objectives": [
            "Discretize the numerical column into bins using the KBinsDiscretizer.",
            "Create a new column 'discretized' in the dataframe, which contains the discretized values of the numerical column.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import KBinsDiscretizer"
        ],
        "function_def": "def discretize_numerical_column(data, numerical_column, bins):\n    discretizer = KBinsDiscretizer(n_bins=bins, encode='ordinal')\n    discretized_values = discretizer.fit_transform(data[[numerical_column]])\n    \n    data['discretized'] = discretized_values\n    \n    return data"
    },
    {
        "function_name": "select_frequent_values",
        "file_name": "data_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "For each unique value in the categorical column, calculate the frequency.",
            "Select the top N unique values with the highest frequency, where N is determined by the threshold (e.g., 0.8 for top 80%).",
            "Create a new column 'frequent_values' in the dataframe, which contains the top N unique values.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def select_frequent_values(df, categorical_column, threshold):\n    frequencies = df[categorical_column].value_counts(normalize=True)\n    cumulative_frequency = frequencies.cumsum()\n    top_n = cumulative_frequency[cumulative_frequency <= threshold].index\n    \n    df['frequent_values'] = df.apply(lambda row: row[categorical_column] if row[categorical_column] in top_n else 'OTHER', axis=1)\n    \n    return df"
    },
    {
        "function_name": "scale_columns",
        "file_name": "data_normalization.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "columns_to_scale": "list[str]"
        },
        "objectives": [
            "Scale the specified columns using the Min-Max Scaler.",
            "Create a new dataframe 'scaled_data' which contains the scaled values of the specified columns.",
            "Return the scaled dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def scale_columns(data, columns_to_scale):\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(data[columns_to_scale])\n    \n    scaled_data = pd.DataFrame(scaled_values, columns=columns_to_scale)\n    \n    return scaled_data"
    },
    {
        "function_name": "identify_significant_categories",
        "file_name": "statistical_tests.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the chi-squared statistic and p-value for the categorical column.",
            "Identify the categories with a p-value below the specified threshold.",
            "Create a new column 'significant_categories' and assign a binary value indicating whether each category is significant.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import chi2_contingency"
        ],
        "function_def": "def identify_significant_categories(data, categorical_column, threshold):\n    categories = data[categorical_column].unique()\n    chi_squared_stats = []\n    p_values = []\n    \n    for category in categories:\n        category_data = pd.crosstab(data[categorical_column], pd.Series([1] * len(data)))\n        chi_squared_stat, p_value, _, _ = chi2_contingency(category_data)\n        chi_squared_stats.append(chi_squared_stat)\n        p_values.append(p_value)\n    \n    significant_categories = [category for category, p_value in zip(categories, p_values) if p_value < threshold]\n    \n    data['significant_categories'] = data[categorical_column].apply(lambda x: 1 if x in significant_categories else 0)\n    \n    return data"
    },
    {
        "function_name": "group_proportions",
        "file_name": "groupby_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "threshold": "int",
            "min_group_size": "int"
        },
        "objectives": [
            "Group the dataframe by the specified column and calculate the size of each group.",
            "Filter out groups with size less than the specified min_group_size.",
            "Identify groups with size above the specified threshold.",
            "Calculate the proportion of each identified group with respect to the total data size.",
            "Return the resulting dataframe with the group names, their sizes, and their proportions."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_proportions(df, group_column, threshold, min_group_size):\n    # Group the dataframe by the specified column and calculate the size of each group\n    group_sizes = df.groupby(group_column).size().to_frame('size')\n    \n    # Filter out groups with size less than the specified min_group_size\n    filtered_groups = group_sizes[group_sizes['size'] >= min_group_size]\n    \n    # Identify groups with size above the specified threshold\n    identified_groups = filtered_groups[filtered_groups['size'] > threshold].index\n    \n    # Calculate the proportion of each identified group with respect to the total data size\n    proportions = (filtered_groups.loc[identified_groups, 'size'] / len(df)).to_frame('proportion')\n    \n    # Return the resulting dataframe with the group names, their sizes, and their proportions\n    return pd.merge(proportions, filtered_groups, left_index=True, right_index=True)"
    },
    {
        "function_name": "identify_points_by_silhouette",
        "file_name": "clustering.py",
        "parameters": {
            "data": "numpy.ndarray",
            "num_clusters": "int",
            "max_iter": "int"
        },
        "objectives": [
            "Perform K-means clustering on the data with the specified number of clusters.",
            "Calculate the silhouette coefficient for each data point.",
            "Identify the points with negative silhouette coefficient.",
            "Return the points with their corresponding cluster labels and silhouette coefficients."
        ],
        "import_lines": [
            "from sklearn.cluster import KMeans",
            "from sklearn.metrics import silhouette_score, silhouette_samples"
        ],
        "function_def": "def identify_points_by_silhouette(data, num_clusters, max_iter):\n    # Perform K-means clustering on the data with the specified number of clusters\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=max_iter)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Calculate the silhouette coefficient for each data point\n    silhouette_coeff = silhouette_samples(data, cluster_labels)\n    \n    # Identify the points with negative silhouette coefficient\n    identified_points = data[silhouette_coeff < 0]\n    \n    # Return the points with their corresponding cluster labels and silhouette coefficients\n    return identified_points, cluster_labels[silhouette_coeff < 0], silhouette_coeff[silhouette_coeff < 0]"
    },
    {
        "function_name": "periodic_trend_detection",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str"
        },
        "objectives": [
            "Detect the periodic trend in the time column using Fast Fourier Transform (FFT).",
            "Extract the top 3 frequencies with the highest amplitude.",
            "Create a new feature for each extracted frequency.",
            "Return the updated dataframe with the new features."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def periodic_trend_detection(df, time_column):\n    # Apply FFT to detect periodic trend\n    fft = np.fft.fft(df[time_column])\n    frequencies = np.fft.fftfreq(len(df), d=1.0)\n    amplitudes = np.abs(fft)\n    \n    # Extract top 3 frequencies with the highest amplitude\n    top_frequencies = frequencies[np.argsort(amplitudes)[-3:]]\n    \n    # Create a new feature for each extracted frequency\n    for frequency in top_frequencies:\n        df[f'frequency_{frequency}'] = df[time_column] * np.cos(2 * np.pi * frequency * np.arange(len(df)))\n    \n    return df"
    },
    {
        "function_name": "categorical_feature_reduction",
        "file_name": "category_reduction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Apply the Label Encoder to the specified categorical column.",
            "Calculate the correlation between the encoded column and the target variable.",
            "Select the top 25% of categories with the highest correlation.",
            "Group the remaining categories into a single category."
        ],
        "import_lines": [
            "from sklearn.preprocessing import LabelEncoder",
            "import pandas as pd"
        ],
        "function_def": "def categorical_feature_reduction(df, categorical_column, threshold):\n    # Apply Label Encoder to the specified categorical column\n    encoder = LabelEncoder()\n    encoded_column = encoder.fit_transform(df[categorical_column])\n    \n    # Calculate correlation\n    correlation = np.corrcoef(encoded_column, df['target'])[0, 1]\n    correlations = [correlation] * len(df[categorical_column].unique())\n    \n    # Select top 25% of categories with the highest correlation\n    top_categories = np.argsort(correlations)[-int(0.25*len(correlations)):]\n    \n    # Group the remaining categories into a single category\n    grouped_df = df.copy()\n    grouped_df[categorical_column] = np.where(grouped_df[categorical_column].isin(top_categories), grouped_df[categorical_column], 'Others')\n    \n    return grouped_df"
    },
    {
        "function_name": "skewness_correction",
        "file_name": "skewness_correction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "alpha": "float"
        },
        "objectives": [
            "Apply the box-cox transformation to the specified column.",
            "Calculate the skewness of the transformed column.",
            "If the absolute value of skewness is greater than the specified alpha, apply the inverse box-cox transformation.",
            "Return the updated dataframe with the transformed column."
        ],
        "import_lines": [
            "from scipy.special import boxcox1p",
            "import pandas as pd"
        ],
        "function_def": "def skewness_correction(df, column_name, alpha):\n    # Apply box-cox transformation\n    df[column_name] = boxcox1p(df[column_name], 0.5)\n    \n    # Calculate skewness\n    skewness = df[column_name].skew()\n    \n    # If absolute value of skewness is greater than alpha, apply inverse box-cox transformation\n    if np.abs(skewness) > alpha:\n        df[column_name] = (df[column_name]**2 - 1) / 2\n    \n    return df"
    },
    {
        "function_name": "binning_with_outlier_handling",
        "file_name": "binning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "bins": "int",
            "strategy": "str"
        },
        "objectives": [
            "Bin the values in the specified column into the specified number of bins.",
            "Apply the specified strategy to handle outliers (either 'clip' or 'remove').",
            "Calculate the mean and standard deviation of the binned values.",
            "Create a new dataframe with the binned values, means, and standard deviations."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def binning_with_outlier_handling(df, column_name, bins, strategy):\n    # Bin the values in the specified column into the specified number of bins\n    binned_df = df[column_name].value_counts(bins=bins, sort=False)\n    \n    # Apply the specified strategy to handle outliers\n    if strategy == 'clip':\n        binned_df = binned_df.clip(lower=binned_df.quantile(0.01), upper=binned_df.quantile(0.99))\n    elif strategy == 'remove':\n        binned_df = binned_df[(binned_df > binned_df.quantile(0.01)) & (binned_df < binned_df.quantile(0.99))]\n    \n    # Calculate the mean and standard deviation of the binned values\n    mean_values = binned_df.mean()\n    std_values = binned_df.std()\n    \n    # Create a new dataframe with the binned values, means, and standard deviations\n    result_df = pd.DataFrame({'binned_values': binned_df, 'mean': mean_values, 'std': std_values})\n    \n    return result_df"
    },
    {
        "function_name": "aggregate_data_by_period",
        "file_name": "data_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str",
            "aggregation_method": "str ('mean', 'median', or 'sum')"
        },
        "objectives": [
            "Convert the date column to datetime format and extract the year and month.",
            "Group the data by the id column and the extracted year and month.",
            "Apply the specified aggregation method to the grouped data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_data_by_period(df, id_column, date_column, aggregation_method):\n    # Convert the date column to datetime format and extract the year and month\n    df[date_column] = pd.to_datetime(df[date_column])\n    df['year'] = df[date_column].dt.year\n    df['month'] = df[date_column].dt.month\n    \n    # Group the data by the id column and the extracted year and month\n    grouped_data = df.groupby([id_column, 'year', 'month'])\n    \n    # Apply the specified aggregation method to the grouped data\n    if aggregation_method == 'mean':\n        aggregated_data = grouped_data.mean()\n    elif aggregation_method == 'median':\n        aggregated_data = grouped_data.median()\n    elif aggregation_method == 'sum':\n        aggregated_data = grouped_data.sum()\n    else:\n        raise ValueError(\"Invalid aggregation method.\")\n    \n    return aggregated_data"
    },
    {
        "function_name": "label_frequent_categories",
        "file_name": "category_labeling.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the frequency of each category in the categorical_column.",
            "Identify the categories with a frequency above the threshold.",
            "Create a new column 'category_label' with a label (0 or 1) for each row, indicating whether the category of the row is above the threshold.",
            "Create a new column 'category_probability' with the probability of each category."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def label_frequent_categories(data, categorical_column, threshold):\n    # Calculate the frequency of each category in the categorical_column\n    category_freq = data[categorical_column].value_counts(normalize=True)\n    \n    # Identify the categories with a frequency above the threshold\n    frequent_categories = category_freq[category_freq > threshold].index\n    \n    # Create a new column 'category_label' with a label (0 or 1) for each row, indicating whether the category of the row is above the threshold\n    data['category_label'] = data[categorical_column].apply(lambda x: 1 if x in frequent_categories else 0)\n    \n    # Create a new column 'category_probability' with the probability of each category\n    data['category_probability'] = data[categorical_column].map(category_freq)\n    \n    return data"
    },
    {
        "function_name": "categorical_feature_importance",
        "file_name": "feature_importance.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "target_column": "str",
            "num_trees": "int"
        },
        "objectives": [
            "Perform categorical encoding using the one-hot encoding scheme.",
            "Use a random forest classifier to model the relationship between the categorical_column and the target_column.",
            "Calculate the feature importance for each category.",
            "Return a dictionary with the feature importance for each category."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.preprocessing import OneHotEncoder"
        ],
        "function_def": "def categorical_feature_importance(data, categorical_column, target_column, num_trees):\n    # Perform one-hot encoding\n    encoder = OneHotEncoder()\n    encoded_data = encoder.fit_transform(data[categorical_column].values.reshape(-1, 1))\n    \n    # Use a random forest classifier to model the relationship\n    model = RandomForestClassifier(n_estimators=num_trees)\n    model.fit(encoded_data, data[target_column])\n    \n    # Calculate the feature importance for each category\n    feature_importance = model.feature_importances_\n    \n    # Return a dictionary with the feature importance for each category\n    return dict(zip(encoder.get_feature_names_out(), feature_importance))"
    },
    {
        "function_name": "sales_analysis",
        "file_name": "sales_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "sales_column": "str"
        },
        "objectives": [
            "Convert the date column to datetime format and extract the year and month.",
            "Group the data by year and month, and calculate the total sales for each group.",
            "Create a new column to indicate whether the total sales for each group is above a certain threshold (e.g., 1000).",
            "Calculate the year-over-year growth rate of sales for each month.",
            "Return the updated dataframe with the growth rates."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def sales_analysis(df, date_column, sales_column):\n    # Convert the date column to datetime format and extract the year and month\n    df['datetime'] = pd.to_datetime(df[date_column])\n    df['year'] = df['datetime'].dt.year\n    df['month'] = df['datetime'].dt.month\n    \n    # Group the data by year and month, and calculate the total sales for each group\n    df['total_sales'] = df.groupby(['year', 'month'])[sales_column].transform('sum')\n    \n    # Create a new column to indicate whether the total sales for each group is above a certain threshold (e.g., 1000)\n    df['above_threshold'] = np.where(df['total_sales'] > 1000, 1, 0)\n    \n    # Calculate the year-over-year growth rate of sales for each month\n    df['growth_rate'] = df.groupby('month')['total_sales'].pct_change() * 100\n    \n    return df"
    },
    {
        "function_name": "keyword_frequency_analysis",
        "file_name": "text_analysis.py",
        "parameters": {
            "text_data": "list",
            "keywords": "list",
            "min_sentence_length": "int",
            "max_sentence_length": "int"
        },
        "objectives": [
            "Remove special characters and punctuation from the text data.",
            "Tokenize the text data into sentences.",
            "Filter out sentences that are shorter than the minimum length or longer than the maximum length.",
            "Count the frequency of each keyword in the remaining sentences."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def keyword_frequency_analysis(text_data, keywords, min_sentence_length, max_sentence_length):\n    # Remove special characters and punctuation from the text data\n    text_data = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in text_data]\n    \n    # Tokenize the text data into sentences\n    sentences = []\n    for text in text_data:\n        sentences.extend(text.split('. '))\n    \n    # Filter out sentences that are shorter than the minimum length or longer than the maximum length\n    filtered_sentences = [sentence for sentence in sentences if min_sentence_length <= len(sentence.split()) <= max_sentence_length]\n    \n    # Count the frequency of each keyword in the remaining sentences\n    keyword_freq = Counter()\n    for sentence in filtered_sentences:\n        for keyword in keywords:\n            if keyword in sentence:\n                keyword_freq[keyword] += 1\n    \n    return keyword_freq"
    },
    {
        "function_name": "category_frequency_mapping",
        "file_name": "categorical_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the frequency of each category in the categorical column.",
            "Identify the categories with frequency less than the specified threshold.",
            "Create a new column that maps the identified categories to a specified label.",
            "Replace the identified categories with the new label in the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def category_frequency_mapping(df, categorical_column, threshold):\n    # Calculate the frequency of each category in the categorical column\n    category_freq = df[categorical_column].value_counts()\n    \n    # Identify the categories with frequency less than the specified threshold\n    low_freq_categories = category_freq[category_freq < threshold].index\n    \n    # Create a new column that maps the identified categories to a specified label\n    df['new_category'] = df[categorical_column].apply(lambda x: 'OTHER' if x in low_freq_categories else x)\n    \n    # Replace the identified categories with the new label in the original dataframe\n    df[categorical_column] = df['new_category']\n    df.drop('new_category', axis=1, inplace=True)\n    \n    return df"
    },
    {
        "function_name": "equal_bucketing",
        "file_name": "data_bucketing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "bucket_column": "str",
            "buckets": "int",
            "distribution_column": "str"
        },
        "objectives": [
            "Create equal-sized buckets for the specified column.",
            "Calculate the cumulative distribution of the values in the distribution column.",
            "Create a new column for the bucket assignment.",
            "Return the dataframe with the bucket assignment."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def equal_bucketing(data, bucket_column, buckets, distribution_column):\n    data[bucket_column] = pd.qcut(data[bucket_column], buckets, duplicates='drop')\n    data['cumulative_distribution'] = data[distribution_column].cumsum() / data[distribution_column].sum()\n    return data"
    },
    {
        "function_name": "calculate_differences",
        "file_name": "data_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "order": "int"
        },
        "objectives": [
            "Calculate the differences of the specified column up to the specified order.",
            "Return a new dataframe with the original column and the calculated differences."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_differences(df, column_name, order):\n    # Calculate the differences of the specified column up to the specified order\n    for i in range(1, order+1):\n        df[f\"{column_name}_diff_{i}\"] = df[column_name].diff(i)\n    \n    return df"
    },
    {
        "function_name": "find_nearest_neighbors",
        "file_name": "similarity_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "feature_columns": "list[str]",
            "k": "int"
        },
        "objectives": [
            "Calculate the similarity between each pair of rows in the feature_columns using cosine similarity.",
            "Identify the k most similar rows for each row.",
            "Create a new column 'nearest_neighbors' with the indices of the k most similar rows for each row."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics.pairwise import cosine_similarity"
        ],
        "function_def": "def find_nearest_neighbors(data, id_column, feature_columns, k):\n    # Step 1: Calculate the similarity between each pair of rows in the feature_columns using cosine similarity\n    similarity_matrix = cosine_similarity(data[feature_columns])\n    \n    # Step 2: Identify the k most similar rows for each row\n    nearest_neighbors = []\n    for i in range(len(data)):\n        nearest_neighbors.append(np.argsort(-similarity_matrix[i])[:k+1])\n    \n    # Step 3: Create a new column 'nearest_neighbors' with the indices of the k most similar rows for each row\n    data['nearest_neighbors'] = nearest_neighbors\n    \n    return data"
    },
    {
        "function_name": "detect_anomalous_transactions",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "customer_data": "pandas.DataFrame",
            "transaction_column": "str",
            "amount_column": "str",
            "time_column": "str",
            "time_window": "int"
        },
        "objectives": [
            "Group the customer data by the transaction ID and calculate the total transaction amount for each group.",
            "Calculate the average transaction amount for each group within a specified time window.",
            "Identify customers with transaction amounts exceeding 3 standard deviations from the average transaction amount within the time window.",
            "Return the resulting dataframe with the customer IDs, transaction IDs, and corresponding transaction amounts."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def detect_anomalous_transactions(customer_data, transaction_column, amount_column, time_column, time_window):\n    # Group the customer data by the transaction ID and calculate the total transaction amount for each group\n    customer_data['total_amount'] = customer_data.groupby(transaction_column)[amount_column].transform('sum')\n    \n    # Calculate the average transaction amount for each group within a specified time window\n    customer_data[time_column] = pd.to_datetime(customer_data[time_column])\n    customer_data.set_index(time_column, inplace=True)\n    customer_data['avg_amount'] = customer_data.groupby(transaction_column)['total_amount'].transform(lambda x: x.rolling(time_window).mean())\n    \n    # Identify customers with transaction amounts exceeding 3 standard deviations from the average transaction amount within the time window\n    customer_data['std_dev'] = customer_data.groupby(transaction_column)['total_amount'].transform(lambda x: x.rolling(time_window).std())\n    customer_data['z_score'] = (customer_data['total_amount'] - customer_data['avg_amount']) / customer_data['std_dev']\n    anomalous_transactions = customer_data[customer_data['z_score'] > 3]\n    \n    # Return the resulting dataframe with the customer IDs, transaction IDs, and corresponding transaction amounts\n    return anomalous_transactions[['customer_id', transaction_column, amount_column]].reset_index(drop=True)"
    },
    {
        "function_name": "analyze_network_structure",
        "file_name": "network_analysis.py",
        "parameters": {
            "network_data": "pandas.DataFrame",
            "node_column": "str",
            "edge_column": "str"
        },
        "objectives": [
            "Construct a graph using the network data and calculate the betweenness centrality for each node.",
            "Identify nodes with betweenness centrality scores exceeding 0.5.",
            "Calculate the community structure of the graph using a predefined community detection algorithm.",
            "Return the resulting dataframe with the node IDs, betweenness centrality scores, and community assignments."
        ],
        "import_lines": [
            "import pandas as pd",
            "import networkx as nx",
            "from networkx.algorithms.community import greedy_modularity_communities"
        ],
        "function_def": "def analyze_network_structure(network_data, node_column, edge_column):\n    # Construct a graph using the network data and calculate the betweenness centrality for each node\n    G = nx.Graph()\n    G.add_edges_from(network_data[[node_column, edge_column]].values)\n    betweenness_centrality = nx.betweenness_centrality(G)\n    \n    # Identify nodes with betweenness centrality scores exceeding 0.5\n    important_nodes = [node for node, centrality in betweenness_centrality.items() if centrality > 0.5]\n    \n    # Calculate the community structure of the graph using a predefined community detection algorithm\n    communities = greedy_modularity_communities(G)\n    \n    # Return the resulting dataframe with the node IDs, betweenness centrality scores, and community assignments\n    node_info = pd.DataFrame({'node_id': list(betweenness_centrality.keys()), 'betweenness_centrality': list(betweenness_centrality.values())})\n    node_info['community'] = [next((i for i, community in enumerate(communities) if node in community), None) for node in node_info['node_id']]\n    return node_info"
    },
    {
        "function_name": "identify_volatile_groups",
        "file_name": "group_analytics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "time_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the moving average of the value column for each id group.",
            "Calculate the standard deviation of the moving average for each id group.",
            "Identify id groups where the standard deviation exceeds the specified threshold.",
            "Return the ids of these groups."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_volatile_groups(df, id_column, value_column, time_column, threshold):\n    # Step 1: Calculate the moving average of the value column for each id group\n    df['moving_average'] = df.groupby(id_column)[value_column].transform(lambda x: x.rolling(window=3).mean())\n    \n    # Step 2: Calculate the standard deviation of the moving average for each id group\n    df['std_dev'] = df.groupby(id_column)['moving_average'].transform(lambda x: x.rolling(window=3).std())\n    \n    # Step 3: Identify id groups where the standard deviation exceeds the specified threshold\n    volatile_groups = df.groupby(id_column)['std_dev'].max().gt(threshold)\n    \n    return volatile_groups[volatile_groups].index"
    },
    {
        "function_name": "resample_mean",
        "file_name": "resample_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_column": "str",
            "value_column": "str",
            "freq": "str"
        },
        "objectives": [
            "Resample the dataframe to the specified frequency.",
            "Calculate the mean of the value column for each resampled period.",
            "Create a new column with the mean values and add it to the dataframe.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_mean(df, datetime_column, value_column, freq):\n    # Convert datetime_column to datetime format\n    df[datetime_column] = pd.to_datetime(df[datetime_column])\n    \n    # Set datetime_column as the index of the dataframe\n    df.set_index(datetime_column, inplace=True)\n    \n    # Resample the dataframe to the specified frequency\n    resampled_df = df.resample(freq).mean()\n    \n    return resampled_df"
    },
    {
        "function_name": "calculate_rolling_std_dev",
        "file_name": "rolling_stats.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "time_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "Convert the time column to a datetime object.",
            "Sort the dataframe by the id column and time column.",
            "Calculate the rolling standard deviation of the target column for each group of 'time_window' number of rows.",
            "Return the resulting dataframe with the rolling standard deviation."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_rolling_std_dev(df, id_column, time_column, target_column, time_window):\n    # Convert the time column to a datetime object\n    df[time_column] = pd.to_datetime(df[time_column])\n    \n    # Sort the dataframe by the id column and time column\n    df = df.sort_values(by=[id_column, time_column])\n    \n    # Calculate the rolling standard deviation of the target column for each group of 'time_window' number of rows\n    df['rolling_std_dev'] = df.groupby(id_column)[target_column].transform(lambda x: x.rolling(time_window).std())\n    \n    return df"
    },
    {
        "function_name": "iterative_scaling",
        "file_name": "feature_scaling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "threshold": "float",
            "max_iterations": "int"
        },
        "objectives": [
            "Perform iterative feature scaling on the specified column using the iterative normalization technique.",
            "Calculate the mean and standard deviation of the scaled column.",
            "Apply the Box-Cox transformation to the scaled column if the skewness is greater than a certain threshold.",
            "Return the scaled and transformed column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.stats import boxcox"
        ],
        "function_def": "def iterative_scaling(df, column_name, threshold, max_iterations):\n    # Perform iterative feature scaling using the iterative normalization technique\n    for _ in range(max_iterations):\n        min_val = df[column_name].min()\n        max_val = df[column_name].max()\n        df[column_name] = (df[column_name] - min_val) / (max_val - min_val)\n    \n    # Calculate the mean and standard deviation of the scaled column\n    mean = df[column_name].mean()\n    std_dev = df[column_name].std()\n    \n    # Apply the Box-Cox transformation if the skewness is greater than a certain threshold\n    skewness = df[column_name].skew()\n    if abs(skewness) > threshold:\n        df[column_name], _ = boxcox(df[column_name] + 1)\n    \n    return df[column_name]"
    },
    {
        "function_name": "forecasting",
        "file_name": "time_series_forecasting.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "forecast_column": "str",
            "look_back": "int",
            "forecast_horizon": "int"
        },
        "objectives": [
            "Perform feature engineering using the lagged values of the forecast_column.",
            "Train a machine learning model to predict the forecast_column using the engineered features.",
            "Forecast the future values of the forecast_column for the specified horizon.",
            "Return the forecasted values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.ensemble import RandomForestRegressor"
        ],
        "function_def": "def forecasting(data, forecast_column, look_back, forecast_horizon):\n    # Perform feature engineering using the lagged values of the forecast_column\n    engineered_features = []\n    for i in range(1, look_back + 1):\n        engineered_features.append(data[forecast_column].shift(i))\n    \n    engineered_features = pd.concat(engineered_features, axis=1)\n    \n    # Train a machine learning model to predict the forecast_column using the engineered features\n    X = engineered_features.dropna()\n    y = data[forecast_column].shift(look_back).dropna()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    \n    # Forecast the future values of the forecast_column for the specified horizon\n    forecasted_values = []\n    for i in range(forecast_horizon):\n        forecast = model.predict(engineered_features[:i+look_back].values[-1:]).flatten()\n        forecasted_values.append(forecast)\n    \n    return forecasted_values"
    },
    {
        "function_name": "weather_data_merge",
        "file_name": "weather_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "weather_column": "str",
            "location_column": "str"
        },
        "objectives": [
            "Extract the year and month from the date column.",
            "Merged the data with the weather data from a separate CSV file based on the location and date.",
            "Create a new column that categorizes the weather into different types (rainy, sunny, cloudy, etc.).",
            "Store the merged data with the new weather category column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def weather_data_merge(data, date_column, weather_column, location_column):\n    # Extract the year and month from the date column\n    data['year'] = data[date_column].dt.year\n    data['month'] = data[date_column].dt.month\n    \n    # Load the weather data from a separate CSV file\n    weather_data = pd.read_csv('weather_data.csv')\n    \n    # Merge the data with the weather data\n    merged_data = pd.merge(data, weather_data, on=['location', 'year', 'month'])\n    \n    # Create a new column that categorizes the weather into different types\n    merged_data['weather_category'] = merged_data[weather_column].apply(lambda x: 'rainy' if x > 50 else 'sunny' if x < 20 else 'cloudy')\n    \n    return merged_data"
    },
    {
        "function_name": "item_recommendation",
        "file_name": "recommendation_system.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "user_column": "str",
            "item_column": "str",
            "rating_column": "str",
            "num_recommendations": "int"
        },
        "objectives": [
            "Create a user-item matrix from the data.",
            "Calculate the similarity between users using a collaborative filtering algorithm.",
            "Generate recommendations for each user based on the similarity.",
            "Return the top N recommended items for each user."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.sparse import csr_matrix",
            "from sklearn.neighbors import NearestNeighbors"
        ],
        "function_def": "def item_recommendation(data, user_column, item_column, rating_column, num_recommendations):\n    # Create a user-item matrix from the data\n    matrix = csr_matrix((data[rating_column], (data[user_column], data[item_column])))\n    \n    # Calculate the similarity between users using a collaborative filtering algorithm\n    model = NearestNeighbors(metric='cosine', algorithm='brute')\n    model.fit(matrix)\n    \n    # Generate recommendations for each user based on the similarity\n    recommendations = []\n    for user in data[user_column].unique():\n        distances, indices = model.kneighbors(matrix[user].reshape(1, -1), n_neighbors=num_recommendations+1)\n        recommended_items = data[item_column].iloc[indices.flatten()[1:]]\n        recommendations.append(recommended_items)\n    \n    return recommendations"
    },
    {
        "function_name": "align_sequences",
        "file_name": "sequence_alignment.py",
        "parameters": {
            "seq_data": "pandas.DataFrame",
            "gap_penalty": "int",
            "match_reward": "int",
            "mismatch_penalty": "int",
            "k": "int"
        },
        "objectives": [
            "Perform a pairwise sequence alignment using the Needleman-Wunsch algorithm.",
            "Identify the top k alignments with the highest scores.",
            "Create a new DataFrame with the aligned sequences and their corresponding scores.",
            "Calculate the similarity between the aligned sequences based on their scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def align_sequences(seq_data, gap_penalty, match_reward, mismatch_penalty, k):\n    # Initialize a 2D matrix for dynamic programming\n    seq1 = seq_data.iloc[0]['sequence']\n    seq2 = seq_data.iloc[1]['sequence']\n    m, n = len(seq1), len(seq2)\n    dp_matrix = np.zeros((m+1, n+1))\n    \n    # Fill the dynamic programming matrix\n    for i in range(m+1):\n        for j in range(n+1):\n            if i == 0:\n                dp_matrix[i, j] = gap_penalty * j\n            elif j == 0:\n                dp_matrix[i, j] = gap_penalty * i\n            elif seq1[i-1] == seq2[j-1]:\n                dp_matrix[i, j] = dp_matrix[i-1, j-1] + match_reward\n            else:\n                dp_matrix[i, j] = max(dp_matrix[i-1, j-1] + mismatch_penalty, dp_matrix[i-1, j] + gap_penalty, dp_matrix[i, j-1] + gap_penalty)\n    \n    # Identify the top k alignments\n    alignments = []\n    while len(alignments) < k:\n        max_score = np.max(dp_matrix)\n        max_idx = np.unravel_index(np.argmax(dp_matrix), dp_matrix.shape)\n        alignments.append((max_idx, max_score))\n        dp_matrix[max_idx] = 0\n    \n    # Create a new DataFrame with the aligned sequences and their corresponding scores\n    aligned_seqs = pd.DataFrame(alignments, columns=['alignment', 'score'])\n    aligned_seqs['sequence1'] = seq1\n    aligned_seqs['sequence2'] = seq2\n    \n    # Calculate the similarity between the aligned sequences\n    similarities = []\n    for _, row in aligned_seqs.iterrows():\n        score = row['score']\n        length = max(len(seq1), len(seq2))\n        similarity = score / length\n        similarities.append(similarity)\n    \n    aligned_seqs['similarity'] = similarities\n    \n    return aligned_seqs"
    },
    {
        "function_name": "detect_frequent_events",
        "file_name": "temporal_patterns.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "timestamp_column": "str",
            "event_column": "str"
        },
        "objectives": [
            "Sort the dataframe by the timestamp column and group it by the id column.",
            "Calculate the time difference between consecutive events for each id.",
            "Identify the ids with a time difference greater than a day and less than a week.",
            "Return the ids with their corresponding average time difference."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_frequent_events(data, id_column, timestamp_column, event_column):\n    # Sort the dataframe by the timestamp column and group it by the id column\n    data = data.sort_values(timestamp_column)\n    data_grouped = data.groupby(id_column)\n    \n    # Calculate the time difference between consecutive events for each id\n    data['time_diff'] = data_grouped[timestamp_column].diff().dt.total_seconds() / (60 * 60 * 24)  # Convert to days\n    \n    # Identify the ids with a time difference greater than a day and less than a week\n    data_filtered = data[(data['time_diff'] > 1) & (data['time_diff'] < 7)]\n    \n    # Return the ids with their corresponding average time difference\n    result = data_filtered.groupby(id_column)['time_diff'].mean().reset_index()\n    return result"
    },
    {
        "function_name": "cluster_assignment",
        "file_name": "clustering_models.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "continuous_columns": "list of str",
            "target_column": "str",
            "num_clusters": "int"
        },
        "objectives": [
            "One-hot encode the categorical columns.",
            "Scale the continuous columns using the StandardScaler.",
            "Concatenate the one-hot encoded categorical data and the scaled continuous data.",
            "Perform k-means clustering on the concatenated data.",
            "Assign a cluster label to each sample based on its cluster assignment.",
            "Calculate the silhouette score for each sample."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder, StandardScaler",
            "from sklearn.cluster import KMeans",
            "from sklearn.metrics import silhouette_score"
        ],
        "function_def": "def cluster_assignment(data, categorical_columns, continuous_columns, target_column, num_clusters):\n    # One-hot encode the categorical columns\n    encoder = OneHotEncoder()\n    encoded_data = pd.DataFrame(encoder.fit_transform(data[categorical_columns]).toarray())\n    \n    # Scale the continuous columns\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data[continuous_columns])\n    \n    # Concatenate the one-hot encoded categorical data and the scaled continuous data\n    concatenated_data = pd.concat([encoded_data, pd.DataFrame(scaled_data)], axis=1)\n    \n    # Perform k-means clustering on the concatenated data\n    model = KMeans(n_clusters=num_clusters)\n    cluster_labels = model.fit_predict(concatenated_data)\n    \n    # Assign a cluster label to each sample based on its cluster assignment\n    data['cluster_label'] = cluster_labels\n    \n    # Calculate the silhouette score for each sample\n    silhouette_scores = silhouette_score(concatenated_data, cluster_labels)\n    \n    return data, silhouette_scores"
    },
    {
        "function_name": "filter_and_select_categories",
        "file_name": "category_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float",
            "max_categories": "int"
        },
        "objectives": [
            "Perform a one-hot encoding of the categorical column.",
            "Apply a threshold to the one-hot encoded columns to filter out rare categories.",
            "Select the top k categories with the highest frequency, where k is determined by the max_categories parameter.",
            "Use the selected one-hot encoded columns as new features in the dataframe.",
            "Drop the original categorical column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_and_select_categories(df, categorical_column, threshold, max_categories):\n    # Perform a one-hot encoding of the categorical column\n    onehot_df = pd.get_dummies(df[categorical_column], drop_first=True)\n    \n    # Apply a threshold to the one-hot encoded columns to filter out rare categories\n    filtered_columns = onehot_df.columns[onehot_df.sum() > threshold]\n    filtered_onehot_df = onehot_df[filtered_columns]\n    \n    # Select the top k categories with the highest frequency\n    top_k_columns = filtered_onehot_df.sum().sort_values(ascending=False).head(max_categories).index\n    selected_onehot_df = filtered_onehot_df[top_k_columns]\n    \n    # Use the selected one-hot encoded columns as new features in the dataframe\n    result = pd.concat([df, selected_onehot_df], axis=1)\n    \n    # Drop the original categorical column\n    result = result.drop(categorical_column, axis=1)\n    \n    return result"
    },
    {
        "function_name": "bayesian_smoothing",
        "file_name": "bayesian_smoothing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "beta": "float",
            "alpha": "float"
        },
        "objectives": [
            "Perform a Bayesian smoothing on the categorical column using the beta and alpha parameters.",
            "Calculate the smoothed probability for each category.",
            "Use the smoothed probabilities as new features in the dataframe.",
            "Return the resulting dataframe with the new features."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def bayesian_smoothing(df, categorical_column, beta, alpha):\n    # Perform a Bayesian smoothing on the categorical column\n    counts = df[categorical_column].value_counts()\n    smoothed_probabilities = (counts + beta) / (len(df) + alpha)\n    \n    # Calculate the smoothed probability for each category\n    smoothed_df = pd.DataFrame(smoothed_probabilities).T\n    \n    # Use the smoothed probabilities as new features in the dataframe\n    result = pd.concat([df, smoothed_df], axis=1)\n    \n    return result"
    },
    {
        "function_name": "select_tfidf_features",
        "file_name": "tfidf_selection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "max_features": "int"
        },
        "objectives": [
            "Perform a TF-IDF transformation on the text column.",
            "Select the top k features with the highest TF-IDF scores, where k is determined by the max_features parameter.",
            "Use the selected TF-IDF features as new features in the dataframe.",
            "Return the resulting dataframe with the new features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def select_tfidf_features(df, text_column, max_features):\n    # Perform a TF-IDF transformation on the text column\n    vectorizer = TfidfVectorizer(max_features=max_features)\n    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n    \n    # Select the top k features with the highest TF-IDF scores\n    top_k_features = tfidf_matrix.toarray()\n    \n    # Use the selected TF-IDF features as new features in the dataframe\n    result = pd.concat([df, pd.DataFrame(top_k_features)], axis=1)\n    \n    return result"
    },
    {
        "function_name": "smoothed_distribution",
        "file_name": "group_smoothing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "feature_column": "str",
            "primary_threshold": "float",
            "secondary_threshold": "float"
        },
        "objectives": [
            "Group the input DataFrame `df` by the `group_column` and calculate the distribution of the `feature_column` values for each group.",
            "For each group, filter the `feature_column` values based on the `primary_threshold` and `secondary_threshold` (values between the two thresholds are to be kept).",
            "Apply exponential smoothing on the filtered values of the `feature_column` for each group.",
            "Return the resulting DataFrame containing the smoothed values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def smoothed_distribution(df, group_column, feature_column, primary_threshold, secondary_threshold):\n    df['primary_filtered'] = np.where((df[feature_column] > primary_threshold) & (df[feature_column] < secondary_threshold), df[feature_column], np.nan)\n    smoothed_values = df.groupby(group_column)['primary_filtered'].transform(lambda x: x.ewm(alpha=0.2, adjust=False).mean())\n    result = pd.DataFrame({'group': df[group_column], 'smoothed_values': smoothed_values}, columns=['group', 'smoothed_values'])\n    return result"
    },
    {
        "function_name": "fraud_detection",
        "file_name": "fraud_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "transaction_column": "str",
            "fraud_threshold": "float"
        },
        "objectives": [
            "Identify unique customer ids from the `id_column`.",
            "For each customer id, calculate the running total of the `transaction_column` values.",
            "Check if the running total at any point exceeds the specified `fraud_threshold`.",
            "Create a new column `fraud_ind` that marks 1 when the running total exceeds the `fraud_threshold`.",
            "Return the resulting DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def fraud_detection(df, id_column, transaction_column, fraud_threshold):\n    df['running_total'] = df.groupby(id_column)[transaction_column].cumsum()\n    df['fraud_ind'] = np.where(df['running_total'] > fraud_threshold, 1, 0)\n    return df"
    },
    {
        "function_name": "phrase_freq",
        "file_name": "phrase_extraction.py",
        "parameters": {
            "text_data": "pandas.Series",
            "target_column": "str"
        },
        "objectives": [
            "Convert the `text_data` into lowercase.",
            "Remove all non-alphanumeric characters.",
            "Tokenize the text into individual words.",
            "Identify common phrases using bigram, trigram, and quadgram analysis.",
            "Count the frequency of each common phrase and store it in the target_column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.tokenize import RegexpTokenizer",
            "from itertools import chain",
            "from collections import Counter",
            "from nltk import ngrams"
        ],
        "function_def": "def phrase_freq(text_data, target_column):\n    text_data = text_data.apply(lambda x: x.lower())\n    text_data = text_data.apply(lambda x: ''.join(e for e in x if e.isalnum() or e==' '))\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokenized_data = text_data.apply(tokenizer.tokenize)\n    bigrams = [list(ngrams(x, 2)) for x in tokenized_data]\n    trigrams = [list(ngrams(x, 3)) for x in tokenized_data]\n    quadgrams = [list(ngrams(x, 4)) for x in tokenized_data]\n    phrases = chain.from_iterable(bigrams+trigrams+quadgrams)\n    phrase_count = Counter([' '.join(gram) for gram in phrases])\n    result = pd.DataFrame(list(phrase_count.items()), columns=['common_phrases', target_column])\n    return result"
    },
    {
        "function_name": "information_gain_analysis",
        "file_name": "information_gain.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list[str]",
            "threshold": "float"
        },
        "objectives": [
            "Apply one-hot encoding to the categorical columns.",
            "Calculate the information gain for each categorical column.",
            "Identify the columns that have an information gain above the specified threshold.",
            "Create a new column that is the sum of the identified columns.",
            "Return the updated dataframe with the new column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder",
            "from sklearn.metrics import mutual_info_score"
        ],
        "function_def": "def information_gain_analysis(data, categorical_columns, threshold):\n    # Apply one-hot encoding to the categorical columns\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    encoded_data = pd.DataFrame(encoder.fit_transform(data[categorical_columns]).toarray())\n    \n    # Calculate the information gain for each categorical column\n    info_gain = []\n    for col in categorical_columns:\n        info_gain.append(mutual_info_score(data[col], data[categorical_columns].apply(lambda x: x.astype('category').cat.codes).sum(axis=1)))\n    \n    # Identify the columns that have an information gain above the specified threshold\n    identified_columns = [column for column, gain in zip(categorical_columns, info_gain) if gain > threshold]\n    \n    # Create a new column that is the sum of the identified columns\n    data['identified_sum'] = data[identified_columns].sum(axis=1)\n    \n    return data"
    },
    {
        "function_name": "sequence_analysis",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "sequences": "list",
            "threshold": "int"
        },
        "objectives": [
            "Calculate the length of each sequence.",
            "Filter out sequences with a length less than the specified threshold.",
            "Calculate the frequency of each element in the remaining sequences.",
            "Create a new dictionary with the frequency results."
        ],
        "import_lines": [
            "import collections"
        ],
        "function_def": "def sequence_analysis(sequences, threshold):\n    # Calculate the length of each sequence\n    lengths = [len(sequence) for sequence in sequences]\n    \n    # Filter out sequences with a length less than the specified threshold\n    filtered_sequences = [sequence for sequence, length in zip(sequences, lengths) if length >= threshold]\n    \n    # Calculate the frequency of each element in the remaining sequences\n    frequency = collections.defaultdict(int)\n    for sequence in filtered_sequences:\n        for element in sequence:\n            frequency[element] += 1\n    \n    return dict(frequency)"
    },
    {
        "function_name": "filter_image",
        "file_name": "image_processing.py",
        "parameters": {
            "image_data": "numpy.ndarray",
            "filter_size": "int"
        },
        "objectives": [
            "Apply a Gaussian filter to the image data.",
            "Apply a median filter to the image data.",
            "Calculate the difference between the original image data and the filtered image data.",
            "Return the filtered image data and the difference image."
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.ndimage import gaussian_filter, median_filter"
        ],
        "function_def": "def filter_image(image_data, filter_size):\n    # Apply a Gaussian filter to the image data\n    gaussian_filtered_image = gaussian_filter(image_data, filter_size)\n    \n    # Apply a median filter to the image data\n    median_filtered_image = median_filter(image_data, filter_size)\n    \n    # Calculate the difference between the original image data and the filtered image data\n    difference_image = np.abs(image_data - gaussian_filtered_image)\n    \n    return gaussian_filtered_image, median_filtered_image, difference_image"
    },
    {
        "function_name": "correlate_data",
        "file_name": "statistical_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column1": "str",
            "column2": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the correlation coefficient between column1 and column2.",
            "Create a new column 'correlated' indicating whether the correlation coefficient is greater than the threshold.",
            "Identify and remove rows where the correlation coefficient is not greater than the threshold.",
            "Return the updated dataframe with the correlated data."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def correlate_data(df, column1, column2, threshold):\n    # Calculate the correlation coefficient between column1 and column2\n    correlation_coefficient = np.corrcoef(df[column1], df[column2])[0, 1]\n    \n    # Create a new column 'correlated' indicating whether the correlation coefficient is greater than the threshold\n    df['correlated'] = np.where(correlation_coefficient > threshold, True, False)\n    \n    # Identify and remove rows where the correlation coefficient is not greater than the threshold\n    df = df[df['correlated']]\n    \n    return df"
    },
    {
        "function_name": "event_based_aggregation",
        "file_name": "event_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_col": "str",
            "time_col": "str",
            "value_col": "str",
            "min_events": "int"
        },
        "objectives": [
            "Calculate the time difference between consecutive events for each id.",
            "Calculate the average time difference for each id.",
            "Identify the ids with average time difference less than the specified threshold.",
            "Remove the ids with less than the specified minimum number of events.",
            "Return a dictionary where the keys are the ids and the values are the average time differences."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def event_based_aggregation(data, id_col, time_col, value_col, min_events):\n    # Calculate the time difference between consecutive events for each id\n    data.sort_values(by=[id_col, time_col], inplace=True)\n    data['time_diff'] = data.groupby(id_col)[time_col].diff()\n    \n    # Calculate the average time difference for each id\n    avg_time_diff = data.groupby(id_col)['time_diff'].mean()\n    \n    # Identify the ids with average time difference less than the specified threshold\n    filtered_ids = avg_time_diff[avg_time_diff < 0].index\n    \n    # Remove the ids with less than the specified minimum number of events\n    filtered_ids = filtered_ids[filtered_ids.isin(data[id_col].value_counts()[data[id_col].value_counts() >= min_events].index)]\n    \n    # Return a dictionary where the keys are the ids and the values are the average time differences\n    result = {}\n    for id in filtered_ids:\n        result[id] = avg_time_diff[id]\n    \n    return result"
    },
    {
        "function_name": "keyword_based_sentence_filtering",
        "file_name": "text_filtering.py",
        "parameters": {
            "text_data": "list of str",
            "keyword_list": "list of str",
            "min_length": "int",
            "max_length": "int"
        },
        "objectives": [
            "Tokenize the text data into words.",
            "Remove stopwords from the tokenized words.",
            "Identify the sentences that contain at least one keyword from the keyword list.",
            "Filter the sentences based on the minimum and maximum length.",
            "Return a list of filtered sentences."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.corpus import stopwords",
            "import nltk"
        ],
        "function_def": "def keyword_based_sentence_filtering(text_data, keyword_list, min_length, max_length):\n    # Tokenize the text data into words\n    tokenized_words = [word.split() for word in text_data]\n    \n    # Remove stopwords from the tokenized words\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [[word for word in words if word not in stop_words] for words in tokenized_words]\n    \n    # Identify the sentences that contain at least one keyword from the keyword list\n    keyword_sentences = []\n    for sentence, words in zip(text_data, filtered_words):\n        if any(keyword in words for keyword in keyword_list):\n            keyword_sentences.append(sentence)\n    \n    # Filter the sentences based on the minimum and maximum length\n    filtered_sentences = [sentence for sentence in keyword_sentences if min_length <= len(sentence.split()) <= max_length]\n    \n    return filtered_sentences"
    },
    {
        "function_name": "exponential_smoothing",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "series": "pandas.Series",
            "window_size": "int"
        },
        "objectives": [
            "Apply an Exponential Smoothing (ES) algorithm to the input series.",
            "Calculate the residuals between the original and smoothed series.",
            "Detect anomalies in the residuals based on their distribution (e.g., median absolute deviation).",
            "Return a new series with the smoothed values and anomaly indicators."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def exponential_smoothing(series, window_size):\n    # Apply Exponential Smoothing\n    alpha = 0.2\n    smoothed_series = series.ewm(alpha=alpha, adjust=False).mean()\n    \n    # Calculate residuals\n    residuals = series - smoothed_series\n    \n    # Detect anomalies\n    median = np.median(residuals)\n    mad = np.median([abs(r - median) for r in residuals])\n    anomaly_indicators = [abs(r - median) > 3 * mad for r in residuals]\n    \n    # Return smoothed series and anomaly indicators\n    return pd.DataFrame({'smoothed': smoothed_series, 'anomaly': anomaly_indicators})"
    },
    {
        "function_name": "select_principal_components",
        "file_name": "dimensionality_reduction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_columns": "list",
            "target_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Perform Principal Component Analysis (PCA) on the feature_columns.",
            "Calculate the explained variance ratio for each principal component.",
            "Select the top principal components that explain at least the specified threshold of the total variance.",
            "Project the original data onto the selected principal components.",
            "Return the projected data and the explained variance ratio."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def select_principal_components(df, feature_columns, target_column, threshold):\n    # Perform Principal Component Analysis (PCA) on the feature_columns\n    pca = PCA(n_components=len(feature_columns))\n    pca.fit(df[feature_columns])\n    \n    # Calculate the explained variance ratio for each principal component\n    explained_variance_ratio = pca.explained_variance_ratio_\n    \n    # Select the top principal components that explain at least the specified threshold of the total variance\n    cumulative_variance = np.cumsum(explained_variance_ratio)\n    n_components = np.argmax(cumulative_variance >= threshold) + 1\n    \n    # Project the original data onto the selected principal components\n    projected_data = pca.transform(df[feature_columns])[:, :n_components]\n    \n    return pd.DataFrame(projected_data), explained_variance_ratio"
    },
    {
        "function_name": "group_mean_scoring",
        "file_name": "group_comparison.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "score_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Group the data by the group_column.",
            "Calculate the mean score for each group using the score_column.",
            "Identify the groups with a mean score greater than the threshold.",
            "Return the identified groups and their corresponding mean scores."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_mean_scoring(data, group_column, score_column, threshold):\n    # Group the data by the group_column\n    grouped_data = data.groupby(group_column)\n    \n    # Calculate the mean score for each group using the score_column\n    mean_scores = grouped_data[score_column].mean()\n    \n    # Identify the groups with a mean score greater than the threshold\n    identified_groups = mean_scores[mean_scores > threshold]\n    \n    return identified_groups"
    },
    {
        "function_name": "treatment_effect",
        "file_name": "causal_inference.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "treatment_column": "str",
            "control_column": "str",
            "outcome_column": "str"
        },
        "objectives": [
            "Calculate the treatment effect using the specified treatment and control columns.",
            "Calculate the average treatment effect using the specified outcome column.",
            "Calculate the standard error of the treatment effect.",
            "Return the treatment effect, average treatment effect, and standard error."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.linear_model import LinearRegression"
        ],
        "function_def": "def treatment_effect(df, treatment_column, control_column, outcome_column):\n    # Calculate the treatment effect using the specified treatment and control columns\n    treatment_df = df[df[treatment_column] == 1]\n    control_df = df[df[control_column] == 1]\n    treatment_effect = np.mean(treatment_df[outcome_column]) - np.mean(control_df[outcome_column])\n    \n    # Calculate the average treatment effect using the specified outcome column\n    X = pd.get_dummies(df[treatment_column], drop_first=True)\n    y = df[outcome_column]\n    model = LinearRegression()\n    model.fit(X, y)\n    average_treatment_effect = np.mean(model.predict(pd.get_dummies(df[treatment_column], drop_first=True)))\n    \n    # Calculate the standard error of the treatment effect\n    std_err = np.std(treatment_df[outcome_column]) / np.sqrt(len(treatment_df))\n    \n    # Return the treatment effect, average treatment effect, and standard error\n    return treatment_effect, average_treatment_effect, std_err"
    },
    {
        "function_name": "kernel_svm",
        "file_name": "kernel_methods.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "target_column": "str",
            "feature_columns": "list[str]",
            "kernel_type": "str"
        },
        "objectives": [
            "One-hot encode the categorical column.",
            "Calculate the kernel matrix using the specified kernel type.",
            "Train a kernel support vector machine (SVM) using the kernel matrix and the specified feature columns.",
            "Return the trained SVM model and the predicted values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.svm import SVC",
            "from sklearn.preprocessing import OneHotEncoder",
            "from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel"
        ],
        "function_def": "def kernel_svm(df, categorical_column, target_column, feature_columns, kernel_type):\n    # One-hot encode the categorical column\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    encoded_data = pd.get_dummies(df[categorical_column], drop_first=True)\n    \n    # Calculate the kernel matrix using the specified kernel type\n    if kernel_type == 'rbf':\n        kernel_matrix = rbf_kernel(encoded_data)\n    elif kernel_type == 'polynomial':\n        kernel_matrix = polynomial_kernel(encoded_data)\n    \n    # Train a kernel support vector machine (SVM) using the kernel matrix and the specified feature columns\n    model = SVC(kernel='precomputed')\n    model.fit(kernel_matrix, df[target_column])\n    \n    # Return the trained SVM model and the predicted values\n    return model, model.predict(kernel_matrix)"
    },
    {
        "function_name": "random_forest",
        "file_name": "ensemble_methods.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_columns": "list[str]",
            "target_column": "str",
            "max_depth": "int"
        },
        "objectives": [
            "Train a random forest model using the specified feature columns and target column.",
            "Calculate the feature importances using the trained model.",
            "Return the trained model and the feature importances."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.metrics import accuracy_score"
        ],
        "function_def": "def random_forest(df, feature_columns, target_column, max_depth):\n    # Train a random forest model using the specified feature columns and target column\n    model = RandomForestClassifier(max_depth=max_depth)\n    model.fit(df[feature_columns], df[target_column])\n    \n    # Calculate the feature importances using the trained model\n    feature_importances = model.feature_importances_\n    \n    # Return the trained model and the feature importances\n    return model, feature_importances"
    },
    {
        "function_name": "extract_time_features",
        "file_name": "temporal_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str"
        },
        "objectives": [
            "Convert the time column to a datetime column and extract the year, month, and day from the time column.",
            "Create separate columns for the year, month, and day.",
            "Calculate the number of days between consecutive dates.",
            "Create a new dataframe containing the extracted time features.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def extract_time_features(df, time_column):\n    # Convert the time column to a datetime column and extract the year, month, and day from the time column\n    df[time_column] = pd.to_datetime(df[time_column])\n    df['year'] = df[time_column].dt.year\n    df['month'] = df[time_column].dt.month\n    df['day'] = df[time_column].dt.day\n    \n    # Calculate the number of days between consecutive dates\n    df['days_diff'] = (df[time_column] - df[time_column].shift()).dt.days\n    \n    # Create a new dataframe containing the extracted time features\n    time_features = df[['year', 'month', 'day', 'days_diff']]\n    \n    return time_features"
    },
    {
        "function_name": "train_gb_classifier",
        "file_name": "model_building.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_columns": "list[str]",
            "target_column": "str"
        },
        "objectives": [
            "Scale the feature columns using RobustScaler.",
            "Select features with a correlation coefficient greater than 0.5 with the target column.",
            "Train a gradient boosting classifier on the selected features and the target column.",
            "Return the trained classifier and the feature coefficients."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import RobustScaler",
            "from sklearn.ensemble import GradientBoostingClassifier",
            "from sklearn.feature_selection import SelectKBest",
            "from sklearn.feature_selection import f_classif"
        ],
        "function_def": "def train_gb_classifier(df, feature_columns, target_column):\n    # Scale the feature columns using RobustScaler\n    scaler = RobustScaler()\n    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n    \n    # Select features with a correlation coefficient greater than 0.5 with the target column\n    selector = SelectKBest(score_func=f_classif, k=5)\n    selector.fit(df[feature_columns], df[target_column])\n    \n    # Train a gradient boosting classifier on the selected features and the target column\n    clf = GradientBoostingClassifier(n_estimators=100)\n    clf.fit(df[feature_columns].loc[:, selector.get_support()], df[target_column])\n    \n    # Return the trained classifier and the feature coefficients\n    return clf, selector.scores_"
    },
    {
        "function_name": "event_duration_statistics",
        "file_name": "event_duration.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "event_column": "str",
            "time_column": "str"
        },
        "objectives": [
            "Calculate the duration between consecutive events for each entity (identified by the id_column).",
            "Identify the maximum and minimum duration for each entity.",
            "Calculate the average and standard deviation of the durations for each entity.",
            "Return a new dataframe with the calculated statistics."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def event_duration_statistics(data, id_column, event_column, time_column):\n    # Calculate the duration between consecutive events for each entity\n    data['next_time'] = data.groupby(id_column)[time_column].shift(-1)\n    data['duration'] = data['next_time'] - data[time_column]\n    \n    # Identify the maximum and minimum duration for each entity\n    data['max_duration'] = data.groupby(id_column)['duration'].transform('max')\n    data['min_duration'] = data.groupby(id_column)['duration'].transform('min')\n    \n    # Calculate the average and standard deviation of the durations for each entity\n    data['avg_duration'] = data.groupby(id_column)['duration'].transform('mean')\n    data['std_duration'] = data.groupby(id_column)['duration'].transform('std')\n    \n    return data[[id_column, 'max_duration', 'min_duration', 'avg_duration', 'std_duration']].drop_duplicates()"
    },
    {
        "function_name": "sentiment_analysis",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "text_column": "str",
            "sentiment_threshold": "float"
        },
        "objectives": [
            "Implement a sentiment analysis model on the text column.",
            "Predict the sentiment of each text sample (positive, negative, or neutral).",
            "Filter out the text samples with a sentiment score below the specified threshold.",
            "Return the filtered dataframe with the predicted sentiments."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
        ],
        "function_def": "def sentiment_analysis(data, text_column, sentiment_threshold):\n    # Implement a sentiment analysis model on the text column\n    sia = SentimentIntensityAnalyzer()\n    sentiments = data[text_column].apply(sia.polarity_scores)\n    \n    # Predict the sentiment of each text sample (positive, negative, or neutral)\n    data['sentiment'] = sentiments.apply(lambda x: 'positive' if x['compound'] > 0.05 else 'negative' if x['compound'] < -0.05 else 'neutral')\n    \n    # Filter out the text samples with a sentiment score below the specified threshold\n    filtered_data = data[data['sentiment'] != 'neutral']\n    filtered_data = filtered_data[filtered_data['sentiment'] == 'positive']\n    filtered_data['sentiment_score'] = filtered_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n    filtered_data = filtered_data[filtered_data['sentiment_score'] > sentiment_threshold].reset_index(drop=True)\n    \n    return filtered_data"
    },
    {
        "function_name": "analyze_cycles",
        "file_name": "cycle_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "cycle_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Convert the time column to datetime format.",
            "Identify cycles with a duration less than the threshold.",
            "Calculate the average value of the cycle column for each cycle.",
            "Create a new column 'average_cycle_value' with the calculated values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def analyze_cycles(data, time_column, cycle_column, threshold):\n    # Convert the time column to datetime format\n    data[time_column] = pd.to_datetime(data[time_column])\n    \n    # Identify cycles with a duration less than the threshold\n    cycle_durations = data.groupby(cycle_column)[time_column].max() - data.groupby(cycle_column)[time_column].min()\n    short_cycles = cycle_durations[cycle_durations < threshold].index\n    \n    # Calculate the average value of the cycle column for each cycle\n    cycle_values = data.groupby(cycle_column)[cycle_column].mean()\n    \n    # Create a new column 'average_cycle_value' with the calculated values\n    data['average_cycle_value'] = data[cycle_column].map(cycle_values)\n    \n    return data"
    },
    {
        "function_name": "identify_best_selling_product",
        "file_name": "sales_insights.py",
        "parameters": {
            "transaction_data": "pandas.DataFrame",
            "product_column": "str",
            "date_column": "str",
            "revenue_column": "str"
        },
        "objectives": [
            "Calculate the total revenue for each product.",
            "Identify the product with the highest total revenue.",
            "Create a new column indicating whether each transaction is for the product with the highest total revenue.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_best_selling_product(transaction_data, product_column, date_column, revenue_column):\n    # Calculate the total revenue for each product\n    total_revenue = transaction_data.groupby(product_column)[revenue_column].sum().reset_index()\n    \n    # Identify the product with the highest total revenue\n    max_revenue = total_revenue[revenue_column].max()\n    max_revenue_product = total_revenue[total_revenue[revenue_column] == max_revenue][product_column].tolist()[0]\n    \n    # Create a new column indicating whether each transaction is for the product with the highest total revenue\n    transaction_data['is_best_selling_product'] = np.where(transaction_data[product_column] == max_revenue_product, True, False)\n    \n    return transaction_data"
    },
    {
        "function_name": "identify_abnormal_sensor_readings",
        "file_name": "sensor_insights.py",
        "parameters": {
            "sensor_data": "pandas.DataFrame",
            "temperature_column": "str",
            "humidity_column": "str"
        },
        "objectives": [
            "Calculate the correlation coefficient between temperature and humidity.",
            "Identify the rows with a temperature value greater than 30 and a humidity value less than 60.",
            "Create a new column indicating whether each row meets the temperature and humidity conditions.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_abnormal_sensor_readings(sensor_data, temperature_column, humidity_column):\n    # Calculate the correlation coefficient between temperature and humidity\n    correlation_coefficient = sensor_data[temperature_column].corr(sensor_data[humidity_column])\n    \n    # Identify the rows with a temperature value greater than 30 and a humidity value less than 60\n    sensor_data['is_abnormal'] = np.where((sensor_data[temperature_column] > 30) & (sensor_data[humidity_column] < 60), True, False)\n    \n    return sensor_data"
    },
    {
        "function_name": "identify_most_engaging_webpage",
        "file_name": "web_insights.py",
        "parameters": {
            "website_data": "pandas.DataFrame",
            "views_column": "str",
            "engagement_column": "str"
        },
        "objectives": [
            "Calculate the engagement rate for each webpage.",
            "Identify the webpage with the highest engagement rate.",
            "Create a new column indicating whether each webpage is the most engaging.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_most_engaging_webpage(website_data, views_column, engagement_column):\n    # Calculate the engagement rate for each webpage\n    engagement_rate = website_data.groupby('webpage')[engagement_column].sum() / website_data.groupby('webpage')[views_column].sum()\n    \n    # Identify the webpage with the highest engagement rate\n    max_engagement_rate = engagement_rate.max()\n    max_engagement_rate_webpage = engagement_rate[engagement_rate == max_engagement_rate].index.tolist()[0]\n    \n    # Create a new column indicating whether each webpage is the most engaging\n    website_data['is_most_engaging'] = np.where(website_data['webpage'] == max_engagement_rate_webpage, True, False)\n    \n    return website_data"
    },
    {
        "function_name": "calculate_gini_impurity",
        "file_name": "gini_impurity.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_column": "str",
            "target_column": "str",
            "max_depth": "int"
        },
        "objectives": [
            "Calculate the Gini impurity between the feature_column and the target_column using a Decision Tree.",
            "Create a new column 'gini_impurity_flag' in the dataframe and assign 1 to rows where the Gini impurity is greater than the threshold, 0 otherwise.",
            "Calculate the feature importance using a Random Forest Classifier.",
            "Create a new column 'feature_importance_flag' in the dataframe and assign 1 to rows where the feature importance is greater than the threshold, 0 otherwise."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.ensemble import RandomForestClassifier"
        ],
        "function_def": "def calculate_gini_impurity(df, feature_column, target_column, max_depth):\n    # Calculate the Gini impurity using a Decision Tree\n    tree = DecisionTreeClassifier(max_depth=max_depth)\n    tree.fit(df[[feature_column]], df[target_column])\n    gini_impurity = tree.tree_.impurity[0]\n    df['gini_impurity_flag'] = (gini_impurity > 0.5).astype(int)\n    \n    # Calculate the feature importance using a Random Forest Classifier\n    forest = RandomForestClassifier(n_estimators=100)\n    forest.fit(df[[feature_column]], df[target_column])\n    feature_importance = forest.feature_importances_[0]\n    df['feature_importance_flag'] = (feature_importance > 0.5).astype(int)\n    return df"
    },
    {
        "function_name": "calculate_silhouette_coef",
        "file_name": "silhouette_coef.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_column": "str",
            "target_column": "str",
            "k": "int"
        },
        "objectives": [
            "Calculate the silhouette coefficient between the feature_column and the target_column using K-Means clustering.",
            "Create a new column 'silhouette_coef_flag' in the dataframe and assign 1 to rows where the silhouette coefficient is greater than the threshold, 0 otherwise.",
            "Calculate the Calinski-Harabasz index using K-Means clustering.",
            "Create a new column 'calinski_harabasz_flag' in the dataframe and assign 1 to rows where the Calinski-Harabasz index is greater than the threshold, 0 otherwise."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans",
            "from sklearn.metrics import silhouette_score, calinski_harabasz_score"
        ],
        "function_def": "def calculate_silhouette_coef(df, feature_column, target_column, k):\n    # Calculate the silhouette coefficient\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(df[[feature_column]])\n    silhouette_coef = silhouette_score(df[[feature_column]], kmeans.labels_)\n    df['silhouette_coef_flag'] = (silhouette_coef > 0.5).astype(int)\n    \n    # Calculate the Calinski-Harabasz index\n    calinski_harabasz = calinski_harabasz_score(df[[feature_column]], kmeans.labels_)\n    df['calinski_harabasz_flag'] = (calinski_harabasz > 0.5).astype(int)\n    return df"
    },
    {
        "function_name": "apply_clustering",
        "file_name": "clustering_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "feature_column": "str",
            "method": "str ('ward', 'kmeans', etc.)"
        },
        "objectives": [
            "Group the dataframe by the id_column.",
            "Apply hierarchical clustering or k-means clustering to the feature_column.",
            "Create a new column with the cluster labels."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans",
            "from sklearn.cluster import AgglomerativeClustering",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def apply_clustering(df, id_column, feature_column, method):\n    # Group dataframe by the id_column\n    grouped_df = df.groupby(id_column)[feature_column]\n    \n    # Apply hierarchical clustering or k-means clustering\n    if method == 'ward':\n        clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n    elif method == 'kmeans':\n        clustering = KMeans(n_clusters=3)\n    else:\n        raise ValueError(\"Invalid clustering method.\")\n    \n    cluster_labels = []\n    for name, group in grouped_df:\n        scaled_group = StandardScaler().fit_transform(group.values.reshape(-1, 1))\n        cluster_labels.extend(clustering.fit_predict(scaled_group))\n    \n    # Create a new column with the cluster labels\n    df['cluster_label'] = cluster_labels\n    \n    return df"
    },
    {
        "function_name": "prepare_data_for_regression",
        "file_name": "regression.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numeric_columns": "list[str]",
            "categorical_columns": "list[str]",
            "target_column": "str"
        },
        "objectives": [
            "Scale the numeric columns using MinMaxScaler.",
            "One-hot encode the categorical columns.",
            "Concatenate the scaled numeric columns and one-hot encoded categorical columns.",
            "Use the concatenated data to train a Linear Regression model to predict the target column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler",
            "from sklearn.preprocessing import OneHotEncoder",
            "from sklearn.compose import ColumnTransformer",
            "from sklearn.linear_model import LinearRegression"
        ],
        "function_def": "def prepare_data_for_regression(df, numeric_columns, categorical_columns, target_column):\n    # Step 1: Scale the numeric columns using MinMaxScaler\n    numeric_transformer = MinMaxScaler()\n    numeric_features = numeric_transformer.fit_transform(df[numeric_columns])\n    \n    # Step 2: One-hot encode the categorical columns\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n    categorical_features = categorical_transformer.fit_transform(df[categorical_columns])\n    \n    # Step 3: Concatenate the scaled numeric columns and one-hot encoded categorical columns\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_columns),\n            ('cat', categorical_transformer, categorical_columns)])\n    preprocessed_data = preprocessor.fit_transform(df)\n    \n    # Step 4: Use the concatenated data to train a Linear Regression model to predict the target column\n    model = LinearRegression()\n    model.fit(preprocessed_data, df[target_column])\n    \n    return model"
    },
    {
        "function_name": "slope_change_areas",
        "file_name": "signal_processing.py",
        "parameters": {
            "arr": "numpy.ndarray",
            "slope_threshold": "float"
        },
        "objectives": [
            "Identify the slope changes in the input array that exceed the specified threshold.",
            "Calculate the areas under the curve between the slope change points.",
            "Return a list of tuples, where each tuple contains the slope change index and the area under the curve."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def slope_change_areas(arr, slope_threshold):\n    slopes = np.diff(arr) / np.diff(np.arange(len(arr)))\n    slope_change_idx = np.where(np.abs(np.diff(slopes)) > slope_threshold)[0] + 1\n    slope_change_idx = np.concatenate([[0], slope_change_idx, [len(arr) - 1]])\n    areas = []\n    for i in range(len(slope_change_idx) - 1):\n        start_idx = slope_change_idx[i]\n        end_idx = slope_change_idx[i + 1]\n        area = np.trapz(arr[start_idx:end_idx], np.arange(start_idx, end_idx))\n        areas.append((slope_change_idx[i], area))\n    return areas"
    },
    {
        "function_name": "identify_large_ids",
        "file_name": "cumulative_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "value_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the cumulative sum of the value column for each id in the id_column.",
            "Identify the ids that have a cumulative sum greater than the threshold.",
            "Return the ids that meet the condition."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_large_ids(df, id_column, value_column, threshold):\n    cumulative_sums = df.groupby(id_column)[value_column].cumsum()\n    large_ids = cumulative_sums[cumulative_sums > threshold].index.get_level_values(0).unique()\n    return large_ids"
    },
    {
        "function_name": "discretize_and_select_bins",
        "file_name": "discretization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "continuous_column": "str",
            "categorical_column": "str",
            "correlation_threshold": "float",
            "max_bins": "int"
        },
        "objectives": [
            "Discretize the continuous column using the KBinsDiscretizer from scikit-learn with a specified number of bins (max_bins).",
            "Calculate the correlation coefficient between each bin and the categorical column using the chi-square statistic.",
            "Select the bins with a correlation coefficient greater than the specified threshold.",
            "Create a new column in the dataframe with the selected bins."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import KBinsDiscretizer",
            "from scipy.stats import chi2_contingency"
        ],
        "function_def": "def discretize_and_select_bins(df, continuous_column, categorical_column, correlation_threshold, max_bins):\n    # Discretize the continuous column using the KBinsDiscretizer\n    discretizer = KBinsDiscretizer(n_bins=max_bins, encode='ordinal')\n    binned_column = discretizer.fit_transform(df[[continuous_column]])\n    \n    # Calculate the correlation coefficient between each bin and the categorical column\n    correlation_coefficients = []\n    for bin_value in np.unique(binned_column):\n        bin_df = df[binned_column == bin_value]\n        contingency_table = pd.crosstab(bin_df[categorical_column], [f'Bin {bin_value}'])\n        chi2, _, _, _ = chi2_contingency(contingency_table)\n        correlation_coefficients.append(chi2)\n    \n    # Select the bins with a correlation coefficient greater than the specified threshold\n    selected_bins = np.where(np.array(correlation_coefficients) > correlation_threshold)[0]\n    \n    # Create a new column in the dataframe with the selected bins\n    df['selected_bins'] = np.where(np.isin(binned_column, selected_bins), binned_column, np.nan)\n    \n    return df"
    },
    {
        "function_name": "extract_relevant_ngrams",
        "file_name": "ngram_extraction.py",
        "parameters": {
            "text_data": "list of str",
            "max_features": "int",
            "ngram_range": "tuple of int",
            "min_df": "float"
        },
        "objectives": [
            "Extract n-grams from the text data using the TfidfVectorizer from scikit-learn.",
            "Filter out n-grams with a minimum document frequency less than the specified threshold (min_df).",
            "Select the top 'max_features' n-grams with the highest TF-IDF scores.",
            "Return a dictionary with the selected n-grams as keys and their corresponding TF-IDF scores as values."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def extract_relevant_ngrams(text_data, max_features, ngram_range, min_df):\n    # Extract n-grams from the text data\n    vectorizer = TfidfVectorizer(ngram_range=ngram_range, min_df=min_df)\n    tfidf = vectorizer.fit_transform(text_data)\n    \n    # Filter out n-grams with a minimum document frequency less than the specified threshold\n    tfidf_scores = np.array(tfidf.sum(axis=0)).ravel()\n    filtered_tfids = tfidf_scores[tfidf_scores != 0]\n    filtered_features = vectorizer.get_feature_names_out()[tfidf_scores != 0]\n    \n    # Select the top 'max_features' n-grams with the highest TF-IDF scores\n    top_features = np.argsort(filtered_tfids)[::-1][:max_features]\n    top_features_scores = filtered_tfids[top_features]\n    \n    return dict(zip(filtered_features[top_features], top_features_scores))"
    },
    {
        "function_name": "temporal_feature_engineering",
        "file_name": "temporal_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "timestamp_column": "str",
            "target_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Sort the dataframe by the id_column and timestamp_column.",
            "Create a new column \"target_lag\" and assign it the value of the target_column from the previous window.",
            "Create a new column \"target_lead\" and assign it the value of the target_column from the next window.",
            "Calculate the moving standard deviation of the target_column for each window.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def temporal_feature_engineering(data, id_column, timestamp_column, target_column, window_size):\n    # Sort dataframe by id_column and timestamp_column\n    sorted_data = data.sort_values([id_column, timestamp_column])\n    \n    # Create new column \"target_lag\" and assign it the value of the target_column from the previous window\n    sorted_data['target_lag'] = sorted_data.groupby(id_column)[target_column].shift(window_size)\n    \n    # Create new column \"target_lead\" and assign it the value of the target_column from the next window\n    sorted_data['target_lead'] = sorted_data.groupby(id_column)[target_column].shift(-window_size)\n    \n    # Calculate the moving standard deviation of the target_column for each window\n    sorted_data['std_dev'] = sorted_data.groupby(id_column)[target_column].transform(lambda x: x.rolling(window_size).std())\n    \n    return sorted_data"
    },
    {
        "function_name": "data_imputation_and_outlier_detection",
        "file_name": "data_imputation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "cols_to_impute": "list[str]",
            "imputation_method": "str ('mean', 'median', or 'knn')"
        },
        "objectives": [
            "Impute missing values in the specified columns using the specified imputation method.",
            "Handle outliers by capping them at the 95th percentile and flooring them at the 5th percentile.",
            "Perform outlier detection using the Local Outlier Factor (LOF) algorithm.",
            "Return the imputed dataframe and the indices of the outliers."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.impute import SimpleImputer",
            "from sklearn.neighbors import LocalOutlierFactor"
        ],
        "function_def": "def data_imputation_and_outlier_detection(data, cols_to_impute, imputation_method):\n    # Impute missing values in the specified columns using the specified imputation method\n    if imputation_method == 'mean':\n        imputer = SimpleImputer(strategy='mean')\n    elif imputation_method == 'median':\n        imputer = SimpleImputer(strategy='median')\n    elif imputation_method == 'knn':\n        from sklearn.impute import KNNImputer\n        imputer = KNNImputer(n_neighbors=5)\n    else:\n        raise ValueError(\"Invalid imputation method.\")\n        \n    imputed_data = pd.DataFrame(imputer.fit_transform(data[cols_to_impute]), columns=cols_to_impute)\n    \n    # Handle outliers by capping them at the 95th percentile and flooring them at the 5th percentile\n    imputed_data = imputed_data.apply(lambda x: x.clip(x.quantile(0.05), x.quantile(0.95)))\n    \n    # Perform outlier detection using the Local Outlier Factor (LOF) algorithm\n    lof = LocalOutlierFactor(n_neighbors=5)\n    outlier_indices = lof.fit_predict(imputed_data)\n    \n    # Return the imputed dataframe and the indices of the outliers\n    return imputed_data, np.where(outlier_indices == -1)[0]"
    },
    {
        "function_name": "train_random_forest_classifer",
        "file_name": "classification.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_columns": "list[str]",
            "target_column": "str",
            "fold": "int"
        },
        "objectives": [
            "Split the data into training and testing sets using StratifiedKFold.",
            "Create a Random Forest Classifier model and train it on the training data.",
            "Evaluate the model's performance using accuracy score, precision score, and recall score.",
            "Return the model's predictions on the testing data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import StratifiedKFold",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.metrics import accuracy_score, precision_score, recall_score"
        ],
        "function_def": "def train_random_forest_classifer(data, feature_columns, target_column, fold):\n    skf = StratifiedKFold(n_splits=fold, shuffle=True, random_state=42)\n    \n    train_indices, test_indices = next(skf.split(data[feature_columns], data[target_column]))\n    \n    X_train, X_test = data[feature_columns].iloc[train_indices], data[feature_columns].iloc[test_indices]\n    y_train, y_test = data[target_column].iloc[train_indices], data[target_column].iloc[test_indices]\n    \n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    \n    print(f'Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}')\n    \n    return y_pred"
    },
    {
        "function_name": "cumulative_duplicate_count",
        "file_name": "duplicate_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column1": "str",
            "column2": "str"
        },
        "objectives": [
            "Identify rows where the values in the specified columns are duplicates.",
            "Create two new columns with the cumulative count of duplicates for each column.",
            "Return the updated DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def cumulative_duplicate_count(data, column1, column2):\n    # Step 1: Identify rows where the values in the specified columns are duplicates\n    duplicates = data.duplicated(subset=[column1], keep=False)\n    \n    # Step 2: Create two new columns with the cumulative count of duplicates for each column\n    data[f'cumulative_duplicates_{column1}'] = duplicates.groupby(data[column1]).cumsum()\n    data[f'cumulative_duplicates_{column2}'] = duplicates.groupby(data[column2]).cumsum()\n    \n    return data"
    },
    {
        "function_name": "strongly_connected_components",
        "file_name": "graph_analysis.py",
        "parameters": {
            "graph": "networkx.DiGraph",
            "node_attribute": "str",
            "edge_attribute": "str"
        },
        "objectives": [
            "Calculate the weighted in-degree and out-degree of each node in the graph.",
            "Identify nodes with a weighted in-degree greater than their weighted out-degree.",
            "Create a new directed graph containing only the identified nodes and the edges between them.",
            "Calculate the strongly connected components of the new graph.",
            "Return the new graph and the strongly connected components."
        ],
        "import_lines": [
            "import networkx as nx",
            "import numpy as np"
        ],
        "function_def": "def strongly_connected_components(graph, node_attribute, edge_attribute):\n    # Calculate the weighted in-degree and out-degree of each node in the graph\n    weighted_in_degree = {node: sum([graph.get_edge_data(neighbor, node)[edge_attribute] for neighbor in graph.predecessors(node)]) for node in graph.nodes}\n    weighted_out_degree = {node: sum([graph.get_edge_data(node, neighbor)[edge_attribute] for neighbor in graph.successors(node)]) for node in graph.nodes}\n    \n    # Identify nodes with a weighted in-degree greater than their weighted out-degree\n    identified_nodes = [node for node, in_degree in weighted_in_degree.items() if in_degree > weighted_out_degree[node]]\n    \n    # Create a new directed graph containing only the identified nodes and the edges between them\n    new_graph = graph.subgraph(identified_nodes)\n    \n    # Calculate the strongly connected components of the new graph\n    scc = list(nx.strongly_connected_components(new_graph))\n    \n    return new_graph, scc"
    },
    {
        "function_name": "interaction_terms",
        "file_name": "feature_engineering.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "numerical_column": "str"
        },
        "objectives": [
            "Perform one-hot encoding on the categorical column.",
            "Calculate the correlation between the numerical column and each categorical feature.",
            "Identify the categorical feature with the highest correlation.",
            "Create a new column containing the interaction term between the numerical column and the identified categorical feature.",
            "Return the updated DataFrame with the new column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def interaction_terms(data, categorical_column, numerical_column):\n    # Perform one-hot encoding on the categorical column\n    one_hot = pd.get_dummies(data[categorical_column], drop_first=True)\n    data = pd.concat([data, one_hot], axis=1)\n    \n    # Calculate the correlation between the numerical column and each categorical feature\n    correlations = data[one_hot.columns].corrwith(data[numerical_column])\n    \n    # Identify the categorical feature with the highest correlation\n    identified_feature = correlations.idxmax()\n    \n    # Create a new column containing the interaction term between the numerical column and the identified categorical feature\n    data['interaction'] = data[numerical_column] * data[identified_feature]\n    \n    return data"
    },
    {
        "function_name": "filter_low_variance_categories",
        "file_name": "category_filters.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "One-hot encode the categorical column.",
            "Calculate the variance of each category.",
            "Filter out categories with variance below the threshold.",
            "Return the filtered dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_low_variance_categories(df, categorical_column, threshold):\n    encoded_df = pd.get_dummies(df, columns=[categorical_column])\n    variances = encoded_df.var()\n    filtered_columns = variances[variances > threshold].index\n    filtered_df = df[filtered_columns]\n    return filtered_df"
    },
    {
        "function_name": "detect_duplicate_points",
        "file_name": "geospatial_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "latitude_column": "str",
            "longitude_column": "str",
            "min_distance": "float"
        },
        "objectives": [
            "Calculate the distance between each point and every other point using the Haversine formula.",
            "Identify the points that are closer than the minimum distance.",
            "Return the indices of these points."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def detect_duplicate_points(df, latitude_column, longitude_column, min_distance):\n    latitudes = np.radians(df[latitude_column])\n    longitudes = np.radians(df[longitude_column])\n    \n    distances = []\n    for i in range(len(latitudes)):\n        dlat = latitudes - latitudes[i]\n        dlon = longitudes - longitudes[i]\n        \n        a = np.sin(dlat/2)**2 + np.cos(latitudes[i]) * np.cos(latitudes) * np.sin(dlon/2)**2\n        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n        \n        distance = 6371 * c  # Earth's radius in km\n        distances.append(distance)\n    \n    distances = np.array(distances)\n    duplicate_points = (distances < min_distance) & (distances != 0)\n    return df[duplicate_points].index"
    },
    {
        "function_name": "overlapOnlyMerge",
        "file_name": "fuzzymerging.py",
        "parameters": {
            "df1": "pandas.DataFrame",
            "df2": "pandas.DataFrame",
            "left_key": "str",
            "right_key": "str",
            "overlap_threshold": "float"
        },
        "objectives": [
            "Merge the two dataframes based on the closest values to the specified key.",
            "Calculate the overlap ratio between the two merged datasets.",
            "Return the merged dataframe if the overlap ratio is within a specified threshold.",
            "Return a new dataframe with non-overlapping rows from both datasets."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def overlapOnlyMerge(df1, df2, left_key, right_key, overlap_threshold):\n    merged_df = pd.merge_asof(df1, df2, on=left_key, by=right_key, direction='nearest')\n    \n    # Calculate the overlap ratio\n    overlap_ratio = len(merged_df) / min(len(df1), len(df2))\n    \n    # Return merged dataframe if overlap ratio is above threshold\n    if overlap_ratio >= overlap_threshold:\n        return merged_df\n    \n    # Otherwise, stack the two non-overlapping dataframes\n    stacked_df = pd.concat([df1[~df1[left_key].isin(merged_df[left_key])], df2[~df2[left_key].isin(merged_df[left_key])]])\n    \n    return stacked_df"
    },
    {
        "function_name": "density_plot_peaks",
        "file_name": "density_plot_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the density plot for each column in the dataframe.",
            "Identify the columns with a density plot peak value above the specified threshold.",
            "Create a new dataframe with the identified columns and their corresponding density plot peak values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.stats import gaussian_kde"
        ],
        "function_def": "def density_plot_peaks(df, threshold):\n    # Calculate the density plot for each column in the dataframe\n    density_plots = {}\n    for col in df.columns:\n        kde = gaussian_kde(df[col])\n        x = np.linspace(df[col].min(), df[col].max(), 100)\n        y = kde(x)\n        density_plots[col] = (x, y)\n    \n    # Identify the columns with a density plot peak value above the specified threshold\n    peak_values = {}\n    for col, (x, y) in density_plots.items():\n        peak_value = np.max(y)\n        if peak_value > threshold:\n            peak_values[col] = peak_value\n    \n    # Create a new dataframe with the identified columns and their corresponding density plot peak values\n    result_df = pd.DataFrame({\n        'column': list(peak_values.keys()),\n        'peak_value': list(peak_values.values())\n    })\n    \n    return result_df"
    },
    {
        "function_name": "calculate_population_within_radius",
        "file_name": "geospatial_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "latitude_column": "str",
            "longitude_column": "str",
            "radius": "float",
            "center_coordinates": "tuple",
            "population_column": "str"
        },
        "objectives": [
            "Calculate the distance between each location and the center coordinates using the Haversine formula.",
            "Identify locations that are within the specified radius from the center coordinates.",
            "Calculate the total population within the radius.",
            "Return the total population and the dataframe with locations within the radius."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def calculate_population_within_radius(data, latitude_column, longitude_column, radius, center_coordinates, population_column):\n    center_latitude, center_longitude = center_coordinates\n    \n    data['distance'] = np.arccos(np.sin(np.radians(data[latitude_column])) * np.sin(np.radians(center_latitude)) +\n                                np.cos(np.radians(data[latitude_column])) * np.cos(np.radians(center_latitude)) *\n                                np.cos(np.radians(data[longitude_column]) - np.radians(center_longitude))) * 6371\n    \n    locations_within_radius = data[data['distance'] <= radius]\n    \n    total_population = locations_within_radius[population_column].sum()\n    \n    return total_population, locations_within_radius"
    },
    {
        "function_name": "analyze_sentiment",
        "file_name": "text_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "text_column": "str",
            "sentiment_label": "str"
        },
        "objectives": [
            "Use a natural language processing (NLP) library to analyze the sentiment of the text in the given column.",
            "Assign a sentiment label (positive, negative, or neutral) to each row based on the analysis.",
            "Create a new column 'sentiment_score' and assign it the sentiment score of the text.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
        ],
        "function_def": "def analyze_sentiment(data, text_column, sentiment_label):\n    sia = SentimentIntensityAnalyzer()\n    data['sentiment_score'] = data[text_column].apply(lambda x: sia.polarity_scores(x)[sentiment_label])\n    data['sentiment_label'] = data['sentiment_score'].apply(lambda x: 'positive' if x > 0.5 else 'negative' if x < -0.5 else 'neutral')\n    return data"
    },
    {
        "function_name": "mutual_information_pca",
        "file_name": "dimensionality_reduction.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_columns": "list of str",
            "target_column": "str"
        },
        "objectives": [
            "Calculate the mutual information between each feature and the target variable.",
            "Select the top 5 features with the highest mutual information.",
            "Create a new dataframe with the selected features and the target variable.",
            "Perform principal component analysis on the selected features.",
            "Return the new dataframe with the selected features and the projected data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import mutual_info_classif",
            "from sklearn.decomposition import PCA",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def mutual_information_pca(data, feature_columns, target_column):\n    # Calculate the mutual information between each feature and the target variable\n    mutual_info = mutual_info_classif(data[feature_columns], data[target_column])\n    \n    # Select the top 5 features with the highest mutual information\n    top_features = sorted(zip(feature_columns, mutual_info), key=lambda x: x[1], reverse=True)[:5]\n    top_features = [x[0] for x in top_features]\n    \n    # Create a new dataframe with the selected features and the target variable\n    selected_data = data[top_features + [target_column]]\n    \n    # Perform principal component analysis on the selected features\n    scaler = StandardScaler()\n    pca = PCA(n_components=2)\n    projected_data = pca.fit_transform(scaler.fit_transform(selected_data[top_features]))\n    \n    # Return the new dataframe with the selected features and the projected data\n    projected_df = pd.DataFrame({'PC1': projected_data[:, 0], 'PC2': projected_data[:, 1]})\n    return selected_data, projected_df"
    },
    {
        "function_name": "scale_and_encode",
        "file_name": "preprocessing_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_to_scale": "str",
            "column_to_encode": "str",
            "target_column": "str"
        },
        "objectives": [
            "Scale the specified column using Min-Max Scaler.",
            "One-hot encode the specified categorical column.",
            "Merge the scaled and encoded columns into the original dataframe.",
            "Calculate the correlation between the scaled column and the target column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def scale_and_encode(df, column_to_scale, column_to_encode, target_column):\n    scaler = MinMaxScaler()\n    df[f'scaled_{column_to_scale}'] = scaler.fit_transform(df[column_to_scale].values.reshape(-1,1))\n    encoded_df = pd.get_dummies(df[column_to_encode])\n    df = pd.concat([df, encoded_df], axis=1)\n    correlation = df[f'scaled_{column_to_scale}'].corr(df[target_column])\n    return df, correlation"
    },
    {
        "function_name": "correlation_coefficient",
        "file_name": "data_transformation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "scaling_column": "str",
            "normalization_column": "str",
            "correlation_threshold": "float"
        },
        "objectives": [
            "Standard scale the values in the `scaling_column` using the `StandardScaler` from scikit-learn.",
            "Normalize the values in the `normalization_column` using the `MinMaxScaler` from scikit-learn.",
            "Calculate the correlation between the scaled and normalized columns.",
            "Create a new dataframe with the correlation coefficient and the corresponding p-value."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler",
            "from scipy.stats import pearsonr"
        ],
        "function_def": "def correlation_coefficient(data, scaling_column, normalization_column, correlation_threshold):\n    # Standard scale the values in the scaling_column\n    scaler = StandardScaler()\n    data['scaled_value'] = scaler.fit_transform(data[[scaling_column]])\n    \n    # Normalize the values in the normalization_column\n    normalizer = MinMaxScaler()\n    data['normalized_value'] = normalizer.fit_transform(data[[normalization_column]])\n    \n    # Calculate the correlation between the scaled and normalized columns\n    correlation_coefficient, p_value = pearsonr(data['scaled_value'], data['normalized_value'])\n    \n    # Create a new dataframe with the correlation coefficient and the corresponding p-value\n    result = pd.DataFrame({'correlation_coefficient': [correlation_coefficient], 'p_value': [p_value]})\n    \n    return result"
    },
    {
        "function_name": "analyze_column_correlations",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "columns": "list[str]",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the correlation matrix of the specified columns.",
            "Identify the pairs of columns with a correlation coefficient greater than the specified threshold.",
            "Create a new DataFrame with the pairs of columns and their corresponding correlation coefficients.",
            "Return the new DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def analyze_column_correlations(data, columns, threshold):\n    # Calculate the correlation matrix of the specified columns\n    corr_matrix = data[columns].corr()\n    \n    # Identify the pairs of columns with a correlation coefficient greater than the specified threshold\n    corr_pairs = []\n    for i in range(len(columns)):\n        for j in range(i+1, len(columns)):\n            if corr_matrix.iloc[i, j] > threshold:\n                corr_pairs.append((columns[i], columns[j], corr_matrix.iloc[i, j]))\n    \n    # Create a new DataFrame with the pairs of columns and their corresponding correlation coefficients\n    corr_df = pd.DataFrame(corr_pairs, columns=['column1', 'column2', 'correlation'])\n    \n    return corr_df"
    },
    {
        "function_name": "calculate_avg_transaction_time",
        "file_name": "transaction_analytics.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Group the dataframe by the id_column and calculate the total number of transactions for each id.",
            "Filter the groups where the total number of transactions is above the threshold.",
            "Within each group, sort the transactions by date and calculate the time difference between consecutive transactions.",
            "Create a new column with the average time difference for each group and add it to the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_avg_transaction_time(data, id_column, date_column, threshold):\n    # Group the dataframe by the id_column and calculate the total number of transactions for each id\n    group_counts = data.groupby(id_column).size().reset_index(name='count')\n    \n    # Filter the groups where the total number of transactions is above the threshold\n    filtered_groups = group_counts[group_counts['count'] > threshold][id_column].tolist()\n    \n    # Within each group, sort the transactions by date and calculate the time difference between consecutive transactions\n    for group in filtered_groups:\n        group_data = data[data[id_column] == group].sort_values(date_column)\n        group_data['time_diff'] = group_data[date_column].diff()\n        \n        # Create a new column with the average time difference for each group and add it to the original dataframe\n        avg_time_diff = group_data['time_diff'].mean()\n        data.loc[data[id_column] == group, 'avg_time_diff'] = avg_time_diff\n    \n    return data"
    },
    {
        "function_name": "numerical_normalizer",
        "file_name": "numerical_normalization.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numerical_columns": "list of str",
            "normalization_method": "str"
        },
        "objectives": [
            "Normalize the numerical columns using the specified normalization method.",
            "Handle outliers by winsorizing them to the 95th percentile.",
            "Create a new dataframe with the normalized numerical columns and add it to the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler",
            "from scipy import stats"
        ],
        "function_def": "def numerical_normalizer(data, numerical_columns, normalization_method):\n    # Normalize the numerical columns using the specified normalization method\n    if normalization_method == 'min-max':\n        scaler = MinMaxScaler()\n        normalized_data = scaler.fit_transform(data[numerical_columns])\n    elif normalization_method == 'standard':\n        normalized_data = stats.zscore(data[numerical_columns])\n    else:\n        raise ValueError(\"Invalid normalization method\")\n    \n    # Handle outliers by winsorizing them to the 95th percentile\n    for column in numerical_columns:\n        data[column] = np.clip(data[column], a_min=data[column].quantile(0.05), a_max=data[column].quantile(0.95))\n    \n    # Create a new dataframe with the normalized numerical columns and add it to the original dataframe\n    normalized_data = pd.DataFrame(normalized_data, columns=numerical_columns)\n    normalized_data.columns = [f\"{column}_{normalization_method}\" for column in normalized_data.columns]\n    data = pd.concat([data, normalized_data], axis=1)\n    \n    return data"
    },
    {
        "function_name": "identify_rolling_sum_exceeded",
        "file_name": "rolling_sum.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "value_column": "str",
            "rolling_window": "int",
            "threshold": "int"
        },
        "objectives": [
            "Calculate the rolling sum of values in the 'value_column' over a specified rolling window.",
            "Identify the rows where the rolling sum exceeds the specified threshold.",
            "Create a new column 'rolling_sum_exceeded' with a flag indicating whether the rolling sum exceeded the threshold.",
            "Return the updated dataframe with the new column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_rolling_sum_exceeded(data, time_column, value_column, rolling_window, threshold):\n    # Calculate the rolling sum of values over a specified rolling window\n    data['rolling_sum'] = data[value_column].rolling(rolling_window).sum()\n    \n    # Identify the rows where the rolling sum exceeds the threshold\n    data['rolling_sum_exceeded'] = data['rolling_sum'] > threshold\n    \n    # Return the updated dataframe with the new column\n    return data"
    },
    {
        "function_name": "image_patch_features",
        "file_name": "image_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "image_column": "str"
        },
        "objectives": [
            "Split each image into 5x5 patches.",
            "Calculate the mean RGB values of each patch.",
            "Create a new dataframe 'patch_features' which contains the mean RGB values of each patch.",
            "Return the new dataframe 'patch_features'."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from PIL import Image"
        ],
        "function_def": "def image_patch_features(df, image_column):\n    # Split each image into 5x5 patches\n    patches = []\n    for img in df[image_column]:\n        img = Image.open(img)\n        patches.extend([img.crop((i*5, j*5, (i+1)*5, (j+1)*5)) for i in range(img.width//5) for j in range(img.height//5)])\n    \n    # Calculate the mean RGB values of each patch\n    patch_features = pd.DataFrame([np.mean(np.array(patch), axis=(0, 1)) for patch in patches], columns=['R', 'G', 'B'])\n    \n    # Create a new dataframe 'patch_features' which contains the mean RGB values of each patch\n    patch_features['image_id'] = df.index.repeat(25)\n    \n    return patch_features"
    },
    {
        "function_name": "flag_events_based_on_sentiment",
        "file_name": "sentiment_analysis.py",
        "parameters": {
            "customer_data": "pandas.DataFrame",
            "sentiment_column": "str",
            "threshold": "float",
            "events_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Calculate the rolling average of the sentiment scores for each customer.",
            "Identify the customers who have a rolling average sentiment score above the threshold.",
            "For these customers, flag the events that occur within a specified window size after the sentiment score exceeds the threshold.",
            "Return the updated dataframe with the flagged events."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def flag_events_based_on_sentiment(customer_data, sentiment_column, threshold, events_column, window_size):\n    # Calculate the rolling average of the sentiment scores for each customer\n    customer_data['rolling_avg_sentiment'] = customer_data.groupby('customer_id')[sentiment_column].transform(lambda x: x.rolling(window_size).mean())\n    \n    # Identify the customers who have a rolling average sentiment score above the threshold\n    high_sentiment_customers = customer_data[customer_data['rolling_avg_sentiment'] > threshold]['customer_id'].unique()\n    \n    # For these customers, flag the events that occur within a specified window size after the sentiment score exceeds the threshold\n    flagged_events = []\n    for customer_id in high_sentiment_customers:\n        customer_data_for_customer = customer_data[customer_data['customer_id'] == customer_id]\n        for index, row in customer_data_for_customer.iterrows():\n            if row['rolling_avg_sentiment'] > threshold:\n                start_index = max(0, index - window_size)\n                end_index = min(len(customer_data_for_customer), index + window_size)\n                flagged_events.extend(customer_data_for_customer.iloc[start_index:end_index][events_column].tolist())\n    \n    # Return the updated dataframe with the flagged events\n    customer_data['flagged_events'] = customer_data[events_column].isin(flagged_events)\n    \n    return customer_data"
    },
    {
        "function_name": "aggregate_time_series",
        "file_name": "time_series_utils.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "time_column": "str",
            "aggregation_column": "str",
            "time_interval": "str (e.g., 'D' for daily, 'W' for weekly, 'M' for monthly)"
        },
        "objectives": [
            "Identify unique customer ids from the id_column.",
            "Group the data by the id_column and time_column to create time intervals based on the time_interval parameter.",
            "For each time interval, calculate the total and average values of the aggregation_column.",
            "Return the resulting DataFrame with the aggregated data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_time_series(data, id_column, time_column, aggregation_column, time_interval):\n    data[time_column] = pd.to_datetime(data[time_column])\n    data['time_interval'] = data[time_column].dt.to_period(time_interval)\n    aggregated_data = data.groupby([id_column, 'time_interval'])[aggregation_column].agg(['sum', 'mean']).reset_index()\n    aggregated_data.columns = [id_column, 'time_interval', 'total', 'average']\n    return aggregated_data"
    },
    {
        "function_name": "anomaly_detection",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "outcome_column": "str",
            "feature_columns": "list",
            "anomaly_threshold": "float"
        },
        "objectives": [
            "Scale the feature columns using StandardScaler.",
            "Apply Local Outlier Factor (LOF) algorithm to the scaled data to detect anomalies.",
            "Identify the data points with an anomaly score greater than the anomaly_threshold.",
            "Return the resulting DataFrame with the predicted anomalies."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.neighbors import LocalOutlierFactor"
        ],
        "function_def": "def anomaly_detection(data, outcome_column, feature_columns, anomaly_threshold):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data[feature_columns])\n    lof = LocalOutlierFactor()\n    anomaly_scores = lof.fit_predict(scaled_data)\n    anomalies = data[anomaly_scores > anomaly_threshold]\n    anomalies['anomaly'] = True\n    return anomalies"
    },
    {
        "function_name": "update_word_freq",
        "file_name": "nlp_utils.py",
        "parameters": {
            "text_data": "list[str]",
            "stop_words": "list[str]",
            "min_freq": "int"
        },
        "objectives": [
            "Remove stop words and punctuation from the text data.",
            "Calculate the frequency of each word in the text data.",
            "Filter the words with frequency less than the minimum frequency threshold.",
            "Return the updated word frequency dictionary."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def update_word_freq(text_data, stop_words, min_freq):\n    # Remove stop words and punctuation from the text data\n    cleaned_text = [' '.join([word for word in re.sub(r'[^\\w\\s]', '', text).split() if word.lower() not in stop_words]) for text in text_data]\n    \n    # Calculate the frequency of each word in the text data\n    word_freq = Counter(' '.join(cleaned_text).split())\n    \n    # Filter the words with frequency less than the minimum frequency threshold\n    updated_word_freq = {word: freq for word, freq in word_freq.items() if freq >= min_freq}\n    \n    return updated_word_freq"
    },
    {
        "function_name": "detect_peak_hours",
        "file_name": "temporal_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "value_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the average value for each hour of the day.",
            "Identify the hours where the average value exceeds the threshold.",
            "Create a new dataframe with the hours and their corresponding average values.",
            "Return the updated dataframe with the identified hours marked."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_peak_hours(data, time_column, value_column, threshold):\n    # Convert time column to datetime type\n    data[time_column] = pd.to_datetime(data[time_column])\n    \n    # Calculate the average value for each hour of the day\n    hourly_values = data.groupby(data[time_column].dt.hour)[value_column].mean().reset_index()\n    \n    # Identify the hours where the average value exceeds the threshold\n    peak_hours = hourly_values[hourly_values[value_column] > threshold]\n    \n    # Create a new dataframe with the hours and their corresponding average values\n    result = pd.merge(hourly_values, peak_hours, how='left', indicator=True)\n    \n    # Mark the identified hours\n    result['is_peak_hour'] = result['_merge'] == 'both'\n    \n    return result"
    },
    {
        "function_name": "filter_and_aggregate_scores",
        "file_name": "scoring.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "score_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the percentile rank of each score in the score_column for each id group.",
            "Identify and remove rows where the score is missing or invalid.",
            "Filter out rows where the score is below the specified threshold.",
            "Calculate the percentage change in score from the previous row for each id group and add it as a new column 'score_change'.",
            "Return the resulting DataFrame with the filtered and aggregated data."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def filter_and_aggregate_scores(df, id_column, score_column, threshold):\n    # Calculate the percentile rank of each score in the score_column for each id group\n    df['percentile_rank'] = df.groupby(id_column)[score_column].rank(method='min', pct=True)\n    \n    # Identify and remove rows where the score is missing or invalid\n    df.dropna(subset=[score_column], inplace=True)\n    df = df[pd.to_numeric(df[score_column], errors='coerce').notnull()]\n    \n    # Filter out rows where the score is below the specified threshold\n    df = df[df[score_column] >= threshold]\n    \n    # Calculate the percentage change in score from the previous row for each id group and add it as a new column 'score_change'\n    df['score_change'] = df.groupby(id_column)[score_column].pct_change() * 100\n    \n    return df"
    },
    {
        "function_name": "supervised_discretization",
        "file_name": "discretization.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numeric_cols": "list of str",
            "supervised_discretization_method": "str ('entropy' or 'kmeans')"
        },
        "objectives": [
            "Discretize the numerical columns in the dataframe using the specified supervised discretization method.",
            "Evaluate the discretization using the corresponding evaluation metric.",
            "Return the discretized dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import mutual_info_classif",
            "from sklearn.preprocessing import KBinsDiscretizer",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def supervised_discretization(data, numeric_cols, supervised_discretization_method):\n    discretized_data = data.copy()\n    \n    # Discretize the numerical columns in the dataframe using the specified supervised discretization method\n    for col in numeric_cols:\n        if supervised_discretization_method == 'entropy':\n            discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')\n            discretized_data[col] = discretizer.fit_transform(data[col].values.reshape(-1, 1))\n        elif supervised_discretization_method == 'kmeans':\n            discretizer = KMeans(n_clusters=5)\n            discretized_data[col] = discretizer.fit_predict(data[col].values.reshape(-1, 1))\n        else:\n            raise ValueError(\"Invalid supervised discretization method\")\n    \n    # Evaluate the discretization using the corresponding evaluation metric\n    evaluation_metric = mutual_info_classif(discretized_data[numeric_cols], discretized_data['target'])\n    \n    return discretized_data"
    },
    {
        "function_name": "categorical_encoding",
        "file_name": "encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "encoding_method": "str (either 'label' or 'onehot')"
        },
        "objectives": [
            "Identify the categorical columns in the DataFrame.",
            "Apply the specified encoding method to the categorical columns.",
            "Calculate the correlation between the encoded categorical variables and the target variable.",
            "Return the encoded DataFrame and the correlation matrix."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder, OneHotEncoder",
            "from sklearn.metrics import mutual_info_score",
            "import numpy as np"
        ],
        "function_def": "def categorical_encoding(df, categorical_columns, encoding_method):\n    if encoding_method == 'label':\n        encoder = LabelEncoder()\n        encoded_df = df.copy()\n        for col in categorical_columns:\n            encoded_df[col] = encoder.fit_transform(encoded_df[col])\n    elif encoding_method == 'onehot':\n        encoder = OneHotEncoder()\n        encoded_data = encoder.fit_transform(df[categorical_columns])\n        encoded_df = pd.concat([df.drop(categorical_columns, axis=1), pd.DataFrame(encoded_data.toarray())], axis=1)\n    \n    target_col = df.columns[-1]\n    correlation_matrix = []\n    for col in encoded_df.columns[:-1]:\n        correlation = mutual_info_score(encoded_df[target_col], encoded_df[col])\n        correlation_matrix.append(correlation)\n    \n    return encoded_df, np.array(correlation_matrix)"
    },
    {
        "function_name": "lsa",
        "file_name": "dimensionality_reduction.py",
        "parameters": {
            "matrix": "numpy.ndarray",
            "k": "int"
        },
        "objectives": [
            "Perform Latent Semantic Analysis (LSA) on the input matrix.",
            "Reduce the dimensionality of the matrix using Singular Value Decomposition (SVD).",
            "Reconstruct the original matrix from the reduced-dimensional representation.",
            "Return the reconstructed matrix."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def lsa(matrix, k):\n    U, sigma, Vt = np.linalg.svd(matrix)\n    U_reduced = U[:, :k]\n    sigma_reduced = np.diag(sigma[:k])\n    Vt_reduced = Vt[:k, :]\n    \n    reconstructed_matrix = np.dot(np.dot(U_reduced, sigma_reduced), Vt_reduced)\n    \n    return reconstructed_matrix"
    },
    {
        "function_name": "confidence_interval",
        "file_name": "stats_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "confidence_level": "float",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the confidence interval for each column in the DataFrame.",
            "Identify the columns where the confidence interval width exceeds the threshold.",
            "Calculate the variance of the columns where the confidence interval width exceeded the threshold.",
            "Return the variance values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import norm"
        ],
        "function_def": "def confidence_interval(df, confidence_level, threshold):\n    interval_widths = []\n    for col in df.columns:\n        mean = df[col].mean()\n        std_dev = df[col].std()\n        interval = norm.interval(confidence_level, loc=mean, scale=std_dev)\n        interval_width = interval[1] - interval[0]\n        interval_widths.append(interval_width)\n    \n    exceeded_threshold = np.where(np.array(interval_widths) > threshold)[0]\n    variance_values = df.iloc[:, exceeded_threshold].var().values\n    \n    return variance_values"
    },
    {
        "function_name": "time_frequency_analysis",
        "file_name": "signal_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "value_column": "str",
            "frequency": "str"
        },
        "objectives": [
            "Perform time-frequency analysis using the Fast Fourier Transform (FFT) on the time-series data.",
            "Extract the frequency components with amplitudes above a certain threshold (e.g., 0.1).",
            "Reconstruct the original signal using the inverse FFT.",
            "Return the reconstructed signal."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def time_frequency_analysis(data, time_column, value_column, frequency):\n    # Perform time-frequency analysis using FFT\n    fft = np.fft.fft(data[value_column])\n    \n    # Extract the frequency components with amplitudes above a certain threshold\n    threshold = 0.1\n    frequency_components = fft[np.abs(fft) > threshold]\n    \n    # Reconstruct the original signal using the inverse FFT\n    reconstructed_signal = np.real(np.fft.ifft(frequency_components))\n    \n    return reconstructed_signal"
    },
    {
        "function_name": "filter_variable_groups",
        "file_name": "group_stats.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str",
            "group_by_columns": "list",
            "metric_column": "str",
            "target_metric": "str",
            "threshold": "float"
        },
        "objectives": [
            "Convert the date column to a datetime object and extract the year.",
            "Group the data by the year, id column, and group by columns, and calculate the sum of the metric column.",
            "For each group, calculate the standard deviation of the metric column.",
            "Filter out the groups where the standard deviation of the metric column exceeds the threshold and return the resulting dataframe with the group by columns, year, and target metric."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_variable_groups(data, id_column, date_column, group_by_columns, metric_column, target_metric, threshold):\n    # Convert the date column to a datetime object and extract the year\n    data['year'] = pd.to_datetime(data[date_column]).dt.year\n    \n    # Group the data by the year, id column, and group by columns, and calculate the sum of the metric column\n    grouped_data = data.groupby(['year', id_column] + group_by_columns)[metric_column].sum().reset_index()\n    \n    # For each group, calculate the standard deviation of the metric column\n    grouped_data['std_dev'] = data.groupby(['year', id_column] + group_by_columns)[metric_column].std().values\n    \n    # Filter out the groups where the standard deviation of the metric column exceeds the threshold\n    filtered_data = grouped_data[grouped_data['std_dev'] <= threshold]\n    \n    # Return the resulting dataframe with the group by columns, year, and target metric\n    return filtered_data[[id_column] + group_by_columns + ['year', target_metric]]"
    },
    {
        "function_name": "rolling_statistics",
        "file_name": "rolling_statistics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "window_size": "int"
        },
        "objectives": [
            "Calculate the rolling statistics (mean, median, mode) for the specified column.",
            "Calculate the rolling standard deviation for the specified column.",
            "Return a new dataframe with the rolling statistics and standard deviation."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy import stats"
        ],
        "function_def": "def rolling_statistics(df, column_name, window_size):\n    # Calculate the rolling statistics (mean, median, mode) for the specified column\n    rolling_mean = df[column_name].rolling(window_size).mean()\n    rolling_median = df[column_name].rolling(window_size).median()\n    rolling_mode = df[column_name].rolling(window_size).apply(lambda x: stats.mode(x)[0])\n    \n    # Calculate the rolling standard deviation for the specified column\n    rolling_std = df[column_name].rolling(window_size).std()\n    \n    # Return a new dataframe with the rolling statistics and standard deviation\n    result = pd.DataFrame({'rolling_mean': rolling_mean, 'rolling_median': rolling_median, 'rolling_mode': rolling_mode, 'rolling_std': rolling_std})\n    \n    return result"
    },
    {
        "function_name": "exclude_high_variance_data",
        "file_name": "data_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "value_column": "str",
            "window_size": "int",
            "threshold": "float"
        },
        "objectives": [
            "Convert the specified date column to datetime format.",
            "Calculate the rolling standard deviation of the value column with the specified window size.",
            "Identify the rows where the rolling standard deviation is greater than the threshold.",
            "Return a new dataframe with the identified rows."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def exclude_high_variance_data(df, date_column, value_column, window_size, threshold):\n    # Step 1: Convert the specified date column to datetime format\n    df[date_column] = pd.to_datetime(df[date_column])\n    \n    # Step 2: Calculate the rolling standard deviation of the value column with the specified window size\n    df['rolling_std'] = df[value_column].rolling(window_size).std()\n    \n    # Step 3: Identify the rows where the rolling standard deviation is greater than the threshold\n    mask = df['rolling_std'] > threshold\n    \n    # Step 4: Return a new dataframe with the identified rows\n    return df[mask]"
    },
    {
        "function_name": "handle_holidays_and_weekends",
        "file_name": "date_utils.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "holiday_list": "list of str"
        },
        "objectives": [
            "Identify dates that fall on holidays or weekends.",
            "Create a new column indicating whether the date is a holiday or weekend.",
            "Extract the month and year from the date column and create new columns.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def handle_holidays_and_weekends(data, date_column, holiday_list):\n    data[date_column] = pd.to_datetime(data[date_column])\n    data['is_holiday_or_weekend'] = data[date_column].dt.dayofweek.isin([5, 6]) | data[date_column].dt.date.isin(holiday_list)\n    data['month'] = data[date_column].dt.month\n    data['year'] = data[date_column].dt.year\n    return data"
    },
    {
        "function_name": "forecast_analysis",
        "file_name": "forecasting.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "forecast_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Create a lagged version of the forecast column with the specified window size.",
            "Calculate the difference between the forecast column and its lagged version.",
            "Identify the top window_size rows with the largest difference values.",
            "Return the updated dataframe with the lagged forecast column and the top rows."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def forecast_analysis(data, time_column, forecast_column, window_size):\n    data[time_column] = pd.to_datetime(data[time_column])\n    data.sort_values(time_column, inplace=True)\n    \n    data[f'lagged_{forecast_column}'] = data[forecast_column].shift(window_size)\n    data['diff'] = data[forecast_column] - data[f'lagged_{forecast_column}']\n    \n    top_rows = data.nlargest(window_size, 'diff')\n    \n    return data, top_rows"
    },
    {
        "function_name": "calculate_lagged_values",
        "file_name": "time_series_preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str",
            "lag_time": "int",
            "agg_columns": "list of str"
        },
        "objectives": [
            "Convert the date_column to datetime format and extract the year.",
            "Group the dataframe by the id_column and the extracted year.",
            "For each group, calculate the lagged values of the specified agg_columns by the specified lag_time.",
            "Return the dataframe with the lagged values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_lagged_values(df, id_column, date_column, lag_time, agg_columns):\n    df[date_column] = pd.to_datetime(df[date_column])\n    df['year'] = df[date_column].dt.year\n    \n    df_lagged = df.copy()\n    for column in agg_columns:\n        df_lagged[f'{column}_lagged'] = df.groupby([id_column, 'year'])[column].transform(lambda x: x.shift(lag_time))\n    \n    return df_lagged"
    },
    {
        "function_name": "calculate_filtered_tfidf",
        "file_name": "text_preprocessing.py",
        "parameters": {
            "text_data": "pandas.Series",
            "min_df": "int",
            "max_df": "int",
            "stop_words": "list of str"
        },
        "objectives": [
            "Preprocess the text data by removing punctuation and converting to lowercase.",
            "Tokenize the text data into words.",
            "Remove stopwords from the tokenized words.",
            "Apply term frequency-inverse document frequency (TF-IDF) transformation to the tokenized words.",
            "Filter the TF-IDF matrix by the specified min_df and max_df.",
            "Return the filtered TF-IDF matrix."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "import re"
        ],
        "function_def": "def calculate_filtered_tfidf(text_data, min_df, max_df, stop_words):\n    text_data = text_data.apply(lambda x: re.sub(r'[^\\w\\s]', '', x).lower())\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=min_df, max_df=max_df)\n    tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n    \n    return tfidf_matrix"
    },
    {
        "function_name": "weekly_average",
        "file_name": "aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "interval": "int"
        },
        "objectives": [
            "Convert the date column to datetime format and extract the week of the year.",
            "Create a new column with the average value of the target variable for each week.",
            "Identify the weeks with the highest average value and create a new column to indicate whether each row belongs to one of these weeks.",
            "Return the updated dataframe with the new columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def weekly_average(df, date_column, interval):\n    # Convert the date column to datetime format and extract the week of the year\n    df['datetime'] = pd.to_datetime(df[date_column])\n    df['week_of_year'] = df['datetime'].dt.isocalendar().week\n    \n    # Create a new column with the average value of the target variable for each week\n    df['average_target'] = df.groupby('week_of_year')['target'].transform('mean')\n    \n    # Identify the weeks with the highest average value\n    top_weeks = df.groupby('week_of_year')['average_target'].mean().nlargest(interval).index\n    \n    # Create a new column to indicate whether each row belongs to one of these weeks\n    df['is_top_week'] = df['week_of_year'].isin(top_weeks)\n    \n    return df"
    },
    {
        "function_name": "discretize_and_score",
        "file_name": "scoring_functions.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numerical_column": "str",
            "bins": "int",
            "scoring_function": "callable"
        },
        "objectives": [
            "Discretize the numerical column into bins using the specified number of bins.",
            "Apply the scoring function to each bin.",
            "Create a new column with the scores.",
            "Return the resulting dataframe with the discretized column and the scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import KBinsDiscretizer"
        ],
        "function_def": "def discretize_and_score(data, numerical_column, bins, scoring_function):\n    # Discretize the numerical column into bins using the specified number of bins\n    discretizer = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='kmeans')\n    discretized_values = discretizer.fit_transform(data[[numerical_column]])\n    data[f'{numerical_column}_discretized'] = discretized_values\n    \n    # Apply the scoring function to each bin\n    scores = scoring_function(data[f'{numerical_column}_discretized'])\n    \n    # Create a new column with the scores\n    data['scores'] = scores\n    \n    # Return the resulting dataframe with the discretized column and the scores\n    return data[[f'{numerical_column}_discretized', 'scores']]"
    },
    {
        "function_name": "recommend_products",
        "file_name": "product_recommendation.py",
        "parameters": {
            "product_data": "pandas.DataFrame",
            "rating_column": "str",
            "review_column": "str",
            "sentiment_threshold": "float"
        },
        "objectives": [
            "Calculate the sentiment score of each review using natural language processing techniques.",
            "Identify reviews with a sentiment score above the specified sentiment_threshold.",
            "Calculate the average rating for each product.",
            "Use the average rating and sentiment score to recommend products to customers."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.sentiment import SentimentIntensityAnalyzer"
        ],
        "function_def": "def recommend_products(product_data, rating_column, review_column, sentiment_threshold):\n    # Calculate the sentiment score of each review\n    sia = SentimentIntensityAnalyzer()\n    product_data['sentiment_score'] = product_data[review_column].apply(lambda x: sia.polarity_scores(x)['compound'])\n    \n    # Identify reviews with a sentiment score above the specified sentiment_threshold\n    positive_reviews = product_data[product_data['sentiment_score'] > sentiment_threshold]\n    \n    # Calculate the average rating for each product\n    product_average_rating = product_data.groupby('product_id')[rating_column].mean()\n    \n    # Use the average rating and sentiment score to recommend products to customers\n    recommended_products = positive_reviews.groupby('product_id')['sentiment_score'].mean().sort_values(ascending=False).index\n    \n    return recommended_products"
    },
    {
        "function_name": "class_probability_analysis",
        "file_name": "machine_learning_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "side_classes": "list",
            "target_column": "str",
            "probability_threshold": "float"
        },
        "objectives": [
            "Create a new column to store the predicted class probabilities.",
            "Train a logistic regression model to predict the class probabilities for each row.",
            "Identify the rows where the predicted probability exceeds the specified threshold.",
            "Group the dataframe by predicted class and calculate the accuracy for each class.",
            "Return a dataframe with the predicted class probabilities and the accuracy for each class."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.metrics import accuracy_score"
        ],
        "function_def": "def class_probability_analysis(df, side_classes, target_column, probability_threshold):\n    # Create a new column to store the predicted class probabilities\n    df['predicted_probability'] = 0\n    \n    # Train a logistic regression model to predict the class probabilities for each row\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    model = LogisticRegression()\n    model.fit(X, y)\n    predicted_probabilities = model.predict_proba(X)\n    \n    # Identify the rows where the predicted probability exceeds the specified threshold\n    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n    df['predicted_class'] = [side_classes[class_] for class_ in predicted_classes]\n    df.loc[df['predicted_probability'] > probability_threshold, 'predicted_class'] = 'high_probability'\n    \n    # Group the dataframe by predicted class and calculate the accuracy for each class\n    accuracy_df = pd.DataFrame({'Class': side_classes, 'Accuracy': [accuracy_score(y, [side_classes[class_] for class_ in predicted_classes]) for class_ in range(len(side_classes))]})\n    \n    return accuracy_df"
    },
    {
        "function_name": "feature_scaling",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numeric_columns": "list of str",
            "categorical_columns": "list of str"
        },
        "objectives": [
            "Scale the numeric data using Standard Scaler.",
            "One-hot encode the categorical data.",
            "Concatenate the scaled numeric data and encoded categorical data.",
            "Return the transformed data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
        ],
        "function_def": "def feature_scaling(df, numeric_columns, categorical_columns):\n    # Scale numeric data\n    scaler = StandardScaler()\n    scaled_numeric_data = scaler.fit_transform(df[numeric_columns])\n    \n    # One-hot encode categorical data\n    encoder = OneHotEncoder()\n    encoded_categorical_data = encoder.fit_transform(df[categorical_columns])\n    \n    # Concatenate scaled numeric data and encoded categorical data\n    transformed_data = pd.concat([pd.DataFrame(scaled_numeric_data, columns=numeric_columns), pd.DataFrame(encoded_categorical_data.toarray()).add_prefix('cat_')], axis=1)\n    \n    return transformed_data"
    },
    {
        "function_name": "analyze_text_sentiment",
        "file_name": "text_sentiment_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "sentiment_column": "str",
            "min_length": "int",
            "max_length": "int"
        },
        "objectives": [
            "Preprocess the text data in the specified column by removing stop words and punctuation.",
            "Calculate the sentiment score for each text using a machine learning model.",
            "Filter out texts with lengths outside the specified range (min_length to max_length).",
            "Return a new dataframe with the filtered texts and their corresponding sentiment scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.linear_model import LogisticRegression",
            "from nltk.sentiment.vader import SentimentIntensityAnalyzer",
            "from nltk.corpus import stopwords",
            "import nltk"
        ],
        "function_def": "def analyze_text_sentiment(df, text_column, sentiment_column, min_length, max_length):\n    stop_words = stopwords.words('english')\n    sia = SentimentIntensityAnalyzer()\n    \n    # Preprocess the text data in the specified column by removing stop words and punctuation\n    df[text_column] = df[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n    \n    # Calculate the sentiment score for each text using a machine learning model\n    sentiment_scores = df[text_column].apply(lambda x: sia.polarity_scores(x)['compound'])\n    \n    # Filter out texts with lengths outside the specified range (min_length to max_length)\n    df = df[(df[text_column].str.len() >= min_length) & (df[text_column].str.len() <= max_length)]\n    \n    return df.assign(sentiment=sentiment_scores)"
    },
    {
        "function_name": "encode_categorical_columns",
        "file_name": "data_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list[str]",
            "cardinality_threshold": "int"
        },
        "objectives": [
            "For each categorical column, calculate the number of unique values.",
            "If the number of unique values is greater than the specified cardinality threshold, convert the column to numerical type using LabelEncoder.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def encode_categorical_columns(df, categorical_columns, cardinality_threshold):\n    # For each categorical column, calculate the number of unique values\n    for column in categorical_columns:\n        unique_values = df[column].nunique()\n        \n        # If the number of unique values is greater than the specified cardinality threshold, convert the column to numerical type using LabelEncoder\n        if unique_values > cardinality_threshold:\n            encoder = LabelEncoder()\n            df[column] = encoder.fit_transform(df[column])\n    \n    return df"
    },
    {
        "function_name": "calculate_sentiment_score",
        "file_name": "sentiment_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "sentiment_column": "str"
        },
        "objectives": [
            "Remove punctuation from the text data.",
            "Convert the text data to lowercase.",
            "Calculate the sentiment score using VADER.",
            "Add the sentiment score to the original dataframe.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import nltk",
            "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
        ],
        "function_def": "def calculate_sentiment_score(df, text_column, sentiment_column):\n    # Remove punctuation from the text data\n    df[text_column] = df[text_column].str.replace(r'[^\\w\\s]', '', regex=True)\n    \n    # Convert the text data to lowercase\n    df[text_column] = df[text_column].str.lower()\n    \n    # Calculate the sentiment score using VADER\n    sia = SentimentIntensityAnalyzer()\n    df[sentiment_column] = df[text_column].apply(lambda x: sia.polarity_scores(x)['compound'])\n    \n    return df"
    },
    {
        "function_name": "permutation_feature_importance",
        "file_name": "feature_importance.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_col": "str",
            "feature_cols": "list of str",
            "target_col": "str"
        },
        "objectives": [
            "Group the data by the id column and calculate the feature importance for each feature column using Permutation Feature Importance.",
            "Create a new dataframe with the feature importance scores for each feature.",
            "Identify the top K features with the highest importance scores.",
            "Create a new dataframe with the top K features and their corresponding importance scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.inspection import permutation_importance",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.ensemble import RandomForestClassifier"
        ],
        "function_def": "def permutation_feature_importance(data, id_col, feature_cols, target_col):\n    X = data[feature_cols]\n    y = data[target_col]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X_train, y_train)\n    importance_scores = permutation_importance(model, X_test, y_test, n_repeats=10)\n    importance_df = pd.DataFrame({'feature': feature_cols, 'importance': importance_scores.importances_mean})\n    top_k_features = importance_df.nlargest(5, 'importance')  # Get top 5 features\n    return top_k_features"
    },
    {
        "function_name": "linear_regression_analysis",
        "file_name": "regression_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "significance_level": "float",
            "confidence_interval": "float",
            "max_r_value": "float"
        },
        "objectives": [
            "Perform linear regression analysis between the specified column and all other columns in the dataframe.",
            "Identify the columns with a significant correlation (based on the specified significance level) and a correlation coefficient greater than the specified max_r_value.",
            "Calculate the confidence interval for the slope of the linear regression line for each identified column.",
            "Create a new dataframe that contains the regression coefficients, standard errors, and confidence intervals for each identified column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.stats import linregress",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def linear_regression_analysis(df, column_name, significance_level, confidence_interval, max_r_value):\n    # Perform linear regression analysis between the specified column and all other columns in the dataframe\n    regression_results = []\n    for col in df.columns:\n        if col != column_name:\n            slope, intercept, r_value, p_value, std_err = linregress(df[column_name], df[col])\n            \n            # Identify the columns with a significant correlation and a correlation coefficient greater than the specified max_r_value\n            if p_value < significance_level and abs(r_value) > max_r_value:\n                # Calculate the confidence interval for the slope of the linear regression line\n                ci = std_err * np.array([1.96, -1.96])  # 95% confidence interval\n                lower_ci = slope + ci[0]\n                upper_ci = slope + ci[1]\n                \n                regression_results.append({\n                    'column': col,\n                    'slope': slope,\n                    'std_err': std_err,\n                    'confidence_interval': (lower_ci, upper_ci)\n                })\n    \n    # Create a new dataframe that contains the regression coefficients, standard errors, and confidence intervals for each identified column\n    regression_df = pd.DataFrame(regression_results)\n    \n    return regression_df"
    },
    {
        "function_name": "validate_time_gaps",
        "file_name": "time_validations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "time_column": "str",
            "max_gap": "int",
            "min_gap": "int"
        },
        "objectives": [
            "Group the dataframe by the group_column and calculate the time differences between consecutive rows within each group.",
            "Identify the rows with time gaps greater than the specified max_gap or less than the specified min_gap.",
            "Create a new column that indicates whether each row has a valid time gap.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def validate_time_gaps(df, group_column, value_column, time_column, max_gap, min_gap):\n    # Group the dataframe by the group_column and calculate the time differences between consecutive rows within each group\n    df['time_diff'] = df.groupby(group_column)[time_column].transform(lambda x: x.diff())\n    \n    # Identify the rows with time gaps greater than the specified max_gap or less than the specified min_gap\n    df['valid_gap'] = (df['time_diff'].dt.days <= max_gap) & (df['time_diff'].dt.days >= min_gap)\n    \n    return df"
    },
    {
        "function_name": "dbscan_clustering",
        "file_name": "clustering_algorithms.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "cluster_column": "str",
            "max_clusters": "int",
            "min_samples": "int"
        },
        "objectives": [
            "Apply the DBSCAN clustering algorithm to the dataframe using the specified cluster_column.",
            "Identify the clusters with a size greater than the specified max_clusters.",
            "Calculate the silhouette coefficient for each cluster.",
            "Create a new column that contains the cluster labels and silhouette coefficients for each row."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import DBSCAN",
            "from sklearn.metrics import silhouette_score"
        ],
        "function_def": "def dbscan_clustering(data, cluster_column, max_clusters, min_samples):\n    # Apply the DBSCAN clustering algorithm to the dataframe\n    clustering = DBSCAN(min_samples=min_samples)\n    cluster_labels = clustering.fit_predict(data[[cluster_column]])\n    \n    # Identify the clusters with a size greater than the specified max_clusters\n    cluster_sizes = pd.Series(cluster_labels).value_counts()\n    large_clusters = cluster_sizes > max_clusters\n    \n    # Calculate the silhouette coefficient for each cluster\n    silhouette_coefficients = silhouette_score(data[[cluster_column]], cluster_labels)\n    \n    # Create a new column that contains the cluster labels and silhouette coefficients for each row\n    data['cluster_label'] = cluster_labels\n    data['silhouette_coefficient'] = silhouette_coefficients\n    \n    return data"
    },
    {
        "function_name": "identify_trend_changes",
        "file_name": "trend_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "value_column": "str",
            "trend_window": "int"
        },
        "objectives": [
            "Calculate the rolling trend of the value column over a window of size trend_window.",
            "Identify the points where the trend changes from increasing to decreasing or vice versa.",
            "Mark these points in a new column 'trend_change'.",
            "Calculate the average value for each trend segment."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_trend_changes(data, date_column, value_column, trend_window):\n    data[date_column] = pd.to_datetime(data[date_column])\n    data['trend'] = data[value_column].rolling(window=trend_window).mean()\n    data['trend_diff'] = data['trend'].diff()\n    data['trend_change'] = (data['trend_diff'] > 0) != (data['trend_diff'].shift(1) > 0)\n    trend_changes = data[data['trend_change']][date_column].tolist()\n    trend_segments = []\n    start = data[date_column].min()\n    for change in trend_changes:\n        segment = data[(data[date_column] >= start) & (data[date_column] < change)]\n        trend_segments.append(segment[value_column].mean())\n        start = change\n    data['trend_segment_avg'] = None\n    start = data[date_column].min()\n    for i, change in enumerate(trend_changes):\n        data.loc[(data[date_column] >= start) & (data[date_column] < change), 'trend_segment_avg'] = trend_segments[i]\n        start = change\n    return data"
    },
    {
        "function_name": "group_labeling",
        "file_name": "group_labeling.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Group the dataframe by the group column and calculate the sum of the value column for each group.",
            "Identify groups where the sum of the value column is above the threshold.",
            "Create a new column 'group_label' and assign it 1 for rows belonging to groups above the threshold, and 0 otherwise.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_labeling(data, group_column, value_column, threshold):\n    group_sums = data.groupby(group_column)[value_column].sum()\n    group_labels = (group_sums > threshold).astype(int)\n    data['group_label'] = data[group_column].map(group_labels)\n    return data"
    },
    {
        "function_name": "resample_and_roll",
        "file_name": "resampling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "value_column": "str",
            "frequency": "str"
        },
        "objectives": [
            "Convert the 'date_column' to datetime format and set it as the dataframe index.",
            "Resample the dataframe by the specified frequency (e.g., 'D' for daily, 'W' for weekly, etc.).",
            "Calculate the rolling mean and rolling standard deviation of the 'value_column' using a window size of 2.",
            "Return the resampled dataframe with the rolling mean and standard deviation."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_roll(df, date_column, value_column, frequency):\n    df[date_column] = pd.to_datetime(df[date_column])\n    df.set_index(date_column, inplace=True)\n    \n    resampled_df = df.resample(frequency).mean()\n    resampled_df['rolling_mean'] = resampled_df[value_column].rolling(2).mean()\n    resampled_df['rolling_std'] = resampled_df[value_column].rolling(2).std()\n    \n    return resampled_df"
    },
    {
        "function_name": "percentile_grouping",
        "file_name": "percentile_grouping.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "value_column": "str",
            "percentile": "float"
        },
        "objectives": [
            "Calculate the specified percentile of the 'value_column'.",
            "Create a new column to indicate whether each value is above or below the calculated percentile.",
            "Calculate the mean and standard deviation of the 'value_column' for each group (above or below the percentile).",
            "Return the dataframe with the processed data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def percentile_grouping(df, value_column, percentile):\n    threshold = df[value_column].quantile(percentile/100)\n    \n    df['above_threshold'] = np.where(df[value_column] > threshold, 1, 0)\n    \n    group_means = df.groupby('above_threshold')[value_column].mean()\n    group_stds = df.groupby('above_threshold')[value_column].std()\n    \n    return df"
    },
    {
        "function_name": "rank_teachers_and_subjects",
        "file_name": "teacher_evaluation.py",
        "parameters": {
            "student_data": "pandas.DataFrame",
            "score_column": "str",
            "subject_column": "str",
            "teacher_column": "str"
        },
        "objectives": [
            "Calculate the average score for each teacher and subject.",
            "Identify the teachers and subjects with the highest average score.",
            "Create a new dataframe with the teachers, subjects, and corresponding average scores.",
            "Rank the teachers and subjects by their average score."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rank_teachers_and_subjects(student_data, score_column, subject_column, teacher_column):\n    avg_score = student_data.groupby([teacher_column, subject_column])[score_column].mean().reset_index()\n    avg_score = avg_score.rename(columns={score_column: 'average_score'})\n    max_score = avg_score['average_score'].max()\n    top_teachers = avg_score[avg_score['average_score'] == max_score]\n    top_teachers = top_teachers[['teacher', 'subject', 'average_score']]\n    top_teachers['rank'] = 1\n    avg_score['rank'] = avg_score['average_score'].rank(method='dense', ascending=False)\n    return top_teachers"
    },
    {
        "function_name": "rank_and_normalize",
        "file_name": "ranking.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "ranking_column": "str",
            "n_top": "int"
        },
        "objectives": [
            "Rank the rows based on the specified column.",
            "Select the top n rows.",
            "Normalize the values in the ranking column to have a mean of 0 and standard deviation of 1."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def rank_and_normalize(df, ranking_column, n_top):\n    # Rank the rows based on the specified column\n    df.sort_values(by=ranking_column, ascending=False, inplace=True)\n    \n    # Select the top n rows\n    top_rows = df.head(n_top)\n    \n    # Normalize the values in the ranking column to have a mean of 0 and standard deviation of 1\n    scaler = StandardScaler()\n    normalized_values = scaler.fit_transform(top_rows[[ranking_column]])\n    top_rows[ranking_column + '_normalized'] = normalized_values\n    \n    return top_rows"
    },
    {
        "function_name": "split_and_calculate_std",
        "file_name": "chunking.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Sort the dataframe by the id_column.",
            "Split the dataframe into chunks of size threshold.",
            "For each chunk, calculate the standard deviation of the numeric columns.",
            "Return the chunks as a list of dataframes with their standard deviations."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_and_calculate_std(df, id_column, threshold):\n    # Sort the dataframe by the id_column\n    df.sort_values(by=id_column, inplace=True)\n    \n    # Split the dataframe into chunks of size threshold\n    chunks = [df[i:i+threshold] for i in range(0, len(df), threshold)]\n    \n    # For each chunk, calculate the standard deviation of the numeric columns\n    chunks_with_std = []\n    for chunk in chunks:\n        numeric_cols = chunk.select_dtypes(include=['int64', 'float64']).columns\n        std = chunk[numeric_cols].std()\n        chunk['std'] = std.mean()\n        chunks_with_std.append(chunk)\n    \n    return chunks_with_std"
    },
    {
        "function_name": "nmf_factorization",
        "file_name": "factorization_models.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "continuous_columns": "list of str",
            "target_column": "str",
            "num_factors": "int"
        },
        "objectives": [
            "One-hot encode the categorical columns using a custom encoding scheme.",
            "Scale the continuous columns using the Min-Max Scaler.",
            "Perform Non-negative Matrix Factorization (NMF) on the concatenated data.",
            "Assign a factor score to each sample based on its factor assignment.",
            "Calculate the Reconstruction Error for each sample."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler",
            "from sklearn.decomposition import NMF",
            "from sklearn.metrics import mean_squared_error"
        ],
        "function_def": "def nmf_factorization(data, categorical_columns, continuous_columns, target_column, num_factors):\n    # One-hot encode the categorical columns\n    encoder = OneHotEncoder()\n    encoded_data = pd.DataFrame(encoder.fit_transform(data[categorical_columns]).toarray())\n    \n    # Scale the continuous columns\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data[continuous_columns])\n    \n    # Concatenate the one-hot encoded categorical data and the scaled continuous data\n    concatenated_data = pd.concat([encoded_data, pd.DataFrame(scaled_data)], axis=1)\n    \n    # Perform NMF on the concatenated data\n    model = NMF(n_components=num_factors)\n    factorizado_data = model.fit_transform(concatenated_data)\n    \n    # Assign a factor score to each sample based on its factor assignment\n    data['factor_score'] = np.sum(factorizado_data, axis=1)\n    \n    # Calculate the Reconstruction Error for each sample\n    reconstructed_data = model.inverse_transform(factorizado_data)\n    reconstruction_error = mean_squared_error(concatenated_data, reconstructed_data)\n    \n    return data, reconstruction_error"
    },
    {
        "function_name": "day_of_week_avg",
        "file_name": "data_quality_check.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "value_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Convert the date column to a datetime object and create a new column with the day of the week.",
            "Group the data by the day of the week and calculate the average of the values in the value column.",
            "Identify the groups where the average value is greater than the threshold.",
            "Create a new column with the average values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def day_of_week_avg(data, date_column, value_column, threshold):\n    # Convert the date column to a datetime object and create a new column with the day of the week\n    data['day_of_week'] = pd.to_datetime(data[date_column]).dt.day_name()\n    \n    # Group the data by the day of the week and calculate the average of the values in the value column\n    avg_values = data.groupby('day_of_week')[value_column].mean()\n    \n    # Identify the groups where the average value is greater than the threshold\n    mask = avg_values > threshold\n    \n    # Create a new column with the average values\n    data['avg_value'] = data['day_of_week'].map(avg_values)\n    \n    return data"
    },
    {
        "function_name": "predict_target_KNN",
        "file_name": "regression.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_columns": "list",
            "target_column": "str",
            "K": "int"
        },
        "objectives": [
            "Scale the feature columns using StandardScaler.",
            "Train a K-Nearest Neighbors regressor on the scaled data.",
            "Use the trained model to predict the target column.",
            "Create a new column with the predicted values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.neighbors import KNeighborsRegressor"
        ],
        "function_def": "def predict_target_KNN(df, feature_columns, target_column, K):\n    # Scale the feature columns using StandardScaler\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(df[feature_columns])\n    \n    # Train a K-Nearest Neighbors regressor on the scaled data\n    model = KNeighborsRegressor(n_neighbors=K)\n    model.fit(scaled_features, df[target_column])\n    \n    # Use the trained model to predict the target column\n    predictions = model.predict(scaled_features)\n    \n    # Create a new column with the predicted values\n    df['predicted_target'] = predictions\n    \n    return df"
    },
    {
        "function_name": "train_model",
        "file_name": "model_training.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_variable": "str",
            "predictor_variables": "list",
            "model_type": "str"
        },
        "objectives": [
            "Perform feature scaling on the predictor variables.",
            "Train a machine learning model (linear regression or decision tree) based on the model type.",
            "Calculate the feature importance for each predictor variable.",
            "Return the trained model and the feature importance."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.tree import DecisionTreeRegressor",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.metrics import mean_squared_error"
        ],
        "function_def": "def train_model(data, target_variable, predictor_variables, model_type):\n    # Perform feature scaling on the predictor variables\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data[predictor_variables])\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(scaled_data, data[target_variable], test_size=0.2, random_state=42)\n    \n    # Train a machine learning model\n    if model_type == 'linear_regression':\n        model = LinearRegression()\n    elif model_type == 'decision_tree':\n        model = DecisionTreeRegressor()\n    model.fit(X_train, y_train)\n    \n    # Calculate feature importance\n    if model_type == 'decision_tree':\n        feature_importance = model.feature_importances_\n    else:\n        feature_importance = np.abs(model.coef_)\n    \n    return model, feature_importance"
    },
    {
        "function_name": "ewma",
        "file_name": "timeseries_analysis.py",
        "parameters": {
            "series": "pandas.Series",
            "thr": "float",
            "window_size": "int"
        },
        "objectives": [
            "Use the moving average of the time series to remove noise and seasonality.",
            "Calculate the root mean square error (RMSE) between the original time series and its moving average.",
            "Calculate the decay factor for the exponentially weighted moving average (EWMA) using the RMSE.",
            "Apply the EWMA to the original time series."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def ewma(series, thr, window_size):\n    # Use the moving average of the time series to remove noise and seasonality\n    moving_average = series.rolling(window_size).mean()\n    \n    # Calculate the root mean square error (RMSE) between the original time series and its moving average\n    rmse = ((series - moving_average) ** 2).mean() ** 0.5\n    \n    # Calculate the decay factor for the exponentially weighted moving average (EWMA) using the RMSE\n    decay_factor = 1 / (1 + rmse ** 2)\n    \n    # Apply the EWMA to the original time series\n    ewma_series = series.ewm(alpha=decay_factor).mean()\n    \n    return ewma_series"
    },
    {
        "function_name": "avg_transaction_amount",
        "file_name": "transaction_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "transaction_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the cumulative sum of the transaction amount for each id.",
            "Identify the ids with a cumulative sum exceeding the threshold.",
            "Calculate the average transaction amount for each id before exceeding the threshold.",
            "Return a dictionary with the id as the key and the average transaction amount as the value."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def avg_transaction_amount(data, id_column, transaction_column, threshold):\n    # Calculate the cumulative sum of the transaction amount for each id\n    data['cumulative_sum'] = data.groupby(id_column)[transaction_column].cumsum()\n    # Identify ids with a cumulative sum exceeding the threshold\n    exceeded_ids = data[data['cumulative_sum'] > threshold][id_column].unique()\n    # Calculate the average transaction amount for each id before exceeding the threshold\n    avg_amounts = {}\n    for id in exceeded_ids:\n        id_data = data[data[id_column] == id]\n        exceeded_idx = id_data[id_data['cumulative_sum'] > threshold].index[0]\n        avg_amounts[id] = id_data.loc[:exceeded_idx-1, transaction_column].mean()\n    return avg_amounts"
    },
    {
        "function_name": "binning_and_entropy",
        "file_name": "binning_entropy.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "value_column": "str",
            "bin_number": "int",
            "method": "str"
        },
        "objectives": [
            "Bin the specified column values into the specified number of bins.",
            "Apply the specified method ('equal_width', 'equal_frequency', or 'kmeans') for binning.",
            "Calculate the entropy of the binned values.",
            "Return the resulting dataframe with the binned values and entropy."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans",
            "from scipy.stats import entropy",
            "import numpy as np"
        ],
        "function_def": "def binning_and_entropy(data, value_column, bin_number, method):\n    if method == 'equal_width':\n        # Apply equal-width binning\n        data['binned_values'] = pd.cut(data[value_column], bins=bin_number)\n    elif method == 'equal_frequency':\n        # Apply equal-frequency binning\n        data['binned_values'] = pd.qcut(data[value_column], q=bin_number)\n    elif method == 'kmeans':\n        # Apply k-means binning\n        kmeans = KMeans(n_clusters=bin_number)\n        data['binned_values'] = kmeans.fit_predict(data[[value_column]])\n    # Calculate the entropy of the binned values\n    binned_counts = data['binned_values'].value_counts()\n    probabilities = binned_counts / len(data)\n    data['entropy'] = entropy(probabilities)\n    return data"
    },
    {
        "function_name": "model_prediction",
        "file_name": "modeling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "feature_columns": "list",
            "outcome_column": "str",
            "method": "str ('logistic_regression' or 'decision_tree')"
        },
        "objectives": [
            "Split the data into training and testing sets (70% for training and 30% for testing).",
            "Train a model using the specified method on the training data to predict the outcome column.",
            "Evaluate the model on the testing data using the area under the receiver operating characteristic curve (AUROC).",
            "Return the predicted probabilities for the testing data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.metrics import roc_auc_score"
        ],
        "function_def": "def model_prediction(df, id_column, feature_columns, outcome_column, method):\n    # Split the data into training and testing sets\n    train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n    \n    # Train a model using the specified method on the training data\n    if method == 'logistic_regression':\n        model = LogisticRegression(max_iter=1000)\n    elif method == 'decision_tree':\n        model = DecisionTreeClassifier()\n    else:\n        raise ValueError(\"Invalid method. Choose 'logistic_regression' or 'decision_tree'.\")\n    \n    model.fit(train_data[feature_columns], train_data[outcome_column])\n    \n    # Evaluate the model on the testing data using the area under the receiver operating characteristic curve (AUROC)\n    predicted_probabilities = model.predict_proba(test_data[feature_columns])[:, 1]\n    auroc = roc_auc_score(test_data[outcome_column], predicted_probabilities)\n    \n    return predicted_probabilities"
    },
    {
        "function_name": "evaluate_data_quality",
        "file_name": "quality_evaluation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "frequency": "str",
            "rolling_window": "int"
        },
        "objectives": [
            "Convert the time column to datetime format and set it as the index.",
            "Resample the dataframe to the specified frequency.",
            "Calculate the rolling standard deviation of each column with the specified rolling window.",
            "Create a new column 'data_quality' and assign it 1 if the rolling standard deviation is within 1% of the average rolling standard deviation, and 0 otherwise."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def evaluate_data_quality(df, time_column, frequency, rolling_window):\n    df[time_column] = pd.to_datetime(df[time_column])\n    df.set_index(time_column, inplace=True)\n    df_resampled = df.resample(frequency).mean()\n    rolling_std = df_resampled.rolling(rolling_window).std()\n    avg_rolling_std = rolling_std.mean()\n    df_resampled['data_quality'] = (rolling_std.sub(avg_rolling_std).abs().div(avg_rolling_std) < 0.01).astype(int)\n    return df_resampled"
    },
    {
        "function_name": "identify_rare_categories",
        "file_name": "categorical_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the category frequency distribution for the specified categorical column.",
            "Identify categories with frequency less than the specified threshold.",
            "Create a new column 'rare_category' and assign it 1 for rows with rare categories, and 0 otherwise.",
            "Return the updated dataframe with the 'rare_category' column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_rare_categories(data, categorical_column, threshold):\n    category_freq = data[categorical_column].value_counts(normalize=True)\n    rare_categories = category_freq[category_freq < threshold].index\n    data['rare_category'] = data[categorical_column].isin(rare_categories).astype(int)\n    return data"
    },
    {
        "function_name": "geographical_insights",
        "file_name": "geographical_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "geographical_column": "str",
            "date_column": "str"
        },
        "objectives": [
            "Identify the unique geographical locations in the data.",
            "Extract the top 5 locations with the most number of entries.",
            "Group the data by date and geographical location.",
            "Calculate the number of entries for each date and location combination."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def geographical_insights(data, geographical_column, date_column):\n    unique_locations = data[geographical_column].unique()\n    top_locations = data[geographical_column].value_counts().head(5).index\n    \n    grouped_data = data.groupby([date_column, geographical_column]).size().reset_index(name='count')\n    \n    insights = {\n        'unique_locations': unique_locations,\n        'top_locations': top_locations,\n        'grouped_data': grouped_data\n    }\n    \n    return insights"
    },
    {
        "function_name": "transform_numeric",
        "file_name": "outlier_reduction.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numeric_column": "str",
            "method": "str (winsorize or log-scale)"
        },
        "objectives": [
            "Apply the specified method to the numeric column to reduce the effect of outliers.",
            "Calculate the skewness and kurtosis of the transformed column.",
            "Create new columns for the skewness and kurtosis.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy import stats"
        ],
        "function_def": "def transform_numeric(data, numeric_column, method):\n    if method == 'winsorize':\n        # Apply winsorization to the numeric column\n        data[numeric_column] = stats.mstats.winsorize(data[numeric_column], limits=[0.1, 0.1])\n    elif method == 'log-scale':\n        # Apply log-scale transformation to the numeric column\n        data[numeric_column] = np.log(data[numeric_column] + 1)\n    else:\n        raise ValueError(\"Invalid method. Please choose 'winsorize' or 'log-scale'.\")\n    \n    # Calculate the skewness and kurtosis of the transformed column\n    skewness = data[numeric_column].skew()\n    kurtosis = data[numeric_column].kurtosis()\n    \n    # Create new columns for the skewness and kurtosis\n    data['skewness'] = skewness\n    data['kurtosis'] = kurtosis\n    \n    return data"
    },
    {
        "function_name": "hybrid_feature_analysis",
        "file_name": "feature_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_column": "str",
            "continuous_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "One-hot encode the categorical column and concatenate it with the original dataframe.",
            "Scale the continuous column using the StandardScaler.",
            "Identify the rows where the scaled continuous column is greater than the specified threshold.",
            "Calculate the correlation between the one-hot encoded categorical column and the scaled continuous column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
            "from sklearn.compose import ColumnTransformer"
        ],
        "function_def": "def hybrid_feature_analysis(data, categorical_column, continuous_column, threshold):\n    # One-hot encode the categorical column and concatenate it with the original dataframe\n    encoder = OneHotEncoder()\n    encoded_column = encoder.fit_transform(data[[categorical_column]])\n    encoded_df = pd.get_dummies(data, columns=[categorical_column])\n    \n    # Scale the continuous column using the StandardScaler\n    scaler = StandardScaler()\n    scaled_column = scaler.fit_transform(data[[continuous_column]])\n    \n    # Identify the rows where the scaled continuous column is greater than the specified threshold\n    filtered_df = encoded_df[scaled_column > threshold]\n    \n    # Calculate the correlation between the one-hot encoded categorical column and the scaled continuous column\n    correlation = pd.concat([encoded_df, pd.DataFrame(scaled_column)], axis=1).corr()\n    \n    return correlation"
    },
    {
        "function_name": "value_filtering",
        "file_name": "value_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Identify the unique values in the specified column.",
            "Calculate the frequency of each unique value.",
            "Filter out the unique values with frequency less than the specified threshold.",
            "Return the resulting dataframe with the filtered unique values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def value_filtering(df, column, threshold):\n    # Identify the unique values in the specified column\n    unique_values = df[column].unique()\n    \n    # Calculate the frequency of each unique value\n    frequency = df[column].value_counts()\n    \n    # Filter out the unique values with frequency less than the specified threshold\n    filtered_values = frequency[frequency > threshold].index\n    \n    # Return the resulting dataframe with the filtered unique values\n    result = df[df[column].isin(filtered_values)]\n    return result"
    },
    {
        "function_name": "windowed_analytics",
        "file_name": "windowed_analytics.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_column": "str",
            "value_column": "str",
            "time_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Group the dataframe by the group column and calculate the moving average of the value column within a specified window size.",
            "Calculate the standard deviation of the value column within the same window size.",
            "Identify the rows where the moving average is greater than the standard deviation.",
            "Return the resulting dataframe with the moving average, standard deviation, and identified rows."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def windowed_analytics(data, group_column, value_column, time_column, window_size):\n    # Group the dataframe by the group column and calculate the moving average of the value column within a specified window size\n    group_df = data.groupby(group_column)[value_column].rolling(window_size).mean().reset_index()\n    \n    # Calculate the standard deviation of the value column within the same window size\n    group_df['std_dev'] = data.groupby(group_column)[value_column].rolling(window_size).std().reset_index()[value_column]\n    \n    # Identify the rows where the moving average is greater than the standard deviation\n    filtered_df = group_df[group_df[value_column] > group_df['std_dev']]\n    \n    return filtered_df"
    },
    {
        "function_name": "is_older_than_threshold",
        "file_name": "date_operations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "target_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "Convert the date column to datetime format.",
            "Calculate the time difference between the date column and the current date.",
            "Identify rows where the time difference is greater than the specified threshold.",
            "Create a new column with a boolean value indicating whether the row is older than the threshold."
        ],
        "import_lines": [
            "import pandas as pd",
            "from datetime import datetime"
        ],
        "function_def": "def is_older_than_threshold(data, date_column, target_column, threshold):\n    # Convert the date column to datetime format\n    data[date_column] = pd.to_datetime(data[date_column])\n    \n    # Calculate the time difference between the date column and the current date\n    data['time_diff'] = (datetime.now() - data[date_column]).dt.days\n    \n    # Identify rows where the time difference is greater than the specified threshold\n    data[target_column] = data['time_diff'] > threshold\n    \n    # Drop the time_diff column\n    data = data.drop('time_diff', axis=1)\n    \n    return data"
    },
    {
        "function_name": "calculate_lift",
        "file_name": "lift_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "target_column": "str",
            "minimum_lift": "float"
        },
        "objectives": [
            "Convert the categorical column to numerical values using LabelEncoder.",
            "Calculate the lift of each category with respect to the target column.",
            "Identify the categories with lift greater than the minimum lift.",
            "Return the categories with high lift."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def calculate_lift(df, categorical_column, target_column, minimum_lift):\n    # Convert categorical column to numerical values using LabelEncoder\n    le = LabelEncoder()\n    df[categorical_column] = le.fit_transform(df[categorical_column])\n    \n    # Calculate the lift of each category with respect to the target column\n    lift_df = df.groupby(categorical_column)[target_column].mean().reset_index()\n    lift_df['lift'] = lift_df[target_column] / lift_df[target_column].mean()\n    \n    # Identify the categories with lift greater than the minimum lift\n    high_lift_categories = lift_df[lift_df['lift'] > minimum_lift]\n    \n    return high_lift_categories"
    },
    {
        "function_name": "logistic_regression_prediction",
        "file_name": "logistic_regression.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "numerical_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "One-hot encode the categorical column.",
            "Scale the numerical column using StandardScaler.",
            "Concatenate the one-hot encoded categorical column and the scaled numerical column.",
            "Perform a logistic regression on the concatenated data to predict the target column.",
            "Return the predicted probabilities and the coefficients of the logistic regression model."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.linear_model import LogisticRegression"
        ],
        "function_def": "def logistic_regression_prediction(df, categorical_column, numerical_column, target_column):\n    # One-hot encode the categorical column\n    categorical_data = pd.get_dummies(df[categorical_column], drop_first=True)\n    \n    # Scale the numerical column\n    numerical_data = df[numerical_column]\n    scaler = StandardScaler()\n    scaled_numerical_data = pd.DataFrame(scaler.fit_transform(numerical_data.values.reshape(-1, 1)), columns=[numerical_column])\n    \n    # Concatenate the one-hot encoded categorical column and the scaled numerical column\n    concatenated_data = pd.concat([categorical_data, scaled_numerical_data], axis=1)\n    \n    # Perform a logistic regression on the concatenated data\n    model = LogisticRegression()\n    model.fit(concatenated_data, df[target_column])\n    \n    # Predict the probabilities\n    predicted_probabilities = model.predict_proba(concatenated_data)\n    \n    # Get the coefficients of the logistic regression model\n    coefficients = model.coef_\n    \n    return predicted_probabilities, coefficients"
    },
    {
        "function_name": "datetime_feature_extraction",
        "file_name": "datetime_preprocessing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "time_column": "str",
            "timezone": "str"
        },
        "objectives": [
            "Convert the date and time columns into a single datetime column.",
            "Localize the datetime column to the specified timezone.",
            "Extract the hour, day, month, and year from the datetime column as new features."
        ],
        "import_lines": [
            "import pandas as pd",
            "import pytz"
        ],
        "function_def": "def datetime_feature_extraction(data, date_column, time_column, timezone):\n    # Convert date and time columns into a single datetime column\n    data['datetime'] = pd.to_datetime(data[date_column] + ' ' + data[time_column])\n    \n    # Localize the datetime column to the specified timezone\n    data['datetime'] = data['datetime'].dt.tz_localize(pytz.timezone(timezone))\n    \n    # Extract hour, day, month, and year as new features\n    data['hour'] = data['datetime'].dt.hour\n    data['day'] = data['datetime'].dt.day\n    data['month'] = data['datetime'].dt.month\n    data['year'] = data['datetime'].dt.year\n    \n    return data"
    },
    {
        "function_name": "remove_outliers_and_scale",
        "file_name": "outlier_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "threshold": "float",
            "num_std": "int"
        },
        "objectives": [
            "Identify the columns in the dataframe that contain outliers based on the Z-score method.",
            "Remove the rows that contain outliers in the identified columns.",
            "Scale the remaining data using StandardScaler.",
            "Clip the scaled data to the specified threshold."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def remove_outliers_and_scale(data, threshold, num_std):\n    outlier_cols = data.columns[data.apply(lambda x: abs(x - x.mean()) > num_std * x.std()).any()]\n    clean_data = data[~data[outlier_cols].apply(lambda x: abs(x - x.mean()) > num_std * x.std()).any(axis=1)]\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(clean_data)\n    scaled_data[scaled_data > threshold] = threshold\n    scaled_data[scaled_data < -threshold] = -threshold\n    return pd.DataFrame(scaled_data, columns=clean_data.columns)"
    },
    {
        "function_name": "gradient_boosting",
        "file_name": "gradient_boosting.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_columns": "list",
            "categorical_columns": "list"
        },
        "objectives": [
            "Scale the numerical columns using the Standard Scaler.",
            "One-hot encode the categorical columns.",
            "Concatenate the scaled and encoded columns.",
            "Train a Gradient Boosting regressor on the concatenated data."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
            "from sklearn.ensemble import GradientBoostingRegressor",
            "from sklearn.metrics import mean_squared_error",
            "import numpy as np"
        ],
        "function_def": "def gradient_boosting(df, numerical_columns, categorical_columns):\n    # Scale the numerical columns using the Standard Scaler\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df[numerical_columns])\n    \n    # One-hot encode the categorical columns\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    encoded_data = encoder.fit_transform(df[categorical_columns])\n    \n    # Concatenate the scaled and encoded columns\n    concatenated_data = np.concatenate((scaled_data, encoded_data.toarray()), axis=1)\n    \n    # Train a Gradient Boosting regressor on the concatenated data\n    X = concatenated_data\n    y = df['target']\n    gb_model = GradientBoostingRegressor()\n    gb_model.fit(X, y)\n    \n    # Calculate the mean squared error\n    y_pred = gb_model.predict(X)\n    mse = mean_squared_error(y, y_pred)\n    \n    return mse"
    },
    {
        "function_name": "data_chunking",
        "file_name": "data_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "chunk_size": "int"
        },
        "objectives": [
            "Split the data into chunks of the specified size.",
            "Calculate the descriptive statistics for each chunk.",
            "Calculate the correlation between each pair of columns.",
            "Identify the chunks with the highest correlation."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def data_chunking(data, chunk_size):\n    # Split the data into chunks of the specified size\n    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n    \n    # Calculate the descriptive statistics for each chunk\n    descriptive_statistics = [chunk.describe() for chunk in chunks]\n    \n    # Calculate the correlation between each pair of columns\n    correlations = [chunk.corr() for chunk in chunks]\n    \n    # Identify the chunks with the highest correlation\n    max_correlations = [correlation.max().max() for correlation in correlations]\n    max_correlation_index = np.argmax(max_correlations)\n    \n    return descriptive_statistics, correlations, chunks[max_correlation_index]"
    },
    {
        "function_name": "cluster_hierarchy",
        "file_name": "hierarchical_clustering.py",
        "parameters": {
            "data": "numpy.ndarray",
            "seed": "int"
        },
        "objectives": [
            "Generate random samples from the data using the k-Means algorithm.",
            "Create a hierarchy of clusters using the AgglomerativeClustering algorithm.",
            "Calculate the within-cluster sum of squares (WCSS) for each level of the hierarchy.",
            "Return the updated dataframe with the cluster labels and the WCSS values."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.cluster import KMeans, AgglomerativeClustering",
            "from sklearn.metrics import silhouette_score"
        ],
        "function_def": "def cluster_hierarchy(data, seed):\n    # Generate random samples from the data using the k-Means algorithm\n    kmeans = KMeans(n_clusters=10, random_state=seed)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Create a hierarchy of clusters using the AgglomerativeClustering algorithm\n    hierarchy = AgglomerativeClustering(n_clusters=10, linkage='ward')\n    hierarchy_labels = hierarchy.fit_predict(data)\n    \n    # Calculate the within-cluster sum of squares (WCSS) for each level of the hierarchy\n    wcss_values = []\n    for level in range(1, 11):\n        hierarchy_level = AgglomerativeClustering(n_clusters=level, linkage='ward')\n        hierarchy_level_labels = hierarchy_level.fit_predict(data)\n        wcss = np.sum([np.sum((data[hierarchy_level_labels == label] - np.mean(data[hierarchy_level_labels == label], axis=0)) ** 2) for label in np.unique(hierarchy_level_labels)])\n        wcss_values.append(wcss)\n    \n    # Return the updated dataframe with the cluster labels and the WCSS values\n    df = pd.DataFrame(np.column_stack((data, cluster_labels, hierarchy_labels)))\n    df.columns = ['feature1', 'feature2', 'cluster_label', 'hierarchy_label']\n    \n    return df, wcss_values"
    },
    {
        "function_name": "cumulative_sum_with_min_values",
        "file_name": "cumulative_calculations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "target_column": "str",
            "min_values": "int"
        },
        "objectives": [
            "Group the dataframe by the group_column.",
            "Calculate the cumulative count of each group.",
            "For each group, calculate the cumulative sum of the target_column until the cumulative count reaches min_values.",
            "Return a new dataframe with the cumulative sums for each group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def cumulative_sum_with_min_values(df, group_column, target_column, min_values):\n    df.sort_values(by=group_column, inplace=True)\n    df['cumulative_count'] = df.groupby(group_column).cumcount()\n    df['cumulative_sum'] = df.groupby(group_column)[target_column].transform(lambda x: x.cumsum().where(x.index >= min_values))\n    return df.dropna(subset=['cumulative_sum']).reset_index(drop=True)[[group_column, 'cumulative_sum']]"
    },
    {
        "function_name": "quantile_means",
        "file_name": "quantile_calculations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "target_column": "str",
            "bins": "int"
        },
        "objectives": [
            "Group the dataframe by the group_column.",
            "For each group, calculate the quantiles of the target_column using binning with bins number of bins.",
            "For each group, calculate the mean of each quantile.",
            "Return a new dataframe with the mean values for each quantile and group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def quantile_means(df, group_column, target_column, bins):\n    df['quantile'] = df.groupby(group_column)[target_column].transform(lambda x: pd.qcut(x, bins, labels=False))\n    df['bin_mean'] = df.groupby([group_column, 'quantile'])[target_column].transform('mean')\n    return df.dropna(subset=['bin_mean']).reset_index(drop=True)[[group_column, 'quantile', 'bin_mean']]"
    },
    {
        "function_name": "calculate_model_performance",
        "file_name": "model_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "predictor_column": "str",
            "apply_log": "bool"
        },
        "objectives": [
            "Calculate the correlation between the target column and the predictor column in the DataFrame.",
            "Apply a log transformation to the predictor column if specified.",
            "Calculate the coefficient of determination (R-squared) between the target column and the predictor column.",
            "Return the correlation and R-squared values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.metrics import r2_score"
        ],
        "function_def": "def calculate_model_performance(df, target_column, predictor_column, apply_log):\n    if apply_log:\n        df[predictor_column] = np.log(df[predictor_column])\n    \n    correlation = df[target_column].corr(df[predictor_column])\n    \n    X = df[predictor_column].values.reshape(-1, 1)\n    y = df[target_column].values\n    model = LinearRegression()\n    model.fit(X, y)\n    y_pred = model.predict(X)\n    r_squared = r2_score(y, y_pred)\n    \n    return correlation, r_squared"
    },
    {
        "function_name": "date_extraction",
        "file_name": "date_extraction.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "granularity": "str"
        },
        "objectives": [
            "Convert the date column to a datetime object.",
            "Extract the relevant date parts based on the granularity (e.g., year, month, day, hour).",
            "Create new columns for each extracted date part.",
            "Return the updated dataframe with the extracted date parts."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def date_extraction(data, date_column, granularity):\n    # Convert the date column to a datetime object\n    data[date_column] = pd.to_datetime(data[date_column])\n    \n    # Extract the relevant date parts based on the granularity\n    if granularity == 'year':\n        data['year'] = data[date_column].dt.year\n    elif granularity == 'month':\n        data['year'] = data[date_column].dt.year\n        data['month'] = data[date_column].dt.month\n    elif granularity == 'day':\n        data['year'] = data[date_column].dt.year\n        data['month'] = data[date_column].dt.month\n        data['day'] = data[date_column].dt.day\n    elif granularity == 'hour':\n        data['year'] = data[date_column].dt.year\n        data['month'] = data[date_column].dt.month\n        data['day'] = data[date_column].dt.day\n        data['hour'] = data[date_column].dt.hour\n    \n    # Return the updated dataframe with the extracted date parts\n    return data"
    },
    {
        "function_name": "knn_imputation",
        "file_name": "missing_value_imputation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "threshold": "int",
            "num_neighbors": "int"
        },
        "objectives": [
            "Identify instances with missing values in the target column.",
            "Impute the missing values using K-Nearest Neighbors (KNN) imputation.",
            "Remove instances with a large number of missing values (more than the threshold).",
            "Return a new dataframe with the imputed values and the number of missing values for each instance."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.impute import KNNImputer"
        ],
        "function_def": "def knn_imputation(data, target_column, threshold, num_neighbors):\n    # Identify instances with missing values in the target column\n    missing_instances = data[data[target_column].isnull()]\n    \n    # Impute missing values using KNN imputation\n    imputer = KNNImputer(n_neighbors=num_neighbors)\n    imputed_values = imputer.fit_transform(data)\n    data[target_column] = imputed_values[:, data.columns.get_loc(target_column)]\n    \n    # Remove instances with a large number of missing values\n    data = data[data.isnull().sum(axis=1) <= threshold]\n    \n    # Return a new dataframe with the imputed values and the number of missing values for each instance\n    return data.assign(num_missing=data.isnull().sum(axis=1))"
    },
    {
        "function_name": "date_filter_and_difference_calculator",
        "file_name": "date_filtering.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "date_column": "str",
            "min_date": "str",
            "max_date": "str"
        },
        "objectives": [
            "Convert the date_column to datetime format.",
            "Filter the data to include only rows where the date is between min_date and max_date.",
            "Calculate the time difference between the date_column and min_date.",
            "Create a new dataframe with the filtered data and a column indicating the time difference."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def date_filter_and_difference_calculator(data, date_column, min_date, max_date):\n    data[date_column] = pd.to_datetime(data[date_column])\n    filtered_data = data[(data[date_column] >= pd.to_datetime(min_date)) & (data[date_column] <= pd.to_datetime(max_date))]\n    time_difference = filtered_data[date_column] - pd.to_datetime(min_date)\n    result = pd.concat([filtered_data, time_difference.dt.days.to_frame('time_difference')], axis=1)\n    return result"
    },
    {
        "function_name": "feature_selector_and_model_trainer",
        "file_name": "model_building.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_columns": "list of str",
            "target_column": "str",
            "num_features": "int"
        },
        "objectives": [
            "Select the top num_features most informative features using the mutual information method.",
            "Train a linear regression model on the selected features and the target_column.",
            "Calculate the R-squared value of the model.",
            "Return the trained model and the R-squared value."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import SelectKBest",
            "from sklearn.feature_selection import mutual_info_regression",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.metrics import r2_score"
        ],
        "function_def": "def feature_selector_and_model_trainer(data, feature_columns, target_column, num_features):\n    selector = SelectKBest(score_func=mutual_info_regression, k=num_features)\n    selector.fit(data[feature_columns], data[target_column])\n    X_selected = selector.transform(data[feature_columns])\n    model = LinearRegression()\n    model.fit(X_selected, data[target_column])\n    y_pred = model.predict(X_selected)\n    r2 = r2_score(data[target_column], y_pred)\n    return model, r2"
    },
    {
        "function_name": "category_filtering",
        "file_name": "category_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "threshold": "int"
        },
        "objectives": [
            "One-hot encode the categorical column.",
            "Calculate the frequency of each category.",
            "Filter out categories with frequency less than the threshold.",
            "Remove rows that contain filtered categories."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def category_filtering(df, categorical_column, threshold):\n    # One-hot encode the categorical column\n    onehot_df = pd.get_dummies(df[categorical_column], drop_first=True)\n    \n    # Calculate the frequency of each category\n    category_freq = onehot_df.sum()\n    \n    # Filter out categories with frequency less than the threshold\n    filtered_categories = category_freq[category_freq >= threshold].index\n    \n    # Remove rows that contain filtered categories\n    filtered_df = onehot_df[filtered_categories]\n    \n    return filtered_df"
    },
    {
        "function_name": "outlier_detection",
        "file_name": "outlier_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "Calculate the correlation between the feature column and the target column.",
            "Perform a regression analysis to model the relationship between the feature column and the target column.",
            "Calculate the residuals of the regression model.",
            "Identify the outliers in the residuals."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.metrics import mean_squared_error"
        ],
        "function_def": "def outlier_detection(df, feature_column, target_column):\n    # Calculate the correlation between the feature column and the target column\n    correlation = np.corrcoef(df[feature_column], df[target_column])[0, 1]\n    \n    # Perform a regression analysis to model the relationship between the feature column and the target column\n    model = LinearRegression()\n    model.fit(df[[feature_column]], df[target_column])\n    \n    # Calculate the residuals of the regression model\n    residuals = df[target_column] - model.predict(df[[feature_column]])\n    \n    # Identify the outliers in the residuals\n    outlier_threshold = 2 * np.std(residuals)\n    outliers = df[np.abs(residuals) > outlier_threshold]\n    \n    return outliers"
    },
    {
        "function_name": "calculate_monthly_averages",
        "file_name": "date_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "start_date": "str",
            "end_date": "str",
            "date_column": "str"
        },
        "objectives": [
            "Filter the DataFrame to include only rows where the date falls within the specified range.",
            "Calculate the average value of a specified column for each month in the filtered DataFrame.",
            "Return the average values as a new DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_monthly_averages(df, start_date, end_date, date_column):\n    # Filter the DataFrame to include only rows where the date falls within the specified range\n    df = df[(df[date_column] >= start_date) & (df[date_column] <= end_date)]\n    \n    # Calculate the average value of a specified column for each month in the filtered DataFrame\n    df[date_column] = pd.to_datetime(df[date_column])\n    df['month'] = df[date_column].dt.strftime('%Y-%m')\n    avg_values = df.groupby('month')['value'].mean().reset_index()\n    \n    return avg_values"
    },
    {
        "function_name": "calculate_edit_distance",
        "file_name": "distance_metrics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "source_column": "str",
            "target_column": "str",
            "max_distance": "int"
        },
        "objectives": [
            "Calculate the edit distance between the source column and the target column for each row.",
            "Create a new column with the edit distance for each row.",
            "Identify rows where the edit distance exceeds the maximum allowed distance.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.metrics import edit_distance"
        ],
        "function_def": "def calculate_edit_distance(df, source_column, target_column, max_distance):\n    # Calculate the edit distance between the source column and the target column for each row\n    df['edit_distance'] = df.apply(lambda row: edit_distance(row[source_column], row[target_column]), axis=1)\n    \n    # Identify rows where the edit distance exceeds the maximum allowed distance\n    exceeding_rows = df[df['edit_distance'] > max_distance]\n    \n    return df"
    },
    {
        "function_name": "3",
        "file_name": "category_filters.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "cardinality_threshold": "int",
            "frequent_threshold": "float"
        },
        "objectives": [
            "Identify categories in the categorical column with cardinality above the specified threshold.",
            "Group the dataframe by these categories and calculate the frequency of each category.",
            "Filter out categories with frequency below the specified threshold.",
            "Return the filtered dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_rare_categories(df, categorical_column, cardinality_threshold, frequent_threshold):\n    # Identify categories in the categorical column with cardinality above the specified threshold\n    high_cardinality_categories = df[categorical_column].value_counts()[lambda x: x >= cardinality_threshold].index\n    \n    # Group the dataframe by these categories and calculate the frequency of each category\n    category_freq = df[categorical_column].value_counts(normalize=True)\n    \n    # Filter out categories with frequency below the specified threshold\n    filtered_categories = category_freq[category_freq >= frequent_threshold].index\n    \n    # Return the filtered dataframe\n    df = df[df[categorical_column].isin(filtered_categories)]\n    \n    return df"
    },
    {
        "function_name": "filter_products_by_sales_and_price",
        "file_name": "sales_insights.py",
        "parameters": {
            "sales_data": "pandas.DataFrame",
            "date_column": "str",
            "product_column": "str",
            "min_sales": "int",
            "max_price": "float"
        },
        "objectives": [
            "Convert the date_column to datetime format.",
            "Filter the sales data to include only rows where the product sales are above min_sales.",
            "Within each product group, calculate the average price.",
            "Filter the product groups where the average price is below max_price.",
            "Create a new column indicating the corresponding product category."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_products_by_sales_and_price(sales_data, date_column, product_column, min_sales, max_price):\n    sales_data[date_column] = pd.to_datetime(sales_data[date_column])\n    \n    # Filter the sales data to include only rows where the product sales are above min_sales\n    filtered_sales = sales_data.groupby(product_column).filter(lambda x: x['quantity'].sum() > min_sales)\n    \n    # Within each product group, calculate the average price\n    avg_price = filtered_sales.groupby(product_column)['price'].mean().reset_index()\n    \n    # Filter the product groups where the average price is below max_price\n    valid_products = avg_price[avg_price['price'] < max_price][product_column].tolist()\n    \n    # Create a new column indicating the corresponding product category\n    sales_data['product_category'] = np.where(sales_data[product_column].isin(valid_products), 'Valid', 'Invalid')\n    \n    return sales_data"
    },
    {
        "function_name": "rolling_average_and_moving_average",
        "file_name": "moving_average_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "datetime_column": "str",
            "values_column": "str",
            "window_size": "int",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the rolling average of the values column over a window of window_size.",
            "Identify the dates where the rolling average is greater than the threshold.",
            "For the identified dates, extract the corresponding values in the values column and calculate the moving average.",
            "Group the result by the moving average and calculate the count of each group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_average_and_moving_average(data, datetime_column, values_column, window_size, threshold):\n    # Convert the datetime column to a datetime object\n    data[datetime_column] = pd.to_datetime(data[datetime_column])\n    \n    # Calculate the rolling average of the values column over a window of window_size\n    rolling_avg = data.set_index(datetime_column)[values_column].rolling(window=window_size, min_periods=1).mean()\n    \n    # Identify the dates where the rolling average is greater than the threshold\n    selected_dates = rolling_avg[rolling_avg > threshold].index\n    \n    # Filter the data for the selected dates\n    selected_data = data[data[datetime_column].isin(selected_dates)]\n    \n    # Calculate the moving average of the values column\n    selected_data['moving_average'] = selected_data[values_column].ewm(span=len(selected_data), adjust=False).mean()\n    \n    # Group the result by the moving average and calculate the count of each group\n    result = selected_data.groupby('moving_average').size()\n    \n    return result"
    },
    {
        "function_name": "keyword_sentences",
        "file_name": "text_analysis.py",
        "parameters": {
            "text": "str",
            "keywords": "list"
        },
        "objectives": [
            "Tokenize the input text into individual sentences.",
            "For each sentence, calculate the number of keywords present.",
            "Identify the sentences with the maximum number of keywords.",
            "Return a list of these sentences."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def keyword_sentences(text, keywords):\n    # Tokenize the input text into individual sentences\n    sentences = re.split(r'[.!?]', text)\n    \n    # For each sentence, calculate the number of keywords present\n    keyword_counts = []\n    for sentence in sentences:\n        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n        keyword_count = sum(1 for token in tokens if token in keywords)\n        keyword_counts.append(keyword_count)\n    \n    # Identify the sentences with the maximum number of keywords\n    max_keyword_count = max(keyword_counts)\n    keyword_sentences = [sentence for sentence, count in zip(sentences, keyword_counts) if count == max_keyword_count]\n    \n    # Return a list of these sentences\n    return keyword_sentences"
    },
    {
        "function_name": "resample_missing_values",
        "file_name": "date_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "freq": "str"
        },
        "objectives": [
            "Convert the date column to a datetime format.",
            "Resample the dataframe to the specified frequency.",
            "Calculate the number of missing values for each resampled period.",
            "Return a dataframe with the resampled periods and the number of missing values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_missing_values(df, date_column, freq):\n    # Convert the date column to a datetime format\n    df[date_column] = pd.to_datetime(df[date_column])\n    \n    # Resample the dataframe to the specified frequency\n    resampled_df = df.resample(freq, on=date_column).count()\n    \n    # Calculate the number of missing values for each resampled period\n    missing_values = resampled_df.apply(lambda row: len(df) - row.sum(), axis=1)\n    \n    # Return a dataframe with the resampled periods and the number of missing values\n    return pd.DataFrame({'date': resampled_df.index, 'missing_values': missing_values})"
    },
    {
        "function_name": "apply_benfords_law",
        "file_name": "data_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the Benford's law digit frequencies for the specified column.",
            "Identify the digit frequencies that are below the specified threshold.",
            "Remove these digit frequencies from the column.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def apply_benfords_law(df, column, threshold):\n    # Calculate the Benford's law digit frequencies for the specified column\n    benford_frequencies = [0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046]\n    \n    # Identify the digit frequencies that are below the specified threshold\n    low_frequency_digits = []\n    for i, frequency in enumerate(benford_frequencies):\n        digit_frequency = df[column].astype(str).str[0].value_counts(normalize=True)[str(i+1)]\n        if digit_frequency < threshold:\n            low_frequency_digits.append(i+1)\n    \n    # Remove these digit frequencies from the column\n    df = df[~df[column].astype(str).str[0].isin([str(digit) for digit in low_frequency_digits])]\n    \n    # Return the updated dataframe\n    return df"
    },
    {
        "function_name": "extrapolate_and_analyze",
        "file_name": "dimensionality_reduction.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "feature_columns": "list of str",
            "target_column": "str"
        },
        "objectives": [
            "Perform extrapolation on the feature columns.",
            "Create a new dataframe with the extrapolated features and the target variable.",
            "Perform similarity analysis on the extrapolated features.",
            "Return the new dataframe with the similarity scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.neighbors import NearestNeighbors"
        ],
        "function_def": "def extrapolate_and_analyze(data, feature_columns, target_column):\n    # Perform extrapolation on the feature columns\n    min_values = data[feature_columns].min()\n    max_values = data[feature_columns].max()\n    extrapolated_data = pd.DataFrame({'feature1': [min_values['feature1'] - 1, max_values['feature1'] + 1], 'feature2': [min_values['feature2'] - 1, max_values['feature2'] + 1]})\n    \n    # Create a new dataframe with the extrapolated features and the target variable\n    new_data = pd.concat([extrapolated_data, data[feature_columns]])\n    new_data[target_column] = [0, 0] + data[target_column].tolist()\n    \n    # Perform similarity analysis on the extrapolated features\n    nn = NearestNeighbors(n_neighbors=5)\n    nn.fit(new_data[feature_columns])\n    similarity_scores = nn.kneighbors(new_data[feature_columns], return_distance=False)\n    \n    # Return the new dataframe with the similarity scores\n    result = pd.DataFrame({'similarity_scores': similarity_scores.flatten()})\n    return new_data, result"
    },
    {
        "function_name": "handle_missing_categorical",
        "file_name": "missing_data_utils.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "categorical_columns": "list",
            "handling_strategy": "str ('impute', 'drop', 'encode')"
        },
        "objectives": [
            "Identify the categorical columns with missing values.",
            "Apply the specified handling strategy (impute, drop, encode) to handle the missing values in the categorical columns.",
            "If imputing, use the most frequent value in each column to impute the missing values.",
            "If dropping, remove the rows with missing values in the categorical columns.",
            "If encoding, use label encoding to encode the categorical columns.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def handle_missing_categorical(data, categorical_columns, handling_strategy):\n    # Identify the categorical columns with missing values\n    categorical_columns_with_missing = [col for col in categorical_columns if data[col].isnull().any()]\n    \n    # Apply the specified handling strategy to handle the missing values in the categorical columns\n    if handling_strategy == 'impute':\n        for col in categorical_columns_with_missing:\n            data[col] = data[col].fillna(data[col].value_counts().index[0])\n    elif handling_strategy == 'drop':\n        data = data.dropna(subset=categorical_columns_with_missing)\n    elif handling_strategy == 'encode':\n        for col in categorical_columns_with_missing:\n            le = LabelEncoder()\n            data[col] = le.fit_transform(data[col])\n    else:\n        raise ValueError(\"Invalid handling strategy. Choose 'impute', 'drop', or 'encode'.\")\n    \n    return data"
    },
    {
        "function_name": "transform_skewed_data",
        "file_name": "data_transformation.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "column_name": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the skewness of the specified column.",
            "If the skewness is greater than the specified threshold, apply log transformation to the column.",
            "If the skewness is less than or equal to the specified threshold, apply standard scaling to the column.",
            "Return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy import stats",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def transform_skewed_data(data, column_name, threshold):\n    # Calculate the skewness of the specified column\n    skewness = stats.skew(data[column_name])\n    \n    # Apply log transformation or standard scaling based on the skewness\n    if skewness > threshold:\n        data[column_name] = data[column_name].apply(lambda x: np.log(x) if x > 0 else 0)\n    else:\n        scaler = StandardScaler()\n        data[column_name] = scaler.fit_transform(data[[column_name]])[:, 0]\n    \n    return data"
    },
    {
        "function_name": "calculate_conditional_entropy",
        "file_name": "informationalysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_column": "str",
            "target_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the mutual information between the categorical column and the target column.",
            "Identify the categories with mutual information greater than the threshold.",
            "Calculate the conditional entropy of the target column given each category.",
            "Return the categories and their corresponding conditional entropies."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics import mutual_info_score",
            "import numpy as np"
        ],
        "function_def": "def calculate_conditional_entropy(df, categorical_column, target_column, threshold):\n    # Calculate the mutual information between the categorical column and the target column\n    mutual_info = mutual_info_score(df[categorical_column], df[target_column])\n    \n    # Identify the categories with mutual information greater than the threshold\n    categories = df[categorical_column].unique()\n    high_info_categories = [cat for cat in categories if mutual_info_score(df[categorical_column] == cat, df[target_column]) > threshold]\n    \n    # Calculate the conditional entropy of the target column given each category\n    conditional_entropies = []\n    for cat in high_info_categories:\n        conditional_entropy = np.sum(-df[target_column][df[categorical_column] == cat].value_counts(normalize=True) * np.log(df[target_column][df[categorical_column] == cat].value_counts(normalize=True)))\n        conditional_entropies.append(conditional_entropy)\n    \n    # Return the categories and their corresponding conditional entropies\n    return high_info_categories, conditional_entropies"
    },
    {
        "function_name": "identify_low_density_points",
        "file_name": "density_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_name": "str",
            "threshold": "float"
        },
        "objectives": [
            "Identify the outliers in the specified column using the IQR method.",
            "Calculate the density of the non-outlier values using the kernel density estimate.",
            "Identify the points with a density less than the threshold.",
            "Return the points and their corresponding densities."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import gaussian_kde",
            "import numpy as np"
        ],
        "function_def": "def identify_low_density_points(df, column_name, threshold):\n    # Identify the outliers in the specified column using the IQR method\n    q1 = df[column_name].quantile(0.25)\n    q3 = df[column_name].quantile(0.75)\n    iqr = q3 - q1\n    outliers = df[column_name][(df[column_name] < q1 - 1.5 * iqr) | (df[column_name] > q3 + 1.5 * iqr)]\n    \n    # Calculate the density of the non-outlier values using the kernel density estimate\n    kde = gaussian_kde(df[column_name][~df[column_name].isin(outliers)])\n    densities = kde.evaluate(df[column_name])\n    \n    # Identify the points with a density less than the threshold\n    low_density_points = df[column_name][densities < threshold]\n    \n    # Return the points and their corresponding densities\n    return low_density_points, densities[densities < threshold]"
    },
    {
        "function_name": "customer_segmentation",
        "file_name": "customer_insights.py",
        "parameters": {
            "customer_data": "pandas.DataFrame",
            "demographic_columns": "list of str"
        },
        "objectives": [
            "Calculate the correlation between each demographic column and the target variable.",
            "Identify the demographic columns with a correlation above 0.5 or below -0.5.",
            "Perform a chi-squared test to check for independence between each pair of demographic columns.",
            "Identify the pairs of demographic columns that are not independent.",
            "Return the updated customer data with new columns indicating the correlation and independence of each demographic column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import chi2_contingency"
        ],
        "function_def": "def customer_segmentation(customer_data, demographic_columns):\n    # Calculate correlation between each demographic column and target variable\n    correlations = customer_data[demographic_columns].corrwith(customer_data['target_variable'])\n    \n    # Identify demographic columns with high correlation\n    highly_correlated_columns = correlations[(correlations > 0.5) | (correlations < -0.5)].index\n    \n    # Perform chi-squared test for independence between each pair of demographic columns\n    independent_columns = []\n    for col1 in demographic_columns:\n        for col2 in demographic_columns:\n            if col1 != col2:\n                contingency_table = pd.crosstab(customer_data[col1], customer_data[col2])\n                chi2_stat, p_value, dof, expected_frequencies = chi2_contingency(contingency_table)\n                if p_value < 0.05:\n                    independent_columns.append((col1, col2))\n    \n    # Update customer data with new columns\n    customer_data['highly_correlated'] = customer_data[demographic_columns].apply(lambda row: 1 if row.name in highly_correlated_columns else 0, axis=1)\n    customer_data['independent'] = customer_data[demographic_columns].apply(lambda row: 1 if (row.name, row.name) in independent_columns else 0, axis=1)\n    \n    return customer_data"
    },
    {
        "function_name": "group_ratio_calculation",
        "file_name": "group_properties.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "max_groups": "int",
            "min_count": "int"
        },
        "objectives": [
            "Group the dataframe by the specified column and calculate the count of each group.",
            "Filter out groups with a count less than the minimum count.",
            "Select the top groups based on their count, up to the maximum number of groups.",
            "Calculate the ratio of each group's count to the total count.",
            "Return a dictionary with the group names as keys and their corresponding ratios as values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_ratio_calculation(df, group_column, max_groups, min_count):\n    group_counts = df.groupby(group_column).size().reset_index(name='count')\n    filtered_groups = group_counts[group_counts['count'] >= min_count].sort_values('count', ascending=False).head(max_groups)\n    ratios = filtered_groups['count'] / df.shape[0]\n    result_dict = dict(zip(filtered_groups[group_column], ratios))\n    \n    return result_dict"
    },
    {
        "function_name": "cost_sensitive_classification",
        "file_name": "classification.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "positive_class": "str",
            "threshold": "float"
        },
        "objectives": [
            "Perform cost-sensitive classification using a logistic regression model.",
            "Calculate the class probabilities for each sample.",
            "Select the samples with a probability of belonging to the positive class greater than the specified threshold.",
            "Return a dataframe with the predicted class and the probability of belonging to the positive class."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.utils.class_weight import compute_class_weight"
        ],
        "function_def": "def cost_sensitive_classification(df, target_column, positive_class, threshold):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n    model = LogisticRegression(class_weight='balanced')\n    model.fit(X, y)\n    predicted_probabilities = model.predict_proba(X)[:, 1]\n    predicted_class = model.predict(X)\n    df['predicted_class'] = predicted_class\n    df['positive_class_probability'] = predicted_probabilities\n    result = df[df['positive_class_probability'] > threshold]\n    \n    return result"
    },
    {
        "function_name": "image_feature_detection",
        "file_name": "image_processing.py",
        "parameters": {
            "arr": "numpy.ndarray",
            "kernel_size": "int"
        },
        "objectives": [
            "Apply a 2D convolution operation to the input array using a kernel of the specified size.",
            "Calculate the magnitude of the gradient of the convolved array.",
            "Identify the local maxima in the gradient magnitude array.",
            "Return the indices of the local maxima."
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.signal import convolve2d"
        ],
        "function_def": "def image_feature_detection(arr, kernel_size):\n    # Apply a 2D convolution operation to the input array\n    kernel = np.ones((kernel_size, kernel_size)) / (kernel_size ** 2)\n    convolved_arr = convolve2d(arr, kernel, mode='same')\n    \n    # Calculate the magnitude of the gradient of the convolved array\n    grad_x = np.gradient(convolved_arr, axis=0)\n    grad_y = np.gradient(convolved_arr, axis=1)\n    grad_mag = np.sqrt(grad_x ** 2 + grad_y ** 2)\n    \n    # Identify the local maxima in the gradient magnitude array\n    local_maxima = np.argwhere(grad_mag == np.maximum(grad_mag, grad_mag.max()))\n    \n    return local_maxima"
    },
    {
        "function_name": "calculate_clv_and_assign_segments",
        "file_name": "customer_lifetime_value.py",
        "parameters": {
            "customer_data": "pandas.DataFrame",
            "transaction_column": "str",
            "revenue_column": "str",
            "date_column": "str"
        },
        "objectives": [
            "Calculate the customer lifetime value (CLV) for each customer.",
            "Identify the top 10% of customers with the highest CLV.",
            "Create a new column in the dataframe that contains the customer segment assignment based on the CLV.",
            "Return the resulting dataframe with the customer information, transaction data, and customer segment assignment."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.ensemble import RandomForestRegressor"
        ],
        "function_def": "def calculate_clv_and_assign_segments(customer_data, transaction_column, revenue_column, date_column):\n    # Calculate the customer lifetime value (CLV) for each customer\n    clv = customer_data.groupby(transaction_column)[revenue_column].sum()\n    \n    # Identify the top 10% of customers with the highest CLV\n    top_customers = clv[clv > clv.quantile(0.9)]\n    \n    # Create a new column in the dataframe that contains the customer segment assignment based on the CLV\n    customer_data['customer_segment'] = customer_data[transaction_column].map(top_customers)\n    \n    return customer_data"
    },
    {
        "function_name": "select_high_volatility_periods",
        "file_name": "volatility_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "granularity": "str",
            "threshold": "float"
        },
        "objectives": [
            "Resample the dataframe to the specified granularity.",
            "Calculate the volatility of the time series data.",
            "Create a new column 'volatility' in the dataframe with the calculated volatility.",
            "Select the time periods where the volatility exceeds the threshold."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def select_high_volatility_periods(df, time_column, granularity, threshold):\n    # Resample dataframe\n    df_resampled = df.resample(granularity).mean()\n    \n    # Calculate volatility\n    df_resampled['volatility'] = df_resampled.std() * np.sqrt(252)\n    \n    # Select high volatility periods\n    high_volatility_periods = df_resampled[df_resampled['volatility'] > threshold]\n    \n    return high_volatility_periods"
    },
    {
        "function_name": "detect_overlapping_events",
        "file_name": "event_detection.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "start_date_column": "str",
            "end_date_column": "str",
            "event_column": "str"
        },
        "objectives": [
            "Calculate the duration of each event.",
            "Identify the events that overlap with each other.",
            "Create a new column to indicate whether each event is overlapping with another event.",
            "Return the updated dataframe with the overlapping events."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_overlapping_events(data, start_date_column, end_date_column, event_column):\n    # Calculate the duration of each event\n    data['event_duration'] = data[end_date_column] - data[start_date_column]\n    \n    # Identify the events that overlap with each other\n    overlapping_events = []\n    for i, row in data.iterrows():\n        for j, other_row in data.iterrows():\n            if i != j and row[start_date_column] < other_row[end_date_column] and row[end_date_column] > other_row[start_date_column]:\n                overlapping_events.append((i, j))\n    \n    # Create a new column to indicate whether each event is overlapping with another event\n    data['is_overlapping'] = data.index.isin([i for i, _ in overlapping_events]) | data.index.isin([j for _, j in overlapping_events])\n    \n    # Return the updated dataframe with the overlapping events\n    return data"
    },
    {
        "function_name": "running_transaction_total",
        "file_name": "transaction_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "transaction_column": "str",
            "timestamp_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "Sort the dataframe by the timestamp column.",
            "Identify the first occurrence of each transaction for each id.",
            "Calculate the total transaction amount for each id.",
            "Create a new column with the target_column name that stores the running total of transactions for each id.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def running_transaction_total(df, id_column, transaction_column, timestamp_column, target_column):\n    df.sort_values(by=timestamp_column, inplace=True)\n    first_transactions = df.loc[df.groupby([id_column, transaction_column])[timestamp_column].idxmin()]\n    total_transactions = df.groupby(id_column)[transaction_column].sum().to_dict()\n    df[target_column] = df.apply(lambda x: total_transactions[x[id_column]] if x.name == first_transactions[first_transactions[id_column] == x[id_column]].index[0] else total_transactions[x[id_column]] - x[transaction_column], axis=1)\n    running_total = df.groupby(id_column)[target_column].transform(pd.Series.cumsum)\n    df[target_column] = running_total\n    return df"
    },
    {
        "function_name": "filter_by_keyword",
        "file_name": "text_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "keyword_column": "str",
            "min_freq": "int"
        },
        "objectives": [
            "Tokenize the text in the text_column and calculate the term frequency-inverse document frequency (TF-IDF) for each word.",
            "Identify the top keywords for each row based on the TF-IDF scores and the keywords provided in the keyword_column.",
            "Filter out rows that do not have any matching keywords.",
            "Return the filtered dataframe with a new column 'keyword_match' indicating whether a match was found."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def filter_by_keyword(df, text_column, keyword_column, min_freq):\n    # Tokenize the text in the text_column and calculate the TF-IDF for each word\n    tfidf = TfidfVectorizer(min_df=min_freq)\n    tfidf_matrix = tfidf.fit_transform(df[text_column])\n    \n    # Identify the top keywords for each row based on the TF-IDF scores and the keywords provided in the keyword_column\n    keyword_match = []\n    for i, row in df.iterrows():\n        keywords = row[keyword_column].split()\n        scores = tfidf_matrix[i].toarray()[0]\n        word_idx = tfidf.vocabulary_\n        idx_word = {v: k for k, v in word_idx.items()}\n        top_keywords = [idx_word[j] for j in np.argsort(-scores)[:len(keywords)]]\n        match = any(keyword in top_keywords for keyword in keywords)\n        keyword_match.append(match)\n    \n    # Filter out rows that do not have any matching keywords\n    df['keyword_match'] = keyword_match\n    df = df[df['keyword_match']]\n    \n    return df"
    },
    {
        "function_name": "detect_anomalous_sensors",
        "file_name": "sensor_data_analysis.py",
        "parameters": {
            "sensor_data": "pandas.DataFrame",
            "id_column": "str",
            "timestamp_column": "str",
            "value_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Identify sensors that have experienced a significant change in value (more than the threshold) within a 5-minute window.",
            "Calculate the average value for each sensor before and after the significant change.",
            "Determine the sensors that have exceeded a specific cumulative sum of values (1000).",
            "Return an updated dataframe with the identified sensors, their corresponding average values, and the cumulative sum of values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_anomalous_sensors(sensor_data, id_column, timestamp_column, value_column, threshold):\n    # Convert timestamp column to datetime type\n    sensor_data[timestamp_column] = pd.to_datetime(sensor_data[timestamp_column])\n    \n    # Identify significant changes in value\n    sensor_data['diff'] = sensor_data.groupby(id_column)[value_column].diff().abs()\n    \n    # Identify sensors with significant changes\n    anomalyMask = (sensor_data['diff'] > threshold)\n    sensor_data['anomaly'] = sensor_data['anomaly'].shift(1) if 'anomaly' in sensor_data else False\n    sensor_data.loc[anomalyMask, 'anomaly'] = True\n    \n    # Calculate average value before and after anomaly\n    sensor_data['before'] = sensor_data.groupby(id_column)[value_column].shift(1)\n    sensor_data['after'] = sensor_data.groupby(id_column)[value_column].shift(-1)\n    \n    # Calculate cumulative sum\n    sensor_data['cumulative_sum'] = sensor_data.groupby(id_column)[value_column].transform(pd.Series.cumsum)\n    \n    # Identify sensors with cumulative sum exceeding threshold\n    anomaly_df = sensor_data[sensor_data['anomaly']][['id', 'before', 'after', 'cumulative_sum']]\n    \n    return anomaly_df"
    },
    {
        "function_name": "merge_reference_values",
        "file_name": "data_mapping.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "time_column": "str",
            "cycle_column": "str",
            "getReference_value_column": "str"
        },
        "objectives": [
            "Merge and map reference values from one dataframe to the specified cycle column in another dataframe.",
            "Identify the differences between the actual values in the specified cycle column and the merged reference values.",
            "Calculate the cumulative sum of these differences for each cycle.",
            "Return the updated dataframe with these calculated cumulative sums."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_reference_values(data, time_column, cycle_column, reference_value_column):\n    # Assuming reference_data is another DataFrame\n    reference_data = pd.DataFrame({'cycle': [1, 2, 3], 'reference_value': [10, 20, 30]})\n    merged_data = pd.merge(data, reference_data, on=cycle_column)\n    merged_data['difference'] = merged_data[cycle_column] - merged_data[reference_value_column]\n    merged_data['cumulative_sum'] = merged_data.groupby(time_column)['difference'].cumsum()\n    return merged_data"
    },
    {
        "function_name": "correlation_based_regression",
        "file_name": "regression.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_column": "str",
            "feature_columns": "list[str]",
            "method": "str (one of 'pearson', 'spearman', 'kendall')"
        },
        "objectives": [
            "Calculate the correlation between each feature column and the target column using the specified method.",
            "Select the top 3 feature columns with the highest correlation.",
            "Create a new dataframe with the selected feature columns and the target column.",
            "Perform a linear regression on the selected feature columns to predict the target column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import LinearRegression",
            "import numpy as np"
        ],
        "function_def": "def correlation_based_regression(data, target_column, feature_columns, method):\n    # Calculate the correlation between each feature column and the target column using the specified method\n    if method == 'pearson':\n        correlations = data[feature_columns].corrwith(data[target_column])\n    elif method == 'spearman':\n        correlations = data[feature_columns].corrwith(data[target_column], method='spearman')\n    elif method == 'kendall':\n        correlations = data[feature_columns].corrwith(data[target_column], method='kendall')\n    \n    # Select the top 3 feature columns with the highest correlation\n    top_features = correlations.sort_values(ascending=False).index[:3]\n    \n    # Create a new dataframe with the selected feature columns and the target column\n    selected_data = data[list(top_features) + [target_column]]\n    \n    # Perform a linear regression on the selected feature columns to predict the target column\n    model = LinearRegression()\n    model.fit(selected_data[top_features], selected_data[target_column])\n    \n    return model"
    },
    {
        "function_name": "pca_analysis",
        "file_name": "pca.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numerical_columns": "list[str]",
            "categorical_columns": "list[str]",
            "encoding_method": "str (one of 'label', 'ordinal', 'onehot')"
        },
        "objectives": [
            "Encode the categorical columns using the specified encoding method.",
            "Scale the numerical columns using StandardScaler.",
            "Concatenate the encoded categorical columns and the scaled numerical columns.",
            "Perform principal component analysis on the concatenated data.",
            "Select the top 2 principal components."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def pca_analysis(data, numerical_columns, categorical_columns, encoding_method):\n    # Encode the categorical columns using the specified encoding method\n    if encoding_method == 'label':\n        encoded_data = data[categorical_columns].apply(LabelEncoder().fit_transform)\n    elif encoding_method == 'ordinal':\n        encoded_data = data[categorical_columns].apply(lambda x: pd.Categorical(x).codes)\n    elif encoding_method == 'onehot':\n        encoded_data = pd.get_dummies(data[categorical_columns], drop_first=True)\n    \n    # Scale the numerical columns using StandardScaler\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data[numerical_columns])\n    scaled_df = pd.DataFrame(scaled_data, columns=numerical_columns)\n    \n    # Concatenate the encoded categorical columns and the scaled numerical columns\n    concatenated_data = pd.concat([encoded_data, scaled_df], axis=1)\n    \n    # Perform principal component analysis on the concatenated data\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(concatenated_data)\n    \n    # Select the top 2 principal components\n    principal_df = pd.DataFrame({'PC1': principal_components[:, 0], 'PC2': principal_components[:, 1]})\n    \n    return principal_df"
    },
    {
        "function_name": "calculate_daily_averages",
        "file_name": "daily_aggregations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "date_column": "str"
        },
        "objectives": [
            "Extract the day of the month from the specified date column.",
            "Calculate the average daily value for each unique ID.",
            "Identify IDs with missing values in the date column.",
            "Return a dataframe containing the average daily value for each ID and a list of IDs with missing dates."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def calculate_daily_averages(data, id_column, date_column):\n    # Extract the day of the month from the specified date column\n    data['day_of_month'] = data[date_column].dt.day\n    \n    # Calculate the average daily value for each unique ID\n    average_daily_values = data.groupby(id_column)['day_of_month'].mean().reset_index()\n    \n    # Identify IDs with missing values in the date column\n    missing_ids = data[data[date_column].isnull()][id_column].unique().tolist()\n    \n    # Return a dataframe containing the average daily value for each ID and a list of IDs with missing dates\n    return average_daily_values, missing_ids"
    },
    {
        "function_name": "aggregate_group_values",
        "file_name": "group_aggregations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "group_by_column": "str",
            "numeric_columns": "list",
            "aggregation_method": "str"
        },
        "objectives": [
            "Group the data by the specified column.",
            "Calculate the aggregated values for each group using the specified method.",
            "Identify the groups with the highest and lowest aggregated values.",
            "Return a dataframe containing the aggregated values for each group and the groups with the highest and lowest values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def aggregate_group_values(data, group_by_column, numeric_columns, aggregation_method):\n    # Group the data by the specified column\n    grouped_data = data.groupby(group_by_column)\n    \n    # Calculate the aggregated values for each group using the specified method\n    if aggregation_method == 'mean':\n        aggregated_values = grouped_data[numeric_columns].mean()\n    elif aggregation_method == 'sum':\n        aggregated_values = grouped_data[numeric_columns].sum()\n    elif aggregation_method == 'max':\n        aggregated_values = grouped_data[numeric_columns].max()\n    else:\n        raise ValueError(\"Invalid aggregation method\")\n    \n    # Identify the groups with the highest and lowest aggregated values\n    highest_group = aggregated_values.idxmax()\n    lowest_group = aggregated_values.idxmin()\n    \n    # Return a dataframe containing the aggregated values for each group and the groups with the highest and lowest values\n    return aggregated_values, highest_group, lowest_group"
    },
    {
        "function_name": "z_scoring",
        "file_name": "z_scoring.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "numerical_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Apply a rolling window to the numerical column and calculate the mean and standard deviation within each window.",
            "Create a new column with the z-scores for each value in the numerical column based on the mean and standard deviation of the rolling window.",
            "Identify the values with a z-score above a certain threshold (e.g., 2) and create a new column to indicate whether each row corresponds to one of these values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def z_scoring(data, numerical_column, window_size):\n    # Step 1: Apply a rolling window to the numerical column and calculate the mean and standard deviation within each window\n    rolling_mean = data[numerical_column].rolling(window_size).mean()\n    rolling_std = data[numerical_column].rolling(window_size).std()\n    \n    # Step 2: Create a new column with the z-scores for each value in the numerical column based on the mean and standard deviation of the rolling window\n    data['z_score'] = (data[numerical_column] - rolling_mean) / rolling_std\n    \n    # Step 3: Identify the values with a z-score above a certain threshold (e.g., 2) and create a new column to indicate whether each row corresponds to one of these values\n    data['is_outlier'] = np.where(data['z_score'] > 2, True, False)\n    \n    return data"
    },
    {
        "function_name": "holiday_target",
        "file_name": "holiday_target.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "holiday_column": "str"
        },
        "objectives": [
            "Convert the date column to datetime format and extract the month and day.",
            "Create a new column indicating whether each row corresponds to a holiday.",
            "Calculate the average value of the target variable for each month and create a new column with this average.",
            "Identify the rows that correspond to holidays and have a target value above the average for the corresponding month."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def holiday_target(df, date_column, holiday_column):\n    # Step 1: Convert the date column to datetime format and extract the month and day\n    df['datetime'] = pd.to_datetime(df[date_column])\n    df['month'] = df['datetime'].dt.month\n    df['day'] = df['datetime'].dt.day\n    \n    # Step 2: Create a new column indicating whether each row corresponds to a holiday\n    df['is_holiday'] = df[holiday_column]\n    \n    # Step 3: Calculate the average value of the target variable for each month and create a new column with this average\n    df['average_target'] = df.groupby('month')['target'].transform('mean')\n    \n    # Step 4: Identify the rows that correspond to holidays and have a target value above the average for the corresponding month\n    df['is_holiday_above_average'] = np.where((df['is_holiday'] == True) & (df['target'] > df['average_target']), True, False)\n    \n    return df"
    },
    {
        "function_name": "minority_class_oversampling",
        "file_name": "class_balance.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "label_column": "str",
            "max_class_size": "int"
        },
        "objectives": [
            "Calculate the class distribution of the label column.",
            "Identify the minority classes with size less than max_class_size.",
            "Perform random oversampling of the minority classes to reach max_class_size.",
            "Return the resulting dataframe with the oversampled minority classes."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.utils import resample"
        ],
        "function_def": "def minority_class_oversampling(data, label_column, max_class_size):\n    # Calculate the class distribution of the label column\n    class_distribution = data[label_column].value_counts()\n    \n    # Identify the minority classes with size less than max_class_size\n    minority_classes = class_distribution[class_distribution < max_class_size].index\n    \n    # Perform random oversampling of the minority classes to reach max_class_size\n    for minority_class in minority_classes:\n        minority_class_data = data[data[label_column] == minority_class]\n        oversampled_data = resample(minority_class_data, replace=True, n_samples=max_class_size)\n        data = pd.concat([data, oversampled_data])\n    \n    # Return the resulting dataframe with the oversampled minority classes\n    return data"
    },
    {
        "function_name": "segment_high_intensity_regions",
        "file_name": "image_analysis.py",
        "parameters": {
            "image_data": "numpy.ndarray",
            "threshold": "float"
        },
        "objectives": [
            "Apply a threshold to the image data to segment out pixels with intensity values above the threshold.",
            "Calculate the area of the segmented region.",
            "Create a mask to highlight the segmented region in the original image."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def segment_high_intensity_regions(image_data, threshold):\n    # Apply a threshold to the image data to segment out pixels with intensity values above the threshold\n    binary_image = np.where(image_data > threshold, 1, 0)\n    \n    # Calculate the area of the segmented region\n    area = np.sum(binary_image)\n    \n    # Create a mask to highlight the segmented region in the original image\n    mask = np.ma.masked_where(binary_image == 0, image_data)\n    \n    return mask, area"
    },
    {
        "function_name": "calculate_frequency_components",
        "file_name": "audio_analysis.py",
        "parameters": {
            "audio_data": "list[float]",
            "sample_rate": "int",
            "window_size": "int"
        },
        "objectives": [
            "Calculate the Fourier Transform of the audio data using a window size of window_size.",
            "Extract the frequency components of the audio data.",
            "Filter out frequency components below a certain threshold (20 Hz).",
            "Return a list of the remaining frequency components."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def calculate_frequency_components(audio_data, sample_rate, window_size):\n    # Calculate the Fourier Transform of the audio data using a window size of window_size\n    fourier_transform = np.fft.fft(audio_data, n=window_size)\n    \n    # Extract the frequency components of the audio data\n    frequency_components = np.abs(fourier_transform)\n    \n    # Filter out frequency components below a certain threshold (20 Hz)\n    threshold_index = int(20 / (sample_rate / window_size))\n    filtered_frequency_components = frequency_components[threshold_index:]\n    \n    return filtered_frequency_components"
    },
    {
        "function_name": "personalized_pagerank",
        "file_name": "network_analysis.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "id_column": "str",
            "connection_column": "str",
            "strength_column": "str"
        },
        "objectives": [
            "Create a graph based on the connection column and the strength column.",
            "Identify the strong connections in the graph with a strength greater than the mean strength.",
            "For each strong connection, calculate the personalized PageRank and return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import networkx as nx"
        ],
        "function_def": "def personalized_pagerank(data, id_column, connection_column, strength_column):\n    # Create a graph based on the connection column and the strength column\n    graph = nx.Graph()\n    for i, row in data.iterrows():\n        graph.add_edge(row[id_column], row[connection_column], weight=row[strength_column])\n    \n    # Identify the strong connections in the graph with a strength greater than the mean strength\n    mean_strength = sum([edge[2]['weight'] for edge in graph.edges(data=True)]) / graph.number_of_edges()\n    strong_connections = [(u, v) for u, v in graph.edges() if graph.get_edge_data(u, v)['weight'] > mean_strength]\n    \n    # For each strong connection, calculate the personalized PageRank\n    personalized_pagerank_values = {}\n    for u, v in strong_connections:\n        pagerank_values = nx.pagerank(graph, personalization={v: 1})\n        personalized_pagerank_values[(u, v)] = pagerank_values\n        \n    return personalized_pagerank_values"
    },
    {
        "function_name": "outlier_detection",
        "file_name": "outlier_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int",
            "threshold": "float"
        },
        "objectives": [
            "Calculate the rolling standard deviation of each numerical column for the specified window size.",
            "Identify the rows where the rolling standard deviation is greater than the specified threshold.",
            "Create a new column 'outlier' to label the identified rows as outliers.",
            "Calculate the correlation matrix of the numerical columns for the non-outlier rows."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def outlier_detection(df, window_size, threshold):\n    # Calculate the rolling standard deviation of each numerical column\n    numerical_cols = df.select_dtypes(include=[np.number]).columns\n    rolling_std = df[numerical_cols].rolling(window_size).std()\n    \n    # Identify the rows where the rolling standard deviation is greater than the specified threshold\n    outlier = (rolling_std > threshold).any(axis=1)\n    \n    # Create a new column 'outlier' to label the identified rows as outliers\n    df['outlier'] = outlier\n    \n    # Calculate the correlation matrix of the numerical columns for the non-outlier rows\n    non_outlier_df = df[~df['outlier']]\n    correlation_matrix = non_outlier_df[numerical_cols].corr()\n    \n    return correlation_matrix"
    }
]