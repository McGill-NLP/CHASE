[
    {
        "function_name": "process_columns",
        "file_name": "column_processing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "col_list": "list"
        },
        "objectives": [
            "Identify columns in \"data\" listed in \"col_list\" that have more than 50% missing values. Call these columns \"drop_cols\".",
            "For the remaining columns in \"col_list\", fill missing values using linear interpolation.",
            "Standardize (mean=0, std=1) these columns.",
            "Return the modified dataframe and the list of dropped columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def process_columns(data, col_list):\n    # Step 1: Identify columns with more than 50% missing values\n    drop_cols = [col for col in col_list if data[col].isnull().mean() > 0.5]\n    \n    # Step 2: Drop identified columns\n    remaining_cols = [col for col in col_list if col not in drop_cols]\n\n    # Step 3: Fill missing values in remaining columns using linear interpolation\n    data[remaining_cols] = data[remaining_cols].interpolate(method='linear')\n\n    # Step 4: Standardize (mean=0, std=1) remaining columns\n    for col in remaining_cols:\n        data[col] = (data[col] - data[col].mean()) / data[col].std()\n\n    return data, drop_cols"
    },
    {
        "function_name": "augment_stock_features",
        "file_name": "stock_analysis.py",
        "parameters": {
            "stock_data": "pandas.DataFrame",
            "feature_list": "list",
            "window": "int"
        },
        "objectives": [
            "For each feature in \"feature_list\", compute rolling mean and rolling standard deviation using the specified \"window\" for the stock_data.",
            "Create new columns in stock_data for each computed rolling mean and rolling standard deviation.",
            "Normalize these new columns such that the minimum value of each is 0 and the maximum is 1.",
            "Return the modified stock_data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def augment_stock_features(stock_data, feature_list, window):\n    for feature in feature_list:\n        rolling_mean = stock_data[feature].rolling(window=window).mean()\n        rolling_std = stock_data[feature].rolling(window=window).std()\n\n        stock_data[f\"{feature}_rolling_mean\"] = rolling_mean\n        stock_data[f\"{feature}_rolling_std\"] = rolling_std\n\n        # Normalize new columns\n        stock_data[f\"{feature}_rolling_mean\"] = (rolling_mean - rolling_mean.min()) / (rolling_mean.max() - rolling_mean.min())\n        stock_data[f\"{feature}_rolling_std\"] = (rolling_std - rolling_std.min()) / (rolling_std.max() - rolling_std.min())\n    \n    return stock_data"
    },
    {
        "function_name": "correlate_and_normalize",
        "file_name": "correlation_operations.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "cols": "list of str"
        },
        "objectives": [
            "Identify pairs of columns in 'data' where one is a string column and the other is a numerical column.",
            "For each such pair, encode the string column using label encoding and then calculate the correlation coefficient between the encoded string column and the numerical column.",
            "Replace the numerical column with its z-score normalized version if the absolute correlation coefficient is above a threshold of 0.5.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder",
            "from scipy.stats import zscore"
        ],
        "function_def": "def correlate_and_normalize(data, cols):\n    threshold = 0.5\n    encoder = LabelEncoder()\n    modified_data = data.copy()\n\n    # Step 1: Identify pairs of columns\n    for i in range(len(cols)):\n        for j in range(i + 1, len(cols)):\n            col1, col2 = cols[i], cols[j]\n            if data[col1].dtype == 'object' and pd.api.types.is_numeric_dtype(data[col2]):\n                encoded_col = encoder.fit_transform(data[col1])\n                correlation = pd.Series(encoded_col).corr(data[col2])\n                if abs(correlation) > threshold:\n                    modified_data[col2] = zscore(data[col2])\n\n    return modified_data"
    },
    {
        "function_name": "summarize_by_intervals",
        "file_name": "interval_summary.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_column": "str",
            "n_splits": "int"
        },
        "objectives": [
            "Discretize the 'datetime_column' into 'n_splits' equal-sized time intervals.",
            "For each interval, calculate the sum of all numerical columns.",
            "Create a new dataframe 'interval_df' with intervals as rows and sums of numerical columns as columns.",
            "Add an additional column 'Interval_Size' specifying the number of rows in each interval."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def summarize_by_intervals(df, datetime_column, n_splits):\n    df[datetime_column] = pd.to_datetime(df[datetime_column])\n    df = df.sort_values(by=datetime_column)\n    \n    # Step 1: Discretize the datetime column\n    df['interval'] = pd.qcut(df[datetime_column], q=n_splits, duplicates='drop')\n    \n    # Step 2: Calculate sum of all numerical columns within each interval\n    interval_sums = df.groupby('interval').sum(numeric_only=True)\n    interval_sizes = df.groupby('interval').size()\n    \n    # Step 3: Create new dataframe interval_df\n    interval_df = pd.DataFrame(interval_sums)\n    interval_df['Interval_Size'] = interval_sizes.values\n\n    return interval_df"
    },
    {
        "function_name": "pairwise_multiplication",
        "file_name": "column_manipulations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Create all possible pairs of numerical columns in the dataframe.",
            "For each pair, add a new derived column in the dataframe which is the product of the two columns.",
            "Replace negative products with zero in all newly added columns.",
            "Return a dataframe containing only the new derived columns along with the 'target_col'."
        ],
        "import_lines": [
            "import pandas as pd",
            "from itertools import combinations"
        ],
        "function_def": "def pairwise_multiplication(df, target_col):\n    numerical_cols = df.select_dtypes(include='number').columns.drop(target_col)\n    new_df = df[[target_col]].copy()\n\n    # Step 1: Create all possible pairs of numerical columns\n    for col1, col2 in combinations(numerical_cols, 2):\n        # Step 2: Add new derived column as product of the two columns\n        new_col_name = f'{col1}_x_{col2}'\n        new_df[new_col_name] = df[col1] * df[col2]\n        \n        # Step 3: Replace negative products with zero\n        new_df[new_col_name] = new_df[new_col_name].apply(lambda x: x if x > 0 else 0)\n\n    return new_df"
    },
    {
        "function_name": "filter_columns_by_std",
        "file_name": "statistics_filters.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "threshold": "int"
        },
        "objectives": [
            "Identify all numeric columns in the dataframe \"df\".",
            "For each numeric column, compute the standard deviation and filter out the columns where the standard deviation is greater than \"threshold\".",
            "Create a new dataframe containing only the filtered columns.",
            "For each row in the new dataframe, compute the sum of the values across all columns and create a new column \"row_sum\" in the new dataframe.",
            "Return the new dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_columns_by_std(df, threshold):\n    # Identify numeric columns\n    numeric_cols = df.select_dtypes(include='number').columns\n    \n    # Initialize an empty list for columns that meet our condition\n    filtered_cols = []\n\n    # Filter out columns with std deviation greater than threshold\n    for col in numeric_cols:\n        if df[col].std() > threshold:\n            filtered_cols.append(col)\n\n    # Create new dataframe\n    new_df = df[filtered_cols]\n\n    # Compute sum across all columns for each row\n    new_df['row_sum'] = new_df.sum(axis=1)\n\n    return new_df"
    },
    {
        "function_name": "calculate_rolling_means",
        "file_name": "rolling_window.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int"
        },
        "objectives": [
            "For each numeric column in the dataframe \"df\", calculate the rolling mean using a window of size \"window_size\".",
            "Create a new column for each numeric column to store these rolling means, appending \"_rolling_mean\" to the original column name.",
            "Replace the values in the new rolling mean columns with 0 for any missing values.",
            "Create another column \"overall_mean\" which contains the mean of all rolling means for each row.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_rolling_means(df, window_size):\n    numeric_cols = df.select_dtypes(include='number').columns\n\n    for col in numeric_cols:\n        df[col + \"_rolling_mean\"] = df[col].rolling(window=window_size, min_periods=1).mean()\n        df[col + \"_rolling_mean\"].fillna(0, inplace=True)\n\n    # Create 'overall_mean' column\n    rolling_mean_cols = [col + \"_rolling_mean\" for col in numeric_cols]\n    df['overall_mean'] = df[rolling_mean_cols].mean(axis=1)\n\n    return df"
    },
    {
        "function_name": "identify_highly_correlated_pairs",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame"
        },
        "objectives": [
            "Identify columns in the dataframe \"df\" with only numeric values.",
            "For each numeric column, calculate its correlation with every other numeric column.",
            "Identify and store the pairs of columns with a correlation coefficient greater than 0.9.",
            "Return a list of these pairs."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_highly_correlated_pairs(df):\n    numeric_cols = df.select_dtypes(include='number').columns\n    correlation_matrix = df[numeric_cols].corr()\n\n    high_correlation_pairs = []\n\n    for i in range(len(correlation_matrix.columns)):\n        for j in range(i+1, len(correlation_matrix.columns)):\n            col1 = correlation_matrix.columns[i]\n            col2 = correlation_matrix.columns[j]\n            if correlation_matrix.iloc[i, j] > 0.9:\n                high_correlation_pairs.append((col1, col2))\n\n    return high_correlation_pairs"
    },
    {
        "function_name": "filter_outliers_iqr",
        "file_name": "outlier_management.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "threshold": "float"
        },
        "objectives": [
            "Identify the numeric columns in the dataframe \"df\".",
            "For each numeric column, calculate the interquartile range (IQR) and filter out the outliers beyond 1.5 times the IQR from the lower and upper quartiles.",
            "Replace identified outliers in the dataframe with NaN values.",
            "Remove rows where more than \"threshold\" proportion of values are NaN."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def filter_outliers_iqr(df, threshold):\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n    \n    for col in numeric_columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        df[col] = df[col].mask((df[col] < lower_bound) | (df[col] > upper_bound), np.nan)\n    \n    df = df[df.isnull().mean(axis=1) <= threshold]\n    \n    return df"
    },
    {
        "function_name": "map_and_one_hot_encode",
        "file_name": "mapping_encode.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_mapping": "Dict[str, Dict]"
        },
        "objectives": [
            "Detect columns in the dataframe \"df\" whose values match the keys in the \"target_mapping\" dictionary.",
            "Apply the mapping transformation to the detected columns based on the respective sub-dictionaries in \"target_mapping\".",
            "One-hot encode all transformed columns.",
            "Combine the resulting one-hot encoded columns back with the original dataframe and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def map_and_one_hot_encode(df, target_mapping):\n    df = df.copy()\n    \n    for col, mapping in target_mapping.items():\n        if col in df.columns:\n            df[col] = df[col].map(mapping)\n            one_hot = pd.get_dummies(df[col], prefix=col)\n            df = pd.concat([df, one_hot], axis=1)\n            df.drop(columns=[col], inplace=True)\n            \n    return df"
    },
    {
        "function_name": "create_interaction_terms",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_subset": "List[str]"
        },
        "objectives": [
            "Create all possible pairwise interaction terms between the columns specified in \"col_subset\".",
            "Generate the product of each pairwise combination as the interaction term.",
            "Append these interaction terms to the original dataframe.",
            "Standardize the interaction terms by removing the mean and scaling to unit variance. Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler",
            "from itertools import combinations"
        ],
        "function_def": "def create_interaction_terms(df, col_subset):\n    interaction_df = df.copy()\n    \n    for col1, col2 in combinations(col_subset, 2):\n        interaction_term = df[col1] * df[col2]\n        interaction_df[f\"{col1}_{col2}_interaction\"] = interaction_term\n    \n    scaler = StandardScaler()\n    interaction_columns = [col for col in interaction_df.columns if '_interaction' in col]\n    interaction_df[interaction_columns] = scaler.fit_transform(interaction_df[interaction_columns])\n    \n    return interaction_df"
    },
    {
        "function_name": "encode_and_merge_categorical",
        "file_name": "categorical_encoder.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_threshold": "float"
        },
        "objectives": [
            "Identify categorical columns in the dataframe \"df\" that have a fraction of unique values greater than \"categorical_threshold\".",
            "For each identified column, encode it using one-hot encoding.",
            "Merge the one-hot encoded columns back into the original dataframe.",
            "Drop the original categorical columns from the dataframe and return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def encode_and_merge_categorical(df, categorical_threshold):\n    categorical_cols = [col for col in df.select_dtypes(include='object').columns if df[col].nunique() / len(df) > categorical_threshold]\n    \n    one_hot_encoded_cols = pd.get_dummies(df[categorical_cols])\n    \n    df = df.drop(columns=categorical_cols)\n    df = pd.concat([df, one_hot_encoded_cols], axis=1)\n    \n    return df"
    },
    {
        "function_name": "select_and_pca",
        "file_name": "pca_transform.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_var": "str",
            "k": "int"
        },
        "objectives": [
            "Identify top \"k\" most correlated features with the target variable \"target_var\" in the dataframe \"df\".",
            "Perform Principal Component Analysis (PCA) on these top \"k\" features and reduce their dimensionality to \"k\" principal components.",
            "Standardize the principal components by removing the mean and scaling to unit variance.",
            "Return the dataframe containing the target variable and these \"k\" standardized principal components."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def select_and_pca(df, target_var, k):\n    correlations = df.corr()[target_var].abs().sort_values(ascending=False)[1:k+1]\n    top_k_features = correlations.index\n    \n    pca = PCA(n_components=k)\n    principal_components = pca.fit_transform(df[top_k_features])\n    \n    scaled_pcs = StandardScaler().fit_transform(principal_components)\n    \n    pc_df = pd.DataFrame(scaled_pcs, columns=[f'PC{i+1}' for i in range(k)])\n    result_df = pd.concat([df[target_var].reset_index(drop=True), pc_df], axis=1)\n    \n    return result_df"
    },
    {
        "function_name": "extract_temporal_features",
        "file_name": "temporal_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "timestamp_col": "str"
        },
        "objectives": [
            "Convert the \"timestamp_col\" in dataframe \"df\" to datetime format.",
            "Extract temporal features such as year, month, day, hour, minute, and weekday from the \"timestamp_col\".",
            "Create a new dataframe by concatenating the extracted temporal features to the original dataframe.",
            "Perform a cyclical transformation on the \"hour\" and \"weekday\" features to capture their cyclical nature and append these transformed features to the dataframe. Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def extract_temporal_features(df, timestamp_col):\n    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n    \n    df['year'] = df[timestamp_col].dt.year\n    df['month'] = df[timestamp_col].dt.month\n    df['day'] = df[timestamp_col].dt.day\n    df['hour'] = df[timestamp_col].dt.hour\n    df['minute'] = df[timestamp_col].dt.minute\n    df['weekday'] = df[timestamp_col].dt.weekday\n    \n    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n    \n    return df"
    },
    {
        "function_name": "handle_missing_values",
        "file_name": "missing_data_handler.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "missing_threshold": "float",
            "strategy": "str"
        },
        "objectives": [
            "Identify columns in the dataframe \"df\" where the fraction of missing values is greater than \"missing_threshold\".",
            "Depending on the \"strategy\", either drop these columns entirely or fill the missing values with the median value of the column.",
            "Standardize the numerical columns by removing the mean and scaling to unit variance.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def handle_missing_values(df, missing_threshold, strategy='drop'):\n    missing_fractions = df.isnull().mean()\n    cols_to_handle = missing_fractions[missing_fractions > missing_threshold].index\n    \n    if strategy == 'drop':\n        df = df.drop(columns=cols_to_handle)\n    elif strategy == 'median':\n        for col in cols_to_handle:\n            df[col].fillna(df[col].median(), inplace=True)\n    \n    numeric_cols = df.select_dtypes(include='number').columns\n    scaled_values = StandardScaler().fit_transform(df[numeric_cols])\n    df[numeric_cols] = scaled_values\n\n    return df"
    },
    {
        "function_name": "add_rolling_features",
        "file_name": "rolling_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_subset": "list of str"
        },
        "objectives": [
            "For each column in \"col_subset\", compute the rolling mean over a specified window.",
            "Add the rolling mean as a new column in the dataframe with an appropriate suffix indicating the window size.",
            "Compute the difference between the original column values and their respective rolling means and store these differences as new columns.",
            "Return the augmented dataframe containing both the rolling means and the differences."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_rolling_features(df, col_subset):\n    window = 5\n    for col in col_subset:\n        rolling_col_name = f\"{col}_rolling_mean_{window}\"\n        diff_col_name = f\"{col}_diff_rolling_mean\"\n        \n        df[rolling_col_name] = df[col].rolling(window=window).mean()\n        df[diff_col_name] = df[col] - df[rolling_col_name]\n    \n    return df"
    },
    {
        "function_name": "clean_and_reduce_features",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_cols": "list",
            "target_col": "str"
        },
        "objectives": [
            "For each column in \"feature_cols\", remove outliers by bounding them within the 1st and 99th percentile of the column.",
            "Compute the correlation of each feature column in \"feature_cols\" with the \"target_col\" and drop the features with correlation below 0.1.",
            "Perform feature scaling to the range [0, 1] for the remaining columns.",
            "Return the modified dataframe and a list of dropped features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def clean_and_reduce_features(df, feature_cols, target_col):\n    dropped_features = []\n\n    # Step 1: Remove outliers by bounding to 1st and 99th percentile\n    for col in feature_cols:\n        lower_bound = df[col].quantile(0.01)\n        upper_bound = df[col].quantile(0.99)\n        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n    \n    # Step 2: Compute correlation and drop features with low correlation\n    for col in feature_cols:\n        correlation = df[col].corr(df[target_col])\n        if abs(correlation) < 0.1:\n            dropped_features.append(col)\n            df.drop(columns=[col], inplace=True)\n\n    # Step 3: Feature scaling to [0, 1] range\n    remaining_cols = [col for col in feature_cols if col not in dropped_features]\n    scaler = MinMaxScaler()\n    df[remaining_cols] = scaler.fit_transform(df[remaining_cols])\n\n    return df, dropped_features"
    },
    {
        "function_name": "time_series_interpolation",
        "file_name": "time_series.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "freq": "str"
        },
        "objectives": [
            "Ensure that the values in the \"time_col\" follow a consistent frequency defined by \"freq\", creating a complete time index if necessary.",
            "For missing timestamps, perform interpolation to estimate missing values in numerical columns while keeping the rest of the columns intact.",
            "Compute and append rolling statistics (mean and standard deviation) with a window size of 5 time periods.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def time_series_interpolation(df, time_col, freq):\n    # Step 1: Create complete time index\n    df[time_col] = pd.to_datetime(df[time_col])\n    df = df.set_index(time_col).asfreq(freq)\n\n    # Step 2: Interpolate missing values in numerical columns\n    numeric_cols = df.select_dtypes(include='number').columns\n    df[numeric_cols] = df[numeric_cols].interpolate(method='linear')\n\n    # Step 3: Compute rolling statistics with a window size of 5 time periods\n    for col in numeric_cols:\n        df[f\"{col}_rolling_mean\"] = df[col].rolling(window=5).mean()\n        df[f\"{col}_rolling_std\"] = df[col].rolling(window=5).std()\n\n    return df.reset_index()"
    },
    {
        "function_name": "discretize_and_smooth",
        "file_name": "data_smoothing.py",
        "parameters": {
            "data": "pandas.DataFrame",
            "target_col": "str",
            "bin_edges": "list"
        },
        "objectives": [
            "Discretize the target column \"target_col\" into bins defined by \"bin_edges\" and create a new column indicating the bin each value belongs to.",
            "For each bin, compute the median of the numerical columns and replace original values with these medians to reduce noise.",
            "Encode the categorical columns using one-hot encoding.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def discretize_and_smooth(data, target_col, bin_edges):\n    # Step 1: Discretize target column into bins\n    bin_labels = range(len(bin_edges) - 1)\n    data[f\"{target_col}_binned\"] = pd.cut(data[target_col], bins=bin_edges, labels=bin_labels)\n\n    # Step 2: Replace values with bin medians\n    numeric_cols = data.select_dtypes(include='number').columns\n    for bin_label in bin_labels:\n        bin_median = data.loc[data[f\"{target_col}_binned\"] == bin_label, numeric_cols].median()\n        data.loc[data[f\"{target_col}_binned\"] == bin_label, numeric_cols] = bin_median\n    \n    # Step 3: One-hot encode categorical columns\n    categorical_cols = data.select_dtypes(include='category').columns\n    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n\n    return data"
    },
    {
        "function_name": "cosine_similarity_pairs",
        "file_name": "similarity_calculations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_pairs": "list of tuple(str, str)"
        },
        "objectives": [
            "Identify pairs of columns in df whose names match any tuple in col_pairs.",
            "Calculate the cosine similarity between the values in each pair of columns.",
            "Create a new column for each pair containing the cosine similarity scores.",
            "Return the modified dataframe with the new columns included."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.metrics.pairwise import cosine_similarity"
        ],
        "function_def": "def cosine_similarity_pairs(df, col_pairs):\n    modified_df = df.copy()\n    \n    for col1, col2 in col_pairs:\n        # Ensure columns exist in the dataframe\n        if col1 in df.columns and col2 in df.columns:\n            # Calculate cosine similarity\n            similarity_matrix = cosine_similarity(df[[col1]], df[[col2]])\n            new_col_name = f'{col1}_cosine_{col2}'\n            modified_df[new_col_name] = similarity_matrix[:, 0]\n\n    return modified_df"
    },
    {
        "function_name": "smooth_time_series",
        "file_name": "time_series_operations.py",
        "parameters": {
            "dataframe": "pandas.DataFrame",
            "window_size": "int"
        },
        "objectives": [
            "Validate if the dataframe contains a \"timestamp\" column in datetime format.",
            "For each numerical column, calculate the rolling average with window size \"window_size\" ignoring NaN values.",
            "Smooth each numerical column by substituting the original values with the rolling averages wherever available.",
            "Return the smoothed dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def smooth_time_series(dataframe, window_size):\n    if 'timestamp' not in dataframe.columns or not pd.api.types.is_datetime64_any_dtype(dataframe['timestamp']):\n        raise ValueError(\"'timestamp' column must be present and in datetime format\")\n\n    smoothed_df = dataframe.copy()\n    \n    for col in dataframe.select_dtypes(include='number').columns:\n        rolling_avg = dataframe[col].rolling(window=window_size, min_periods=1).mean()\n        smoothed_df[col] = rolling_avg\n\n    return smoothed_df"
    },
    {
        "function_name": "encode_and_normalize",
        "file_name": "preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "label_col": "str"
        },
        "objectives": [
            "Identify categorical columns in the dataframe \"df\".",
            "Encode each categorical column using one-hot encoding, and append these encoded columns to the dataframe, dropping the original categorical columns.",
            "Normalize all numerical columns to a 0-1 range.",
            "Calculate the correlation matrix between the label column and all other columns, and create a new dataframe with this information.",
            "Return the modified dataframe and the correlation matrix dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler"
        ],
        "function_def": "def encode_and_normalize(df, label_col):\n    # Identify categorical columns\n    cat_cols = df.select_dtypes(include='object').columns.tolist()\n    \n    # One-hot encode categorical columns\n    df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n    \n    # Normalize numerical columns\n    num_cols = df_encoded.select_dtypes(include='number').columns.tolist()\n    num_cols.remove(label_col)\n    scaler = MinMaxScaler()\n    df_encoded[num_cols] = scaler.fit_transform(df_encoded[num_cols])\n    \n    # Calculate correlation matrix\n    corr_matrix = df_encoded.corr()[[label_col]].drop(label_col)\n    \n    return df_encoded, corr_matrix"
    },
    {
        "function_name": "group_statistics",
        "file_name": "group_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str"
        },
        "objectives": [
            "Group the dataframe \"df\" by the column \"group_col\".",
            "For each group, compute the mean and standard deviation for all numerical columns, and create a new dataframe with the group-wise statistics.",
            "Generate a new column in the original dataframe that contains the Z-score for each numerical column using the group-wise mean and standard deviation.",
            "Return the modified dataframe and the group-wise statistics dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_statistics(df, group_col):\n    # Group the dataframe by the group column\n    grouped = df.groupby(group_col)\n    \n    # Compute mean and std deviation for each group\n    group_stats = grouped.mean().add_suffix('_mean').join(grouped.std().add_suffix('_std'))\n    \n    # Calculate Z-score for each numerical column in each group\n    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n    for col in numeric_cols:\n        df[col + '_zscore'] = (df[col] - df[group_col].map(group_stats[col + '_mean'])) / df[group_col].map(group_stats[col + '_std'])\n    \n    return df, group_stats"
    },
    {
        "function_name": "generate_lagged_features",
        "file_name": "lag_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "lag": "int"
        },
        "objectives": [
            "Identify all numerical columns in the dataframe \"df\".",
            "Create lagged versions of each numerical column with a lag specified by \"lag\".",
            "Calculate the change rate for each lagged column by taking the difference between the original and lagged columns.",
            "Create an \"average_change_rate\" column that contains the mean of all change rates for each row.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def generate_lagged_features(df, lag):\n    # Identify numerical columns\n    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n    \n    # Create lagged features\n    for col in numeric_cols:\n        df[col + '_lagged'] = df[col].shift(lag)\n        df[col + '_change_rate'] = df[col] - df[col + '_lagged']\n    \n    # Calculate average change rate for each row\n    change_rate_cols = [col + '_change_rate' for col in numeric_cols]\n    df['average_change_rate'] = df[change_rate_cols].mean(axis=1)\n    \n    return df"
    },
    {
        "function_name": "transform_and_standardize",
        "file_name": "transformations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "factor": "float"
        },
        "objectives": [
            "Identify columns in the dataframe \"df\" that are highly skewed (|skewness| > 1).",
            "Apply a log transformation to these columns to reduce skewness, adding \"_log\" to the column names.",
            "Standardize all numerical columns to have zero mean and unit variance.",
            "Calculate and return the skewness of each transformed numerical column.",
            "Return the modified dataframe and a dictionary of skewness values after transformation."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def transform_and_standardize(df, factor):\n    # Identify highly skewed columns\n    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n    skewed_cols = [col for col in numeric_cols if abs(df[col].skew()) > 1]\n    \n    # Apply log transformation to reduce skewness\n    for col in skewed_cols:\n        df[col + '_log'] = np.log1p(df[col])\n    \n    # Standardize all numerical columns\n    all_cols = df.select_dtypes(include='number').columns\n    scaler = StandardScaler()\n    df[all_cols] = scaler.fit_transform(df[all_cols])\n    \n    # Calculate skewness of transformed columns\n    skewness_dict = {col: df[col].skew() for col in df.columns}\n    \n    return df, skewness_dict"
    },
    {
        "function_name": "categorical_correlation",
        "file_name": "categorical_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list"
        },
        "objectives": [
            "Validate that all specified categorical columns are present in the dataframe.",
            "Calculate the pairwise correlation between the specified categorical columns using Cram\u00e9r's V.",
            "Store the correlations in a new DataFrame with pairs of columns as rows and correlations as columns.",
            "Return the correlation DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.stats import chi2_contingency"
        ],
        "function_def": "def cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x, y)\n    chi2 = chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r, k = confusion_matrix.shape\n    return np.sqrt(phi2 / min((k - 1), (r - 1)))"
    },
    {
        "function_name": "normalize_skewed_columns",
        "file_name": "transformation_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list"
        },
        "objectives": [
            "Validate that all specified columns are present in the dataframe.",
            "Calculate the skewness and kurtosis for each column.",
            "Normalize the columns using a Box-Cox transformation if skewness is greater than a threshold (e.g., 1).",
            "Return the dataframe with normalized columns and separate lists of skewness and kurtosis values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy import stats"
        ],
        "function_def": "def normalize_skewed_columns(df, columns):\n    skewness_vals = []\n    kurtosis_vals = []\n    \n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not in the dataframe\")\n        \n        # Calculate skewness and kurtosis\n        skewness = df[col].skew()\n        kurtosis = df[col].kurtosis()\n        \n        skewness_vals.append((col, skewness))\n        kurtosis_vals.append((col, kurtosis))\n        \n        # Step 3: Normalization if skewness is high\n        if abs(skewness) > 1:\n            df[col], _ = stats.boxcox(df[col].dropna() + 1)  # Adding 1 to shift values to positive for Box-Cox\n    \n    return df, skewness_vals, kurtosis_vals"
    },
    {
        "function_name": "resample_and_aggregate",
        "file_name": "time_series_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "timestamp_column": "str",
            "freq": "str"
        },
        "objectives": [
            "Validate that the dataframe contains a \"timestamp_column\" in datetime format.",
            "Resample the dataframe based on the specified frequency \"freq\" for the timestamp column.",
            "Aggregate the numerical columns using sum, mean, and maximum for each resample window.",
            "Return the resampled and aggregated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_aggregate(df, timestamp_column, freq):\n    if timestamp_column not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[timestamp_column]):\n        raise ValueError(\"'timestamp_column' must be present and in datetime format\")\n    \n    df = df.set_index(timestamp_column)\n    df_resampled = df.resample(freq).agg(['sum', 'mean', 'max'])\n    \n    return df_resampled"
    },
    {
        "function_name": "filter_by_category_proportion",
        "file_name": "category_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "percentage": "float"
        },
        "objectives": [
            "Check that the target column exists in the dataframe and that it is categorical.",
            "Encode the target column using one-hot encoding.",
            "For each newly created one-hot encoded column, calculate the proportion of each category in the entire dataframe.",
            "Filter the dataframe to keep only the rows where the proportion of the target category is less than the specified percentage.",
            "Return the filtered dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_by_category_proportion(df, target_column, percentage):\n    if target_column not in df.columns:\n        raise ValueError(f\"Column {target_column} is not in the dataframe\")\n    if not pd.api.types.is_categorical_dtype(df[target_column]):\n        raise ValueError(\"Target column must be categorical\")\n    \n    # Step 2: One-hot encode the target column\n    one_hot_encoded = pd.get_dummies(df[target_column], prefix=target_column)\n    \n    # Step 3: Calculate the proportion for each category\n    category_proportions = one_hot_encoded.mean()\n    \n    # Step 4: Filter the dataframe\n    keep_columns = category_proportions[category_proportions < percentage].index\n    df_filtered = df[df[target_column].apply(lambda x: f\"{target_column}_{x}\" in keep_columns)]\n    \n    return df_filtered"
    },
    {
        "function_name": "quantile_labels",
        "file_name": "quantile_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "quantiles": "list"
        },
        "objectives": [
            "Calculate specified quantiles for the numerical columns in the dataframe.",
            "For each quantile, generate a new column indicating whether the value is below, within, or above the quantile range.",
            "Label new columns as '<original_column>_quantile_<quantile_value>'.",
            "Return a dataframe containing these quantile-derived columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def quantile_labels(df, target_col, quantiles):\n    numerical_cols = df.select_dtypes(include='number').columns.drop(target_col)\n    labels_df = pd.DataFrame()\n\n    for col in numerical_cols:\n        col_vals = df[col]\n        # Step 1: Calculate specified quantiles\n        quantile_vals = np.quantile(col_vals.dropna(), quantiles)\n        \n        for i, quantile in enumerate(quantiles):\n            new_col_name = f'{col}_quantile_{quantile:.2f}'\n            labels_df[new_col_name] = pd.cut(col_vals, bins=[-np.inf, quantile_vals[i], np.inf], labels=['BelowQuantile', 'AboveQuantile'], include_lowest=True).astype(str)\n    \n    # Step 4: Return dataframe with quantile-derived columns\n    return labels_df"
    },
    {
        "function_name": "calculate_z_scores",
        "file_name": "zscore_calculations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_column": "str",
            "target_columns": "list"
        },
        "objectives": [
            "Group the dataframe by the specified column.",
            "For each grouping, calculate the z-score for the target columns.",
            "Create new columns for the z-scores with '_zscore' suffixed.",
            "Return the dataframe with the z-score columns included."
        ],
        "import_lines": [
            "import pandas as pd",
            "import scipy.stats as stats"
        ],
        "function_def": "def calculate_z_scores(df, group_column, target_columns):\n    group_df = df.groupby(group_column).transform(lambda x: stats.zscore(x, nan_policy='omit'))\n    \n    for col in target_columns:\n        # Step 1: Calculate z-score for each grouping\n        new_col_name = f'{col}_zscore'\n        df[new_col_name] = group_df[col]\n\n    # Step 4: Return modified dataframe\n    return df"
    },
    {
        "function_name": "weekly_aggregation_and_rolling_mean",
        "file_name": "date_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "agg_dict": "dict"
        },
        "objectives": [
            "Convert the 'date_col' to datetime format and set it as the dataframe index.",
            "Resample the dataframe into weekly periods, aggregating using the operations specified in 'agg_dict'.",
            "Forward fill missing values after resampling.",
            "Calculate rolling mean for each aggregated column over a 4-week window and add them as new columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def weekly_aggregation_and_rolling_mean(df, date_col, agg_dict):\n    # Step 1: Convert date_col to datetime and set as index\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.set_index(date_col, inplace=True)\n\n    # Step 2: Resample to weekly periods and aggregate \n    weekly_df = df.resample('W').agg(agg_dict)\n\n    # Step 3: Forward fill missing values\n    weekly_df.ffill(inplace=True)\n\n    # Step 4: Calculate rolling mean over a 4-week window\n    for col in agg_dict.keys():\n        weekly_df[f'{col}_4wk_roll_mean'] = weekly_df[col].rolling(window=4).mean()\n\n    return weekly_df"
    },
    {
        "function_name": "rename_and_clean_columns",
        "file_name": "data_cleaning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_map": "dict"
        },
        "objectives": [
            "Rename columns based on the mapping provided in 'col_map'.",
            "Detect columns with high cardinality of unique values (greater than 50 unique values) and drop them from the dataframe.",
            "Impute missing values in numerical columns with the median and categorical columns with the mode.",
            "Return the modified dataframe and a list of columns that were dropped."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rename_and_clean_columns(df, col_map):\n    # Step 1: Rename columns based on col_map\n    df.rename(columns=col_map, inplace=True)\n    \n    # Step 2: Detect and drop high cardinality columns\n    high_card_cols = [col for col in df.columns if df[col].nunique() > 50]\n    df.drop(columns=high_card_cols, inplace=True)\n\n    # Step 3: Impute missing values\n    for col in df.select_dtypes(include='number').columns:\n        df[col].fillna(df[col].median(), inplace=True)\n    \n    for col in df.select_dtypes(include='object').columns:\n        df[col].fillna(df[col].mode()[0], inplace=True)\n    \n    return df, high_card_cols"
    },
    {
        "function_name": "generate_time_series_features",
        "file_name": "feature_generation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Generate lag features for the 'target_col' for the past 3 periods.",
            "Create cumulative sum and cumulative mean features for 'target_col' grouped by 'id_col'.",
            "Compute day of the week, week of the year, and month of the year features from the DataFrame index if it's a datetime index.",
            "Combine all new features with the original dataframe and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def generate_time_series_features(df, id_col, target_col):\n    df.sort_index(inplace=True)\n    \n    # Step 1: Generate lag features for the past 3 periods\n    for lag in range(1, 4):\n        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n    \n    # Step 2: Create cumulative sum and mean features grouped by id_col\n    df[f'{target_col}_cum_sum'] = df.groupby(id_col)[target_col].cumsum()\n    df[f'{target_col}_cum_mean'] = df.groupby(id_col)[target_col].expanding().mean().reset_index(level=0, drop=True)\n\n    # Step 3: Compute date-related features if index is datetime\n    if isinstance(df.index, pd.DatetimeIndex):\n        df['day_of_week'] = df.index.day_of_week\n        df['week_of_year'] = df.index.isocalendar().week\n        df['month_of_year'] = df.index.month\n\n    return df"
    },
    {
        "function_name": "target_encoding_with_kfold",
        "file_name": "encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_cols": "list"
        },
        "objectives": [
            "Perform target encoding on categorical columns specified in 'cat_cols' by replacing each category with the mean of the target variable.",
            "Implement k-fold cross-validated target encoding to prevent data leakage.",
            "Store the encoding mapping for each column.",
            "Return the modified dataframe and the encoding mappings."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import KFold"
        ],
        "function_def": "def target_encoding_with_kfold(df, cat_cols):\n    target_col = 'target'  # assuming 'target' column exists for this example\n    encoding_mappings = {}\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n    for col in cat_cols:\n        col_mapping = {}\n        df[f'{col}_encoded'] = 0\n        \n        for train_idx, val_idx in kf.split(df):\n            train_fold, val_fold = df.iloc[train_idx], df.iloc[val_idx]\n            mean_target = train_fold.groupby(col)[target_col].mean()\n            val_fold_encoded = val_fold[col].map(mean_target)\n            df.loc[val_fold.index, f'{col}_encoded'] = val_fold_encoded\n            \n            col_mapping.update(mean_target.to_dict())\n        \n        encoding_mappings[col] = col_mapping\n\n    df.drop(columns=cat_cols, inplace=True)\n    \n    return df, encoding_mappings"
    },
    {
        "function_name": "create_standardized_pivot_table",
        "file_name": "pivot_table.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "pivot_col": "str",
            "value_col": "str",
            "agg_funcs": "list"
        },
        "objectives": [
            "Validate that `pivot_col` and `value_col` are present in the dataframe.",
            "Create a pivot table where the index is the unique values of `pivot_col` and the columns are the aggregation functions applied on `value_col`.",
            "Ensure each aggregation function has a unique column name in the pivot table.",
            "Standardize the pivot table by removing mean and scaling to unit variance, and return the result."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def create_standardized_pivot_table(df, pivot_col, value_col, agg_funcs):\n    # Validate presence of columns\n    if pivot_col not in df.columns or value_col not in df.columns:\n        raise ValueError(f\"Columns '{pivot_col}' or '{value_col}' not found in DataFrame\")\n\n    # Create pivot table\n    pivot_table = df.pivot_table(index=pivot_col, values=value_col, aggfunc=agg_funcs)\n\n    # Standardize pivot table\n    scaler = StandardScaler()\n    standardized_pivot = pd.DataFrame(scaler.fit_transform(pivot_table), index=pivot_table.index, columns=pivot_table.columns)\n\n    return standardized_pivot"
    },
    {
        "function_name": "leave_one_out_mean",
        "file_name": "cross_validation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Confirm the presence of `target_col` in the dataframe.",
            "Implement a leave-one-out cross-validation scheme to compute the mean of `target_col` excluding the value of the current row.",
            "Create a new column `LOO_Target_Mean` to store the computed means.",
            "Return the dataframe with the added `LOO_Target_Mean` column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def leave_one_out_mean(df, target_col):\n    # Validate presence of column\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n\n    # Implement leave-one-out cross-validation\n    loo_mean = (df[target_col].sum() - df[target_col]) / (len(df) - 1)\n    df['LOO_Target_Mean'] = loo_mean\n\n    return df"
    },
    {
        "function_name": "equal_frequency_binning",
        "file_name": "binning_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "n_bins": "int"
        },
        "objectives": [
            "Validate the presence of `target_col` and confirm it is numeric.",
            "Create `n_bins` equal-frequency bins for `target_col` and label them.",
            "Calculate the mean and standard deviation of each bin.",
            "Generate a summary DataFrame with bin labels, mean, and standard deviation, and return it."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def equal_frequency_binning(df, target_col, n_bins):\n    # Validate presence and type of target column\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' is either missing or not numeric in the DataFrame\")\n    \n    # Create equal-frequency bins\n    df['bin'] = pd.qcut(df[target_col], q=n_bins, labels=False, duplicates='drop')\n\n    # Calculate mean and std for each bin\n    bin_summary = df.groupby('bin')[target_col].agg(['mean', 'std']).reset_index()\n\n    return bin_summary"
    },
    {
        "function_name": "preprocess_categorical_and_normalize",
        "file_name": "data_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame"
        },
        "objectives": [
            "Identify columns with categorical data and convert them to numerical data using one-hot encoding.",
            "Normalize all numerical columns to have zero mean and unit variance.",
            "Identify any missing values and fill them using the mean of the respective columns.",
            "Return the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def preprocess_categorical_and_normalize(df):\n    # Step 1: One-hot encode categorical columns\n    df = pd.get_dummies(df, drop_first=True)\n    \n    # Step 2: Normalize numerical columns\n    num_cols = df.select_dtypes(include='number').columns\n    scaler = StandardScaler()\n    df[num_cols] = scaler.fit_transform(df[num_cols])\n    \n    # Step 3: Fill missing values with mean of the column\n    for col in df.columns:\n        if df[col].isnull().any():\n            df[col].fillna(df[col].mean(), inplace=True)\n    \n    return df"
    },
    {
        "function_name": "generate_polynomial_features",
        "file_name": "feature_generation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Create polynomial features (degree of 2 and 3) from all numerical columns except the target column.",
            "For each polynomial feature created, compute its correlation with the target column.",
            "Filter and retain only those polynomial features with a correlation coefficient greater than 0.7 in the final dataframe.",
            "Return the dataframe containing the high-correlation polynomial features and the target column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from itertools import combinations",
            "from sklearn.preprocessing import PolynomialFeatures"
        ],
        "function_def": "def generate_polynomial_features(df, target_col):\n    numerical_cols = df.select_dtypes(include='number').columns.drop(target_col)\n    df_target = df[[target_col]].copy()\n    \n    poly = PolynomialFeatures(degree=3, include_bias=False)\n    poly_features = poly.fit_transform(df[numerical_cols])\n    poly_feature_names = poly.get_feature_names_out(numerical_cols)\n    \n    poly_df = pd.DataFrame(poly_features, columns=poly_feature_names)\n    \n    # Compute correlation with target column\n    high_correlation_features = []\n    for col in poly_df.columns:\n        correlation = poly_df[col].corr(df_target[target_col])\n        if correlation > 0.7:\n            high_correlation_features.append(col)\n    \n    return pd.concat([df_target, poly_df[high_correlation_features]], axis=1)"
    },
    {
        "function_name": "add_outlier_indicators",
        "file_name": "outlier_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "outlier_std_threshold": "float"
        },
        "objectives": [
            "Identify numeric columns in the dataframe \u201cdf\u201d and calculate the Z-score for each value.",
            "Identify and mark outliers that have Z-scores greater than \"outlier_std_threshold\".",
            "Create new columns indicating the presence of outliers (1 for outlier, 0 for non-outlier) for each numeric column.",
            "Return a dataframe with these new outlier-indicator columns added."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def add_outlier_indicators(df, outlier_std_threshold):\n    numeric_cols = df.select_dtypes(include='number').columns.copy()\n    \n    for col in numeric_cols:\n        z_scores = (df[col] - df[col].mean()) / df[col].std()\n        outlier_col_name = f'{col}_outliers'\n        df[outlier_col_name] = np.where(np.abs(z_scores) > outlier_std_threshold, 1, 0)\n    \n    return df"
    },
    {
        "function_name": "compute_rolling_stats",
        "file_name": "time_series_processing.py",
        "parameters": {
            "df": "pandas.DataFrame"
        },
        "objectives": [
            "Identify columns in the dataframe with time-series data.",
            "Apply a rolling window approach to compute the rolling mean and standard deviation for these columns.",
            "Create new columns holding these rolling statistics.",
            "Return the dataframe with the original columns and the new rolling statistics columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_rolling_stats(df):\n    time_series_cols = df.select_dtypes(include=['datetime', 'number']).columns\n    \n    for col in time_series_cols:\n        if pd.api.types.is_datetime64_any_dtype(df[col]) or pd.api.types.is_timedelta64_dtype(df[col]):\n            continue  # Skip datetime columns for rolling stats\n        \n        df[f'{col}_rolling_mean'] = df[col].rolling(window=3, min_periods=1).mean()\n        df[f'{col}_rolling_std'] = df[col].rolling(window=3, min_periods=1).std()\n    \n    return df"
    },
    {
        "function_name": "add_cumulative_target_mean",
        "file_name": "cumulative_statistics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Validate that `id_col` and `target_col` are present in the dataframe.",
            "Group the dataframe by `id_col` and compute the cumulative sum and cumulative count of `target_col`.",
            "Compute the cumulative mean of `target_col` for each group by dividing the cumulative sum by the cumulative count.",
            "Add a new column `Cumulative_Target_Mean` to the dataframe containing these calculated cumulative means."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_cumulative_target_mean(df, id_col, target_col):\n    # Validate presence of columns\n    if id_col not in df.columns or target_col not in df.columns:\n        raise ValueError(f\"Columns '{id_col}' or '{target_col}' not found in DataFrame\")\n\n    # Compute cumulative sums and counts within each group\n    df['Cumulative_Sum'] = df.groupby(id_col)[target_col].cumsum()\n    df['Cumulative_Count'] = df.groupby(id_col).cumcount() + 1\n\n    # Compute cumulative means\n    df['Cumulative_Target_Mean'] = df['Cumulative_Sum'] / df['Cumulative_Count']\n\n    # Drop intermediate columns\n    df.drop(columns=['Cumulative_Sum', 'Cumulative_Count'], inplace=True)\n\n    return df"
    },
    {
        "function_name": "calculate_days_since_reference",
        "file_name": "date_calculations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "reference_date": "str",
            "date_col": "str"
        },
        "objectives": [
            "Validate the presence of `date_col` in the dataframe and ensure it contains datetime values.",
            "Convert the `date_col` to datetime if it is not already in datetime format.",
            "Calculate the difference in days between each date in `date_col` and the `reference_date`.",
            "Add a new column `Days_Since_Reference` to the dataframe with these calculated differences."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_days_since_reference(df, reference_date, date_col):\n    # Validate presence of column\n    if date_col not in df.columns:\n        raise ValueError(f\"Column '{date_col}' not found in DataFrame\")\n\n    # Convert date_col and reference_date to datetime\n    df[date_col] = pd.to_datetime(df[date_col])\n    reference_date = pd.to_datetime(reference_date)\n\n    # Calculate difference in days\n    df['Days_Since_Reference'] = (df[date_col] - reference_date).dt.days\n\n    return df"
    },
    {
        "function_name": "scale_winsorize_and_normalize",
        "file_name": "scaling_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns_to_scale": "list",
            "quantile_range": "tuple"
        },
        "objectives": [
            "Ensure that all specified columns to scale are present in the dataframe and are numerical.",
            "Perform Winsorization on these columns based on the quantile range to limit the influence of outliers.",
            "Normalize the Winsorized columns using Min-Max scaling (0 to 1).",
            "Return the modified dataframe with scaled columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler",
            "from scipy.stats.mstats import winsorize"
        ],
        "function_def": "def scale_winsorize_and_normalize(df, columns_to_scale, quantile_range):\n    # Step 1: Ensure columns are present and numerical\n    for col in columns_to_scale:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not numerical\")\n    \n    # Step 2: Perform Winsorization\n    for col in columns_to_scale:\n        limits = quantile_range if isinstance(quantile_range, tuple) and len(quantile_range) == 2 else (0.05, 0.05)\n        df[col] = winsorize(df[col], limits=limits)\n    \n    # Step 3: Normalize using Min-Max scaling\n    scaler = MinMaxScaler()\n    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n    \n    return df"
    },
    {
        "function_name": "grouped_binning",
        "file_name": "group_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "target_col": "str",
            "n_bins": "int"
        },
        "objectives": [
            "Ensure that the group and target columns are present in the dataframe and target column is numerical.",
            "Group the dataframe by the specified group_col.",
            "For each group, bin the target column into n_bins equal-sized bins.",
            "Calculate and add a new column containing the bin indices for each row in the dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def grouped_binning(df, group_col, target_col, n_bins):\n    # Step 1: Ensure columns are present and valid\n    if group_col not in df.columns or target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(\"Columns or dtype are invalid\")\n    \n    # Step 2: Group by the specified column\n    df['bin_index'] = np.nan\n\n    # Step 3: Bin the target column within each group\n    groups = df.groupby(group_col)\n    for name, group in groups:\n        bins = pd.qcut(group[target_col], n_bins, labels=False, duplicates='drop')\n        df.loc[group.index, 'bin_index'] = bins\n\n    return df"
    },
    {
        "function_name": "fill_missing_with_reference",
        "file_name": "missing_data.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "missing_col": "str",
            "reference_cols": "list",
            "method": "str"
        },
        "objectives": [
            "Ensure the missing column and reference columns are present and valid in the dataframe.",
            "Fill missing values in the specified column using the specified method ('mean', 'median', or 'mode') of the reference columns.",
            "If the method is 'mode', ensure the reference columns are categorical and apply the mode appropriately.",
            "Return the modified dataframe with the filled missing values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def fill_missing_with_reference(df, missing_col, reference_cols, method):\n    # Step 1: Ensure columns are present and valid\n    if missing_col not in df.columns or not all(col in df.columns for col in reference_cols):\n        raise ValueError(\"Columns are invalid\")\n\n    # Step 2: Fill missing values based on method\n    fill_value = None\n    if method == 'mean':\n        fill_value = df[reference_cols].mean(axis=1)\n    elif method == 'median':\n        fill_value = df[reference_cols].median(axis=1)\n    elif method == 'mode':\n        fill_value = df[reference_cols].mode(axis=1).iloc[0]\n    else:\n        raise ValueError(\"Method must be 'mean', 'median', or 'mode'\")\n    \n    if fill_value is not None:\n        df[missing_col] = df[missing_col].fillna(fill_value)\n    \n    return df"
    },
    {
        "function_name": "compute_column_ranges",
        "file_name": "range_computations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "range_cols": "list"
        },
        "objectives": [
            "Verify that the specified range columns exist in the dataframe and are numerical.",
            "Compute the range (max - min) for each specified column.",
            "Calculate the average range for all specified columns and store this value in a new column 'average_range'.",
            "Return the modified dataframe with the new column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_column_ranges(df, range_cols):\n    # Step 1: Ensure columns are present and numerical\n    for col in range_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not numerical\")\n    \n    # Step 2: Compute the range for each column\n    ranges = {}\n    for col in range_cols:\n        ranges[col] = df[col].max() - df[col].min()\n    \n    # Step 3: Calculate the average range\n    avg_range = sum(ranges.values()) / len(ranges)\n    \n    # Step 4: Store average range in new column\n    df['average_range'] = avg_range\n    \n    return df"
    },
    {
        "function_name": "flag_high_rolling_sums",
        "file_name": "rolling_sum_flag.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Identify all rows in the dataframe where the specified column's value exceeds the given threshold.",
            "For those rows, compute the rolling sum with a window size equal to the number of such rows.",
            "Append a new column to the dataframe containing these rolling sums.",
            "Create a boolean mask column indicating whether each rolling sum is above or below the threshold. Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def flag_high_rolling_sums(df, col, threshold):\n    rows = df[df[col] > threshold]\n    window_size = len(rows)\n    \n    rolling_sums = rows[col].rolling(window=window_size).sum().reset_index(drop=True)\n    \n    df['rolling_sum'] = pd.Series(index=rows.index, data=rolling_sums)\n    df['rolling_sum_flag'] = df['rolling_sum'] > threshold\n    \n    return df"
    },
    {
        "function_name": "log_transform_and_quantile_binning",
        "file_name": "log_binning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column": "str"
        },
        "objectives": [
            "Apply a log transformation to the specified column, handling zero and negative values appropriately.",
            "Calculate the quantile bins for the log-transformed values and label each bin.",
            "Add a new column in the dataframe with the labeled quantile bins.",
            "Ensure that the new column has a consistent categorical order, and return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def log_transform_and_quantile_binning(df, column):\n    df[f'{column}_log_transformed'] = df[column].apply(lambda x: np.log(x + 1) if x >= 0 else np.nan)\n    \n    quantiles = pd.qcut(df[f'{column}_log_transformed'].dropna(), q=4, labels=False)  # Default 4 quantiles\n    \n    df[f'{column}_quantile_bin'] = pd.qcut(df[f'{column}_log_transformed'], q=4)\n    \n    return df"
    },
    {
        "function_name": "cumulative_sum_post_sort",
        "file_name": "cumsum_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "sort_column": "str"
        },
        "objectives": [
            "Sort the dataframe based on the values in the specified \"sort_column\".",
            "Reset the index post sorting to ensure the indices reflect the sort order.",
            "Compute the cumulative sum of the numeric columns.",
            "Append these cumulative sums as new columns to the dataframe, prefixed with \"cumsum_\". Return the augmented dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def cumulative_sum_post_sort(df, sort_column):\n    sorted_df = df.sort_values(by=sort_column).reset_index(drop=True)\n    \n    cumsum_df = sorted_df.copy()\n    numeric_columns = sorted_df.select_dtypes(include='number').columns\n    \n    for col in numeric_columns:\n        cumsum_df[f'cumsum_{col}'] = sorted_df[col].cumsum()\n    \n    return cumsum_df"
    },
    {
        "function_name": "discretize_by_percentile",
        "file_name": "percentile_binning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "percentiles": "list"
        },
        "objectives": [
            "Validate that the target column exists and is numerical.",
            "Calculate percentile-based bin edges based on the provided percentiles.",
            "Discretize the target column into the calculated percentile bins.",
            "Return the modified dataframe along with the percentile-based bin edges."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def discretize_by_percentile(df, target_col, percentiles):\n    # Step 1: Validate target column\n    if target_col not in df.columns or not np.issubdtype(df[target_col].dtype, np.number):\n        raise ValueError(f\"Target column {target_col} must be present and of numerical type.\")\n    \n    # Step 2: Calculate percentile-based bin edges\n    bin_edges = np.percentile(df[target_col], percentiles)\n    \n    # Step 3: Discretize target column into bins\n    bin_labels = range(len(bin_edges) - 1)\n    df[f\"{target_col}_percentile_bin\"] = pd.cut(df[target_col], bins=bin_edges, labels=bin_labels, include_lowest=True)\n    \n    return df, bin_edges"
    },
    {
        "function_name": "rank_within_groups",
        "file_name": "group_ranking.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_by_cols": "list",
            "rank_col": "str"
        },
        "objectives": [
            "Validate that all specified group-by columns and rank column are present in the dataframe.",
            "Group the dataframe by the specified columns.",
            "Within each group, rank the data based on the specified rank column.",
            "Return the dataframe with a new column indicating the rank of each entry within its group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rank_within_groups(df, group_by_cols, rank_col):\n    # Step 1: Validate columns\n    missing_columns = [col for col in group_by_cols + [rank_col] if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"These columns are not in the dataframe: {missing_columns}\")\n    \n    # Step 2: Group by specified columns and rank within each group\n    df['rank'] = df.groupby(group_by_cols)[rank_col].rank(method='first')\n    \n    return df"
    },
    {
        "function_name": "expand_datetime_features",
        "file_name": "datetime_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str"
        },
        "objectives": [
            "Ensure the \"datetime_col\" is present in the dataframe and convert it to datetime format.",
            "Create new columns for year, month, day, hour, minute, and second from the datetime column.",
            "Calculate the time elapsed in seconds between consecutive entries in the datetime column.",
            "Return the dataframe with the new datetime feature columns and elapsed time column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def expand_datetime_features(df, datetime_col):\n    if datetime_col not in df.columns:\n        raise ValueError(f\"Column {datetime_col} is not in the dataframe\")\n    \n    df[datetime_col] = pd.to_datetime(df[datetime_col])\n    df['year'] = df[datetime_col].dt.year\n    df['month'] = df[datetime_col].dt.month\n    df['day'] = df[datetime_col].dt.day\n    df['hour'] = df[datetime_col].dt.hour\n    df['minute'] = df[datetime_col].dt.minute\n    df['second'] = df[datetime_col].dt.second\n    df['elapsed_seconds'] = df[datetime_col].diff().dt.total_seconds().fillna(0)\n    \n    return df"
    },
    {
        "function_name": "text_to_word_freq",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str"
        },
        "objectives": [
            "Validate the presence of the specified text column in the dataframe.",
            "Split the text into individual words and calculate the frequency of each word.",
            "Create a new column for each of the top N most frequent words (optional parameter, default 10).",
            "For each new word-column, set the value to the count of the word in each row's text.",
            "Return the modified dataframe with new word-frequency columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from collections import Counter",
            "from sklearn.feature_extraction.text import CountVectorizer"
        ],
        "function_def": "def text_to_word_freq(df, text_col, top_n=10):\n    if text_col not in df.columns:\n        raise ValueError(f\"Column {text_col} is not in the dataframe\")\n    \n    text_data = df[text_col].astype(str)\n    vectorizer = CountVectorizer(max_features=top_n)\n    word_matrix = vectorizer.fit_transform(text_data)\n    \n    words = vectorizer.get_feature_names_out()\n    word_freq_df = pd.DataFrame(word_matrix.toarray(), columns=words)\n    \n    return pd.concat([df, word_freq_df], axis=1)"
    },
    {
        "function_name": "inner_join_dataframes",
        "file_name": "data_merging.py",
        "parameters": {
            "df1": "pandas.DataFrame",
            "df2": "pandas.DataFrame",
            "key": "str"
        },
        "objectives": [
            "Validate that the key column is present in both dataframes.",
            "Perform an inner join on the two dataframes using the key column.",
            "Detect and handle missing or mismatched data in the key column.",
            "Return the merged dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def inner_join_dataframes(df1, df2, key):\n    if key not in df1.columns or key not in df2.columns:\n        raise ValueError(f\"Column {key} is not in both dataframes\")\n    \n    merged_df = pd.merge(df1, df2, on=key, how='inner')\n    \n    missing_in_df1 = df1[~df1[key].isin(merged_df[key])]\n    missing_in_df2 = df2[~df2[key].isin(merged_df[key])]\n    \n    if not missing_in_df1.empty or not missing_in_df2.empty:\n        print(\"Missing data detected in the key column. Handling missing data...\")\n    \n    merged_df = merged_df.fillna(method='ffill').fillna(method='bfill')\n    \n    return merged_df"
    },
    {
        "function_name": "select_top_correlated_features",
        "file_name": "feature_selection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Ensure the \"target_col\" is present in the dataframe.",
            "Calculate the correlation of each numeric column with the target column.",
            "Select the top N columns that have the highest absolute correlation with the target column.",
            "Return a new dataframe containing the target column and the selected top N columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def select_top_correlated_features(df, target_col, top_n=5):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column {target_col} is not in the dataframe\")\n    \n    correlations = df.corr()[target_col].abs().sort_values(ascending=False)\n    \n    top_columns = correlations.index[1:top_n+1]  # Exclude the target column itself\n    selected_df = df[[target_col] + list(top_columns)]\n    \n    return selected_df"
    },
    {
        "function_name": "normalize_and_cap_outliers",
        "file_name": "normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_cols": "list"
        },
        "objectives": [
            "Identify ranges in the columns specified in numerical_cols.",
            "Standardize these columns using min-max scaling from 0 to 1.",
            "Cap the extreme values at 1st and 99th percentiles to remove outliers.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def normalize_and_cap_outliers(df, numerical_cols):\n    for col in numerical_cols:\n        # Step 1: Identify range\n        min_val = df[col].min()\n        max_val = df[col].max()\n        \n        # Step 2: Min-max scaling\n        df[col] = (df[col] - min_val) / (max_val - min_val)\n        \n        # Step 3: Cap extreme values\n        low_cap = df[col].quantile(0.01)\n        high_cap = df[col].quantile(0.99)\n        df[col] = df[col].clip(lower=low_cap, upper=high_cap)\n    \n    return df"
    },
    {
        "function_name": "impute_missing_values",
        "file_name": "imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "missing_val_strategy": "str (one of \"mean\", \"median\", \"mode\")"
        },
        "objectives": [
            "Identify and list all columns in the dataframe `df` that contain missing values.",
            "Apply the specified `missing_val_strategy` to impute the missing values in each column.",
            "Log the number of missing values before and after imputation for auditing purposes.",
            "Return the modified dataframe and a log of the audit information."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def impute_missing_values(df, target_col, missing_val_strategy):\n    if missing_val_strategy not in [\"mean\", \"median\", \"mode\"]:\n        raise ValueError(\"Invalid missing value strategy. Choose from 'mean', 'median', 'mode'.\")\n\n    missing_before = df.isnull().sum()\n\n    for col in df.columns:\n        if df[col].isnull().sum() > 0:\n            if missing_val_strategy == \"mean\":\n                df[col].fillna(df[col].mean(), inplace=True)\n            elif missing_val_strategy == \"median\":\n                df[col].fillna(df[col].median(), inplace=True)\n            elif missing_val_strategy == \"mode\":\n                df[col].fillna(df[col].mode()[0], inplace=True)\n\n    missing_after = df.isnull().sum()\n    \n    audit_log = pd.DataFrame({\n        \"column\": df.columns,\n        \"missing_before\": missing_before,\n        \"missing_after\": missing_after\n    })\n    \n    audit_log = audit_log[audit_log[\"missing_before\"] > 0]\n\n    return df, audit_log"
    },
    {
        "function_name": "limit_categories",
        "file_name": "category_limitation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_cols": "list of str",
            "top_n": "int"
        },
        "objectives": [
            "Validate that all specified categorical columns are present in the dataframe.",
            "Identify the top `top_n` most frequent categories in each categorical column.",
            "Replace all non-top `top_n` categories with a special value \"Other\".",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def limit_categories(df, category_cols, top_n):\n    missing_columns = [col for col in category_cols if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"The following columns are not in the dataframe: {missing_columns}\")\n\n    for col in category_cols:\n        top_categories = df[col].value_counts().nlargest(top_n).index\n        df[col] = df[col].apply(lambda x: x if x in top_categories else \"Other\")\n    \n    return df"
    },
    {
        "function_name": "aggregate_features",
        "file_name": "feature_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "feature_cols": "list of str",
            "agg_funcs": "dict"
        },
        "objectives": [
            "Group the dataframe by the unique identifier column `id_col`.",
            "For each group, apply the specified aggregation functions to the columns listed in `feature_cols` using the dictionary `agg_funcs`.",
            "Flatten the resulting multi-level column index to create a single level with appropriate names.",
            "Return the aggregated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_features(df, id_col, feature_cols, agg_funcs):\n    if id_col not in df.columns:\n        raise ValueError(f\"The identifier column '{id_col}' is not in the dataframe.\")\n       \n    missing_columns = [col for col in feature_cols if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"The following feature columns are not in the dataframe: {missing_columns}\")\n\n    aggregated_df = df.groupby(id_col)[feature_cols].agg(agg_funcs)\n    aggregated_df.columns = ['_'.join(col) for col in aggregated_df.columns]\n\n    return aggregated_df.reset_index()"
    },
    {
        "function_name": "add_hierarchical_cumsum_rank",
        "file_name": "hierarchical_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "hierarchy_cols": "list",
            "value_col": "str"
        },
        "objectives": [
            "Confirm that all `hierarchy_cols` exist in the dataframe and that `value_col` is numeric.",
            "Create multi-level groups based on the hierarchy columns in ascending order.",
            "Calculate cumulative sum and rank within each group for the `value_col`.",
            "Add new columns for cumulative sum and rank, suffixed with '_cumsum' and '_rank', respectively, and return the dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_hierarchical_cumsum_rank(df, hierarchy_cols, value_col):\n    for col in hierarchy_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    if value_col not in df.columns:\n        raise ValueError(f\"Column '{value_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"Column '{value_col}' must be numeric\")\n\n    grouped = df.groupby(hierarchy_cols)\n    \n    df[f'{value_col}_cumsum'] = grouped[value_col].cumsum()\n    df[f'{value_col}_rank'] = grouped[value_col].rank()\n    \n    return df"
    },
    {
        "function_name": "resample_and_aggregate",
        "file_name": "time_series_resampling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "windows": "list"
        },
        "objectives": [
            "Ensure the presence and datetime type of the `time_col`.",
            "For each window duration in `windows`, resample the dataframe and calculate the sum, mean, min, and max of all numeric columns.",
            "Convert the resampled data to a flat dataframe and rename columns to indicate the aggregation performed and the window size.",
            "Merge all resampled dataframes to get a final dataframe and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_aggregate(df, time_col, windows):\n    if time_col not in df.columns:\n        raise ValueError(f\"Column '{time_col}' not found in DataFrame\")\n    df[time_col] = pd.to_datetime(df[time_col])\n    \n    results = []\n    \n    for window in windows:\n        resampled = df.resample(window, on=time_col).agg(['sum', 'mean', 'min', 'max'])\n        resampled.columns = ['_'.join(col) + f'_{window}' for col in resampled.columns]\n        results.append(resampled)\n    \n    final_result = pd.concat(results, axis=1)\n    final_result.reset_index(inplace=True)\n    \n    return final_result"
    },
    {
        "function_name": "impute_and_calculate_range",
        "file_name": "data_cleaning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "cols_to_impute": "List[str]"
        },
        "objectives": [
            "Group the dataframe by 'group_col'.",
            "For each group, impute missing values in 'cols_to_impute' using forward-fill followed by backward-fill.",
            "After imputation, calculate the difference between the max and min values for each 'cols_to_impute'.",
            "Add these calculated differences as new columns in the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_and_calculate_range(df, group_col, cols_to_impute):\n    df = df.copy()\n    grouped = df.groupby(group_col)\n    \n    for col in cols_to_impute:\n        df[col] = grouped[col].ffill().bfill()\n        \n        max_vals = grouped[col].transform('max')\n        min_vals = grouped[col].transform('min')\n        df[f'{col}_range'] = max_vals - min_vals\n    \n    return df"
    },
    {
        "function_name": "rolling_stats_and_outliers",
        "file_name": "feature_generation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "window_size": "int"
        },
        "objectives": [
            "Calculate rolling mean, rolling median, and rolling standard deviation for 'target_col' with a given 'window_size'.",
            "Add these rolling statistics as new columns in the original dataframe.",
            "Identify and mark outliers in 'target_col' if they lie beyond 3 standard deviations from the rolling mean.",
            "Return the modified dataframe with rolling statistics and outlier marks."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_stats_and_outliers(df, target_col, window_size):\n    df = df.copy()\n    \n    df[f'{target_col}_rolling_mean'] = df[target_col].rolling(window=window_size).mean()\n    df[f'{target_col}_rolling_median'] = df[target_col].rolling(window=window_size).median()\n    df[f'{target_col}_rolling_std'] = df[target_col].rolling(window=window_size).std()\n    \n    rolling_mean = df[f'{target_col}_rolling_mean']\n    rolling_std = df[f'{target_col}_rolling_std']\n    \n    df['outlier'] = ((df[target_col] < rolling_mean - 3*rolling_std) | (df[target_col] > rolling_mean + 3*rolling_std)).astype(int)\n    \n    return df"
    },
    {
        "function_name": "hierarchical_aggregation_and_scaling",
        "file_name": "aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "agg_dict": "Dict[str, List[str]]"
        },
        "objectives": [
            "For each unique value in 'cat_col', aggregate the dataframe according to the aggregation rules specified in 'agg_dict'.",
            "Convert these aggregated metrics into a hierarchical multi-indexed dataframe where the first level is 'cat_col' values, and the second level is the aggregation metrics.",
            "Standardize the values in the hierarchical dataframe by scaling each metric to zero mean and unit variance.",
            "Return the hierarchical multi-indexed standardized dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def hierarchical_aggregation_and_scaling(df, cat_col, agg_dict):\n    agg_df = df.groupby(cat_col).agg(agg_dict)\n    \n    hierarchical_index = agg_df.index\n    agg_df = agg_df.reset_index()\n    agg_df.columns = ['_'.join(col).strip() if type(col) is tuple else col for col in agg_df.columns.values]\n    \n    scaler = StandardScaler()\n    agg_df_scaled = scaler.fit_transform(agg_df.iloc[:, 1:])\n    \n    standardized_df = pd.DataFrame(agg_df_scaled, columns=agg_df.columns[1:], index=hierarchical_index)\n    \n    return standardized_df"
    },
    {
        "function_name": "categorical_numeric_stats",
        "file_name": "category_stats.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list of str",
            "numeric_col": "str"
        },
        "objectives": [],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def categorical_numeric_stats(df, categorical_cols, numeric_col):\n    df = df.copy()\n    \n    for cat_col in categorical_cols:\n        unique_categories = df[cat_col].unique()\n        for category in unique_categories:\n            cat_mask = df[cat_col] == category\n            \n            category_mean = df.loc[cat_mask, numeric_col].mean()\n            category_median = df.loc[cat_mask, numeric_col].median()\n            category_std = df.loc[cat_mask, numeric_col].std()\n            \n            df.loc[cat_mask, f'{numeric_col}_{category}_mean'] = category_mean\n            df.loc[cat_mask, f'{numeric_col}_{category}_median'] = category_median\n            df.loc[cat_mask, f'{numeric_col}_{category}_std'] = category_std\n            \n            df.loc[cat_mask, numeric_col] = df.loc[cat_mask, numeric_col] - category_mean\n    \n    return df"
    },
    {
        "function_name": "resample_and_aggregate",
        "file_name": "resampling_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str",
            "freq": "str"
        },
        "objectives": [],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_aggregate(df, datetime_col, freq):\n    df = df.copy()\n    df[datetime_col] = pd.to_datetime(df[datetime_col])\n    \n    resampled_df = df.resample(freq, on=datetime_col).agg(['sum', 'count', 'mean'])\n    resampled_df.columns = ['_'.join(col).strip() for col in resampled_df.columns.values]\n    resampled_df['duration_days'] = resampled_df.index.to_series().diff().dt.days.fillna(0)\n    \n    return resampled_df.reset_index()"
    },
    {
        "function_name": "polynomial_feature_selection",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "dependent_var": "str",
            "independent_vars": "list of str"
        },
        "objectives": [],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def polynomial_feature_selection(df, dependent_var, independent_vars):\n    df = df.copy()\n    threshold = 0.7\n    selected_features = []\n    \n    for col in independent_vars:\n        df[f'{col}_squared'] = df[col] ** 2\n        df[f'{col}_cubed'] = df[col] ** 3\n        \n        for poly_col in [col, f'{col}_squared', f'{col}_cubed']:\n            correlation = df[poly_col].corr(df[dependent_var])\n            if abs(correlation) >= threshold:\n                selected_features.append(poly_col)\n    \n    return df[selected_features + [dependent_var]]"
    },
    {
        "function_name": "temporal_averages",
        "file_name": "temporal_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "target_col": "str"
        },
        "objectives": [],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def temporal_averages(df, date_col, target_col):\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df['year'] = df[date_col].dt.year\n    df['month'] = df[date_col].dt.month\n    df['week'] = df[date_col].dt.isocalendar().week\n    df['day_of_week'] = df[date_col].dt.dayofweek\n    df['quarter'] = df[date_col].dt.quarter\n    \n    temporal_aggregations = ['year', 'month', 'week', 'day_of_week', 'quarter']\n    \n    for time_feature in temporal_aggregations:\n        group_avgs = df.groupby(time_feature)[target_col].transform('mean')\n        df[f'{target_col}_avg_by_{time_feature}'] = group_avgs\n    \n    return df"
    },
    {
        "function_name": "remove_and_normalize_outliers",
        "file_name": "outlier_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "outlier_cols": "list of str",
            "threshold": "float"
        },
        "objectives": [
            "Identify rows with outliers in specified columns based on a given threshold.",
            "For each identified row, replace the outlier with the median value of its respective column.",
            "Normalize the columns by subtracting the mean and dividing by the standard deviation for those columns.",
            "Return the cleaned and normalized dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_and_normalize_outliers(df, outlier_cols, threshold):\n    if not all(col in df.columns for col in outlier_cols):\n        missing_cols = [col for col in outlier_cols if col not in df.columns]\n        raise ValueError(f\"Columns {missing_cols} are not in the dataframe.\")\n    \n    for col in outlier_cols:\n        median = df[col].median()\n        outliers = df[col].abs() > threshold\n        df.loc[outliers, col] = median\n    \n    df[outlier_cols] = (df[outlier_cols] - df[outlier_cols].mean()) / df[outlier_cols].std()\n    \n    return df"
    },
    {
        "function_name": "apply_moving_average",
        "file_name": "moving_average.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int"
        },
        "objectives": [
            "Identify all numeric columns in the dataframe.",
            "Apply a moving average filter of a specified window size to these numeric columns.",
            "Calculate and append the weighted moving average for each column.",
            "Return the dataframe with the appended moving average columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def apply_moving_average(df, window_size):\n    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n    \n    for col in numeric_cols:\n        df[col + '_ma'] = df[col].rolling(window=window_size, min_periods=1).mean()\n        \n        weights = np.arange(1, window_size + 1)\n        weighted_ma = df[col].rolling(window=window_size).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n        df[col + '_wma'] = weighted_ma\n    \n    return df"
    },
    {
        "function_name": "apply_operation_to_numeric",
        "file_name": "numeric_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "operation": "str"
        },
        "objectives": [
            "Identify all columns that are numeric in the dataframe.",
            "Apply an operation (specified by a string, e.g., \"log\", \"sqrt\", \"square\") to all numeric columns.",
            "Handle any potential issues raised by invalid inputs for the specified operation (e.g., taking the log of a negative number).",
            "Return the dataframe with the transformed numeric columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def apply_operation_to_numeric(df, operation):\n    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n    \n    transformation_dict = {\n        \"log\": np.log,\n        \"sqrt\": np.sqrt,\n        \"square\": np.square\n    }\n    \n    if operation not in transformation_dict:\n        raise ValueError(f\"Unsupported operation '{operation}'. Supported operations are: {list(transformation_dict.keys())}\")\n    \n    for col in numeric_cols:\n        try:\n            df[col] = df[col].apply(transformation_dict[operation])\n        except Exception as e:\n            print(f\"Could not apply operation '{operation}' on column '{col}'. Reason: {e}\")\n    \n    return df"
    },
    {
        "function_name": "encode_rare_categories",
        "file_name": "category_encoder.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "rare_thresh": "int"
        },
        "objectives": [
            "Identify categories in the categorical columns that appear less frequently than \"rare_thresh\" times.",
            "Group rare categories into a single category labeled 'Rare'.",
            "Encode the categorical columns using one-hot encoding.",
            "Return the modified dataframe with rare categories grouped and encoded."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def encode_rare_categories(df, categorical_columns, rare_thresh):\n    for col in categorical_columns:\n        value_counts = df[col].value_counts()\n        rare_categories = value_counts[value_counts < rare_thresh].index\n        df[col] = df[col].apply(lambda x: 'Rare' if x in rare_categories else x)\n    \n    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n    \n    return df"
    },
    {
        "function_name": "create_interaction_terms",
        "file_name": "interaction_creator.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "interaction_columns": "list of tuple"
        },
        "objectives": [
            "For each tuple in \"interaction_columns\", create a new column representing the interaction effect (i.e., product) of the specified columns.",
            "Standardize the newly created interaction columns.",
            "Ensure that the column names are appropriately labeled to reflect the interaction.",
            "Return the dataframe with the new interaction columns added and standardized."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def create_interaction_terms(df, interaction_columns):\n    for col1, col2 in interaction_columns:\n        interaction_term = f\"{col1}_x_{col2}\"\n        df[interaction_term] = df[col1] * df[col2]\n    \n    interaction_cols = df.columns[df.columns.str.contains('_x_')]\n    scaler = StandardScaler()\n    df[interaction_cols] = scaler.fit_transform(df[interaction_cols])\n    \n    return df"
    },
    {
        "function_name": "apply_operations_from_log",
        "file_name": "log_processor.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "operation_log": "str"
        },
        "objectives": [
            "Parse the \"operation_log\" which contains a series of operations to apply to the dataframe.",
            "Each operation should follow a specific syntax, e.g., 'fillna:column_name,mean', 'dropna:column_name'.",
            "Apply each operation in the order they appear in the log.",
            "Validate syntax and raise errors for unrecognized operations.",
            "Return the dataframe after applying all specified operations."
        ],
        "import_lines": [
            "import pandas as pd",
            "import re"
        ],
        "function_def": "def apply_operations_from_log(df, operation_log):\n    operations = operation_log.split(';')\n    \n    for op in operations:\n        action, params = op.split(':')\n        \n        if action == 'fillna':\n            col, method = params.split(',')\n            if method == 'mean':\n                df[col].fillna(df[col].mean(), inplace=True)\n            elif method == 'median':\n                df[col].fillna(df[col].median(), inplace=True)\n            else:\n                raise ValueError(f\"Unknown fillna method: {method}\")\n        elif action == 'dropna':\n            if ',' in params:\n                col, axis = params.split(',')\n                df.dropna(subset=[col], axis=int(axis), inplace=True)\n            else:\n                col = params\n                df.dropna(subset=[col], inplace=True)\n        else:\n            raise ValueError(f\"Unknown operation: {action}\")\n    \n    return df"
    },
    {
        "function_name": "resample_and_fill_missing",
        "file_name": "time_series.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str"
        },
        "objectives": [
            "Parse and convert the \"time_col\" as a datetime object.",
            "Set the \"time_col\" as the DataFrame\u2019s index.",
            "Resample the DataFrame by month, calculating the monthly average for all numeric columns.",
            "Forward-fill and backward-fill any missing values after resampling."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_fill_missing(df, time_col):\n    # Parse and convert the time column to datetime\n    df[time_col] = pd.to_datetime(df[time_col])\n    \n    # Set the time column as the DataFrame's index\n    df.set_index(time_col, inplace=True)\n    \n    # Resample by month and calculate monthly average\n    df_resampled = df.resample('M').mean()\n    \n    # Forward-fill and backward-fill missing values\n    df_resampled.ffill(inplace=True)\n    df_resampled.bfill(inplace=True)\n    \n    return df_resampled"
    },
    {
        "function_name": "feature_engineering_pipeline",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_cols": "list of str",
            "target_col": "str"
        },
        "objectives": [
            "Perform a K-nearest neighbors imputation to fill in missing values in the \"feature_cols\".",
            "Normalize the \"feature_cols\" to range [0, 1].",
            "Select the top 3 features most correlated with the \"target_col\".",
            "Perform a principal component analysis (PCA) to reduce dimensions to 2D space while preserving 95% variance."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.impute import KNNImputer",
            "from sklearn.preprocessing import MinMaxScaler",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def feature_engineering_pipeline(df, feature_cols, target_col):\n    # K-nearest neighbors imputation for filling missing values\n    imputer = KNNImputer(n_neighbors=5)\n    df[feature_cols] = imputer.fit_transform(df[feature_cols])\n    \n    # Normalize feature columns to range [0, 1]\n    scaler = MinMaxScaler()\n    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n    \n    # Select top 3 features most correlated with the target column\n    corrs = df[feature_cols].corrwith(df[target_col]).abs()\n    top_features = corrs.nlargest(3).index.tolist()\n    \n    # Perform PCA to reduce dimensions while preserving 95% variance\n    pca = PCA(n_components=0.95)\n    pca_features = pca.fit_transform(df[top_features])\n    \n    return pd.DataFrame(pca_features, columns=[f\"PCA_{i}\" for i in range(pca_features.shape[1])])"
    },
    {
        "function_name": "discretize_and_calculate_stats",
        "file_name": "binning_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "bins": "int"
        },
        "objectives": [
            "Ensure the presence and numeric type of `target_col`.",
            "Discretize the values in `target_col` into specified `bins` using quantiles.",
            "For each bin, calculate the mean and standard deviation of all numeric columns.",
            "Return a dataframe with bins and calculated statistics, renaming columns to indicate the bin."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def discretize_and_calculate_stats(df, target_col, bins):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n    \n    df = df.copy()\n    df['quantile_bin'] = pd.qcut(df[target_col], bins)\n    \n    statistics_df = df.groupby('quantile_bin').agg(['mean', 'std'])\n    statistics_df.columns = ['_'.join(col) + '_binned' for col in statistics_df.columns]\n    \n    return statistics_df.reset_index().rename(columns={'quantile_bin': 'bin'})"
    },
    {
        "function_name": "scale_within_categories",
        "file_name": "scaling_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "num_col": "str",
            "scale_type": "str (possible values"
        },
        "objectives": [
            "Ensure the presence of `cat_col` and `num_col`, and validate their data types.",
            "For each unique category in `cat_col`, apply the specified scaling type to `num_col`.",
            "Return a dataframe with scaled numeric column within each category.",
            "Append a suffix to `num_col` to indicate the scaling type."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
        ],
        "function_def": "def scale_within_categories(df, cat_col, num_col, scale_type):\n    if cat_col not in df.columns or num_col not in df.columns:\n        raise ValueError(f\"Columns '{cat_col}' or '{num_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[num_col]):\n        raise ValueError(f\"Column '{num_col}' must be numeric\")\n\n    df = df.copy()\n    scaler = None\n    scaled_col_name = f\"{num_col}_{scale_type}\"\n\n    if scale_type == 'min-max':\n        scaler = MinMaxScaler()\n    elif scale_type == 'standard':\n        scaler = StandardScaler()\n    else:\n        raise ValueError(f\"Unknown scaling type: {scale_type}\")\n\n    df[scaled_col_name] = df.groupby(cat_col)[num_col].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten())\n\n    return df"
    },
    {
        "function_name": "filter_with_conditions",
        "file_name": "conditional_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "condition_list": "list"
        },
        "objectives": [
            "Validate the syntax of each condition in `condition_list`, where each condition is of the form `column_name:operation:value`.",
            "Apply each condition sequentially and filter the dataframe based on the cumulative conditions.",
            "Raise an error for unrecognized operations or columns.",
            "Return the filtered dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_with_conditions(df, condition_list):\n    df = df.copy()\n    \n    for condition in condition_list:\n        col, op, value = condition.split(':')\n        \n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        \n        if op == 'eq':\n            df = df[df[col] == value]\n        elif op == 'neq':\n            df = df[df[col] != value]\n        elif op == 'gt':\n            df = df[df[col] > float(value)]\n        elif op == 'lt':\n            df = df[df[col] < float(value)]\n        elif op == 'ge':\n            df = df[df[col] >= float(value)]\n        elif op == 'le':\n            df = df[df[col] <= float(value)]\n        else:\n            raise ValueError(f\"Unknown operation: {op}\")\n    \n    return df"
    },
    {
        "function_name": "group_and_aggregate",
        "file_name": "grouping_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_cols": "list",
            "agg_dict": "dict"
        },
        "objectives": [
            "Ensure the presence of `group_cols` and their types as str.",
            "Validate the `agg_dict` which contains columns and their aggregation functions.",
            "Group the dataframe by `group_cols` and apply the specified aggregation functions to the respective columns.",
            "Return the aggregated dataframe with a MultiIndex in columns representing the aggregate functions used."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_and_aggregate(df, group_cols, agg_dict):\n    for col in group_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Group column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_string_dtype(df[col]):\n            raise ValueError(f\"Group column '{col}' must be of string type\")\n    \n    for col, funcs in agg_dict.items():\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not all(func in ['sum', 'mean', 'min', 'max', 'count', 'std', 'median'] for func in funcs):\n            raise ValueError(f\"Invalid aggregation function in column '{col}'\")\n\n    grouped_df = df.groupby(group_cols).agg(agg_dict)\n\n    return grouped_df"
    },
    {
        "function_name": "handle_outliers",
        "file_name": "outliers_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str"
        },
        "objectives": [
            "Ensure the presence of the \"target_column\" and validate its data type.",
            "Calculate outliers using the IQR method and Z-score method.",
            "Impute outliers with the median value of the respective column.",
            "Return the dataframe with outliers handled and the total count of outliers before imputation."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy import stats"
        ],
        "function_def": "def handle_outliers(df, target_column):\n    if target_column not in df.columns:\n        raise ValueError(f\"Column '{target_column}' not found in DataFrame\")\n    \n    if not pd.api.types.is_numeric_dtype(df[target_column]):\n        raise ValueError(f\"Column '{target_column}' must be numeric\")\n    \n    target_data = df[target_column]\n    \n    # Calculate IQR\n    Q1 = target_data.quantile(0.25)\n    Q3 = target_data.quantile(0.75)\n    IQR = Q3 - Q1\n\n    # Calculate outliers\n    iqr_outliers = ((target_data < (Q1 - 1.5 * IQR)) | (target_data > (Q3 + 1.5 * IQR)))\n    z_score_outliers = np.abs(stats.zscore(target_data)) > 3\n    \n    total_outliers = np.any([iqr_outliers, z_score_outliers], axis=0).sum()\n    \n    # Impute with median\n    median_value = target_data.median()\n    df.loc[iqr_outliers | z_score_outliers, target_column] = median_value\n    \n    return df, total_outliers"
    },
    {
        "function_name": "consolidate_infrequent_categories",
        "file_name": "category_consolidation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "threshold": "float"
        },
        "objectives": [
            "Ensure the presence of all columns in \"categorical_columns\" and validate they are of categorical type.",
            "For each categorical column, identify infrequent categories that occur less than the threshold proportion.",
            "Replace these infrequent categories with the string 'Other'.",
            "Return the dataframe with infrequent categories consolidated in each specified categorical column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def consolidate_infrequent_categories(df, categorical_columns, threshold):\n    consolidated_df = df.copy()\n    \n    for col in categorical_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n\n        if not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be categorical\")\n\n        cat_counts = df[col].value_counts(normalize=True)\n        infrequent_categories = cat_counts[cat_counts < threshold].index\n        \n        consolidated_df[col] = df[col].apply(lambda x: 'Other' if x in infrequent_categories else x)\n    \n    return consolidated_df"
    },
    {
        "function_name": "k_means_clustering",
        "file_name": "clustering_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "n_clusters": "int"
        },
        "objectives": [
            "Ensure the dataframe has only numeric columns.",
            "Normalize the numeric data using Z-score normalization.",
            "Apply K-Means clustering to partition the data into \"n_clusters\" clusters.",
            "Append a new column to the dataframe indicating the cluster each row belongs to."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def k_means_clustering(df, n_clusters):\n    numeric_columns = df.select_dtypes(include='number').columns\n    \n    if len(numeric_columns) == 0:\n        raise ValueError(\"DataFrame does not contain any numeric columns for clustering\")\n    \n    df = df.copy()\n    \n    scaler = StandardScaler()\n    normalized_data = scaler.fit_transform(df[numeric_columns])\n    \n    kmeans = KMeans(n_clusters=n_clusters)\n    clusters = kmeans.fit_predict(normalized_data)\n    \n    df['cluster'] = clusters\n    \n    return df"
    },
    {
        "function_name": "scale_columns",
        "file_name": "scaling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_scale": "List[str]",
            "scale_type": "str"
        },
        "objectives": [
            "Validate that each column in 'cols_to_scale' exists in the dataframe and is numeric.",
            "Perform standard scaling if 'scale_type' is 'standard' and min-max scaling if 'scale_type' is 'minmax'.",
            "If an inappropriate 'scale_type' is provided, raise a ValueError.",
            "Return the dataframe with new columns for each scaled column appended with '_scaled'."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
        ],
        "function_def": "def scale_columns(df, cols_to_scale, scale_type):\n    if scale_type not in {'standard', 'minmax'}:\n        raise ValueError(\"scale_type must be either 'standard' or 'minmax'\")\n    \n    scaler = StandardScaler() if scale_type == 'standard' else MinMaxScaler()\n    \n    for col in cols_to_scale:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not numeric or not in dataframe\")\n        \n        scaled_data = scaler.fit_transform(df[[col]])\n        df[f'{col}_scaled'] = scaled_data\n    \n    return df"
    },
    {
        "function_name": "compute_moving_averages",
        "file_name": "moving_averages.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_name": "str",
            "window_size": "int"
        },
        "objectives": [
            "Ensure 'col_name' exists in the dataframe and is numeric.",
            "Apply a rolling window of size 'window_size' to compute the rolling mean for the specified column.",
            "Compute the exponentially weighted moving average (EWMA) using a span of 'window_size' for the same column.",
            "Return the dataframe with two new columns: 'rolling_mean' and 'ewma'."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_moving_averages(df, col_name, window_size):\n    if col_name not in df.columns or not pd.api.types.is_numeric_dtype(df[col_name]):\n        raise ValueError(f\"Column {col_name} is not numeric or not in dataframe\")\n    \n    df['rolling_mean'] = df[col_name].rolling(window=window_size).mean()\n    \n    df['ewma'] = df[col_name].ewm(span=window_size).mean()\n    \n    return df"
    },
    {
        "function_name": "calculate_yoy_growth",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Ensure that 'date_col' exists in the dataframe and is a datetime type.",
            "Resample data by month and calculate the mean of 'target_col' for each resampled period.",
            "Calculate the year-over-year growth rate of the target column's monthly means.",
            "Return a dataframe with two new columns: 'monthly_mean' and 'yoy_growth'."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_yoy_growth(df, date_col, target_col):\n    if date_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column {date_col} is not datetime or not in dataframe\")\n    \n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column {target_col} is not numeric or not in dataframe\")\n    \n    df.set_index(date_col, inplace=True)\n    monthly_mean = df[target_col].resample('M').mean()\n    \n    yoy_growth = monthly_mean.pct_change(periods=12)\n    \n    result_df = pd.DataFrame({\n        'monthly_mean': monthly_mean,\n        'yoy_growth': yoy_growth\n    }).reset_index()\n    \n    df.reset_index(drop=True, inplace=True)\n    \n    return result_df"
    },
    {
        "function_name": "compute_weighted_averages",
        "file_name": "weighted_averages.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns_list": "list",
            "weights": "dict"
        },
        "objectives": [
            "Calculate weighted averages for the specified 'columns_list' using the 'weights' dictionary.",
            "For each weighted average, generate deviation columns that express the difference between actual values and weighted averages.",
            "Summarize the deviations by computing the mean absolute deviation for each column.",
            "Return the modified DataFrame including the weighted averages and deviation columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_weighted_averages(df, columns_list, weights):\n    df = df.copy()\n\n    # Step 1: Calculate weighted averages\n    for column in columns_list:\n        if column not in weights:\n            raise ValueError(f\"No weights provided for column: {column}\")\n\n        weighted_avg = df[column] * weights[column]\n        weighted_avg_col = f\"{column}_weighted_avg\"\n        df[weighted_avg_col] = weighted_avg.mean()\n\n    # Step 2: Generate deviation columns\n    for column in columns_list:\n        deviation_col = f\"{column}_deviation\"\n        weighted_avg_col = f\"{column}_weighted_avg\"\n        df[deviation_col] = df[column] - df[weighted_avg_col]\n\n    # Step 3: Summarize deviations\n    dev_summary = {}\n    for column in columns_list:\n        deviation_col = f\"{column}_deviation\"\n        dev_summary[f\"{deviation_col}_mean_abs_dev\"] = df[deviation_col].abs().mean()\n\n    return df, dev_summary"
    },
    {
        "function_name": "resample_and_normalize",
        "file_name": "resampling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "freq": "str"
        },
        "objectives": [
            "Resample the DataFrame based on the specified 'time_col' and 'freq'.",
            "Calculate the total, mean, and standard deviation for numeric columns in each resampled period.",
            "Normalize the resampled DataFrame such that the minimum value of each column is 0 and the maximum is 1.",
            "Return the resampled and normalized DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_normalize(df, time_col, freq):\n    df = df.copy()\n\n    # Step 1: Resample the DataFrame\n    df.set_index(time_col, inplace=True)\n    resampled_df = df.resample(freq).agg(['sum', 'mean', 'std'])\n    resampled_df.columns = ['_'.join(col).strip() for col in resampled_df.columns.values]\n\n    # Step 2: Normalize the resampled DataFrame\n    for col in resampled_df.columns:\n        min_col = resampled_df[col].min()\n        max_col = resampled_df[col].max()\n        resampled_df[col] = (resampled_df[col] - min_col) / (max_col - min_col)\n\n    return resampled_df"
    },
    {
        "function_name": "merge_and_clean",
        "file_name": "merging.py",
        "parameters": {
            "main_df": "pandas.DataFrame",
            "aux_df": "pandas.DataFrame",
            "key_column": "str"
        },
        "objectives": [
            "Perform an outer join on 'main_df' and 'aux_df' based on the 'key_column'.",
            "Detect and handle duplicate rows generated by the join operation by keeping the most recent entry based on a 'timestamp' column.",
            "Fill missing values in the resulting DataFrame using forward fill and backward fill methods.",
            "Return the merged and cleaned DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_and_clean(main_df, aux_df, key_column):\n    main_df = main_df.copy()\n    aux_df = aux_df.copy()\n\n    # Step 1: Perform outer join\n    merged_df = pd.merge(main_df, aux_df, on=key_column, how='outer', suffixes=('', '_aux'))\n\n    # Step 2: Handle duplicates\n    if 'timestamp' not in merged_df.columns:\n        raise ValueError(\"Timestamp column is required to handle duplicates\")\n\n    merged_df.sort_values(by='timestamp', ascending=False, inplace=True)\n    merged_df.drop_duplicates(subset=[key_column], keep='first', inplace=True)\n\n    # Step 3: Fill missing values\n    merged_df.fillna(method='ffill', inplace=True)\n    merged_df.fillna(method='bfill', inplace=True)\n\n    return merged_df"
    },
    {
        "function_name": "fill_missing_values",
        "file_name": "missing_value_handler.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "fill_method": "str",
            "exclude_cols": "list of str"
        },
        "objectives": [
            "Scan through the dataframe 'df' and identify columns with missing values.",
            "Exclude columns specified in 'exclude_cols' from any processing.",
            "Apply the 'fill_method' (either 'mean', 'median', or 'mode') to fill the missing values in the identified columns.",
            "Return a dataframe with missing values filled accordingly."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def fill_missing_values(df, fill_method, exclude_cols):\n    if fill_method not in ['mean', 'median', 'mode']:\n        raise ValueError(\"Invalid fill method. Choose from 'mean', 'median', or 'mode'.\")\n\n    for col in df.columns:\n        if col in exclude_cols:\n            continue\n        if df[col].isnull().any():\n            if fill_method == 'mean':\n                fill_value = df[col].mean()\n            elif fill_method == 'median':\n                fill_value = df[col].median()\n            else:  # mode\n                fill_value = df[col].mode()[0]\n            df[col].fillna(fill_value, inplace=True)\n    \n    return df"
    },
    {
        "function_name": "bin_numeric_values",
        "file_name": "binner.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "bucket_count": "int"
        },
        "objectives": [
            "Identify and validate that the 'target_column' exists in the dataframe 'df' and is numeric.",
            "Create 'bucket_count' bins or buckets and assign each value in the target column to a corresponding bin.",
            "Add a new column to the dataframe indicating the bin assignment for each row.",
            "Return the modified dataframe with the new bin-assignment column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_numeric_values(df, target_column, bucket_count):\n    if target_column not in df.columns:\n        raise ValueError(f\"Column {target_column} is not in the dataframe\")\n    if not pd.api.types.is_numeric_dtype(df[target_column]):\n        raise ValueError(f\"Column {target_column} must be numeric\")\n    \n    bins = np.linspace(df[target_column].min(), df[target_column].max(), bucket_count + 1)\n    bin_labels = [f'bin_{i}' for i in range(1, bucket_count + 1)]\n    \n    df[f'{target_column}_bin'] = pd.cut(df[target_column], bins=bins, labels=bin_labels, include_lowest=True)\n    \n    return df"
    },
    {
        "function_name": "encode_categorical_values",
        "file_name": "categorical_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "method": "str"
        },
        "objectives": [
            "Identify and verify the existence of 'categorical_columns' in the dataframe 'df'.",
            "Apply the specified 'method' ('onehot' or 'label') to encode the categorical variables.",
            "For 'onehot' encoding, generate binary columns for each category.",
            "For 'label' encoding, convert categories into integer labels.",
            "Return the modified dataframe with encoded categorical columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def encode_categorical_values(df, categorical_columns, method):\n    if method not in ['onehot', 'label']:\n        raise ValueError(\"Invalid method. Choose from 'onehot' or 'label'.\")\n\n    df = df.copy()\n    \n    for col in categorical_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not in the dataframe\")\n        \n        if method == 'onehot':\n            onehot = pd.get_dummies(df[col], prefix=col)\n            df = pd.concat([df, onehot], axis=1).drop(col, axis=1)\n        else:  # label\n            encoder = LabelEncoder()\n            df[col] = encoder.fit_transform(df[col])\n    \n    return df"
    },
    {
        "function_name": "frequency_encode_high_cardinality",
        "file_name": "high_cardinality_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "k": "int"
        },
        "objectives": [
            "Identify columns with high cardinality (having unique values greater than 'k').",
            "Encode such columns using frequency encoding, replacing each unique value with the frequency of its occurrence in the column.",
            "Add encoded columns to the dataframe with suffix '_freq' and remove original high cardinality columns.",
            "Return the modified dataframe with frequency-encoded columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def frequency_encode_high_cardinality(df, k):\n    df = df.copy()\n    high_card_cols = [col for col in df.columns if df[col].nunique() > k]\n    \n    for col in high_card_cols:\n        freq_map = df[col].value_counts() / len(df)\n        df[f'{col}_freq'] = df[col].map(freq_map)\n        df = df.drop(col, axis=1)\n    \n    return df"
    },
    {
        "function_name": "handle_missing_and_outliers",
        "file_name": "missing_values_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "fill_method": "str"
        },
        "objectives": [
            "Identify missing values in the column specified by \"target_col\".",
            "Fill missing values using the specified \"fill_method\" ('mean', 'median', 'mode', 'ffill', 'bfill').",
            "Detect and mark any outliers in the filled column using the IQR method.",
            "Return a modified dataframe with missing values handled and a new column indicating outliers."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def handle_missing_and_outliers(df, target_col, fill_method):\n    df = df.copy()\n    \n    if fill_method == 'mean':\n        fill_value = df[target_col].mean()\n    elif fill_method == 'median':\n        fill_value = df[target_col].median()\n    elif fill_method == 'mode':\n        fill_value = df[target_col].mode()[0]\n    else:\n        fill_value = None\n    \n    if fill_value is not None:\n        df[target_col].fillna(fill_value, inplace=True)\n    else:\n        df[target_col].ffill(inplace=True)\n        df[target_col].bfill(inplace=True)\n    \n    Q1 = df[target_col].quantile(0.25)\n    Q3 = df[target_col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    df[f\"{target_col}_outlier\"] = ((df[target_col] < lower_bound) | (df[target_col] > upper_bound)).astype(int)\n    \n    return df"
    },
    {
        "function_name": "extract_and_aggregate_date",
        "file_name": "date_aggregator.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "new_column_prefix": "str"
        },
        "objectives": [
            "Validate `date_column` presence and ensure it's of datetime type.",
            "Extract year, month, and day from the `date_column` and create new columns with specified `new_column_prefix`.",
            "Group by the newly created year and month columns, and calculate the mean of all numeric columns.",
            "Return the dataframe with new columns and aggregated statistics."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def extract_and_aggregate_date(df, date_column, new_column_prefix):\n    if date_column not in df.columns:\n        raise ValueError(f\"Column '{date_column}' not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[date_column]):\n        raise ValueError(f\"Column '{date_column}' must be of datetime type\")\n    \n    df = df.copy()\n    df[f'{new_column_prefix}_year'] = df[date_column].dt.year\n    df[f'{new_column_prefix}_month'] = df[date_column].dt.month\n    df[f'{new_column_prefix}_day'] = df[date_column].dt.day\n    \n    aggregated_df = df.groupby([f'{new_column_prefix}_year', f'{new_column_prefix}_month']).mean().reset_index()\n    \n    return aggregated_df"
    },
    {
        "function_name": "multi_level_groupby",
        "file_name": "group_aggregator.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_columns": "list",
            "agg_spec": "dict"
        },
        "objectives": [
            "Validate that all `group_columns` are present in the DataFrame.",
            "Apply multi-level grouping based on the `group_columns`.",
            "Perform specified aggregate operations, defined in `agg_spec`, on each group.",
            "Return the aggregated result as a DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def multi_level_groupby(df, group_columns, agg_spec):\n    missing_cols = [col for col in group_columns if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Columns {missing_cols} not found in DataFrame\")\n    \n    df = df.copy()\n    grouped_df = df.groupby(group_columns).agg(agg_spec)\n    \n    return grouped_df.reset_index()"
    },
    {
        "function_name": "create_frequency_table",
        "file_name": "category_analyzer.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list"
        },
        "objectives": [
            "Ensure all columns listed in `categorical_columns` are present and categorical.",
            "Calculate the frequency and proportion of each category in the categorical columns.",
            "Create a frequency table containing both frequency and proportion for each category.",
            "Return the frequency table as a DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_frequency_table(df, categorical_columns):\n    missing_cols = [col for col in categorical_columns if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Columns {missing_cols} not found in DataFrame\")\n    non_categorical_cols = [col for col in categorical_columns if not pd.api.types.is_categorical_dtype(df[col])]\n    if non_categorical_cols:\n        raise ValueError(f\"Columns {non_categorical_cols} must be of categorical type\")\n    \n    freq_table = pd.DataFrame()\n    for col in categorical_columns:\n        freq = df[col].value_counts().reset_index()\n        freq.columns = [col, 'frequency']\n        freq['proportion'] = freq['frequency'] / len(df)\n        freq_table = pd.concat([freq_table, freq], axis=0)\n    \n    return freq_table.reset_index(drop=True)"
    },
    {
        "function_name": "enhance_timestamp_features",
        "file_name": "timestamp_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "timestamp_column": "str"
        },
        "objectives": [
            "Convert the specified timestamp column to pandas datetime.",
            "Extract year, month, day, hour, minute, and second as separate columns.",
            "Generate a series of boolean columns indicating weekends, holidays, and leap years.",
            "Return the enhanced dataframe with the new columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from pandas.tseries.holiday import USFederalHolidayCalendar"
        ],
        "function_def": "def enhance_timestamp_features(df, timestamp_column):\n    df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n    \n    df['year'] = df[timestamp_column].dt.year\n    df['month'] = df[timestamp_column].dt.month\n    df['day'] = df[timestamp_column].dt.day\n    df['hour'] = df[timestamp_column].dt.hour\n    df['minute'] = df[timestamp_column].dt.minute\n    df['second'] = df[timestamp_column].dt.second\n    \n    df['is_weekend'] = df[timestamp_column].dt.weekday >= 5\n    \n    cal = USFederalHolidayCalendar()\n    holidays = cal.holidays(start=df[timestamp_column].min(), end=df[timestamp_column].max())\n    df['is_holiday'] = df[timestamp_column].isin(holidays)\n    \n    df['is_leap_year'] = df[timestamp_column].dt.is_leap_year\n\n    return df"
    },
    {
        "function_name": "label_encode_and_z_score",
        "file_name": "encoding_zscore.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_cols": "list",
            "numerical_cols": "list"
        },
        "objectives": [
            "Perform label encoding on specified categorical columns.",
            "Compute the Z-score for the specified numerical columns.",
            "Identify and handle anomalies by capping values beyond 3 standard deviations.",
            "Return the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def label_encode_and_z_score(df, cat_cols, numerical_cols):\n    for col in cat_cols:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col].astype(str))\n    \n    for col in numerical_cols:\n        col_mean = df[col].mean()\n        col_std = df[col].std()\n        df[col] = (df[col] - col_mean) / col_std\n        df[col] = df[col].clip(lower=-3, upper=3)\n    \n    return df"
    },
    {
        "function_name": "group_aggregate_and_merge",
        "file_name": "group_aggregate.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_columns": "list",
            "agg_dict": "dict"
        },
        "objectives": [
            "Group the dataframe by specified columns.",
            "Perform aggregation based on the provided dictionary (agg_dict).",
            "Compute the percentage of each group relative to the overall size.",
            "Merge these aggregated metrics back to the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_aggregate_and_merge(df, group_columns, agg_dict):\n    grouped_df = df.groupby(group_columns).agg(agg_dict).reset_index()\n    \n    group_size = grouped_df.groupby(group_columns).size().reset_index(name='group_size')\n    total_size = df.shape[0]\n    group_size['group_percent'] = group_size['group_size'] / total_size * 100\n    \n    grouped_df = pd.merge(grouped_df, group_size, on=group_columns)\n    df = pd.merge(df, grouped_df, on=group_columns, how='left')\n    \n    return df"
    },
    {
        "function_name": "drop_and_impute_missing_values",
        "file_name": "missing_values.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "missing_threshold": "float"
        },
        "objectives": [
            "Identify columns with missing values exceeding the specified threshold.",
            "Drop columns exceeding the threshold.",
            "Impute remaining missing values with median for numerical and mode for categorical.",
            "Return the cleaned dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def drop_and_impute_missing_values(df, missing_threshold):\n    missing_percent = df.isnull().mean()\n    cols_to_drop = missing_percent[missing_percent > missing_threshold].index\n    df = df.drop(columns=cols_to_drop)\n    \n    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n    cat_cols = df.select_dtypes(include=['object']).columns\n    \n    for col in num_cols:\n        df[col].fillna(df[col].median(), inplace=True)\n        \n    for col in cat_cols:\n        df[col].fillna(df[col].mode()[0], inplace=True)\n    \n    return df"
    },
    {
        "function_name": "compute_top_value_frequencies",
        "file_name": "frequency_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_threshold": "int"
        },
        "objectives": [
            "Identify columns in the dataframe `df` with more than `col_threshold` unique values.",
            "For each identified column, compute the frequency count of the top 5 most common values.",
            "Normalize these frequencies to lie between 0 and 1.",
            "Return a dictionary with column names as keys and the normalized frequency counts of the top 5 values as values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_top_value_frequencies(df, col_threshold):\n    freq_dict = {}\n    \n    # Identify columns with more than `col_threshold` unique values\n    for col in df.columns:\n        if df[col].nunique() > col_threshold:\n            # Compute frequency of top 5 most common values\n            top_values = df[col].value_counts().head(5)\n            \n            # Normalize the frequencies\n            normalized_freq = top_values / top_values.sum()\n            \n            freq_dict[col] = normalized_freq.to_dict()\n\n    return freq_dict"
    },
    {
        "function_name": "groupwise_fill_missing",
        "file_name": "group_fill.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_by_col": "str",
            "fill_strategy": "str"
        },
        "objectives": [
            "Group the dataframe `df` by the `group_by_col`.",
            "For each group, sort the rows based on the index.",
            "Apply the specified `fill_strategy` (one of 'ffill', 'bfill', 'interpolate') to fill missing values within each group.",
            "Return the modified dataframe, ensuring the original order of rows is preserved."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def groupwise_fill_missing(df, group_by_col, fill_strategy):\n    if fill_strategy not in [\"ffill\", \"bfill\", \"interpolate\"]:\n        raise ValueError(\"Invalid fill strategy. Choose from 'ffill', 'bfill', 'interpolate'.\")\n    \n    # Group by specified column\n    grouped = df.groupby(group_by_col)\n    \n    # Sort rows within each group by index and apply fill strategy\n    if fill_strategy == \"ffill\":\n        df = grouped.apply(lambda group: group.sort_index().ffill())\n    elif fill_strategy == \"bfill\":\n        df = grouped.apply(lambda group: group.sort_index().bfill())\n    elif fill_strategy == \"interpolate\":\n        df = grouped.apply(lambda group: group.sort_index().interpolate())\n    \n    return df.sort_index()"
    },
    {
        "function_name": "standardize_numerical_columns",
        "file_name": "standardization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Standardize numerical columns in the dataframe by removing the mean and scaling to unit variance.",
            "Add new columns with \"_std\" suffix for each standardized column.",
            "Create a summary DataFrame containing mean and variance for each original numerical column.",
            "Return the modified dataframe and the summary DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def standardize_numerical_columns(df, target_col):\n    numerical_cols = df.select_dtypes(include='number').columns.drop(target_col)\n    \n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df[numerical_cols])\n    \n    # Add new standardized columns to dataframe\n    for i, col in enumerate(numerical_cols):\n        df[col + '_std'] = scaled_data[:, i]\n    \n    # Create summary DataFrame\n    summary_df = pd.DataFrame({\n        'column': numerical_cols,\n        'mean': scaler.mean_,\n        'variance': scaler.var_\n    })\n    \n    return df, summary_df"
    },
    {
        "function_name": "discretize_columns",
        "file_name": "discretization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "bins": "int"
        },
        "objectives": [
            "Identify all numerical columns in the dataframe `df`.",
            "For each numerical column, discretize the column into `bins` equal-width bins.",
            "Create new columns with \"_binned\" suffix representing the bin number for each original value.",
            "Return the modified dataframe with the newly created binned columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def discretize_columns(df, bins):\n    numerical_cols = df.select_dtypes(include='number').columns\n    \n    # Discretize each numerical column into specified number of bins\n    for col in numerical_cols:\n        binned_col = pd.cut(df[col], bins=bins, labels=False)\n        df[col + '_binned'] = binned_col\n    \n    return df"
    },
    {
        "function_name": "daily_resampling_and_rolling_average",
        "file_name": "time_series_preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str"
        },
        "objectives": [
            "Validate that the `date_col` is a column in the dataframe and is of datetime type.",
            "Resample data to a daily frequency, filling missing dates with forward fill method.",
            "Calculate a rolling 7-day average for each numeric column in the dataframe.",
            "Create a new dataframe with the original columns plus the 7-day rolling averages as additional columns, with '_7d_avg' as suffix."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def daily_resampling_and_rolling_average(df, date_col):\n    if date_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column {date_col} must be a datetime column in the dataframe\")\n\n    df.set_index(date_col, inplace=True)\n    df_resampled = df.resample('D').ffill()\n\n    rolling_avg_df = df_resampled.rolling(window=7).mean().add_suffix('_7d_avg')\n\n    result_df = pd.concat([df_resampled, rolling_avg_df], axis=1)\n\n    return result_df.reset_index()"
    },
    {
        "function_name": "group_aggregate_and_variance",
        "file_name": "aggregation_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "groupby_col": "str",
            "agg_dict": "Dict[str, str]"
        },
        "objectives": [
            "Group the dataframe by the specified `groupby_col`.",
            "Perform aggregation operations as per the given `agg_dict`.",
            "Calculate the variance for each numeric column within each group.",
            "Return a new dataframe with aggregation metrics along with calculated variances."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_aggregate_and_variance(df, groupby_col, agg_dict):\n    if groupby_col not in df.columns:\n        raise ValueError(f\"Column {groupby_col} is not in the dataframe\")\n\n    grouped_df = df.groupby(groupby_col).agg(agg_dict)\n    \n    variance_df = df.groupby(groupby_col).var().add_suffix('_variance')\n    \n    result_df = pd.concat([grouped_df, variance_df], axis=1)\n\n    return result_df"
    },
    {
        "function_name": "linear_interpolation_and_logging",
        "file_name": "missing_value_interpolation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns_to_interpolate": "List[str]"
        },
        "objectives": [
            "Validate that all specified columns exist in the dataframe and are numeric.",
            "Detect and report any missing values in the specified columns.",
            "Interpolate missing values linearly within each specified column.",
            "Return the dataframe with interpolated values and a log of previously missing positions."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def linear_interpolation_and_logging(df, columns_to_interpolate):\n    missing_value_log = {}\n\n    for col in columns_to_interpolate:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not a numeric column in the dataframe\")\n\n        missing_indices = df[df[col].isnull()].index.tolist()\n        if missing_indices:\n            missing_value_log[col] = missing_indices\n        \n        df[col] = df[col].interpolate(method='linear')\n\n    return df, missing_value_log"
    },
    {
        "function_name": "min_max_scaling_and_quartile_binning",
        "file_name": "scaling_and_binning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "List[str]"
        },
        "objectives": [
            "Ensure all specified columns exist in the dataframe.",
            "Perform normalization on the specified columns based on min-max scaling.",
            "Bin the values into quartiles for each column.",
            "Create new columns for each of the original columns representing the quartile bins, with '_quartile_bin' as suffix."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def min_max_scaling_and_quartile_binning(df, columns):\n    for col in columns:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not a numeric column in the dataframe\")\n\n    scaler = MinMaxScaler()\n    df[columns] = scaler.fit_transform(df[columns])\n    \n    for col in columns:\n        df[col + '_quartile_bin'] = pd.qcut(df[col], 4, labels=False)\n    \n    return df"
    },
    {
        "function_name": "transform_columns",
        "file_name": "preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_cols": "list of str",
            "num_cols": "list of str",
            "target_col": "str"
        },
        "objectives": [
            "For each categorical column in cat_cols, calculate the frequency count of each unique value.",
            "Replace the original categorical values in df with these frequency counts.",
            "For each numerical column in num_cols, normalize the data using z-score standardization.",
            "Return the modified dataframe with transformed categorical and numerical columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import zscore"
        ],
        "function_def": "def transform_columns(df, cat_cols, num_cols, target_col):\n    df = df.copy()\n    \n    # Step 1: Calculate frequency count for categorical columns\n    for col in cat_cols:\n        col_freq_map = df[col].value_counts().to_dict()\n        df[col] = df[col].map(col_freq_map)\n    \n    # Step 2: Normalize numerical columns using z-score\n    df[num_cols] = df[num_cols].apply(zscore)\n    \n    return df"
    },
    {
        "function_name": "aggregate_grouped_data",
        "file_name": "group_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "agg_methods": "dict"
        },
        "objectives": [
            "Group the dataframe \"df\" by \"group_col\".",
            "Aggregate the grouped data based on specified methods in \"agg_methods\" for each corresponding column.",
            "Handle cases where the aggregation methods specified are not valid for the data types of the columns.",
            "Return the aggregated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_grouped_data(df, group_col, agg_methods):\n    grouped_df = df.groupby(group_col)\n    \n    # Step 1: Validate aggregation methods\n    valid_agg_methods = ['sum', 'mean', 'max', 'min', 'count', 'std']\n    for col, method in agg_methods.items():\n        if method not in valid_agg_methods:\n            raise ValueError(f\"Unsupported aggregation method '{method}' for column '{col}'. Supported methods: {valid_agg_methods}\")\n    \n    # Step 2: Apply aggregation\n    agg_dict = {col: method for col, method in agg_methods.items() if col in df.columns}\n    aggregated_df = grouped_df.agg(agg_dict)\n    \n    return aggregated_df.reset_index()"
    },
    {
        "function_name": "add_rolling_statistics",
        "file_name": "rolling_stats.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int",
            "target_col": "str"
        },
        "objectives": [
            "Calculate rolling mean for \"target_col\" based on \"window_size\".",
            "Calculate exponentially weighted average for \"target_col\".",
            "Create a new column in the dataframe which includes the rolling mean and a new one for the exponentially weighted average.",
            "Return the modified dataframe with these additional rolling statistics."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_rolling_statistics(df, window_size, target_col):\n    df = df.copy()\n    \n    # Step 1: Calculate rolling mean\n    rolling_mean_col = f'{target_col}_rolling_mean'\n    df[rolling_mean_col] = df[target_col].rolling(window=window_size).mean()\n    \n    # Step 2: Calculate exponentially weighted average\n    ewm_col = f'{target_col}_ewm'\n    df[ewm_col] = df[target_col].ewm(span=window_size, adjust=False).mean()\n    \n    return df"
    },
    {
        "function_name": "detect_discontinuous_blocks",
        "file_name": "time_block_detector.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "group_col": "str"
        },
        "objectives": [
            "Convert the input time column to datetime and sort the dataframe by group and time.",
            "Group the dataframe by the specified column and compute the time difference between consecutive rows within each group.",
            "Identify rows where the time difference exceeds a predefined threshold (e.g., 1 day).",
            "Create a new column indicating whether the row is the start of a new group of continuous data based on the time difference threshold."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_discontinuous_blocks(df, time_col, group_col):\n    df[time_col] = pd.to_datetime(df[time_col])\n    df = df.sort_values(by=[group_col, time_col])\n    \n    df['time_diff'] = df.groupby(group_col)[time_col].diff()\n    threshold = pd.Timedelta(days=1)\n    df['is_new_block'] = df['time_diff'] > threshold\n    \n    return df"
    },
    {
        "function_name": "category_combination_stats",
        "file_name": "category_combination_stats.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categories": "list of str",
            "agg_col": "str"
        },
        "objectives": [
            "Identify unique combinations of values from the specified category columns.",
            "Calculate the total sum, mean, and standard deviation of the aggregation column for those unique category combinations.",
            "Generate a new dataframe with columns representing these statistics.",
            "Return the new dataframe with the calculated statistics for each unique category combination."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def category_combination_stats(df, categories, agg_col):\n    grouped = df.groupby(categories)[agg_col].agg(['sum', 'mean', 'std']).reset_index()\n    return grouped"
    },
    {
        "function_name": "standardize_and_encode",
        "file_name": "preprocessing_utilities.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_cols": "list",
            "category_cols": "list"
        },
        "objectives": [
            "Check if the specified columns exist and are of the correct type (datetime for datetime_cols and categorical/text for category_cols).",
            "Convert the datetime columns to a common timezone (e.g., UTC).",
            "One-hot encode the categorical columns.",
            "Return the modified dataframe with datetime columns standardized and categorical columns one-hot encoded."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def standardize_and_encode(df, datetime_cols, category_cols):\n    # Step 1: Column presence and type checks\n    for col in datetime_cols:\n        if col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not datetime\")\n\n    for col in category_cols:\n        if col not in df.columns or not pd.api.types.is_string_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not categorical/text\")\n    \n    # Step 2: Convert datetime columns to UTC\n    for col in datetime_cols:\n        df[col] = df[col].dt.tz_convert('UTC')\n\n    # Step 3: One-hot encode categorical columns\n    df = pd.get_dummies(df, columns=category_cols)\n\n    return df"
    },
    {
        "function_name": "detect_anomalies",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Ensure the specified target column exists and is numerical.",
            "Compute the rolling mean and rolling standard deviation for the target column over a 5-period window.",
            "Detect anomalies where the current value deviates more than 'threshold' times the rolling standard deviation from the rolling mean.",
            "Return a dataframe with an additional column 'anomaly' indicating anomaly points as True/False."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_anomalies(df, target_col, threshold):\n    # Step 1: Ensure column presence and type checks\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column {target_col} is not present or not numerical\")\n    \n    # Step 2: Compute rolling mean and rolling standard deviation\n    df['rolling_mean'] = df[target_col].rolling(window=5).mean()\n    df['rolling_std'] = df[target_col].rolling(window=5).std()\n    \n    # Step 3: Detect anomalies\n    df['anomaly'] = abs(df[target_col] - df['rolling_mean']) > threshold * df['rolling_std']\n    \n    # Drop temporary columns\n    df.drop(columns=['rolling_mean', 'rolling_std'], inplace=True)\n    \n    return df"
    },
    {
        "function_name": "discretize_column",
        "file_name": "feature_discretization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "input_col": "str",
            "n_bins": "int",
            "strategy": "str"
        },
        "objectives": [
            "Check if the specified input column exists and is numerical.",
            "Discretize the input column into 'n_bins' bins using the specified binning strategy ('uniform', 'quantile', 'kmeans').",
            "Assign labels for the bins such as 'Bin_1', 'Bin_2', etc.",
            "Return the modified dataframe with a new column 'binned_<input_col>' representing the bin labels."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import KBinsDiscretizer"
        ],
        "function_def": "def discretize_column(df, input_col, n_bins, strategy):\n    # Step 1: Ensure column presence and type checks\n    if input_col not in df.columns or not pd.api.types.is_numeric_dtype(df[input_col]):\n        raise ValueError(f\"Column {input_col} is not present or not numerical\")\n    \n    # Step 2: Discretize the column\n    kb = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n    df['binned_' + input_col] = kb.fit_transform(df[[input_col]]).astype(int)\n    \n    # Step 3: Assign labels to bins\n    bin_labels = [f'Bin_{i+1}' for i in range(n_bins)]\n    df['binned_' + input_col] = df['binned_' + input_col].map(lambda x: bin_labels[x])\n    \n    return df"
    },
    {
        "function_name": "robust_scale_and_polynomial_transform",
        "file_name": "advanced_scaling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_cols": "list",
            "target_col": "str"
        },
        "objectives": [
            "Ensure that all specified feature columns and the target column exist and are numerical.",
            "Apply a robust scaling method to the feature columns, which scales the data according to the IQR (Interquartile Range).",
            "Perform polynomial feature transformation on the feature columns up to a degree of 3.",
            "Return the dataframe with scaled and polynomial transformed features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import RobustScaler, PolynomialFeatures"
        ],
        "function_def": "def robust_scale_and_polynomial_transform(df, feature_cols, target_col):\n    # Step 1: Ensure column presence and type checks\n    for col in feature_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not numerical\")\n    \n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column {target_col} is not present or not numerical\")\n    \n    # Step 2: Apply robust scaling to the feature columns\n    scaler = RobustScaler()\n    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n    \n    # Step 3: Polynomial feature transformation\n    poly = PolynomialFeatures(degree=3, include_bias=False)\n    poly_features = poly.fit_transform(df[feature_cols])\n    \n    # Create a new DataFrame with polynomial features\n    poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(feature_cols))\n    poly_df[target_col] = df[target_col]\n    \n    return poly_df"
    },
    {
        "function_name": "resample_and_interpolate",
        "file_name": "resample_interpolate.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "new_freq": "str"
        },
        "objectives": [
            "Convert the 'time_col' to a pandas datetime object.",
            "Resample the dataframe based on 'new_freq' which can be 'D', 'W', 'M', etc.",
            "Interpolate missing data linearly after resampling.",
            "Flag periods where any value was originally missing and return the modified dataframe with the new flag column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_interpolate(df, time_col, new_freq):\n    df[time_col] = pd.to_datetime(df[time_col])\n    df.set_index(time_col, inplace=True)\n    \n    resampled_df = df.resample(new_freq).interpolate(method='linear')\n    original_index = df.index\n    \n    resampled_df['was_missing'] = ~resampled_df.index.isin(original_index)\n    \n    return resampled_df.reset_index()"
    },
    {
        "function_name": "bin_numerical_data",
        "file_name": "bin_numerical.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_cols": "list",
            "criteria_col": "str"
        },
        "objectives": [
            "Filter rows in the dataframe based on whether the 'criteria_col' values are above its median.",
            "Within these filtered rows, create data bins for each column in 'numerical_cols' using quartiles.",
            "Replace the values in these columns with the corresponding quartile labels.",
            "Return the modified dataframe with binned data for the specified numerical columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def bin_numerical_data(df, numerical_cols, criteria_col):\n    median_val = df[criteria_col].median()\n    filtered_df = df[df[criteria_col] > median_val]\n    \n    for col in numerical_cols:\n        bins = pd.qcut(filtered_df[col], 4, labels=False)\n        df.loc[filtered_df.index, f\"{col}_binned\"] = bins\n    \n    return df"
    },
    {
        "function_name": "correlate_numeric_with_category",
        "file_name": "category_numeric_corr.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "numerical_col": "str"
        },
        "objectives": [
            "Encode the 'category_col' using one-hot encoding.",
            "For each resulting dummy variable, calculate the correlation with the 'numerical_col'.",
            "Select the dummy variable with the highest absolute correlation.",
            "Create a new column indicating the correspondence of each row to this dummy variable and return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def correlate_numeric_with_category(df, category_col, numerical_col):\n    dummies = pd.get_dummies(df[category_col], prefix=category_col)\n    correlations = dummies.corrwith(df[numerical_col]).abs()\n    \n    max_corr_col = correlations.idxmax()\n    df[f'correlated_with_{numerical_col}'] = dummies[max_corr_col]\n    \n    return df"
    },
    {
        "function_name": "adjust_target_with_bias",
        "file_name": "bias_adjustment.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "bias_factor": "float"
        },
        "objectives": [
            "Confirm that `target_col` exists in the dataframe and is numeric.",
            "Calculate the mean and standard deviation of `target_col`.",
            "Adjust the `target_col` values by adding a bias term (`bias_factor` * standard deviation).",
            "Create and return a new column `'adjusted_[target_col]'` with the bias-adjusted values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def adjust_target_with_bias(df, target_col, bias_factor):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n\n    # Calculate mean and standard deviation\n    mean_val = df[target_col].mean()\n    std_val = df[target_col].std()\n\n    # Adjust target column with bias\n    df[f'adjusted_{target_col}'] = df[target_col] + (std_val * bias_factor)\n\n    return df"
    },
    {
        "function_name": "impute_missing_values_with_pattern",
        "file_name": "missing_value_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "pattern": "str"
        },
        "objectives": [
            "Identify columns whose names match the given regex pattern.",
            "For each matching column, compute the percentage of missing values.",
            "Impute missing values using the column mean.",
            "Add a boolean column indicating whether the original value was imputed."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "import re"
        ],
        "function_def": "def impute_missing_values_with_pattern(df, pattern):\n    matching_cols = [col for col in df.columns if re.search(pattern, col)]\n    \n    for col in matching_cols:\n        missing_percentage = df[col].isnull().mean() * 100\n        mean_value = df[col].mean()\n        df[f'{col}_was_imputed'] = df[col].isnull()\n        df[col].fillna(mean_value, inplace=True)\n    \n    return df"
    },
    {
        "function_name": "group_and_standardize",
        "file_name": "group_scaling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "numeric_cols": "List[str]"
        },
        "objectives": [
            "Group the dataframe by category_col.",
            "For each group, standardize the numeric columns using z-score scaling.",
            "Calculate the within-group variance for each numeric column.",
            "Append the group variance as new columns in the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def group_and_standardize(df, category_col, numeric_cols):\n    df = df.copy()\n    grouped = df.groupby(category_col)\n    \n    for name, group in grouped:\n        scaler = StandardScaler()\n        scaled_data = scaler.fit_transform(group[numeric_cols])\n        group[numeric_cols] = scaled_data\n\n        for col in numeric_cols:\n            df.loc[group.index, f'{col}_group_variance'] = group[col].var()\n    \n    return df"
    },
    {
        "function_name": "temporal_aggregation",
        "file_name": "temporal_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "agg_funcs": "List[str]"
        },
        "objectives": [
            "Convert 'time_column' into a datetime type and extract temporal features (year, month, day, weekday, hour).",
            "For each temporal feature, perform the specified aggregate functions in 'agg_funcs' on numerical columns.",
            "Summarize the results in a multi-indexed dataframe where the first level is the temporal features and the second level is the aggregation.",
            "Return the multi-indexed summarized dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def temporal_aggregation(df, time_column, agg_funcs):\n    if not pd.api.types.is_datetime64_any_dtype(df[time_column]):\n        df[time_column] = pd.to_datetime(df[time_column])\n    \n    temporal_features = ['year', 'month', 'day', 'weekday', 'hour']\n    temp_df = df.copy()\n    temp_df['year'] = df[time_column].dt.year\n    temp_df['month'] = df[time_column].dt.month\n    temp_df['day'] = df[time_column].dt.day\n    temp_df['weekday'] = df[time_column].dt.weekday\n    temp_df['hour'] = df[time_column].dt.hour\n    \n    agg_df = temp_df.groupby(temporal_features).agg(agg_funcs)\n    \n    return agg_df"
    },
    {
        "function_name": "filter_aggregate_impute",
        "file_name": "time_series_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "min_date": "str",
            "max_date": "str",
            "date_col": "str"
        },
        "objectives": [
            "Filter rows based on the specified date range in the date_col.",
            "Group the DataFrame by 'year' and 'month' from date_col and calculate the average of all numerical columns for each group.",
            "For each group, identify and replace missing numeric values with the group average.",
            "Return the aggregated and imputed DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_aggregate_impute(df, min_date, max_date, date_col):\n    # Step 1: Filter rows based on the date range\n    df_filtered = df[(df[date_col] >= min_date) & (df[date_col] <= max_date)]\n    \n    # Step 2: Group by 'year' and 'month'\n    df_filtered[date_col] = pd.to_datetime(df_filtered[date_col])\n    df_filtered['year'] = df_filtered[date_col].dt.year\n    df_filtered['month'] = df_filtered[date_col].dt.month\n    \n    grouped = df_filtered.groupby(['year', 'month']).mean()\n    \n    # Step 3: Impute missing numeric values with group average\n    grouped_imputed = grouped.apply(lambda x: x.fillna(x.mean()))\n    \n    return grouped_imputed.reset_index()"
    },
    {
        "function_name": "impute_by_group_mean",
        "file_name": "missing_value_imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "threshold": "float",
            "group_key": "str"
        },
        "objectives": [
            "Identify numerical columns where more than the specified threshold % of data points are missing.",
            "For identified columns, compute the mean of remaining values grouped by the group_key.",
            "Impute missing values using these group means.",
            "Return the DataFrame with the missing values imputed for the identified columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_by_group_mean(df, threshold, group_key):\n    df = df.copy()\n    \n    # Step 1: Identify numerical columns with more than threshold % missing values\n    missing_threshold = df.isnull().mean()\n    columns_to_impute = missing_threshold[missing_threshold > threshold].index\n    \n    for col in columns_to_impute:\n        # Step 2: Compute group mean\n        group_mean = df.groupby(group_key)[col].transform('mean')\n        \n        # Step 3: Impute missing values using group mean\n        df[col].fillna(group_mean, inplace=True)\n\n    return df"
    },
    {
        "function_name": "categorical_proportions",
        "file_name": "category_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_cols": "List[str]",
            "group_col": "str"
        },
        "objectives": [
            "Ensure all columns in 'category_cols' are present in the DataFrame and are of categorical data type.",
            "Group the DataFrame by 'group_col', and within each group, calculate the frequency of each category level in 'category_cols'.",
            "Normalize these frequencies by the total number of entries in each group to get the proportions.",
            "Return a DataFrame with the normalized category proportions for each group, renaming columns to indicate the original category and level."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def categorical_proportions(df, category_cols, group_col):\n    for col in category_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be categorical\")\n\n    group_counts = df.groupby(group_col).size()\n    \n    prop_df = df.groupby(group_col)[category_cols].apply(lambda x: x.apply(lambda y: y.value_counts(normalize=True))).unstack().fillna(0)\n    \n    grouped_indices = prop_df.index.get_level_values(0).unique()\n    final_df = prop_df.loc[grouped_indices]\n    final_df.columns = ['_'.join(map(str, col)) for col in final_df.columns]\n    \n    return final_df.reset_index()"
    },
    {
        "function_name": "split_and_explode_column",
        "file_name": "text_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_to_split": "str",
            "delimiter": "str"
        },
        "objectives": [
            "Ensure the column 'col_to_split' exists in the DataFrame and is of string data type.",
            "Split the values in 'col_to_split' into multiple columns based on the provided 'delimiter'.",
            "Assign appropriate names to the newly created columns based on their position after the split.",
            "Concatenate the new columns with the original DataFrame and drop the original column used for splitting."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_and_explode_column(df, col_to_split, delimiter):\n    if col_to_split not in df.columns:\n        raise ValueError(f\"Column '{col_to_split}' not found in DataFrame\")\n    if not pd.api.types.is_string_dtype(df[col_to_split]):\n        raise ValueError(f\"Column '{col_to_split}' must be of string type\")\n    \n    split_df = df[col_to_split].str.split(delimiter, expand=True)\n    \n    new_column_names = [f\"{col_to_split}_part{i+1}\" for i in range(split_df.shape[1])]\n    split_df.columns = new_column_names\n    \n    df_result = pd.concat([df.drop(columns=[col_to_split]), split_df], axis=1)\n    \n    return df_result"
    },
    {
        "function_name": "rolling_window_averages",
        "file_name": "time_series_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "window": "str"
        },
        "objectives": [
            "Ensure 'time_col' is present in the DataFrame and is of datetime data type.",
            "Set the 'time_col' as the index of the DataFrame.",
            "Compute rolling window averages for all numeric columns using the specified window size.",
            "Return a DataFrame containing the rolling averages."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_window_averages(df, time_col, window):\n    if time_col not in df.columns:\n        raise ValueError(f\"Column '{time_col}' not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n        raise ValueError(f\"Column '{time_col}' must be of datetime type\")\n    \n    df = df.set_index(time_col)\n    \n    rolling_df = df.rolling(window).mean()\n    \n    return rolling_df.reset_index()"
    },
    {
        "function_name": "normalize_and_threshold_binary",
        "file_name": "data_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Ensure 'target_col' is present in the DataFrame and is of numeric type.",
            "Normalize 'target_col' by subtracting the minimum value and dividing by the range.",
            "Create a binary column indicating whether the normalized value exceeds the threshold.",
            "Return the DataFrame with the new binary column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def normalize_and_threshold_binary(df, target_col, threshold):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n    \n    df = df.copy()\n    \n    min_val = df[target_col].min()\n    range_val = df[target_col].max() - df[target_col].min()\n    \n    df[target_col + '_normalized'] = (df[target_col] - min_val) / range_val\n    \n    df[target_col + '_binary'] = df[target_col + '_normalized'] > threshold\n    \n    return df"
    },
    {
        "function_name": "compute_pairwise_correlations",
        "file_name": "correlation_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_pairs": "List[Tuple[str, str]]"
        },
        "objectives": [
            "Ensure all columns in 'col_pairs' exist in the DataFrame and are of numeric type.",
            "For each pair, compute the Pearson correlation coefficient.",
            "Create a correlation matrix to store the computed correlation coefficients.",
            "Return the correlation matrix as a DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_pairwise_correlations(df, col_pairs):\n    for col1, col2 in col_pairs:\n        if col1 not in df.columns or col2 not in df.columns:\n            raise ValueError(f\"Columns '{col1}' or '{col2}' not found in DataFrame\")\n        if not (pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2])):\n            raise ValueError(f\"Columns '{col1}' and '{col2}' must be numeric\")\n    \n    correlation_data = {\n        f\"{col1}_{col2}_correlation\": df[[col1, col2]].corr().iloc[0, 1]\n        for col1, col2 in col_pairs\n    }\n\n    correlation_df = pd.DataFrame([correlation_data])\n    \n    return correlation_df"
    },
    {
        "function_name": "add_quantile_thresholds",
        "file_name": "quantile_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numeric_cols": "list of str",
            "quantiles": "list of float"
        },
        "objectives": [
            "Identify and ensure that all 'numeric_cols' exist in the dataframe and are numeric.",
            "For each column in 'numeric_cols', calculate the specified quantiles.",
            "Create new DataFrame columns with quantile thresholds for each specified quantile.",
            "Merge the quantile columns back into the original DataFrame and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_quantile_thresholds(df, numeric_cols, quantiles):\n    df = df.copy()\n    \n    for col in numeric_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n        \n        for quantile in quantiles:\n            quantile_value = df[col].quantile(quantile)\n            df[f'{col}_q{int(quantile*100)}'] = df[col].apply(lambda x: x > quantile_value)\n            \n    return df"
    },
    {
        "function_name": "split_and_balance_dataset",
        "file_name": "data_splitting.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "ratio": "float"
        },
        "objectives": [
            "Verify the existence of 'target_col' and ensure it is numeric.",
            "Split the DataFrame based on a specified ratio into a training and testing set.",
            "Within both sets, balance the classes of 'target_col' to be as close to the original distribution as possible.",
            "Return both the training and testing DataFrames."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split"
        ],
        "function_def": "def split_and_balance_dataset(df, target_col, ratio):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n    \n    train_df, test_df = train_test_split(df, test_size=ratio, stratify=df[target_col])\n    \n    return train_df, test_df"
    },
    {
        "function_name": "detect_and_count_keyword",
        "file_name": "text_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "keyword": "str"
        },
        "objectives": [
            "Ensure the column 'text_col' exists in the DataFrame and is of string type.",
            "Create a binary column indicating the presence of a specified 'keyword' in 'text_col'.",
            "Count the number of keyword occurrences in the DataFrame.",
            "Return the modified DataFrame along with the keyword count."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_and_count_keyword(df, text_col, keyword):\n    if text_col not in df.columns:\n        raise ValueError(f\"Column '{text_col}' not found in DataFrame\")\n    if not pd.api.types.is_string_dtype(df[text_col]):\n        raise ValueError(f\"Column '{text_col}' must be of string type\")\n    \n    df = df.copy()\n    df[f'{keyword}_exists'] = df[text_col].str.contains(keyword)\n    keyword_count = df[f'{keyword}_exists'].sum()\n    \n    return df, keyword_count"
    },
    {
        "function_name": "replace_high_percentage_with_sum",
        "file_name": "percentage_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "percentage_col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Identify rows where the specified 'percentage_col' exceeds the given threshold.",
            "For those rows, calculate the sum of all other numeric columns.",
            "Replace the values in the 'percentage_col' of these rows with the sum.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def replace_high_percentage_with_sum(df, percentage_col, threshold):\n    df = df.copy()\n\n    # Step 1: Identify rows where the percentage exceeds the threshold\n    high_percentage_rows = df[percentage_col] > threshold\n\n    # Step 2: Calculate the sum of all other numeric columns for these rows\n    other_numeric_cols = df.select_dtypes(include=[float, int]).columns.drop(percentage_col)\n    sums = df.loc[high_percentage_rows, other_numeric_cols].sum(axis=1)\n\n    # Step 3: Replace the values in 'percentage_col' with the sums\n    df.loc[high_percentage_rows, percentage_col] = sums\n\n    return df"
    },
    {
        "function_name": "create_lag_feature",
        "file_name": "lag_feature.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "value_col": "str",
            "lag": "int"
        },
        "objectives": [
            "For each unique identifier in 'id_col', create a lag feature for 'value_col' based on the specified lag.",
            "Interpolate any missing values resulting from the lag creation using cubic interpolation.",
            "Compute the first derivative of the lagged values.",
            "Return the modified DataFrame with the new lag feature and its first derivative."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_lag_feature(df, id_col, value_col, lag):\n    df = df.copy()\n\n    # Step 1: Create a lag feature for each unique identifier\n    df['lagged'] = df.groupby(id_col)[value_col].shift(lag)\n\n    # Step 2: Interpolate missing values using cubic interpolation\n    df['lagged'].interpolate(method='cubic', inplace=True)\n\n    # Step 3: Compute the first derivative of the lagged values\n    df['lagged_derivative'] = df['lagged'].diff()\n\n    return df"
    },
    {
        "function_name": "sample_and_interact",
        "file_name": "sampling_interaction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "sample_size": "int"
        },
        "objectives": [
            "Randomly sample the DataFrame without replacement to create a new DataFrame of the given 'sample_size'.",
            "For the sampled DataFrame, create pairwise feature interactions (multiplicative).",
            "Standardize these interaction terms (mean=0, std=1).",
            "Return the sampled and transformed DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def sample_and_interact(df, sample_size):\n    df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n\n    # Step 1: Create pairwise feature interactions\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                interaction_col = col1 + '_x_' + col2\n                df[interaction_col] = df[col1] * df[col2]\n\n    # Step 2: Standardize interaction terms\n    interaction_terms = [col for col in df.columns if '_x_' in col]\n    for col in interaction_terms:\n        df[col] = (df[col] - df[col].mean()) / df[col].std()\n\n    return df"
    },
    {
        "function_name": "compute_tf_idf",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "word_list": "list"
        },
        "objectives": [
            "Tokenize the text in the specified 'text_col'.",
            "Compute the term frequency for words in 'word_list' for each document.",
            "Calculate TF-IDF for these words using the entire DataFrame.",
            "Return the DataFrame augmented with the TF-IDF values for each document."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def compute_tf_idf(df, text_col, word_list):\n    df = df.copy()\n\n    # Step 1: Tokenize the text\n    vectorizer = TfidfVectorizer(vocabulary=word_list)\n    tf_idf_matrix = vectorizer.fit_transform(df[text_col])\n\n    # Step 2: Extract TF-IDF values\n    tf_idf_df = pd.DataFrame(tf_idf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    # Step 3: Append TF-IDF values to the original DataFrame\n    df = pd.concat([df, tf_idf_df], axis=1)\n\n    return df"
    },
    {
        "function_name": "identify_low_entropy_columns",
        "file_name": "entropy_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list",
            "target_col": "str"
        },
        "objectives": [
            "Ensure that all specified categorical columns and target column are present in the dataframe and are categorical.",
            "Calculate the entropy for each categorical column with respect to the target column.",
            "Identify the columns with entropy value below a defined threshold, suggesting strong predictive power.",
            "Return a list of these columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import entropy",
            "import numpy as np"
        ],
        "function_def": "def identify_low_entropy_columns(df, categorical_cols, target_col, threshold=0.5):\n    # Step 1: Ensure columns are present and categorical\n    for col in categorical_cols:\n        if col not in df.columns or not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not categorical\")\n\n    if target_col not in df.columns or not pd.api.types.is_categorical_dtype(df[target_col]):\n        raise ValueError(f\"Target column {target_col} is not present or not categorical\")\n\n    low_entropy_columns = []\n    \n    # Step 2: Calculate entropy for each categorical column with respect to the target column\n    for col in categorical_cols:\n        freq_df = pd.crosstab(df[col], df[target_col])\n        cond_entropy = entropy(freq_df, axis=1, base=2).mean()\n        \n        # Step 3: Identify columns with entropy value below threshold\n        if cond_entropy < threshold:\n            low_entropy_columns.append(col)\n    \n    return low_entropy_columns"
    },
    {
        "function_name": "extract_and_moving_average",
        "file_name": "datetime_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str"
        },
        "objectives": [
            "Validate that the specified datetime column exists and is of datetime type.",
            "Extract the year, month, day, and hour as separate columns from the datetime column.",
            "Generate a moving average for each newly created column with a defined window size.",
            "Return the modified dataframe with the extracted columns and their moving averages."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def extract_and_moving_average(df, datetime_col, window_size=3):\n    # Step 1: Validate datetime column\n    if datetime_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n        raise ValueError(f\"Column {datetime_col} is not present or not of datetime type\")\n    \n    # Step 2: Extract year, month, day, hour\n    df['year'] = df[datetime_col].dt.year\n    df['month'] = df[datetime_col].dt.month\n    df['day'] = df[datetime_col].dt.day\n    df['hour'] = df[datetime_col].dt.hour\n    \n    # Step 3: Generate moving averages\n    for col in ['year', 'month', 'day', 'hour']:\n        df[f'ma_{col}'] = df[col].rolling(window=window_size).mean()\n    \n    return df"
    },
    {
        "function_name": "normalize_high_entropy_columns",
        "file_name": "entropy_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols": "list"
        },
        "objectives": [
            "Ensure that all specified columns are present in the dataframe and are numerical.",
            "Perform Shannon entropy calculation for each column in the list.",
            "Normalize the columns with the highest entropy using Z-score normalization.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import entropy, zscore"
        ],
        "function_def": "def normalize_high_entropy_columns(df, cols):\n    # Step 1: Ensure columns are present and numerical\n    for col in cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not numerical\")\n    \n    highest_entropy_col = None\n    highest_entropy_value = -1\n    \n    # Step 2: Calculate entropy for each column and identify the column with highest entropy\n    for col in cols:\n        col_entropy = entropy(df[col].value_counts(), base=2)\n        if col_entropy > highest_entropy_value:\n            highest_entropy_value = col_entropy\n            highest_entropy_col = col\n    \n    # Step 3: Normalize using Z-score\n    if highest_entropy_col:\n        df[highest_entropy_col] = zscore(df[highest_entropy_col])\n\n    return df"
    },
    {
        "function_name": "detect_high_correlations",
        "file_name": "correlation_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_pairs": "list of tuple",
            "method": "str"
        },
        "objectives": [
            "Validate that tuples in column_pairs exist as column names in the dataframe.",
            "Calculate the correlation between each pair of columns for numeric types using Pearson or Spearman method.",
            "Flag pairs where the correlation exceeds the threshold of 0.8 in magnitude.",
            "Return a dictionary with column pairs as keys and their correlation values as values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_high_correlations(df, column_pairs, method):\n    if method not in ['pearson', 'spearman']:\n        raise ValueError(\"Invalid method. Choose from 'pearson' or 'spearman'.\")\n    \n    correlation_dict = {}\n    \n    for col1, col2 in column_pairs:\n        if col1 not in df.columns or col2 not in df.columns:\n            raise ValueError(f\"One of the columns in pair ({col1}, {col2}) is not in the dataframe\")\n        \n        if df[col1].dtype.kind not in 'biufc' or df[col2].dtype.kind not in 'biufc':\n            raise ValueError(f\"Columns {col1} and {col2} must be numeric\")\n        \n        corr = df[[col1, col2]].corr(method=method).iloc[0, 1]\n        if abs(corr) > 0.8:\n            correlation_dict[(col1, col2)] = corr\n            \n    return correlation_dict"
    },
    {
        "function_name": "tfidf_transform_text",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_columns": "list of str",
            "max_features": "int"
        },
        "objectives": [
            "Validate the presence of text_columns in the dataframe.",
            "Tokenize the text in each column and remove stop-words.",
            "Apply TF-IDF (Term Frequency-Inverse Document Frequency) vectorization with a predefined 'max_features'.",
            "Return a modified dataframe with original text columns replaced by their TF-IDF features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def tfidf_transform_text(df, text_columns, max_features):\n    if not isinstance(text_columns, list) or not all(isinstance(col, str) for col in text_columns):\n        raise ValueError(\"text_columns must be a list of strings\")\n    \n    vectorizers = {col: TfidfVectorizer(max_features=max_features, stop_words='english') for col in text_columns}\n    \n    for col in text_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not in the dataframe\")\n        \n        tfidf_matrix = vectorizers[col].fit_transform(df[col].fillna(''))\n        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizers[col].get_feature_names_out(), index=df.index)\n        df = df.drop(columns=[col])\n        df = pd.concat([df, tfidf_df], axis=1)\n    \n    return df"
    },
    {
        "function_name": "impute_from_reference",
        "file_name": "data_imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "reference_df": "pandas.DataFrame",
            "join_columns": "list of str",
            "columns_to_impute": "list of str"
        },
        "objectives": [
            "Validate join_columns and columns_to_impute exist in both dataframes.",
            "Perform a left join on the join_columns between df and reference_df.",
            "Use the reference dataframe to replace NaN values in columns_to_impute of df.",
            "Return the modified dataframe with imputed values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_from_reference(df, reference_df, join_columns, columns_to_impute):\n    if not all(col in df.columns and col in reference_df.columns for col in join_columns):\n        raise ValueError(\"One or more join columns are not present in both dataframes\")\n    \n    if not all(col in df.columns and col in reference_df.columns for col in columns_to_impute):\n        raise ValueError(\"One or more columns to impute are not present in both dataframes\")\n    \n    # Perform left join\n    merged_df = pd.merge(df, reference_df[join_columns + columns_to_impute], on=join_columns, how='left', suffixes=('', '_ref'))\n    \n    # Replace NaNs with reference values\n    for col in columns_to_impute:\n        merged_df[col] = merged_df[col].combine_first(merged_df.pop(f'{col}_ref'))\n    \n    return merged_df"
    },
    {
        "function_name": "tfidf_feature_extraction",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_cols": "list",
            "max_features": "int"
        },
        "objectives": [
            "Ensure that all specified text columns exist in the DataFrame and are of string data type.",
            "Tokenize the text in each specified column and create a term frequency-inverse document frequency (TF-IDF) matrix.",
            "Select the top max_features based on TF-IDF scores for each text column and reduce the matrix accordingly.",
            "Return a DataFrame containing the TF-IDF features for each text column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def tfidf_feature_extraction(df, text_cols, max_features=1000):\n    # Step 1: Ensure text columns exist and are of string type\n    for col in text_cols:\n        if col not in df.columns or not df[col].dtype == 'object':\n            raise ValueError(f\"Column {col} is not present or not of string type\")\n    \n    # Step 2: Tokenize and create TF-IDF matrix\n    tfidf_matrices = {}\n    for col in text_cols:\n        vectorizer = TfidfVectorizer(max_features=max_features)\n        tfidf_matrix = vectorizer.fit_transform(df[col].astype(str))\n        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n        tfidf_df.columns = [f\"{col}_{feature}\" for feature in tfidf_df.columns]\n        tfidf_matrices[col] = tfidf_df\n    \n    # Step 3: Concatenate all TF-IDF matrices\n    result_df = pd.concat([df] + [tfidf_matrices[col] for col in text_cols], axis=1)\n    result_df.drop(columns=text_cols, inplace=True)\n    \n    return result_df"
    },
    {
        "function_name": "impute_missing_values",
        "file_name": "imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "impute_cols": "list",
            "method": "str"
        },
        "objectives": [
            "Ensure the specified impute_cols exist in the DataFrame and contain at least some missing values.",
            "Based on the specified method, impute the missing values for these columns. Choose from 'mean', 'median', or 'mode' for numeric columns and 'most_frequent' for categorical columns.",
            "After imputation, ensure no missing values remain in the specified columns.",
            "Return the DataFrame with imputed values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.impute import SimpleImputer"
        ],
        "function_def": "def impute_missing_values(df, impute_cols, method):\n    # Step 1: Ensure specified columns exist and contain missing values\n    for col in impute_cols:\n        if col not in df.columns or not df[col].isnull().any():\n            raise ValueError(f\"Column {col} is not present or does not contain missing values\")\n            \n    # Step 2: Impute missing values based on method\n    for col in impute_cols:\n        if pd.api.types.is_numeric_dtype(df[col]):\n            if method in ['mean', 'median', 'mode']:\n                imputer = SimpleImputer(strategy=method if method != 'mode' else 'most_frequent')\n            else:\n                raise ValueError(\"Invalid method for numeric columns. Choose from 'mean', 'median', 'mode'\")\n        else:\n            if method == 'most_frequent':\n                imputer = SimpleImputer(strategy='most_frequent')\n            else:\n                raise ValueError(\"Invalid method for categorical columns. Use 'most_frequent'\")\n        \n        df[col] = imputer.fit_transform(df[[col]])\n    \n    # Step 3: Ensure no missing values remain in these columns\n    if df[impute_cols].isnull().any().any():\n        raise ValueError(f\"Imputation failed, missing values remain in columns {impute_cols}\")\n    \n    return df"
    },
    {
        "function_name": "ewma_target",
        "file_name": "ewma_computation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "date_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Ensure the presence and correct datatypes (date for `date_col` and numeric for `target_col`) of the columns.",
            "Group the dataframe by `id_col` and sort each group by `date_col`.",
            "Calculate the exponentially weighted moving average (EWMA) of `target_col` for each group.",
            "Add a new column `EWMA_Target` to the original dataframe with these calculated EWMA values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def ewma_target(df, id_col, date_col, target_col):\n    if id_col not in df.columns or date_col not in df.columns or target_col not in df.columns:\n        raise ValueError(\"One or more specified columns not found in the DataFrame\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column '{date_col}' must be of datetime type\")\n    \n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n    \n    df = df.sort_values(by=[id_col, date_col])\n    \n    ewm_grouped = df.groupby(id_col)[target_col].transform(lambda x: x.ewm(span=10, adjust=False).mean())\n    df['EWMA_Target'] = ewm_grouped\n    \n    return df"
    },
    {
        "function_name": "generate_polynomial_and_dummy_features",
        "file_name": "feature_expansion.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_cols": "List[str]",
            "target_col": "str"
        },
        "objectives": [
            "Create dummy variables for each categorical column specified.",
            "Generate polynomial features for the target column up to a specified degree.",
            "Concatenate the original dataframe with the dummy variables and polynomial features.",
            "Return the concatenated dataframe with the target column normalized."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
        ],
        "function_def": "def generate_polynomial_and_dummy_features(df, cat_cols, target_col):\n    df = df.copy()\n    \n    dummy_df = pd.get_dummies(df[cat_cols], drop_first=True)\n    df = pd.concat([df.drop(columns=cat_cols), dummy_df], axis=1)\n    \n    poly = PolynomialFeatures(degree=2)\n    poly_features = poly.fit_transform(df[[target_col]])\n    poly_columns = [f\"{target_col}_poly_{i}\" for i in range(poly_features.shape[1])]\n    \n    poly_df = pd.DataFrame(poly_features, columns=poly_columns)\n    \n    df = pd.concat([df, poly_df], axis=1).drop(columns=[target_col])\n    \n    scaler = StandardScaler()\n    df[poly_columns] = scaler.fit_transform(df[poly_columns])\n    \n    return df"
    },
    {
        "function_name": "apply_pca",
        "file_name": "dimensionality_reduction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_cols": "List[str]",
            "n_components": "int"
        },
        "objectives": [
            "Standardize the specified feature columns.",
            "Apply Principal Component Analysis (PCA) to reduce the dimensionality to \"n_components\".",
            "Project the standardized data onto the principal components.",
            "Return a new dataframe with the principal components added as new columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.decomposition import PCA",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def apply_pca(df, feature_cols, n_components):\n    df = df.copy()\n    \n    scaler = StandardScaler()\n    standardized_features = scaler.fit_transform(df[feature_cols])\n    \n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(standardized_features)\n    \n    pca_columns = [f\"PC{i+1}\" for i in range(n_components)]\n    pca_df = pd.DataFrame(principal_components, columns=pca_columns)\n    \n    df = pd.concat([df, pca_df], axis=1)\n    \n    return df"
    },
    {
        "function_name": "filter_encode_categories",
        "file_name": "categorical_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_columns": "list",
            "min_support": "float"
        },
        "objectives": [
            "Ensure categorical columns are present and of object type.",
            "Filter out rare categories that do not meet the minimum support threshold.",
            "Encode remaining categories using one-hot encoding.",
            "Return the modified DataFrame with one-hot encoded categorical variables."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_encode_categories(df, cat_columns, min_support):\n    # Step 1: Ensure columns are present and categorical.\n    for col in cat_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' is not found in the DataFrame\")\n        if not pd.api.types.is_object_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be of object type\")\n    \n    for col in cat_columns:\n        # Step 2: Filter Rare Categories\n        value_counts = df[col].value_counts(normalize=True)\n        valid_categories = value_counts[value_counts >= min_support].index\n        df[col] = df[col].apply(lambda x: x if x in valid_categories else None)\n        \n        # Step 3: One-Hot Encoding\n        one_hot_encoded = pd.get_dummies(df[col], prefix=col)\n        df = df.drop(col, axis=1)\n        df = pd.concat([df, one_hot_encoded], axis=1)\n        \n    return df"
    },
    {
        "function_name": "bin_column_and_label",
        "file_name": "binning_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col": "str",
            "bins": "int",
            "prefix": "str"
        },
        "objectives": [
            "Create equal-width bins for the values in the specified column.",
            "Label each bin with integers starting from 0 to bins-1.",
            "Add an additional column to the dataframe with the binned labels and prefix the column name with the provided prefix.",
            "Handle any missing values in the original column by assigning them to a separate bin labeled as \"NaN_<prefix>\"."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_column_and_label(df, col, bins, prefix):\n    # Step 1: Create equal-width bins\n    bin_edges = pd.cut(df[col], bins=bins, labels=False)\n    \n    # Step 2: Handle missing values\n    bin_edges = bin_edges.astype('float')\n    bin_edges[df[col].isna()] = np.nan\n    \n    # Step 3: Add the binned labels with prefix\n    bin_col_name = f'{prefix}_{col}_bins'\n    df[bin_col_name] = bin_edges\n    df[bin_col_name] = df[bin_col_name].apply(lambda x: f'NaN_{prefix}' if pd.isna(x) else int(x))\n    \n    return df"
    },
    {
        "function_name": "monthly_group_and_fillna",
        "file_name": "grouping_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "agg_col": "str"
        },
        "objectives": [
            "Confirm that the date column is in datetime format and aggregate column is numeric.",
            "Group the dataframe by month using the date column, calculating the mean of the aggregated column for each group.",
            "Detect if there's any missing month in the resulting grouped dataframe and fill them with NaN for the aggregated column.",
            "Return the transformed dataframe containing monthly means, including the NaN-filled months."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def monthly_group_and_fillna(df, date_col, agg_col):\n    # Step 1: Confirm that the date column is in datetime format\n    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        df[date_col] = pd.to_datetime(df[date_col])\n    \n    # Step 2: Ensure the aggregate column is numeric\n    if not pd.api.types.is_numeric_dtype(df[agg_col]):\n        raise ValueError(f\"Column '{agg_col}' must be numeric\")\n    \n    # Step 3: Group by month and calculate the mean\n    df = df.set_index(date_col)\n    monthly_means = df[agg_col].resample('M').mean()\n    \n    # Step 4: Detect and fill missing months with NaN\n    monthly_means = monthly_means.asfreq('M')\n    \n    result_df = monthly_means.reset_index().rename(columns={agg_col: 'monthly_mean'})\n    \n    return result_df"
    },
    {
        "function_name": "mean_by_top_categories",
        "file_name": "category_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "num_col": "str",
            "top_n": "int"
        },
        "objectives": [
            "Confirm that the categorical column exists and the numeric column is numeric.",
            "Identify and rank the top N most frequent categories in the categorical column.",
            "Calculate the mean of the numeric column for each of the top N categories.",
            "Return a dataframe that contains each of the top N categories along with their respective means of the numeric column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def mean_by_top_categories(df, cat_col, num_col, top_n):\n    # Step 1: Confirm that the categorical column exists\n    if cat_col not in df.columns:\n        raise ValueError(f\"Column '{cat_col}' not found in DataFrame\")\n    \n    # Step 2: Ensure the numeric column is numeric\n    if not pd.api.types.is_numeric_dtype(df[num_col]):\n        raise ValueError(f\"Column '{num_col}' must be numeric\")\n    \n    # Step 3: Identify top N most frequent categories\n    top_categories = df[cat_col].value_counts().nlargest(top_n).index\n    \n    # Step 4: Calculate the mean for each of the top N categories\n    means = df[df[cat_col].isin(top_categories)].groupby(cat_col)[num_col].mean().reset_index()\n    \n    result_df = means.rename(columns={num_col: f'{num_col}_mean'})\n    \n    return result_df"
    },
    {
        "function_name": "segment_time_series_and_summarize",
        "file_name": "time_segment_summary.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "num_segments": "int"
        },
        "objectives": [
            "Ensure the `date_col` exists, is in datetime format, and the DataFrame is sorted by this column.",
            "Slice the DataFrame into `num_segments` segments based on the date range, ensuring each segment covers an equal portion of the date range.",
            "Calculate summary statistics (mean, median, STD, min, max) for each segment for all numeric columns.",
            "Return a DataFrame with these statistics and a segment identifier."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def segment_time_series_and_summarize(df, date_col, num_segments):\n    if date_col not in df.columns:\n        raise ValueError(f\"Column '{date_col}' not found in DataFrame\")\n    \n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(by=date_col)\n    \n    # Split the DataFrame into segments\n    total_range = df[date_col].max() - df[date_col].min()\n    segment_duration = total_range / num_segments\n    segments = []\n    \n    for i in range(num_segments):\n        segment_start = df[date_col].min() + i * segment_duration\n        segment_end = segment_start + segment_duration\n        segment = df[(df[date_col] >= segment_start) & (df[date_col] < segment_end)]\n        summary = segment.describe().T[['mean', '50%', 'std', 'min', 'max']]\n        summary['segment'] = i\n        segments.append(summary)\n    \n    summary_df = pd.concat(segments)\n    summary_df = summary_df.reset_index().rename(columns={'50%': 'median'})\n    return summary_df"
    },
    {
        "function_name": "bin_and_stat_summary",
        "file_name": "binning_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "n_bins": "int",
            "method": "str"
        },
        "objectives": [
            "Validate the existence of `target_col` and ensure it is numeric.",
            "Bin the data in `target_col` into `n_bins` using the specified `method` (either 'equal_width' or 'equal_frequency').",
            "For each bin, calculate and return the mean and standard deviation of all numeric columns.",
            "Add bin labels back to the original DataFrame for further analysis."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_and_stat_summary(df, target_col, n_bins, method='equal_width'):\n    if target_col not in df.columns or not np.issubdtype(df[target_col].dtype, np.number):\n        raise ValueError(f\"Column '{target_col}' not found or not numeric in DataFrame\")\n    \n    if method == 'equal_width':\n        df['bin'] = pd.cut(df[target_col], bins=n_bins, labels=False)\n    elif method == 'equal_frequency':\n        df['bin'] = pd.qcut(df[target_col], q=n_bins, labels=False)\n    else:\n        raise ValueError(\"Invalid binning method. Choose from 'equal_width' or 'equal_frequency'.\")\n    \n    bin_stats = df.groupby('bin').agg([\"mean\", \"std\"])\n    bin_stats.columns = ['_'.join(col) for col in bin_stats.columns]\n    return bin_stats.reset_index()"
    },
    {
        "function_name": "rolling_avg_multi_patterns",
        "file_name": "rolling_averages.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_patterns": "list"
        },
        "objectives": [
            "Iterate through the list of column name patterns and search for matches in the DataFrame columns.",
            "For each matched column, calculate rolling averages with different window sizes (e.g., [3, 5, 10]).",
            "Create new columns for each rolling average and rename them accordingly.",
            "Return the modified DataFrame with rolling average columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_avg_multi_patterns(df, col_patterns):\n    matching_cols = [col for col in df.columns if any(p in col for p in col_patterns)]\n    \n    if not matching_cols:\n        raise ValueError(\"No matching columns found\")\n    \n    window_sizes = [3, 5, 10]\n    for col in matching_cols:\n        for window in window_sizes:\n            df[f'{col}_rolling_{window}'] = df[col].rolling(window=window).mean()\n    \n    return df"
    },
    {
        "function_name": "top_n_categories_filter",
        "file_name": "categorical_filter.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "n": "int"
        },
        "objectives": [
            "Validate the presence of `target_col` and ensure it is categorical.",
            "Identify the top `n` most frequent value counts in `target_col`.",
            "Filter the DataFrame to only include these top `n` categories.",
            "Return the filtered DataFrame and a mapping of original to filtered category counts."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def top_n_categories_filter(df, target_col, n):\n    if target_col not in df.columns or not pd.api.types.is_categorical_dtype(df[target_col]):\n        raise ValueError(f\"Target column '{target_col}' not found or not categorical in DataFrame\")\n    \n    top_n_categories = df[target_col].value_counts().nlargest(n).index\n    filtered_df = df[df[target_col].isin(top_n_categories)]\n    category_counts = filtered_df[target_col].value_counts().to_dict()\n    \n    return filtered_df, category_counts"
    },
    {
        "function_name": "merge_string_columns",
        "file_name": "merging.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_merge": "list of str",
            "new_col_name": "str"
        },
        "objectives": [
            "Validate that all columns in `cols_to_merge` exist in the dataframe.",
            "Concatenate the string representations of the values in `cols_to_merge` into a single column.",
            "Create a new column with the merged values, named `new_col_name`.",
            "Return the modified dataframe with the new concatenated column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_string_columns(df, cols_to_merge, new_col_name):\n    for col in cols_to_merge:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n\n    df = df.copy()\n    df[new_col_name] = df[cols_to_merge].astype(str).agg('-'.join, axis=1)\n\n    return df"
    },
    {
        "function_name": "replace_outliers",
        "file_name": "outliers.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list of str",
            "tolerance": "float"
        },
        "objectives": [
            "Validate that all `columns` exist and are numeric.",
            "Identify outliers in each column based on the specified `tolerance` using the IQR method.",
            "Replace identified outliers with the median of the respective column.",
            "Return the modified dataframe with the outliers replaced."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def replace_outliers(df, columns, tolerance):\n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n\n    df = df.copy()\n\n    for col in columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - (tolerance * IQR)\n        upper_bound = Q3 + (tolerance * IQR)\n        \n        median_value = df[col].median()\n        outliers = ~((df[col] >= lower_bound) & (df[col] <= upper_bound))\n        \n        df.loc[outliers, col] = median_value\n\n    return df"
    },
    {
        "function_name": "impute_nulls",
        "file_name": "imputation_utilities.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list of str",
            "threshold": "int"
        },
        "objectives": [
            "Identify columns from `columns` with more than `threshold` number of null values.",
            "For each identified column, calculate the central value (mean/median based on numerical/categorical data).",
            "Replace null values in those columns with the calculated central value.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_nulls(df, columns, threshold):\n    # Identify columns with more than `threshold` null values\n    cols_to_impute = [col for col in columns if df[col].isnull().sum() > threshold]\n    \n    # Impute null values with central value (mean/median)\n    for col in cols_to_impute:\n        if pd.api.types.is_numeric_dtype(df[col]):\n            central_value = df[col].mean()\n        else:\n            central_value = df[col].mode()[0]  # Assuming mode() returns a series\n        \n        df[col].fillna(central_value, inplace=True)\n    \n    return df"
    },
    {
        "function_name": "apply_transformations",
        "file_name": "data_transformations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "transformation_specs": "dict"
        },
        "objectives": [
            "Iterate through the key-value pairs in `transformation_specs` where each key is a column name and the value is a transformation function.",
            "Apply the specified transformation function to each column.",
            "For each transformed column, calculate a new column's mean and variance.",
            "Return a new dataframe with both the original and transformed columns, as well as their mean and variance."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def apply_transformations(df, transformation_specs):\n    result_df = df.copy()\n    \n    for col, transform_func in transformation_specs.items():\n        if col in df.columns:\n            transformed_col = df[col].apply(transform_func)\n            result_df[col + '_transformed'] = transformed_col\n            result_df[col + '_transformed_mean'] = transformed_col.mean()\n            result_df[col + '_transformed_variance'] = transformed_col.var()\n    \n    return result_df"
    },
    {
        "function_name": "daily_cumulative_stats",
        "file_name": "time_series.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "start_date": "str",
            "end_date": "str",
            "agg_column": "str"
        },
        "objectives": [
            "Filter the dataframe `df` to include rows where the date column falls between `start_date` and `end_date`.",
            "For the filtered rows, group by each unique day.",
            "Calculate the cumulative sum and cumulative product of the `agg_column` for each day.",
            "Return a new dataframe containing the daily cumulative sum and product columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def daily_cumulative_stats(df, start_date, end_date, agg_column):\n    # Filter rows based on date range\n    date_range_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n    \n    # Group by each unique day\n    date_range_df['date'] = pd.to_datetime(date_range_df['date'])\n    date_range_df = date_range_df.set_index('date').resample('D')\n    \n    daily_cumsum = date_range_df[agg_column].cumsum()\n    daily_cumprod = date_range_df[agg_column].cumprod()\n    \n    # Combine cumulative sum and product into a new dataframe\n    result_df = pd.DataFrame({'cumulative_sum': daily_cumsum, 'cumulative_product': daily_cumprod})\n    \n    return result_df"
    },
    {
        "function_name": "remove_duplicates_and_count",
        "file_name": "deduplication.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Identify duplicates in the dataframe `df` when compared against the `target_col`.",
            "For each identified duplicate, create a column with the count of duplicates.",
            "Drop rows from the dataframe if they are identified as duplicates but only keep the first occurrence.",
            "Return the modified dataframe with the duplicate count column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_duplicates_and_count(df, target_col):\n    # Identify duplicates\n    duplicates = df.duplicated(subset=target_col, keep=False)\n    df['duplicate_count'] = 0\n    \n    # Count duplicates and mark them\n    df.loc[duplicates, 'duplicate_count'] = df.loc[duplicates, target_col].map(df.loc[duplicates, target_col].value_counts())\n    \n    # Drop duplicate rows, keeping the first occurrence\n    df = df.drop_duplicates(subset=target_col, keep='first')\n    \n    return df"
    },
    {
        "function_name": "grouped_timeseries_analysis",
        "file_name": "group_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "time_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Sort the dataframe based on `group_col` and `time_col` in ascending order.",
            "For each group in `group_col`, compute the cumulative sum of `value_col`.",
            "Calculate the exponentially weighted moving average for `value_col` in each group.",
            "Detect outliers in the `value_col` for each group using Z-score and remove them from the dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.stats import zscore"
        ],
        "function_def": "def grouped_timeseries_analysis(df, group_col, time_col, value_col):\n    # Sort the dataframe by group_col and time_col\n    df = df.sort_values(by=[group_col, time_col])\n    \n    # Compute the cumulative sum of value_col for each group\n    df['cumulative_sum'] = df.groupby(group_col)[value_col].cumsum()\n    \n    # Calculate the exponentially weighted moving average for value_col within each group\n    df['ewm'] = df.groupby(group_col)[value_col].transform(lambda x: x.ewm(span=10, adjust=False).mean())\n    \n    # Compute Z-scores and remove outliers\n    df['z_score'] = df.groupby(group_col)[value_col].transform(lambda x: zscore(x))\n    df = df[df['z_score'].abs() <= 3]\n    \n    return df.drop(columns=['z_score'])"
    },
    {
        "function_name": "sensor_data_anomaly_detection",
        "file_name": "sensor_analysis.py",
        "parameters": {
            "sensor_data": "pandas.DataFrame",
            "id_col": "str",
            "timestamp_col": "str",
            "measurement_col": "str"
        },
        "objectives": [
            "Sort the data based on `id_col` and `timestamp_col`.",
            "Detect and interpolate missing values in `measurement_col` for each sensor.",
            "Calculate and append a column for the rate of change of `measurement_col` over time.",
            "Identify periods of anomalous behavior using a simple moving average and flag these periods."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def sensor_data_anomaly_detection(sensor_data, id_col, timestamp_col, measurement_col):\n    # Sort the data based on id_col and timestamp_col\n    sensor_data = sensor_data.sort_values(by=[id_col, timestamp_col])\n    \n    # Interpolate missing values in measurement_col\n    sensor_data[measurement_col] = sensor_data.groupby(id_col)[measurement_col].transform(lambda x: x.interpolate(method='linear'))\n    \n    # Calculate the rate of change of measurement_col over time\n    sensor_data['rate_of_change'] = sensor_data.groupby(id_col)[measurement_col].diff() / sensor_data.groupby(id_col)[timestamp_col].diff().dt.total_seconds()\n    \n    # Identify periods of anomalous behavior using simple moving average\n    sensor_data['moving_avg'] = sensor_data.groupby(id_col)[measurement_col].transform(lambda x: x.rolling(window=5).mean())\n    sensor_data['anomaly_flag'] = ((sensor_data[measurement_col] - sensor_data['moving_avg']).abs() > 2 * sensor_data.groupby(id_col)[measurement_col].transform('std')).astype(int)\n    \n    return sensor_data.drop(columns=['moving_avg'])"
    },
    {
        "function_name": "label_encode_and_one_hot",
        "file_name": "encoding_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "encode_cols": "list"
        },
        "objectives": [
            "Verify that target and encode columns exist in the DataFrame and are of appropriate types.",
            "Apply label encoding to target column, assigning an integer value for each unique category.",
            "Transform input data by applying one-hot encoding to specified encode columns.",
            "Return the dataframe with the encoded columns and label-encoded target column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def label_encode_and_one_hot(df, target_col, encode_cols):\n    # Step 1: Validate presence and types of columns\n    if target_col not in df.columns:\n        raise ValueError(f\"Target column '{target_col}' is not found in the DataFrame\")\n    if not pd.api.types.is_object_dtype(df[target_col]):\n        raise ValueError(f\"Target column '{target_col}' must be of object type\")\n\n    for col in encode_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' is not found in the DataFrame\")\n\n    # Step 2: Apply label encoding to the target column\n    le = LabelEncoder()\n    df[target_col] = le.fit_transform(df[target_col])\n\n    # Step 3: Apply one-hot encoding to encode columns\n    df = pd.get_dummies(df, columns=encode_cols, prefix=encode_cols, drop_first=True)\n    \n    return df"
    },
    {
        "function_name": "split_and_summarize",
        "file_name": "data_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "threshold": "float",
            "target_col": "str"
        },
        "objectives": [
            "Validate that the target column is present and boolean type.",
            "Separate data into two groups based on the threshold of a specified numeric column.",
            "Calculate individual statistics such as mean, median, and standard deviation for both groups.",
            "Return a dictionary with statistical summaries for each group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_and_summarize(df, threshold, target_col):\n    # Step 1: Validate target column presence and type\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' is not found in the DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]) and not pd.api.types.is_bool_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be boolean or numeric\")\n\n    # Step 2: Separate data into two groups based on threshold\n    group_1 = df[df[target_col] >= threshold]\n    group_2 = df[df[target_col] < threshold]\n\n    # Step 3: Calculate statistical summaries\n    summary = {\n        'Group 1': {\n            'mean': group_1.mean().to_dict(),\n            'median': group_1.median().to_dict(),\n            'std': group_1.std().to_dict()\n        },\n        'Group 2': {\n            'mean': group_2.mean().to_dict(),\n            'median': group_2.median().to_dict(),\n            'std': group_2.std().to_dict()\n        }\n    }\n    \n    return summary"
    },
    {
        "function_name": "label_encode_and_pca",
        "file_name": "dimensionality_reduction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "List[str]"
        },
        "objectives": [
            "categorical_cols: List[str]",
            "Validate that each column in `categorical_cols` exists in the dataframe.",
            "Encode each categorical column individually via label encoding.",
            "Perform PCA on the entire dataframe and reduce the dimensions to 2.",
            "Return the transformed dataframe with the two PCA components."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def label_encode_and_pca(df, categorical_cols):\n    missing_cols = [col for col in categorical_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Columns {missing_cols} are not in the DataFrame\")\n\n    # Label encode categorical columns\n    le = LabelEncoder()\n\n    for col in categorical_cols:\n        df[col] = le.fit_transform(df[col])\n\n    # Apply PCA\n    pca = PCA(n_components=2)\n    pca_components = pca.fit_transform(df)\n\n    pca_df = pd.DataFrame(pca_components, columns=['PCA1', 'PCA2'])\n\n    return pca_df"
    },
    {
        "function_name": "vectorize_text_columns",
        "file_name": "text_vectorizer.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_columns": "list",
            "tfidf_max_features": "int",
            "ngrams": "tuple"
        },
        "objectives": [
            "Validate that the specified text columns exist in the dataframe and are of string type.",
            "Vectorize each specified text column using TF-IDF with a given number of maximum features and n-grams range.",
            "For each text column, create new features in the dataframe corresponding to the TF-IDF vectorized values.",
            "Return the modified dataframe with new TF-IDF features for each text column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def vectorize_text_columns(df, text_columns, tfidf_max_features, ngrams):\n    # Step 1: Validate columns\n    for col in text_columns:\n        if col not in df.columns or not pd.api.types.is_string_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not of string type\")\n    \n    # Step 2: Vectorize using TF-IDF\n    for col in text_columns:\n        vectorizer = TfidfVectorizer(max_features=tfidf_max_features, ngram_range=ngrams)\n        tfidf_matrix = vectorizer.fit_transform(df[col].values.astype('U'))\n        \n        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f'{col}_tfidf_{i}' for i in range(tfidf_matrix.shape[1])])\n        df = pd.concat([df, tfidf_df], axis=1)\n    \n    return df"
    },
    {
        "function_name": "clean_and_normalize_target",
        "file_name": "data_cleaning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Identify and fill missing values in the specified \"target_col\" by using the mode (most frequent value) of the column.",
            "Normalize the \"target_col\" by scaling its values to lie between 0 and 1.",
            "Detect and remove outliers from the \"target_col\" by using the IQR (Interquartile Range) method.",
            "Return the modified DataFrame and a list of indices corresponding to the removed outliers."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def clean_and_normalize_target(df, target_col):\n    df = df.copy()\n    \n    # Fill missing values with mode\n    mode_value = df[target_col].mode()[0]\n    df[target_col].fillna(mode_value, inplace=True)\n    \n    # Normalize target column\n    scaler = MinMaxScaler()\n    df[target_col] = scaler.fit_transform(df[[target_col]])\n    \n    # Remove outliers using IQR method\n    Q1 = df[target_col].quantile(0.25)\n    Q3 = df[target_col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outlier_indices = df[(df[target_col] < lower_bound) | (df[target_col] > upper_bound)].index.tolist()\n    df = df[(df[target_col] >= lower_bound) & (df[target_col] <= upper_bound)]\n    \n    return df, outlier_indices"
    },
    {
        "function_name": "replace_with_frequency",
        "file_name": "frequency_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_cols": "List[str]"
        },
        "objectives": [
            "Identify categorical columns listed in \"cat_cols\".",
            "For each categorical column, compute the frequency of each unique value.",
            "Replace the original values in each categorical column with their corresponding frequency.",
            "Create new columns named after the original columns with \"_freq\" suffix containing these frequencies.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def replace_with_frequency(df, cat_cols):\n    df = df.copy()\n    \n    for col in cat_cols:\n        freq = df[col].value_counts().to_dict()\n        df[col + '_freq'] = df[col].map(freq)\n    \n    return df"
    },
    {
        "function_name": "equal_width_binning",
        "file_name": "binning_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_to_bin": "str",
            "n_bins": "int"
        },
        "objectives": [
            "Perform binning on the specified column \"col_to_bin\" into \"n_bins\" equal-width bins.",
            "Label the bins in sequential order starting from 0.",
            "Create a new column with the same name as \"col_to_bin\", but appended with \"_binned\", containing the bin labels.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def equal_width_binning(df, col_to_bin, n_bins):\n    df = df.copy()\n    \n    bin_labels = range(n_bins)\n    df[col_to_bin + '_binned'] = pd.cut(df[col_to_bin], bins=n_bins, labels=bin_labels)\n    \n    return df"
    },
    {
        "function_name": "exponential_smoothing",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Ensure `time_col` is a datetime type and `value_col` is numeric.",
            "Resample the DataFrame to a daily frequency, filling in missing dates with NaN.",
            "Apply exponential smoothing on the `value_col` to reduce noise.",
            "Return the resampled DataFrame with the smoothed values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def exponential_smoothing(df, time_col, value_col, smoothing_factor=0.2):\n    if not all(col in df.columns for col in [time_col, value_col]):\n        raise ValueError(f\"Columns {time_col} and/or {value_col} not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n        raise ValueError(f\"Column {time_col} must be of datetime type\")\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"Column {value_col} must be numeric\")\n    \n    # Resample to daily frequency\n    df = df.set_index(time_col).resample('D').asfreq()\n\n    # Exponential smoothing\n    df[value_col + '_smoothed'] = df[value_col].ewm(alpha=smoothing_factor).mean()\n\n    return df.reset_index()"
    },
    {
        "function_name": "monthly_cumulative_sum",
        "file_name": "cumulative_sum.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Ensure `date_col` is of datetime type and `value_col` is numeric.",
            "Extract year, month, and day from `date_col` and create separate columns for each.",
            "Calculate and add a new column representing the cumulative sum of `value_col` grouped by month and year.",
            "Return the modified DataFrame with the new columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def monthly_cumulative_sum(df, date_col, value_col):\n    if not all(col in df.columns for col in [date_col, value_col]):\n        raise ValueError(f\"Columns {date_col} and/or {value_col} not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column {date_col} must be of datetime type\")\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"Column {value_col} must be numeric\")\n    \n    df['year'] = df[date_col].dt.year\n    df['month'] = df[date_col].dt.month\n    df['day'] = df[date_col].dt.day\n    \n    df['monthly_cumulative_sum'] = df.groupby(['year', 'month'])[value_col].cumsum()\n    \n    return df"
    },
    {
        "function_name": "create_lag_features",
        "file_name": "lag_feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "date_col": "str",
            "params": "list of str"
        },
        "objectives": [
            "Convert the 'date_col' to datetime format.",
            "Sort the dataframe by 'id_col' and 'date_col'.",
            "Create lag features for each parameter in 'params', where lag is defined as the previous row's value in the same group of 'id_col'.",
            "Fill missing lag values with the mean of the respective parameter within the same group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_lag_features(df, id_col, date_col, params):\n    # Step 1: Convert 'date_col' to datetime format\n    df[date_col] = pd.to_datetime(df[date_col])\n    \n    # Step 2: Sort by 'id_col' and 'date_col'\n    df.sort_values(by=[id_col, date_col], inplace=True)\n\n    # Step 3: Create lag features\n    for param in params:\n        df[f'lag_{param}'] = df.groupby(id_col)[param].shift(1)\n    \n    # Step 4: Fill missing lag values with mean of respective parameter within the same group\n    for param in params:\n        df[f'lag_{param}'] = df.groupby(id_col)[f'lag_{param}'].transform(lambda x: x.fillna(x.mean()))\n\n    return df"
    },
    {
        "function_name": "standardize_handle_pca",
        "file_name": "transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols": "list of str",
            "strategies": "dict"
        },
        "objectives": [
            "Standardize specified columns using z-score normalization.",
            "For each specified column, handle missing values with different strategies ('mean', 'median', 'mode') as specified in 'strategies'.",
            "After handling missing values, perform a principal component analysis (PCA) transformation on the standardized columns.",
            "Return the transformed dataframe with principal components."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, normalize",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def standardize_handle_pca(df, cols, strategies):\n    # Step 1: Standardize columns using z-score normalization\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n\n    # Step 2: Handle missing values according to specified strategies\n    for col, strat in strategies.items():\n        if strat == 'mean':\n            df[col].fillna(df[col].mean(), inplace=True)\n        elif strat == 'median':\n            df[col].fillna(df[col].median(), inplace=True)\n        elif strat == 'mode':\n            df[col].fillna(df[col].mode()[0], inplace=True)\n\n    # Step 3: Perform PCA transformation\n    pca = PCA()\n    principal_components = pca.fit_transform(df[cols])\n    df_pca = pd.DataFrame(data=principal_components, columns=[f'pc{i+1}' for i in range(principal_components.shape[1])])\n\n    df.reset_index(drop=True, inplace=True)\n    df_pca.reset_index(drop=True, inplace=True)\n    df = pd.concat([df, df_pca], axis=1)\n\n    return df"
    },
    {
        "function_name": "preprocess_numerical_columns",
        "file_name": "numerical_preprocessor.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_columns": "list",
            "fill_strategy": "str"
        },
        "objectives": [
            "Validate that the specified numerical columns exist and are of numerical type.",
            "Handle missing values in the numerical columns based on the given fill strategy (\"mean\", \"median\", \"mode\", or \"constant\").",
            "Normalize the numerical columns using min-max scaling.",
            "Create new columns containing the z-score for each numerical column and return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import zscore"
        ],
        "function_def": "def preprocess_numerical_columns(df, numerical_columns, fill_strategy):\n    # Step 1: Validate columns\n    for col in numerical_columns:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not of numerical type\")\n\n    # Step 2: Handle missing values\n    if fill_strategy == 'mean':\n        fill_value = df[numerical_columns].mean()\n    elif fill_strategy == 'median':\n        fill_value = df[numerical_columns].median()\n    elif fill_strategy == 'mode':\n        fill_value = df[numerical_columns].mode().iloc[0]\n    elif fill_strategy == 'constant':\n        fill_value = -999  # Assumes a placeholder constant\n    else:\n        raise ValueError(\"Invalid fill_strategy specified\")\n\n    df[numerical_columns] = df[numerical_columns].fillna(fill_value)\n\n    # Step 3: Normalize using min-max scaling\n    df[numerical_columns] = (df[numerical_columns] - df[numerical_columns].min()) / (df[numerical_columns].max() - df[numerical_columns].min())\n\n    # Step 4: Z-score normalization\n    for col in numerical_columns:\n        df[f'{col}_zscore'] = zscore(df[col])\n\n    return df"
    },
    {
        "function_name": "aggregate_and_rank",
        "file_name": "grouping_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "groupby_columns": "list",
            "agg_columns": "list"
        },
        "objectives": [
            "Validate that specified columns for grouping and aggregation exist in the dataframe.",
            "Perform groupby operation on specified columns and calculate aggregates (mean, median, sum) for specified aggregation columns.",
            "Merge the aggregated results back to the original dataframe.",
            "Introduce a ranking column for each group based on one of the aggregation columns and return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_and_rank(df, groupby_columns, agg_columns):\n    # Step 1: Validate columns\n    for col in groupby_columns + agg_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not present in the dataframe\")\n\n    # Step 2: Groupby and aggregate\n    agg_funcs = {col: ['mean', 'median', 'sum'] for col in agg_columns}\n    grouped_df = df.groupby(groupby_columns).agg(agg_funcs)\n\n    # Flatten MultiIndex columns\n    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n    grouped_df = grouped_df.reset_index()\n\n    # Step 3: Merge back with the original dataframe\n    df = df.merge(grouped_df, on=groupby_columns, how='left')\n\n    # Step 4: Create ranking column for one of the aggregation columns\n    rank_col = agg_columns[0]\n    df[rank_col + '_rank'] = df.groupby(groupby_columns)[rank_col].rank(method='first', ascending=False)\n\n    return df"
    },
    {
        "function_name": "extract_top_words",
        "file_name": "text_feature_extractor.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "top_n_words": "int"
        },
        "objectives": [
            "Validate that the specified text column exists and is of string type.",
            "Tokenize the text column into words.",
            "Calculate the frequency of each word and select the top 'n' most frequent words.",
            "For each of the top 'n' words, create a binary feature in the dataframe indicating the presence of that word in each text entry.",
            "Return the modified dataframe with the new binary features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from collections import Counter",
            "import re"
        ],
        "function_def": "def extract_top_words(df, text_column, top_n_words):\n    # Step 1: Validate column\n    if text_column not in df.columns or not pd.api.types.is_string_dtype(df[text_column]):\n        raise ValueError(f\"Column {text_column} is not present or not of string type\")\n\n    # Step 2: Tokenize the text column\n    all_words = df[text_column].apply(lambda x: re.findall(r'\\b\\w+\\b', x.lower())).sum()\n\n    # Step 3: Calculate word frequency and select top 'n' frequent words\n    top_words = dict(Counter(all_words).most_common(top_n_words))\n\n    # Step 4: Create binary features for top words\n    for word in top_words:\n        df[f'contains_{word}'] = df[text_column].apply(lambda x: int(word in x.lower().split()))\n\n    return df"
    },
    {
        "function_name": "impute_and_standardize",
        "file_name": "data_imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Identify and fill missing values in each column by imputing with suitable statistics: mean for numeric columns and mode for categorical columns.",
            "For \"target_col\", handle missing values by filling them with a placeholder value \"missing\" and convert it into a categorical type.",
            "Perform one-hot encoding on all categorical columns.",
            "Standardize all numeric columns using Z-score normalization.",
            "Return the modified dataframe and a list of imputation strategies used for each column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def impute_and_standardize(df, target_col):\n    imputation_strategies = {}\n\n    # Step 1: Fill missing values\n    for col in df.columns:\n        if col == target_col:\n            df[col].fillna('missing', inplace=True)\n            df[col] = df[col].astype('category')\n        elif df[col].dtype in ['int64', 'float64']:\n            mean_value = df[col].mean()\n            df[col].fillna(mean_value, inplace=True)\n            imputation_strategies[col] = 'mean'\n        else:\n            mode_value = df[col].mode()[0]\n            df[col].fillna(mode_value, inplace=True)\n            imputation_strategies[col] = 'mode'\n    \n    # Step 2: One-hot encode categorical columns\n    categorical_cols = df.select_dtypes(include=['category']).columns\n    df = pd.get_dummies(df, columns=categorical_cols)\n    \n    # Step 3: Standardize numeric columns\n    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    scaler = StandardScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    \n    return df, imputation_strategies"
    },
    {
        "function_name": "impute_and_scale",
        "file_name": "feature_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_features": "list of str"
        },
        "objectives": [
            "Validate that all `numerical_features` exist and are numeric.",
            "Impute missing values in each specified feature column using its respective median value.",
            "Scale the features to mean 0 and variance 1.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def impute_and_scale(df, numerical_features):\n    # Step 1: Validate columns\n    for col in numerical_features:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n\n    df = df.copy()\n\n    # Step 2: Impute missing values\n    for col in numerical_features:\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n\n    # Step 3: Scale features\n    scaler = StandardScaler()\n    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n    \n    return df"
    },
    {
        "function_name": "resample_and_interpolate",
        "file_name": "datetime_manipulation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str",
            "freq": "str"
        },
        "objectives": [
            "Ensure the specified datetime column exists and is of datetime type.",
            "Resample the dataframe according to the provided frequency (e.g., 'D' for daily, 'M' for monthly) using the datetime column.",
            "Aggregate numeric columns using their mean and interpolate missing time intervals.",
            "Return the resampled dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_interpolate(df, datetime_col, freq):\n    if datetime_col not in df.columns:\n        raise ValueError(f\"Datetime column '{datetime_col}' is not in the dataframe.\")\n    if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n        raise ValueError(f\"Column '{datetime_col}' must be of datetime type.\")\n    \n    df = df.set_index(datetime_col)\n    resampled_df = df.resample(freq).mean()\n    interpolated_df = resampled_df.interpolate(method='time')\n    \n    return interpolated_df.reset_index()"
    },
    {
        "function_name": "generate_n_grams",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "n_gram_range": "tuple"
        },
        "objectives": [
            "Ensure the text column exists and is of string type.",
            "Tokenize the text column into n-grams within the specified range.",
            "Count the frequency of each n-gram and create a new column for each n-gram.",
            "Return the modified DataFrame with the n-gram columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import CountVectorizer"
        ],
        "function_def": "def generate_n_grams(df, text_col, n_gram_range):\n    if text_col not in df.columns:\n        raise ValueError(f\"Text column '{text_col}' is not in the dataframe.\")\n    if not pd.api.types.is_string_dtype(df[text_col]):\n        raise ValueError(f\"Column '{text_col}' must be of string type.\")\n    \n    vectorizer = CountVectorizer(ngram_range=n_gram_range)\n    n_gram_matrix = vectorizer.fit_transform(df[text_col])\n    n_gram_df = pd.DataFrame(n_gram_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n    df = pd.concat([df.reset_index(drop=True), n_gram_df], axis=1)\n\n    return df"
    },
    {
        "function_name": "bin_and_label_distribution",
        "file_name": "numeric_binning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "num_column": "str",
            "bins": "int",
            "labels": "list"
        },
        "objectives": [
            "Ensure the numeric column exists and is of numeric type.",
            "Bin the numeric column into a specified number of bins.",
            "Label the bins using the provided labels.",
            "Return a modified DataFrame with a new column for the labeled bins."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def bin_and_label_distribution(df, num_column, bins, labels):\n    if num_column not in df.columns:\n        raise ValueError(f\"Numeric column '{num_column}' is not in the dataframe.\")\n    if not pd.api.types.is_numeric_dtype(df[num_column]):\n        raise ValueError(f\"Column '{num_column}' must be of numeric type.\")\n    \n    df['binned'] = pd.cut(df[num_column], bins=bins, labels=labels)\n    \n    return df"
    },
    {
        "function_name": "normalize_within_categories",
        "file_name": "category_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_column": "str",
            "num_column": "str"
        },
        "objectives": [
            "Validate `cat_column` and `num_column` presence and ensure they are of the correct types (categorical and numeric, respectively).",
            "For each category in `cat_column`, calculate the mean and standard deviation of `num_column`.",
            "Normalize entries in `num_column` within each category based on the calculated mean and standard deviation.",
            "Return the transformed dataframe with the normalized values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def normalize_within_categories(df, cat_column, num_column):\n    if cat_column not in df.columns:\n        raise ValueError(f\"Column '{cat_column}' not found in DataFrame\")\n    if num_column not in df.columns:\n        raise ValueError(f\"Column '{num_column}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[num_column]):\n        raise ValueError(f\"Column '{num_column}' must be of numeric type\")\n    if not pd.api.types.is_categorical_dtype(df[cat_column]) and not df[cat_column].dtype == 'object':\n        raise ValueError(f\"Column '{cat_column}' must be of categorical type\")\n\n    df = df.copy()\n    for category in df[cat_column].unique():\n        subset = df[df[cat_column] == category]\n        mean = subset[num_column].mean()\n        std = subset[num_column].std()\n        df.loc[df[cat_column] == category, num_column] = (subset[num_column] - mean) / std\n\n    return df"
    },
    {
        "function_name": "calculate_token_frequencies",
        "file_name": "token_frequencies.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str"
        },
        "objectives": [
            "Validate `text_column` presence and ensure it's of string type.",
            "Tokenize the text in `text_column` and remove common stopwords.",
            "Calculate the frequencies of each token and create a new column with token frequencies.",
            "Return the dataframe with the token frequency column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS",
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def calculate_token_frequencies(df, text_column):\n    if text_column not in df.columns:\n        raise ValueError(f\"Column '{text_column}' not found in DataFrame\")\n    if not pd.api.types.is_string_dtype(df[text_column]):\n        raise ValueError(f\"Column '{text_column}' must be of string type\")\n\n    df = df.copy()\n    token_counts = []\n    \n    for text in df[text_column]:\n        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n        filtered_tokens = [token for token in tokens if token not in ENGLISH_STOP_WORDS]\n        token_count = Counter(filtered_tokens)\n        token_counts.append(token_count)\n    \n    df[f'{text_column}_token_freq'] = token_counts\n\n    return df"
    },
    {
        "function_name": "join_and_aggregate",
        "file_name": "join_and_aggregate.py",
        "parameters": {
            "df1": "pandas.DataFrame",
            "df2": "pandas.DataFrame",
            "join_column": "str",
            "agg_column": "str"
        },
        "objectives": [
            "Validate `join_column` presence in both dataframes and `agg_column` presence in `df2`.",
            "Perform an inner join on `df1` and `df2` based on `join_column`.",
            "Aggregate the `agg_column` (sum) for repeated entries in the joined dataframe.",
            "Return the resulting dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def join_and_aggregate(df1, df2, join_column, agg_column):\n    if join_column not in df1.columns:\n        raise ValueError(f\"Column '{join_column}' not found in df1\")\n    if join_column not in df2.columns:\n        raise ValueError(f\"Column '{join_column}' not found in df2\")\n    if agg_column not in df2.columns:\n        raise ValueError(f\"Column '{agg_column}' not found in df2\")\n\n    # Step 2: Perform inner join\n    joined_df = pd.merge(df1, df2, on=join_column, how='inner')\n\n    # Step 3: Aggregate agg_column by summing up\n    aggregated_df = joined_df.groupby(join_column).agg({agg_column: 'sum'}).reset_index()\n\n    return aggregated_df"
    },
    {
        "function_name": "target_encode_columns",
        "file_name": "target_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list"
        },
        "objectives": [
            "Validate `categorical_columns` presence and ensure they are of categorical type.",
            "Encode categorical variables using target encoding.",
            "Ensure that the encoding process does not introduce data leakage by using cross-validation.",
            "Return the dataframe with the target-encoded columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import KFold"
        ],
        "function_def": "def target_encode_columns(df, categorical_columns):\n    for col in categorical_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_categorical_dtype(df[col]) and not df[col].dtype == 'object':\n            raise ValueError(f\"Column '{col}' must be of categorical type\")\n\n    df = df.copy()\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n    for col in categorical_columns:\n        target_mean = df.groupby(col)['target'].mean()\n        df[col + '_enc'] = df[col].map(target_mean)\n\n        for train_index, valid_index in kf.split(df):\n            train_df, valid_df = df.iloc[train_index], df.iloc[valid_index]\n            enc_map = train_df.groupby(col)['target'].mean()\n            df.iloc[valid_index, df.columns.get_loc(col + '_enc')] = valid_df[col].map(enc_map).values\n\n    return df"
    },
    {
        "function_name": "handle_categorical_and_numerical",
        "file_name": "column_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_cols": "List[str]",
            "num_cols": "List[str]"
        },
        "objectives": [
            "Validate that all columns in 'cat_cols' are of categorical type and all 'num_cols' are of numeric type.",
            "Perform one-hot encoding on each column in 'cat_cols'.",
            "For each column in 'num_cols', replace outliers (values 3 standard deviations from the mean) with the median value of the column.",
            "Return the modified DataFrame with one-hot encoded categorical columns and outlier-free numerical columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def handle_categorical_and_numerical(df, cat_cols, num_cols):\n    df = df.copy()\n    \n    for col in cat_cols:\n        if col not in df.columns or not df[col].dtype.name == 'category':\n            raise ValueError(f\"Column '{col}' must be categorical and exist in DataFrame\")\n    \n    for col in num_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric and exist in DataFrame\")\n    \n    df = pd.get_dummies(df, columns=cat_cols)\n    \n    for col in num_cols:\n        col_mean = df[col].mean()\n        col_std = df[col].std()\n        col_median = df[col].median()\n        \n        df[col] = df[col].apply(lambda x: col_median if abs(x - col_mean) > 3 * col_std else x)\n    \n    return df"
    },
    {
        "function_name": "feature_selection_and_model_training",
        "file_name": "model_training_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_cols": "list",
            "target_col": "str"
        },
        "objectives": [
            "Ensure all specified feature columns and the target column are present in the dataframe and are numerical.",
            "Perform feature selection using Recursive Feature Elimination with Cross-Validation (RFECV).",
            "Train a linear regression model using the selected features.",
            "Return the fitted model and the selected features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import RFECV",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.model_selection import train_test_split"
        ],
        "function_def": "def feature_selection_and_model_training(df, feature_cols, target_col):\n    for col in feature_cols + [target_col]:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not numerical\")\n\n    X = df[feature_cols]\n    y = df[target_col]\n\n    model = LinearRegression()\n    rfecv = RFECV(estimator=model, step=1, cv=5, scoring='neg_mean_squared_error')\n\n    rfecv.fit(X, y)\n\n    selected_features = [feature for feature, support in zip(feature_cols, rfecv.support_) if support]\n\n    model.fit(X[selected_features], y)\n\n    return model, selected_features"
    },
    {
        "function_name": "identify_high_variance_categorical_columns",
        "file_name": "feature_selection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "list of str",
            "target_column": "str",
            "variance_threshold": "float"
        },
        "objectives": [
            "Validate that categorical_columns and target_column exist in df.",
            "For each column in categorical_columns, compute the variance of the target_column grouped by the categorical column.",
            "Identify and store columns where the variance exceeds the variance_threshold.",
            "Return a list of these columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_high_variance_categorical_columns(df, categorical_columns, target_column, variance_threshold):\n    if not all(col in df.columns for col in categorical_columns + [target_column]):\n        raise ValueError(\"One or more specified columns are not present in the dataframe\")\n    \n    high_variance_columns = []\n    \n    for col in categorical_columns:\n        variance = df.groupby(col)[target_column].var()\n        if variance.max() > variance_threshold:\n            high_variance_columns.append(col)\n    \n    return high_variance_columns"
    },
    {
        "function_name": "map_values_to_reference_dict",
        "file_name": "value_mapping.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "reference_dict": "dict"
        },
        "objectives": [
            "Validate that the keys of reference_dict exist as columns in df.",
            "For each key in reference_dict, map the values in the corresponding column of df to the values in reference_dict.",
            "Replace any values in the columns that do not exist in reference_dict with NaN.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def map_values_to_reference_dict(df, reference_dict):\n    for col in reference_dict.keys():\n        if col not in df.columns:\n            raise ValueError(f\"The column '{col}' specified in the reference dictionary is not present in the dataframe\")\n        df[col] = df[col].map(reference_dict[col]).fillna(np.nan)\n    \n    return df"
    },
    {
        "function_name": "category_percentile_summary",
        "file_name": "category_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "numerical_col": "str",
            "target_percentile": "float"
        },
        "objectives": [
            "Calculate percentile for each category in `category_col` using values from `numerical_col`.",
            "Identify categories where the percentile value exceeds the `target_percentile`.",
            "Generate a summary DataFrame listing these categories and corresponding percentile values.",
            "Return the modified DataFrame and the summary DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def category_percentile_summary(df, category_col, numerical_col, target_percentile):\n    # Step 1: Calculate percentile for each category\n    percentile_values = df.groupby(category_col)[numerical_col].quantile(target_percentile).reset_index()\n    \n    # Step 2: Identify categories with percentile value exceeding target_percentile\n    high_percentiles = percentile_values[percentile_values[numerical_col] > df[numerical_col].quantile(target_percentile)]\n    \n    # Step 3: Generate summary DataFrame\n    summary_df = high_percentiles.rename(columns={numerical_col: f'{target_percentile}_percentile_value'})\n    \n    # Step 4: Return modified DataFrame and summary DataFrame\n    return df, summary_df"
    },
    {
        "function_name": "rolling_mean_difference",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "value_col": "str",
            "rolling_window": "int"
        },
        "objectives": [
            "Sort the DataFrame based on `time_col`.",
            "Compute a rolling mean for `value_col` based on the specified `rolling_window`.",
            "Calculate the difference between the actual value and the rolling mean.",
            "Append a column indicating whether the difference exceeds a threshold (2 standard deviations).",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def rolling_mean_difference(df, time_col, value_col, rolling_window):\n    # Step 1: Sort the DataFrame based on time_col\n    df = df.sort_values(by=[time_col])\n    \n    # Step 2: Compute rolling mean\n    df['rolling_mean'] = df[value_col].rolling(rolling_window).mean()\n    \n    # Step 3: Calculate difference between actual value and rolling mean\n    df['mean_diff'] = df[value_col] - df['rolling_mean']\n    \n    # Step 4: Append column indicating if difference exceeds threshold\n    threshold = 2 * df[value_col].std()\n    df['exceeds_threshold'] = (df['mean_diff'].abs() > threshold).astype(int)\n    \n    return df"
    },
    {
        "function_name": "clean_and_normalize_column",
        "file_name": "data_cleaning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_to_clean": "str"
        },
        "objectives": [
            "Identify numeric columns containing invalid values (strings or NaNs).",
            "Replace invalid values with the mean of the column (after removing invalid values).",
            "Standardize the cleaned column using z-score normalization.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def clean_and_normalize_column(df, col_to_clean):\n    # Step 1: Identify and convert invalid values to NaN\n    df[col_to_clean] = pd.to_numeric(df[col_to_clean], errors='coerce')\n    \n    # Step 2: Replace invalid values with column mean\n    col_mean = df[col_to_clean].mean()\n    df[col_to_clean].fillna(col_mean, inplace=True)\n    \n    # Step 3: Z-score normalization\n    col_std = df[col_to_clean].std()\n    df[col_to_clean] = (df[col_to_clean] - col_mean) / col_std\n    \n    return df"
    },
    {
        "function_name": "detect_and_remove_outliers",
        "file_name": "outlier_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "grouping_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Group the DataFrame by `grouping_col` and calculate the median of `value_col` for each group.",
            "Create a new column to flag entries that deviate from the median by more than three times the interquartile range (IQR).",
            "Filter out these flagged entries from the DataFrame.",
            "Return the filtered DataFrame along with a summary of the number of flagged entries per group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_and_remove_outliers(df, grouping_col, value_col):\n    # Step 1: Calculate median and IQR for each group\n    grouped = df.groupby(grouping_col)[value_col]\n    medians = grouped.median().reset_index(name=f'{value_col}_median')\n    iqr = grouped.quantile(0.75) - grouped.quantile(0.25)\n    \n    # Step 2: Flag entries deviating by more than 3*IQR\n    deviations = df[value_col] - df.merge(medians, on=grouping_col)[f'{value_col}_median']\n    threshold = 3 * iqr.reset_index(name=f'{value_col}_iqr')[f'{value_col}_iqr']\n    df['flag'] = deviations.abs() > threshold[0]\n    \n    # Step 3: Filter out flagged entries\n    filtered_df = df[df['flag'] == False].drop(columns=['flag'])\n    \n    # Step 4: Summary of flagged entries per group\n    summary = df[df['flag']].groupby(grouping_col)['flag'].count().reset_index(name='flag_count')\n    \n    return filtered_df, summary"
    },
    {
        "function_name": "categorical_interaction_means",
        "file_name": "categorical_preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list",
            "numerical_col": "str"
        },
        "objectives": [
            "Ensure the specified categorical columns exist in the dataframe and have categorical data.",
            "Compute the mean value of \"numerical_col\" for each combination of values from the categorical columns.",
            "Normalize these computed mean values using Min-Max normalization.",
            "Return the modified DataFrame with normalized mean values for each combination of categorical values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def categorical_interaction_means(df, categorical_cols, numerical_col):\n    # Step 1: Ensure the specified columns exist and are of correct types\n    for col in categorical_cols:\n        if col not in df.columns or not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not categorical\")\n\n    if numerical_col not in df.columns or not pd.api.types.is_numeric_dtype(df[numerical_col]):\n        raise ValueError(f\"Column {numerical_col} is not present or not numerical\")\n    \n    # Step 2: Compute mean values for each combination of categorical columns\n    grouped_means = df.groupby(categorical_cols)[numerical_col].mean().reset_index()\n    grouped_means[f\"{numerical_col}_mean\"] = grouped_means[numerical_col]\n    grouped_means.drop(columns=[numerical_col], inplace=True)\n    \n    # Step 3: Apply Min-Max normalization to the computed mean values\n    scaler = MinMaxScaler()\n    grouped_means[f\"{numerical_col}_mean_normalized\"] = scaler.fit_transform(grouped_means[[f\"{numerical_col}_mean\"]])\n    \n    return grouped_means"
    },
    {
        "function_name": "resample_and_ohlc",
        "file_name": "resampling_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str",
            "freq": "str"
        },
        "objectives": [
            "Ensure the \"datetime_col\" is present in the dataframe and convert it to datetime format.",
            "Resample the dataframe based on the specified frequency (\"freq\").",
            "For each resampled period, calculate and append OHLC (Open, High, Low, Close) values of another specified column.",
            "Return the resampled dataframe with OHLC values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_ohlc(df, datetime_col, target_col, freq):\n    # Ensure the column is present and convert to datetime\n    if datetime_col not in df.columns:\n        raise ValueError(f\"Column {datetime_col} is not in the dataframe\")\n    df[datetime_col] = pd.to_datetime(df[datetime_col])\n    \n    # Check if target column is present and numerical\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column {target_col} is not present or not numerical\")\n    \n    # Set datetime column as index\n    df.set_index(datetime_col, inplace=True)\n    \n    # Resample and calculate OHLC\n    ohlc_dict = {\n        'Open': 'first',\n        'High': 'max',\n        'Low': 'min',\n        'Close': 'last'\n    }\n    resampled = df[target_col].resample(freq).apply(ohlc_dict)\n    resampled.columns = [f\"{target_col}_{col}\" for col in resampled.columns]\n    \n    return resampled"
    },
    {
        "function_name": "interpolate_low_values",
        "file_name": "interpolation_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "low_v_thresh": "float"
        },
        "objectives": [
            "Ensure the \"target_col\" is present in the dataframe and is numerical.",
            "Identify records in the dataframe with values in \"target_col\" below the specified threshold.",
            "Perform linear interpolation to estimate the missing values below the threshold.",
            "Return the modified dataframe with interpolated values replacing the lower-threshold original values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def interpolate_low_values(df, target_col, low_v_thresh):\n    # Step 1: Ensure the column is present and numerical\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column {target_col} is not present or not numerical\")\n    \n    # Step 2: Identify low value records\n    low_values_mask = df[target_col] < low_v_thresh\n    \n    # Step 3: Apply interpolation\n    df.loc[low_values_mask, target_col] = df[target_col].interpolate()\n    \n    return df"
    },
    {
        "function_name": "weighted_category_frequency",
        "file_name": "category_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "weight_col": "str"
        },
        "objectives": [
            "Validate that `cat_col` is a categorical column and `weight_col` is numeric.",
            "Calculate the weighted frequency of each category in `cat_col` using `weight_col`.",
            "Create a new dataframe with the category and its corresponding weighted frequency.",
            "Return the new dataframe sorted by weighted frequency in descending order."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def weighted_category_frequency(df, cat_col, weight_col):\n    if cat_col not in df.columns:\n        raise ValueError(f\"Column '{cat_col}' not found in DataFrame\")\n    if not pd.api.types.is_categorical_dtype(df[cat_col]):\n        raise ValueError(f\"Column '{cat_col}' must be categorical\")\n    if weight_col not in df.columns:\n        raise ValueError(f\"Column '{weight_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[weight_col]):\n        raise ValueError(f\"Column '{weight_col}' must be numeric\")\n    \n    df = df.copy()\n    weighted_freq = df.groupby(cat_col)[weight_col].sum().reset_index()\n    weighted_freq.rename(columns={weight_col: 'weighted_frequency'}, inplace=True)\n    \n    return weighted_freq.sort_values(by='weighted_frequency', ascending=False)"
    },
    {
        "function_name": "rolling_window_features",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int",
            "target_col": "str"
        },
        "objectives": [
            "Validate `target_col` presence and ensure it is of numeric type.",
            "Apply a rolling window technique to `target_col` to generate feature columns: mean, standard deviation, and sum within the window size.",
            "Shift the original `target_col` by window size to avoid look-ahead bias.",
            "Return the modified dataframe with the new rolling window features and the shifted original `target_col`."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_window_features(df, window_size, target_col):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be of numeric type\")\n    \n    df = df.copy()\n    rolling_window = df[target_col].rolling(window=window_size, min_periods=1)\n    \n    df[f'{target_col}_rolling_mean'] = rolling_window.mean()\n    df[f'{target_col}_rolling_std'] = rolling_window.std().fillna(0)\n    df[f'{target_col}_rolling_sum'] = rolling_window.sum()\n    df[target_col] = df[target_col].shift(window_size)\n    \n    return df"
    },
    {
        "function_name": "token_frequency_indicators",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "top_n": "int"
        },
        "objectives": [
            "Validate `text_col` presence and ensure it is of string type.",
            "Tokenize the text in `text_col` and generate a frequency distribution of tokens.",
            "Filter out the top_n most common tokens and assign these tokens to new columns with binary presence indicators (1 if present, 0 if not).",
            "Return the modified dataframe with the new token presence indicator columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from collections import Counter",
            "import re"
        ],
        "function_def": "def token_frequency_indicators(df, text_col, top_n):\n    if text_col not in df.columns:\n        raise ValueError(f\"Column '{text_col}' not found in DataFrame\")\n    if not pd.api.types.is_string_dtype(df[text_col]):\n        raise ValueError(f\"Column '{text_col}' must be of string type\")\n    \n    df = df.copy()\n    all_tokens = df[text_col].apply(lambda x: re.findall(r'\\w+', x.lower())).sum()\n    top_tokens = [token for token, _ in Counter(all_tokens).most_common(top_n)]\n    \n    for token in top_tokens:\n        df[f'has_{token}'] = df[text_col].apply(lambda x: int(token in x.lower()))\n    \n    return df"
    },
    {
        "function_name": "clip_outliers",
        "file_name": "outlier_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_clip": "List[str]",
            "lower_quantile": "float",
            "upper_quantile": "float"
        },
        "objectives": [
            "Validate that all columns in `cols_to_clip` exist in the dataframe and are numeric.",
            "Calculate the quantiles for each column specified in `cols_to_clip`.",
            "Clip the values of these columns based on the calculated quantiles.",
            "Return the dataframe with the clipped values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def clip_outliers(df, cols_to_clip, lower_quantile, upper_quantile):\n    df = df.copy()\n    \n    for col in cols_to_clip:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n    \n    for col in cols_to_clip:\n        lower_bound = df[col].quantile(lower_quantile)\n        upper_bound = df[col].quantile(upper_quantile)\n        df[col] = df[col].clip(lower_bound, upper_bound)\n    \n    return df"
    },
    {
        "function_name": "bin_target_variable",
        "file_name": "target_binning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_variable": "str",
            "bins": "int"
        },
        "objectives": [
            "Validate if the `target_variable` is present and numerical.",
            "Calculate bin edges for the specified number of bins using the target variable.",
            "Apply binning to the target variable and create a categorical column with bin labels.",
            "Return the modified dataframe with the new binned column and bin edges."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_target_variable(df, target_variable, bins):\n    # Validate target variable\n    if target_variable not in df.columns:\n        raise ValueError(f\"Column {target_variable} not found in DataFrame\")\n    \n    if not pd.api.types.is_numeric_dtype(df[target_variable]):\n        raise ValueError(f\"Column {target_variable} must be of numeric type\")\n    \n    # Calculate bin edges\n    bin_edges = np.linspace(df[target_variable].min(), df[target_variable].max(), bins + 1)\n    \n    # Apply binning\n    df[target_variable + '_binned'] = pd.cut(df[target_variable], bins=bin_edges, labels=False, include_lowest=True)\n    \n    return df, bin_edges.tolist()"
    },
    {
        "function_name": "normalize_column_with_null_handling",
        "file_name": "normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column": "str",
            "new_range": "tuple",
            "null_strategy": "str"
        },
        "objectives": [
            "Normalize the specified column's values to a new range given by `new_range`.",
            "Handle missing values in the specified column according to the given `null_strategy` (\"mean\", \"median\", \"drop\").",
            "Add a new column indicating whether the original value was missing.",
            "Return the updated dataframe with the normalized column and the missing value indicator column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def normalize_column_with_null_handling(df, column, new_range, null_strategy):\n    min_val, max_val = df[column].min(), df[column].max()\n    new_min, new_max = new_range\n\n    if null_strategy == \"mean\":\n        fill_value = df[column].mean()\n    elif null_strategy == \"median\":\n        fill_value = df[column].median()\n    elif null_strategy == \"drop\":\n        df = df.dropna(subset=[column])\n        fill_value = None\n    else:\n        raise ValueError(\"Invalid null_strategy. Use 'mean', 'median', or 'drop'.\")\n\n    df[f'{column}_missing'] = df[column].isna().astype(int)\n    \n    if fill_value is not None:\n        df[column].fillna(fill_value, inplace=True)\n    \n    df[column] = ((df[column] - min_val) / (max_val - min_val)) * (new_max - new_min) + new_min\n\n    return df"
    },
    {
        "function_name": "groupby_rolling_mean",
        "file_name": "rolling_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "groupby_cols": "list",
            "agg_col": "str",
            "window_size": "int"
        },
        "objectives": [
            "Validate that the groupby columns and aggregation column exist in the dataframe.",
            "Group the dataframe by specified columns and calculate the rolling mean on the aggregation column.",
            "Handle edge cases where the window size is larger than the group size.",
            "Store the rolling mean in a new column and return the updated dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def groupby_rolling_mean(df, groupby_cols, agg_col, window_size):\n    for col in groupby_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' is not found in the DataFrame\")\n    if agg_col not in df.columns:\n        raise ValueError(f\"Aggregation column '{agg_col}' is not found in the DataFrame\")\n\n    df = df.sort_values(groupby_cols)\n    df[f'{agg_col}_rolling_mean'] = df.groupby(groupby_cols)[agg_col].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n\n    return df"
    },
    {
        "function_name": "winsorize_column",
        "file_name": "winsorization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column": "str",
            "lower_limit": "float",
            "upper_limit": "float"
        },
        "objectives": [
            "Validate that the given column exists in the dataframe.",
            "Apply Winsorization to the specified column, capping outliers to the lower and upper limits.",
            "Ensure that any missing values in the specified column are preserved, rather than being capped.",
            "Return the updated dataframe with the winsorized column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def winsorize_column(df, column, lower_limit, upper_limit):\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' is not found in the DataFrame\")\n\n    original_na = df[column].isna()\n    df[column] = np.where(df[column] < lower_limit, lower_limit, df[column])\n    df[column] = np.where(df[column] > upper_limit, upper_limit, df[column])\n    \n    df[column] = df[column].where(~original_na, np.nan)\n\n    return df"
    },
    {
        "function_name": "outlier_analysis",
        "file_name": "outlier_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Identify and mark outliers in the specified column using z-score method.",
            "Exclude marked outliers from the calculations of mean and median for the column.",
            "Compute and return a dictionary containing mean, median, and outlier count of the column.",
            "Include a check for the minimal number of non-missing values required to perform calculations."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def outlier_analysis(df, column, threshold):\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[column]):\n        raise ValueError(f\"Column '{column}' must be numeric\")\n\n    col_vals = df[column].dropna()\n    if len(col_vals) < 3:\n        raise ValueError(f\"Not enough non-missing values in '{column}' to perform analysis\")\n    \n    # Step 1: Identify and mark outliers using z-score method\n    z_scores = np.abs((col_vals - col_vals.mean()) / col_vals.std())\n    outliers = z_scores > threshold\n    \n    # Step 2: Exclude marked outliers from the calculations of mean and median\n    filtered_vals = col_vals[~outliers]\n    \n    # Step 3: Compute mean, median and outlier count\n    stats = {\n        'mean': filtered_vals.mean(),\n        'median': filtered_vals.median(),\n        'outlier_count': outliers.sum()\n    }\n    \n    return stats"
    },
    {
        "function_name": "row_wise_percent_change",
        "file_name": "row_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_cols": "list of str",
            "new_col": "str"
        },
        "objectives": [
            "Validate that all specified target columns are present in the DataFrame and are numeric.",
            "Create a new column by computing the row-wise percent change of each target column.",
            "Apply a user-defined constant offset to avoid infinite values in case of division by zero.",
            "Return the DataFrame containing the new column with row-wise percent changes."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def row_wise_percent_change(df, target_cols, new_col, offset=1.0):\n    missing_columns = [col for col in target_cols if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"The following columns are not in the dataframe: {missing_columns}\")\n    \n    if not all(pd.api.types.is_numeric_dtype(df[col]) for col in target_cols):\n        raise ValueError(\"All target columns must be numeric\")\n    \n    pct_changes = (df[target_cols].pct_change(axis=1) + offset).dropna(axis=1, how='all')\n    \n    df[new_col] = pct_changes.mean(axis=1)\n    \n    return df"
    },
    {
        "function_name": "label_early_dates",
        "file_name": "date_labelling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col": "str"
        },
        "objectives": [
            "Validate the presence of the specified column in the DataFrame.",
            "Compute the date range length between the minimum and maximum dates in `col`.",
            "Create a new boolean column indicating whether each date is within the first 10% of the total range length.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def label_early_dates(df, col):\n    if col not in df.columns:\n        raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[col]):\n        df[col] = pd.to_datetime(df[col])\n    \n    date_range_length = (df[col].max() - df[col].min()).days\n    threshold_date = df[col].min() + pd.to_timedelta(int(0.1 * date_range_length), unit='d')\n    \n    df['is_early'] = df[col] <= threshold_date\n    \n    return df"
    },
    {
        "function_name": "mode_of_sparse_columns",
        "file_name": "column_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "new_col": "str"
        },
        "objectives": [
            "Validate the presence of the unique identifier column (`id_col`).",
            "Compute the number of unique values in each column and store that in a dictionary.",
            "Create a new column based on the mode of the columns having the smallest number of unique values when grouped by `id_col`.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def mode_of_sparse_columns(df, id_col, new_col):\n    if id_col not in df.columns:\n        raise ValueError(f\"Column '{id_col}' not found in DataFrame\")\n    \n    unique_counts = {col: df[col].nunique() for col in df.columns if df[col].dtype == 'O'}\n    \n    sparse_column = min(unique_counts, key=unique_counts.get)\n    \n    df[new_col] = df.groupby(id_col)[sparse_column].transform(lambda x: x.mode()[0] if not x.mode().empty else None)\n    \n    return df"
    },
    {
        "function_name": "rolling_mean_normalize",
        "file_name": "numeric_transformations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "num_col": "str"
        },
        "objectives": [
            "Validate the presence of the specified numeric column in the DataFrame.",
            "Compute the rolling mean for the column with a window size of 5 and fill null values caused by rolling operation forwards.",
            "Normalize the rolling mean column to have values between 0 and 1.",
            "Return the modified DataFrame with the new normalized rolling mean column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_mean_normalize(df, num_col):\n    if num_col not in df.columns:\n        raise ValueError(f\"Column '{num_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[num_col]):\n        raise ValueError(f\"Column '{num_col}' must be numeric\")\n    \n    rolling_mean_col = f\"{num_col}_rolling_mean\"\n    df[rolling_mean_col] = df[num_col].rolling(window=5).mean().fillna(method='bfill')\n    \n    min_val = df[rolling_mean_col].min()\n    max_val = df[rolling_mean_col].max()\n    \n    df[rolling_mean_col] = (df[rolling_mean_col] - min_val) / (max_val - min_val)\n    \n    return df"
    },
    {
        "function_name": "log_transform_and_normalize",
        "file_name": "transformer.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "new_col": "str"
        },
        "objectives": [
            "Verify that the specified target column exists in the dataframe and is of numerical type.",
            "Create a new column by applying the logarithm function to the target column values.",
            "Normalize the newly created column using z-score normalization.",
            "Return the modified dataframe with the new normalized column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import zscore",
            "import numpy as np"
        ],
        "function_def": "def log_transform_and_normalize(df, target_col, new_col):\n    # Step 1: Verify that the target column exists and is numerical\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column {target_col} is not present or not of numerical type\")\n\n    # Step 2: Apply logarithm function to the target column values\n    df[new_col] = np.log1p(df[target_col])\n\n    # Step 3: Normalize the new column using z-score normalization\n    df[new_col] = zscore(df[new_col])\n    \n    return df"
    },
    {
        "function_name": "summarize_time_series",
        "file_name": "time_series_summary.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "target_col": "str",
            "time_window": "str"
        },
        "objectives": [
            "Validate that `date_col` is a valid datetime column.",
            "Group data by the specified `time_window` (e.g., 'daily', 'weekly', 'monthly') and calculate the sum of `target_col` within each group.",
            "Combine the grouped sums into a new DataFrame with `date_col` adjusted to the corresponding time window's start date.",
            "Return the new DataFrame containing the summarized data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def summarize_time_series(df, date_col, target_col, time_window):\n    if date_col not in df.columns or target_col not in df.columns:\n        raise ValueError(\"Specified columns are not in the DataFrame\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column '{date_col}' must be a datetime type\")\n    \n    df = df.copy()\n    \n    time_window_dict = {\n        'daily': 'D',\n        'weekly': 'W',\n        'monthly': 'M'\n    }\n    \n    if time_window not in time_window_dict:\n        raise ValueError(\"Invalid time window. Use 'daily', 'weekly', or 'monthly'\")\n    \n    time_window_code = time_window_dict[time_window]\n    df.set_index(date_col, inplace=True)\n    grouped_df = df[target_col].resample(time_window_code).sum().reset_index()\n    \n    return grouped_df"
    },
    {
        "function_name": "apply_weighted_sum",
        "file_name": "weighted_sum.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "label_col": "str",
            "value_col": "List[str]",
            "weight": "float"
        },
        "objectives": [
            "Validate that all specified columns exist in the DataFrame.",
            "Multiply values in `value_col` by `weight`.",
            "Sum the weighted values to compute a weighted sum for each row.",
            "Create a new column with the computed weighted sum.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def apply_weighted_sum(df, label_col, value_col, weight):\n    if label_col not in df.columns:\n        raise ValueError(f\"Column '{label_col}' not found in DataFrame\")\n    \n    for col in value_col:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n    \n    df = df.copy()\n    for col in value_col:\n        df[col] *= weight\n    \n    df['weighted_sum'] = df[value_col].sum(axis=1)\n    \n    return df"
    },
    {
        "function_name": "normalize_segment_metrics",
        "file_name": "segment_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "segment_col": "str",
            "metrics": "list of str"
        },
        "objectives": [
            "Validate the presence of `segment_col` and ensure all columns in `metrics` exist and are numeric.",
            "Normalize the columns in `metrics` within each unique segment in `segment_col`.",
            "Add new columns with suffix '_normalized' for each metric to store the normalized values.",
            "Return the DataFrame with the new normalized columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def normalize_segment_metrics(df, segment_col, metrics):\n    if segment_col not in df.columns:\n        raise ValueError(f\"Column '{segment_col}' not found in DataFrame\")\n    for metric in metrics:\n        if metric not in df.columns:\n            raise ValueError(f\"Metric column '{metric}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[metric]):\n            raise ValueError(f\"Metric column '{metric}' must be numeric\")\n    \n    df = df.copy()\n    for segment in df[segment_col].unique():\n        segment_mask = df[segment_col] == segment\n        segment_data = df.loc[segment_mask, metrics]\n        normalized_data = (segment_data - segment_data.mean()) / segment_data.std()\n        for metric in metrics:\n            df.loc[segment_mask, f\"{metric}_normalized\"] = normalized_data[metric]\n    \n    return df"
    },
    {
        "function_name": "compute_weighted_average",
        "file_name": "weighted_average.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "weight_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Validate that the specified group column, weight column, and target column exist.",
            "Compute the weighted average of the target column for each group defined by the group column.",
            "Create a new DataFrame with the group column and its corresponding weighted average.",
            "Return this new DataFrame sorted by the weighted average in descending order."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_weighted_average(df, group_col, weight_col, target_col):\n    missing_columns = [col for col in [group_col, weight_col, target_col] if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"The following columns are not in the DataFrame: {missing_columns}\")\n\n    grouped = df.groupby(group_col).apply(lambda x: (x[weight_col] * x[target_col]).sum() / x[weight_col].sum())\n    weighted_average_df = grouped.reset_index()\n    weighted_average_df.columns = [group_col, 'weighted_average']\n    \n    weighted_average_df = weighted_average_df.sort_values(by='weighted_average', ascending=False).reset_index(drop=True)\n    \n    return weighted_average_df"
    },
    {
        "function_name": "add_top_n_word_counts",
        "file_name": "word_counts.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "n": "int"
        },
        "objectives": [
            "Validate that the `text_col` exists and is a string type.",
            "Tokenize the text in `text_col` and compute the frequency of each word.",
            "Extract the top `n` most frequent words and create new columns for each word count.",
            "Return the modified DataFrame with these additional columns indicating the counts of the top `n` words."
        ],
        "import_lines": [
            "import pandas as pd",
            "from collections import Counter",
            "from sklearn.feature_extraction.text import CountVectorizer"
        ],
        "function_def": "def add_top_n_word_counts(df, text_col, n):\n    if text_col not in df.columns:\n        raise ValueError(f\"Column {text_col} not found in DataFrame\")\n    if not pd.api.types.is_string_dtype(df[text_col]):\n        raise ValueError(f\"Column {text_col} must be of string type\")\n    \n    vectorizer = CountVectorizer(max_features=n)\n    word_counts = vectorizer.fit_transform(df[text_col])\n    top_words = vectorizer.get_feature_names_out()\n    \n    word_count_df = pd.DataFrame(word_counts.toarray(), columns=top_words)\n    df = pd.concat([df.reset_index(drop=True), word_count_df.reset_index(drop=True)], axis=1)\n    \n    return df"
    },
    {
        "function_name": "filter_rare_categories",
        "file_name": "category_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list of str",
            "threshold": "float"
        },
        "objectives": [
            "Validate that all `categorical_cols` exist and are of object dtype.",
            "For each categorical column, compute the frequency distribution of each category.",
            "Filter out categories that have a frequency below the specified `threshold` percentage of the total.",
            "Replace categories under the threshold with the value 'Other' and return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_rare_categories(df, categorical_cols, threshold):\n    df = df.copy()\n    for col in categorical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_object_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be of object dtype\")\n        \n        total_count = len(df[col])\n        frequency = df[col].value_counts(normalize=True)\n        rare_categories = frequency[frequency < threshold].index.tolist()\n        \n        df[col] = df[col].apply(lambda x: 'Other' if x in rare_categories else x)\n    \n    return df"
    },
    {
        "function_name": "create_lagged_features",
        "file_name": "feature_lagging.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "features": "list of str",
            "periods": "list of int"
        },
        "objectives": [
            "Validate that `date_col` exists, is of datetime type, and that all `features` exist and are numeric.",
            "Create lagged versions of each feature in `features`, lagged by each period in `periods`.",
            "Name the new columns with the pattern \"{feature}_lag_{period}\".",
            "Ensure the DataFrame is sorted by `date_col` before creating lags, then drop NaNs resulting from lag operations, and return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_lagged_features(df, date_col, features, periods):\n    if date_col not in df.columns:\n        raise ValueError(f\"Column '{date_col}' not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column '{date_col}' must be of datetime type\")\n\n    for feature in features:\n        if feature not in df.columns:\n            raise ValueError(f\"Feature column '{feature}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[feature]):\n            raise ValueError(f\"Feature column '{feature}' must be numeric\")\n    \n    df = df.copy()\n    df.sort_values(by=date_col, inplace=True)\n\n    for period in periods:\n        for feature in features:\n            df[f\"{feature}_lag_{period}\"] = df[feature].shift(period)\n    \n    df.dropna(inplace=True)\n    return df"
    },
    {
        "function_name": "conditional_mean",
        "file_name": "conditional_statistics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "value_col": "str",
            "conditions": "dict"
        },
        "objectives": [
            "Validate that `value_col` exists and all keys in `conditions` exist as columns in the DataFrame.",
            "Evaluate a set of complex conditions specified in `conditions`, which map column names to functions that return boolean Series.",
            "Filter the DataFrame based on these conditions, ensuring all rows meet all specified conditions.",
            "Compute the mean of the `value_col` for the rows that meet the conditions, and return this mean value."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def conditional_mean(df, value_col, conditions):\n    if value_col not in df.columns:\n        raise ValueError(f\"Value column '{value_col}' not found in DataFrame\")\n    \n    for col in conditions.keys():\n        if col not in df.columns:\n            raise ValueError(f\"Condition column '{col}' not found in DataFrame\")\n    \n    df = df.copy()\n    condition_mask = pd.Series([True] * len(df))\n    \n    for col, condition_fn in conditions.items():\n        condition_mask &= condition_fn(df[col])\n    \n    filtered_df = df[condition_mask]\n    value_mean = filtered_df[value_col].mean()\n    \n    return value_mean"
    },
    {
        "function_name": "resample_and_count_events",
        "file_name": "event_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "event_id_col": "str",
            "count_col": "str"
        },
        "objectives": [
            "Ensure 'time_col' and 'event_id_col' exist and are of appropriate types.",
            "Group the DataFrame by 'event_id_col' and resample by 'time_col' on a daily basis.",
            "Count the occurrences of each event per day to populate 'count_col'.",
            "Return the resampled DataFrame with event counts."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_count_events(df, time_col, event_id_col, count_col):\n    # Step 1: Validate 'time_col' and 'event_id_col'\n    if time_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n        raise ValueError(f\"Column '{time_col}' must be present and of datetime type in DataFrame\")\n    if event_id_col not in df.columns:\n        raise ValueError(f\"Column '{event_id_col}' must be present in DataFrame\")\n    \n    # Step 2: Set index to 'time_col' and group by 'event_id_col'\n    df.set_index(time_col, inplace=True)\n    grouped_df = df.groupby(event_id_col).resample('D').size().reset_index(name=count_col)\n    \n    return grouped_df"
    },
    {
        "function_name": "create_interaction_terms",
        "file_name": "data_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Validate that 'categorical_col' is present and of categorical type.",
            "One-hot encode the categorical column.",
            "Create interaction terms between one-hot encoded columns and target column.",
            "Return the DataFrame with interaction terms."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_interaction_terms(df, categorical_col, target_col):\n    # Step 1: Validate 'categorical_col'\n    if categorical_col not in df.columns or not pd.api.types.is_categorical_dtype(df[categorical_col]):\n        raise ValueError(f\"Column '{categorical_col}' must be present and of categorical type in DataFrame\")\n    \n    # Step 2: One-hot encode the categorical column\n    one_hot_encoded = pd.get_dummies(df[categorical_col], prefix=categorical_col)\n    \n    # Step 3: Create interaction terms\n    for col in one_hot_encoded.columns:\n        df[f'{col}_x_{target_col}'] = one_hot_encoded[col] * df[target_col]\n    \n    return df"
    },
    {
        "function_name": "pivot_and_summarize",
        "file_name": "pivot_summary.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "pivot_columns": "List[str]",
            "value_column": "str",
            "agg_funcs": "List[str]"
        },
        "objectives": [
            "Perform a pivot operation on `df` using `pivot_columns` and aggregate `value_column` using specified aggregation functions in `agg_funcs`.",
            "Create summary statistics using each aggregation for the pivoted data.",
            "Flatten the resulting multi-level column indices and preserve meaningful column names.",
            "Merge the pivoted and aggregated results back into the original dataframe without altering row order."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def pivot_and_summarize(df, pivot_columns, value_column, agg_funcs):\n    pivot_df = df.pivot_table(index=pivot_columns, values=value_column, aggfunc=agg_funcs)\n    \n    pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]\n    pivot_df.reset_index(inplace=True)\n    \n    df = pd.merge(df, pivot_df, on=pivot_columns, how='left')\n    return df"
    },
    {
        "function_name": "tfidf_with_trends",
        "file_name": "tfidf_trends.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_columns": "List[str]",
            "trends_history": "pandas.Series"
        },
        "objectives": [
            "For each specified `text_columns`, tokenize the text contents.",
            "Count the occurrences of each unique token and create a term frequency dataframe.",
            "Normalize the terms using TF-IDF (Term Frequency-Inverse Document Frequency) method.",
            "Merge TF-IDF scores back into the original dataframe, ensuring terms have historical relevance based on `trends_history`."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer"
        ],
        "function_def": "def tfidf_with_trends(df, text_columns, trends_history):\n    df = df.copy()\n    \n    for col in text_columns:\n        if col in df.columns:\n            tfidf = TfidfVectorizer()\n            tfidf_matrix = tfidf.fit_transform(df[col].fillna(''))\n            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n            \n            # Multiply TF-IDF scores by trends history weights\n            for term in tfidf_df.columns:\n                if term in trends_history.index:\n                    tfidf_df[term] *= trends_history[term]\n                    \n            tfidf_df = tfidf_df.add_prefix(f'{col}_tfidf_')\n            df = pd.concat([df, tfidf_df], axis=1)\n            \n    return df"
    },
    {
        "function_name": "handle_outliers",
        "file_name": "outlier_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "reference_col": "str"
        },
        "objectives": [
            "Confirm that the 'reference_col' exists and is either datetime or numerical type.",
            "Compute Z-scores for all numerical columns based on interquartile range scaling with respect to 'reference_col'.",
            "Detect and replace outliers (Z-score > 3 or < -3) in these columns with the median.",
            "Return the modified DataFrame with outliers handled based on Z-scores."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def handle_outliers(df, reference_col):\n    if reference_col not in df.columns or not pd.api.types.is_numeric_dtype(df[reference_col]):\n        raise ValueError(f\"Column '{reference_col}' not found or not of numerical type\")\n    \n    modified_df = df.copy()\n    for col in df.select_dtypes(include='number').columns:\n        if col != reference_col:\n            Q1 = df[col].quantile(0.25)\n            Q3 = df[col].quantile(0.75)\n            IQR = Q3 - Q1\n\n            if IQR == 0:\n                continue\n\n            median = df[col].median()\n            z_scores = (df[col] - median) / IQR\n            modified_df.loc[z_scores.abs() > 3, col] = median\n    return modified_df"
    },
    {
        "function_name": "normalize_categorical_frequencies",
        "file_name": "frequency_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list"
        },
        "objectives": [
            "Ensure all specified categorical columns are present in the DataFrame and are not null values.",
            "Calculate the frequency of each category in the specified columns.",
            "Normalize these frequencies to obtain probability distributions.",
            "Return a dictionary mapping columns to their respective probability distributions."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def normalize_categorical_frequencies(df, categorical_cols):\n    for col in categorical_cols:\n        if col not in df.columns or df[col].isnull().any():\n            raise ValueError(f\"Column '{col}' is not present or contains null values\")\n\n    normalized_frequencies = {}\n\n    for col in categorical_cols:\n        freq = df[col].value_counts(normalize=True)\n        normalized_frequencies[col] = freq.to_dict()\n    \n    return normalized_frequencies"
    },
    {
        "function_name": "fill_and_encode",
        "file_name": "data_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "specific_cols": "list"
        },
        "objectives": [
            "Validate that all columns in `specific_cols` exist in the dataframe.",
            "Handle missing values in these columns by performing forward fill followed by backward fill.",
            "Detect and encode categorical columns within `specific_cols` using one-hot encoding.",
            "Return the modified dataframe with missing values handled and categorical columns encoded."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def fill_and_encode(df, specific_cols):\n    # Validate that all specific_cols exist in the dataframe\n    missing_columns = [col for col in specific_cols if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"Columns {missing_columns} not found in DataFrame\")\n    \n    df = df.copy()\n    \n    # Handle missing values by forward fill followed by backward fill\n    df[specific_cols] = df[specific_cols].ffill().bfill()\n    \n    # Handle categorical columns with one-hot encoding\n    for col in specific_cols:\n        if pd.api.types.is_categorical_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):\n            df = pd.get_dummies(df, columns=[col], drop_first=True)\n    \n    return df"
    },
    {
        "function_name": "add_time_features",
        "file_name": "time_feature_extraction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str"
        },
        "objectives": [
            "Validate that `datetime_col` exists and is of datetime type.",
            "Derive and add new time-based features including day, month, year, day of week, and is_weekend from `datetime_col`.",
            "Handle any missing values in `datetime_col` by filling them with the mode of the column.",
            "Return the modified dataframe with the newly created time features."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_time_features(df, datetime_col):\n    if datetime_col not in df.columns:\n        raise ValueError(f\"Column '{datetime_col}' not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n        raise ValueError(f\"Column '{datetime_col}' must be of datetime type\")\n    \n    df = df.copy()\n    \n    # Handle missing values in `datetime_col` by filling them with the mode\n    mode = df[datetime_col].mode()[0]\n    df[datetime_col].fillna(mode, inplace=True)\n    \n    # Derive new time-based features\n    df['day'] = df[datetime_col].dt.day\n    df['month'] = df[datetime_col].dt.month\n    df['year'] = df[datetime_col].dt.year\n    df['day_of_week'] = df[datetime_col].dt.dayofweek\n    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n    \n    return df"
    },
    {
        "function_name": "combine_features_with_pca",
        "file_name": "pca_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_prefix": "str"
        },
        "objectives": [
            "Identify columns that start with `feature_prefix`.",
            "Combine these columns using Principal Component Analysis (PCA) and reduce them to a single feature.",
            "Preserve the variance explained by the first principal component (should be above a threshold of 90%).",
            "Return the modified dataframe with the new PCA feature added."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def combine_features_with_pca(df, feature_prefix):\n    # Identify columns that start with `feature_prefix`\n    feature_cols = [col for col in df.columns if col.startswith(feature_prefix)]\n    if not feature_cols:\n        raise ValueError(f\"No columns found with prefix '{feature_prefix}'\")\n    \n    df = df.copy()\n    \n    # Combine these columns using PCA\n    pca = PCA(n_components=1)\n    principal_component = pca.fit_transform(df[feature_cols])\n    variance_explained = pca.explained_variance_ratio_[0]\n    \n    if variance_explained < 0.90:\n        raise ValueError(f\"Principal component does not explain at least 90% of the variance\")\n    \n    # Add the new PCA feature to the dataframe\n    df[f'{feature_prefix}_pca'] = principal_component\n    \n    return df"
    },
    {
        "function_name": "filter_and_aggregate_categories",
        "file_name": "category_aggregator.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "agg_col": "str",
            "threshold": "int"
        },
        "objectives": [
            "Validate presence and types for `cat_col` and `agg_col`, ensuring `agg_col` is numeric.",
            "Identify unique categories in `cat_col` that appear more frequently than `threshold` times.",
            "Aggregate (sum) `agg_col` for the remaining categories after filtering out categories below the threshold.",
            "Append the aggregate values as new columns in the original dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_and_aggregate_categories(df, cat_col, agg_col, threshold):\n    if cat_col not in df.columns:\n        raise ValueError(f\"Column '{cat_col}' not found in DataFrame\")\n    if agg_col not in df.columns:\n        raise ValueError(f\"Column '{agg_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[agg_col]):\n        raise ValueError(f\"Column '{agg_col}' must be numeric\")\n    \n    category_counts = df[cat_col].value_counts()\n    filtered_categories = category_counts[category_counts > threshold].index\n    \n    agg_values = df[df[cat_col].isin(filtered_categories)].groupby(cat_col)[agg_col].sum().reset_index()\n    for _, row in agg_values.iterrows():\n        df.loc[df[cat_col] == row[cat_col], f'{agg_col}_sum'] = row[agg_col]\n    \n    return df"
    },
    {
        "function_name": "impute_and_evaluate",
        "file_name": "imputation_evaluator.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "impute_col": "str"
        },
        "objectives": [
            "Validate `target_col` and `impute_col` presence, and ensure `impute_col` is numeric.",
            "Identify missing values in `target_col`.",
            "Impute missing values in `target_col` using the mean of `impute_col` for the same row.",
            "Calculate the R-squared value for the imputed `target_col` against `impute_col`."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.metrics import r2_score"
        ],
        "function_def": "def impute_and_evaluate(df, target_col, impute_col):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if impute_col not in df.columns:\n        raise ValueError(f\"Column '{impute_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[impute_col]):\n        raise ValueError(f\"Column '{impute_col}' must be numeric\")\n    \n    df = df.copy()\n    missing_mask = df[target_col].isnull()\n    df.loc[missing_mask, target_col] = df.loc[missing_mask, impute_col].mean()\n    \n    r2 = r2_score(df[impute_col], df[target_col])\n    print(f\"R-squared value for imputed '{target_col}': {r2}\")\n    \n    return df"
    },
    {
        "function_name": "pivot_and_summarize",
        "file_name": "pivot_summarizer.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "pivot_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Validate `pivot_col` and `value_col` presence and ensure `value_col` is numeric.",
            "Pivot the dataframe with unique values from `pivot_col` as columns and sum of `value_col` as values.",
            "Identify and remove any columns from the pivot table that contain zero sum.",
            "Append the pivot table to the original dataframe as new rows with an identifying prefix."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def pivot_and_summarize(df, pivot_col, value_col):\n    if pivot_col not in df.columns:\n        raise ValueError(f\"Column '{pivot_col}' not found in DataFrame\")\n    if value_col not in df.columns:\n        raise ValueError(f\"Column '{value_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"Column '{value_col}' must be numeric\")\n    \n    pivot_df = df.pivot_table(index=df.index, columns=pivot_col, values=value_col, aggfunc='sum').fillna(0)\n    pivot_df = pivot_df.loc[:, (pivot_df != 0).any(axis=0)]\n    \n    pivot_df.columns = [f'{col}_pivot_sum' for col in pivot_df.columns]\n    \n    result_df = pd.concat([df, pivot_df.reset_index(drop=True)], axis=0)\n    \n    return result_df.reset_index(drop=True)"
    },
    {
        "function_name": "compute_time_differences",
        "file_name": "time_difference_calculator.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str",
            "grp_col": "str"
        },
        "objectives": [
            "Validate `datetime_col` and `grp_col` presence, ensuring `datetime_col` is of datetime type.",
            "Create new columns for year, month, and day extracted from the `datetime_col`.",
            "Group by the `grp_col` and within each group, compute the time difference in days for consecutive rows based on the `datetime_col`.",
            "Return the dataframe with new datetime-related columns and the computed time differences."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_time_differences(df, datetime_col, grp_col):\n    if datetime_col not in df.columns:\n        raise ValueError(f\"Column '{datetime_col}' not found in DataFrame\")\n    if grp_col not in df.columns:\n        raise ValueError(f\"Column '{grp_col}' not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n        raise ValueError(f\"Column '{datetime_col}' must be of datetime type\")\n    \n    df = df.copy()\n    df['year'] = df[datetime_col].dt.year\n    df['month'] = df[datetime_col].dt.month\n    df['day'] = df[datetime_col].dt.day\n    \n    df['time_diff'] = df.groupby(grp_col)[datetime_col].diff().dt.days\n    \n    return df"
    },
    {
        "function_name": "add_z_score",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "new_col_name": "str"
        },
        "objectives": [
            "Ensure 'target_col' is present in the DataFrame and is numeric.",
            "Compute the z-score for each value in 'target_col'.",
            "Create a new column with 'new_col_name' to store these z-scores.",
            "Return the DataFrame with the new z-score column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import zscore"
        ],
        "function_def": "def add_z_score(df, target_col, new_col_name):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n\n    df[new_col_name] = zscore(df[target_col])\n    \n    return df"
    },
    {
        "function_name": "filter_text_by_frequency",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "min_freq": "int"
        },
        "objectives": [
            "Ensure 'text_col' is present in the DataFrame.",
            "Split the text data in 'text_col' into individual words.",
            "Calculate the frequency of each word across the entire DataFrame.",
            "Filter out words that appear less than 'min_freq' times, returning a DataFrame with filtered words."
        ],
        "import_lines": [
            "import pandas as pd",
            "from collections import Counter"
        ],
        "function_def": "def filter_text_by_frequency(df, text_col, min_freq):\n    if text_col not in df.columns:\n        raise ValueError(f\"Column '{text_col}' not found in DataFrame\")\n\n    all_words = ' '.join(df[text_col]).split()\n    word_freq = Counter(all_words)\n    \n    df[f'{text_col}_filtered'] = df[text_col].apply(lambda x: ' '.join([word for word in x.split() if word_freq[word] >= min_freq]))\n    \n    return df"
    },
    {
        "function_name": "categorical_summary",
        "file_name": "data_summary.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_column": "str",
            "categories": "list"
        },
        "objectives": [
            "Convert `datetime_column` to datetime type and extract year, month, and day into separate columns.",
            "Pivot the dataframe to create a summary table for each category in `categories`.",
            "In the summary table, calculate the count, mean, and standard deviation for each numeric column by category.",
            "Return the summary table."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def categorical_summary(df, datetime_column, categories):\n    # Step 1: Convert to datetime and extract year, month, day\n    df[datetime_column] = pd.to_datetime(df[datetime_column])\n    df['year'] = df[datetime_column].dt.year\n    df['month'] = df[datetime_column].dt.month\n    df['day'] = df[datetime_column].dt.day\n    \n    # Step 2: Pivot to create a summary table for each category\n    summary_tables = {}\n    for category in categories:\n        category_summary = df.groupby(category).agg({\n            col: ['count', 'mean', 'std'] for col in df.select_dtypes(include=['number']).columns\n        })\n        summary_tables[category] = category_summary\n    \n    return summary_tables"
    },
    {
        "function_name": "temporal_feature_engineering",
        "file_name": "time_series_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "value_col": "str"
        },
        "objectives": [
            "Identify and interpolate missing values in `value_col` using linear interpolation.",
            "Calculate the rolling mean, median, and standard deviation for a window of 7 days for `value_col`.",
            "Create lagged versions of the `value_col` with lags of 1, 2, and 3 periods.",
            "Return the transformed DataFrame with interpolated, rolling, and lagged features."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def temporal_feature_engineering(df, value_col):\n    # Step 1: Interpolate missing values\n    df[value_col] = df[value_col].interpolate(method='linear')\n    \n    # Step 2: Calculate rolling statistics\n    df[f'{value_col}_rolling_mean'] = df[value_col].rolling(window=7).mean()\n    df[f'{value_col}_rolling_median'] = df[value_col].rolling(window=7).median()\n    df[f'{value_col}_rolling_std'] = df[value_col].rolling(window=7).std()\n    \n    # Step 3: Create lagged features\n    df[f'{value_col}_lag1'] = df[value_col].shift(1)\n    df[f'{value_col}_lag2'] = df[value_col].shift(2)\n    df[f'{value_col}_lag3'] = df[value_col].shift(3)\n    \n    return df"
    },
    {
        "function_name": "standardize_and_calculate_skewness",
        "file_name": "normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "num_cols": "list"
        },
        "objectives": [
            "Validate that all columns in num_cols exist and are numerical.",
            "Standardize each specified numerical column (z-score normalization).",
            "Calculate each column's skewness and add the results to a dictionary.",
            "Return the modified dataframe and the skewness dictionary."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.stats import skew"
        ],
        "function_def": "def standardize_and_calculate_skewness(df, num_cols):\n    # Step 1: Validate numerical columns\n    for col in num_cols:\n        if col not in df.columns or not np.issubdtype(df[col].dtype, np.number):\n            raise ValueError(f\"Column {col} must be present and of numerical type.\")\n    \n    skewness_dict = {}\n    \n    # Step 2: Standardize each column\n    for col in num_cols:\n        df[col] = (df[col] - df[col].mean()) / df[col].std()\n    \n    # Step 3: Calculate skewness\n    for col in num_cols:\n        skewness_dict[col] = skew(df[col].dropna())\n    \n    return df, skewness_dict"
    },
    {
        "function_name": "transform_and_impute",
        "file_name": "data_transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_cols": "list",
            "categorial_cols": "list",
            "n_bins": "int",
            "missing_value_replacement": "dict"
        },
        "objectives": [
            "Replace missing values in 'numerical_cols' and 'categorical_cols' based on 'missing_value_replacement' dict.",
            "Bin the numerical columns into 'n_bins' equally sized bins and add these as new categorical columns.",
            "Generate dummy variables for the original and binned categorical columns.",
            "Combine all transformed columns back into the original dataframe and return it."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def transform_and_impute(df, numerical_cols, categorial_cols, n_bins, missing_value_replacement):\n    # Step 1: Replace missing values\n    df.fillna(missing_value_replacement, inplace=True)\n    \n    # Step 2: Bin numerical columns\n    for col in numerical_cols:\n        df[f'{col}_binned'] = pd.cut(df[col], bins=n_bins, labels=False)\n    \n    # Step 3: Generate dummy variables\n    categorical_features = categorial_cols + [f'{col}_binned' for col in numerical_cols]\n    df = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n    \n    return df"
    },
    {
        "function_name": "forward_fill_and_interaction",
        "file_name": "data_enrichment.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list",
            "step": "int"
        },
        "objectives": [
            "Apply a forward fill on the specified 'columns' to propagate the last valid observation forward.",
            "Apply user-defined transformations (like scaling, log) to these columns.",
            "Generate pairwise interaction terms between transformed columns.",
            "Append these interaction terms to the original dataframe and return it."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def forward_fill_and_interaction(df, columns, step):\n    df[columns] = df[columns].ffill(limit=step)\n    \n    for col in columns:\n        df[f'{col}_scaled'] = (df[col] - df[col].mean()) / df[col].std()\n        df[f'{col}_log'] = np.log1p(df[col])\n    \n    interaction_terms = pd.DataFrame(index=df.index)\n    for i, col1 in enumerate(columns):\n        for col2 in columns[i+1:]:\n            interaction_terms[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n    \n    return pd.concat([df, interaction_terms], axis=1)"
    },
    {
        "function_name": "user_action_time_analysis",
        "file_name": "user_activity.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "user_col": "str",
            "action_col": "str",
            "time_col": "str"
        },
        "objectives": [
            "Ensure user, action, and time columns exist and have appropriate data types (string for user and action, datetime for time).",
            "Calculate the time difference in seconds between consecutive actions of the same user.",
            "Create a new column with cumulative time spent by users on their actions.",
            "Return the DataFrame with the new cumulative time column added."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def user_action_time_analysis(df, user_col, action_col, time_col):\n    if user_col not in df.columns or not pd.api.types.is_string_dtype(df[user_col]):\n        raise ValueError(f\"Column '{user_col}' is not a string column in DataFrame\")\n    if action_col not in df.columns or not pd.api.types.is_string_dtype(df[action_col]):\n        raise ValueError(f\"Column '{action_col}' is not a string column in DataFrame\")\n    if time_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n        raise ValueError(f\"Column '{time_col}' is not a datetime column in DataFrame\")\n    \n    df = df.sort_values(by=[user_col, time_col])\n    df['time_diff_seconds'] = df.groupby(user_col)[time_col].diff().dt.total_seconds().fillna(0)\n\n    df['cumulative_time_seconds'] = df.groupby(user_col)['time_diff_seconds'].cumsum()\n\n    return df"
    },
    {
        "function_name": "consecutive_date_differences",
        "file_name": "date_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_cols": "list"
        },
        "objectives": [
            "Calculate the difference in days between consecutive date columns in 'date_cols'.",
            "Create a new column for each date difference.",
            "Identify and handle rows with negative date differences.",
            "Return the modified dataframe with these date difference columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def consecutive_date_differences(df, date_cols):\n    df = df.copy()\n    \n    # Step 1: Convert date columns to datetime if not already\n    for col in date_cols:\n        df[col] = pd.to_datetime(df[col])\n    \n    # Step 2: Calculate differences between consecutive date columns\n    for i in range(1, len(date_cols)):\n        date_diff_col = f'{date_cols[i-1]}_to_{date_cols[i]}_days'\n        df[date_diff_col] = (df[date_cols[i]] - df[date_cols[i-1]]).dt.days\n        \n        # Step 3: Handle negative differences by setting them to 0\n        df[date_diff_col] = df[date_diff_col].apply(lambda x: x if x >= 0 else 0)\n    \n    return df"
    },
    {
        "function_name": "impute_missing_values",
        "file_name": "imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_impute": "list",
            "method": "str"
        },
        "objectives": [
            "Identify gaps (i.e., NaN values) in the columns listed in 'cols_to_impute'.",
            "Impute these gaps using the specified 'method' which could be 'mean', 'median', or 'mode'.",
            "Check and handle edge cases where entire columns could be NaN.",
            "Return the modified dataframe with imputed data."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_missing_values(df, cols_to_impute, method):\n    df = df.copy()\n    \n    for col in cols_to_impute:\n        if method == 'mean':\n            fill_value = df[col].mean()\n        elif method == 'median':\n            fill_value = df[col].median()\n        elif method == 'mode':\n            fill_value = df[col].mode()[0]\n        else:\n            raise ValueError(\"Method must be either 'mean', 'median', or 'mode'\")\n        \n        # Step 1: Identify gaps\n        if df[col].isna().sum() == len(df[col]):\n            raise ValueError(f\"Entire column '{col}' is NaN\")\n        \n        # Step 2: Impute using specified method\n        df[col] = df[col].fillna(fill_value)\n    \n    return df"
    },
    {
        "function_name": "compute_average_time_between_events",
        "file_name": "event_time_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "event_col": "str",
            "timestamp_col": "str"
        },
        "objectives": [
            "Validate that `id_col`, `event_col`, and `timestamp_col` are present in the dataframe and that `timestamp_col` is a datetime type.",
            "Sort the dataframe by `id_col` and `timestamp_col`.",
            "Compute the time difference in minutes for consecutive events for the same ID, forming a new column `time_diff_minutes`.",
            "Compute the average time between events for the same ID and store it in a new dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_average_time_between_events(df, id_col, event_col, timestamp_col):\n    if id_col not in df.columns:\n        raise ValueError(f\"Column '{id_col}' not found in the DataFrame\")\n    if event_col not in df.columns:\n        raise ValueError(f\"Column '{event_col}' not found in the DataFrame\")\n    if timestamp_col not in df.columns:\n        raise ValueError(f\"Column '{timestamp_col}' not found in the DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n        raise ValueError(f\"Column '{timestamp_col}' must be of datetime type\")\n    \n    # Step 2: Sort the DataFrame\n    df = df.sort_values(by=[id_col, timestamp_col])\n    \n    # Step 3: Compute time difference in minutes for consecutive events\n    df['time_diff_minutes'] = df.groupby(id_col)[timestamp_col].diff().dt.total_seconds() / 60.0\n    \n    # Step 4: Compute average time between events\n    avg_time_between_events = df.groupby(id_col)['time_diff_minutes'].mean().reset_index()\n    avg_time_between_events = avg_time_between_events.rename(columns={'time_diff_minutes': 'avg_time_diff_minutes'})\n    \n    return avg_time_between_events"
    },
    {
        "function_name": "outlier_detection_and_correction",
        "file_name": "outlier_handler.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "value_col": "str"
        },
        "objectives": [
            "Validate the presence of `value_col` in the dataframe.",
            "Identify the presence of outliers using the IQR method and mark them in a new column `is_outlier`.",
            "Replace outliers with the median value of `value_col`.",
            "Return the cleaned dataframe and a count of total outliers detected."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def outlier_detection_and_correction(df, value_col):\n    if value_col not in df.columns:\n        raise ValueError(f\"Column '{value_col}' not found in the DataFrame\")\n    \n    # Step 2: Identify outliers using IQR method\n    Q1 = df[value_col].quantile(0.25)\n    Q3 = df[value_col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    df['is_outlier'] = (df[value_col] < lower_bound) | (df[value_col] > upper_bound)\n    \n    # Step 3: Replace outliers with the median value\n    median_value = df[value_col].median()\n    df.loc[df['is_outlier'], value_col] = median_value\n    \n    # Step 4: Return cleaned df and outlier count\n    outlier_count = df['is_outlier'].sum()\n    return df.drop(columns=['is_outlier']), outlier_count"
    },
    {
        "function_name": "find_and_group_regex_matches",
        "file_name": "regex_grouping.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list",
            "regex": "str",
            "groupby_col": "str"
        },
        "objectives": [
            "Identify all rows in the dataframe where any of the specified columns contain a match to the given regex pattern.",
            "For those rows, group them by the specified 'groupby_col' and calculate the count of rows in each group that match the regex pattern.",
            "Append a new column to the dataframe that holds the percentage of rows in each group that match the regex pattern.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import re"
        ],
        "function_def": "def find_and_group_regex_matches(df, columns, regex, groupby_col):\n    if not all(col in df.columns for col in columns):\n        missing_cols = [col for col in columns if col not in df.columns]\n        raise ValueError(f\"Columns {missing_cols} are not in the dataframe.\")\n    \n    regex_matches = df[columns].apply(lambda x: x.astype(str).str.contains(regex, na=False, case=True))\n    df['matches'] = regex_matches.any(axis=1)\n    \n    grouped = df.groupby(groupby_col)['matches'].agg(['sum', 'count'])\n    grouped['match_percentage'] = (grouped['sum'] / grouped['count']) * 100\n    \n    group_match_map = grouped['match_percentage'].to_dict()\n    df['group_match_percentage'] = df[groupby_col].map(group_match_map)\n    \n    return df"
    },
    {
        "function_name": "calculate_event_gaps",
        "file_name": "event_gaps.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "time_col": "str",
            "event_col": "str"
        },
        "objectives": [
            "Convert the time column to datetime format.",
            "Sort the dataframe by the id_col and time_col.",
            "For each unique id in the id_col, calculate the time difference between consecutive events.",
            "Create a new column to store these time differences, and if the time exceeds a certain threshold, mark it as an event gap.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_event_gaps(df, id_col, time_col, event_col):\n    df[time_col] = pd.to_datetime(df[time_col])\n    df = df.sort_values(by=[id_col, time_col])\n    \n    df['time_diff'] = df.groupby(id_col)[time_col].diff().dt.total_seconds()\n    threshold = df['time_diff'].median() * 2\n    df['event_gap'] = df['time_diff'] > threshold\n    \n    return df"
    },
    {
        "function_name": "string_categorization_and_target_scaling",
        "file_name": "categorical_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "string_cols": "list",
            "categories": "list",
            "target_col": "str"
        },
        "objectives": [
            "For each string column specified, categorize its values based on the given categories list.",
            "Create one-hot encoded columns for each category in each string column.",
            "Find the frequency of each category in the new one-hot encoded columns.",
            "Normalize the target column by its range, scaling the values between 0 and 1.",
            "Return the modified dataframe with new one-hot encoded columns and the normalized target column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def string_categorization_and_target_scaling(df, string_cols, categories, target_col):\n    for col in string_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not in the dataframe.\")\n        \n        for category in categories:\n            df[f\"{col}_{category}\"] = df[col].apply(lambda x: 1 if category in str(x) else 0)\n        \n        df.drop(columns=[col], inplace=True)\n    \n    for new_col in [f\"{col}_{category}\" for col in string_cols for category in categories]:\n        df[new_col] = df[new_col] / df[new_col].sum()\n    \n    df[f\"{target_col}_scaled\"] = (df[target_col] - df[target_col].min()) / (df[target_col].max() - df[target_col].min())\n    \n    return df"
    },
    {
        "function_name": "bin_and_compute_statistics",
        "file_name": "binned_statistics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "numeric_cols": "list",
            "bin_col": "str",
            "bins": "int"
        },
        "objectives": [
            "Ensure the existence and proper types of the category_col, numeric_cols and bin_col, and that the data types are appropriate.",
            "Perform binning on the specified bin_col and calculate the mean and standard deviation of numeric_cols for each bin.",
            "Create a new DataFrame with bin ranges as index and the calculated statistics as columns.",
            "Add columns indicating the percentage of each bin's numeric values relative to the whole dataset."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_and_compute_statistics(df, category_col, numeric_cols, bin_col, bins):\n    # Step 1: Validate columns\n    if category_col not in df.columns or not pd.api.types.is_categorical_dtype(df[category_col]):\n        raise ValueError(f\"Category column {category_col} is not present or not categorical\")\n    \n    for col in numeric_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Numeric column {col} is not present or not numeric\")\n\n    if bin_col not in df.columns or not pd.api.types.is_numeric_dtype(df[bin_col]):\n        raise ValueError(f\"Bin column {bin_col} is not present or not numeric\")\n\n    # Step 2: Perform binning and calculate statistics\n    df['bin'] = pd.cut(df[bin_col], bins=bins)\n    bin_stats = df.groupby('bin')[numeric_cols].agg(['mean', 'std'])\n    \n    # Flatten columns\n    bin_stats.columns = ['_'.join(col).strip() for col in bin_stats.columns.values]\n    \n    # Step 3: Calculate percentage contribution of each bin's numeric values\n    total_values = df[numeric_cols].sum()\n    bin_totals = df.groupby('bin')[numeric_cols].sum()\n    bin_percentages = bin_totals.div(total_values).multiply(100).add_suffix('_percent')\n    \n    result_df = pd.concat([bin_stats, bin_percentages], axis=1)\n    \n    return result_df.reset_index()"
    },
    {
        "function_name": "weighted_aggregates",
        "file_name": "weighted_statistics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "groupby_col": "str",
            "weight_col": "str",
            "target_cols": "list"
        },
        "objectives": [
            "Ensure groupby_col, weight_col and target_cols exist and weights are numeric and positive.",
            "Compute weighted mean and variance for target columns within each group defined by groupby_col.",
            "Return the DataFrame aggregated by the weighted statistics.",
            "Handle edge cases where weights or groups might have no valid entries."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def weighted_aggregates(df, groupby_col, weight_col, target_cols):\n    # Step 1: Validate presence and type of columns\n    if groupby_col not in df.columns:\n        raise ValueError(f\"Groupby column {groupby_col} is not present\")\n    \n    if weight_col not in df.columns or not pd.api.types.is_numeric_dtype(df[weight_col]):\n        raise ValueError(f\"Weight column {weight_col} is not present or not numeric\")\n    \n    for col in target_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Target column {col} is not present or not numeric\")\n    \n    grouped = df.groupby(groupby_col)\n    \n    weighted_stats = []\n    # Step 2: Compute weighted statistics\n    for name, group in grouped:\n        weight = group[weight_col]\n        stat_dict = {}\n        stat_dict[groupby_col] = name\n        \n        for col in target_cols:\n            weighted_mean = (group[col] * weight).sum() / weight.sum()\n            weighted_var = ((group[col] - weighted_mean)**2 * weight).sum() / weight.sum()\n            \n            stat_dict[f\"{col}_weighted_mean\"] = weighted_mean\n            stat_dict[f\"{col}_weighted_var\"] = weighted_var\n        \n        weighted_stats.append(stat_dict)\n    \n    # Convert list of dictionaries to DataFrame\n    result_df = pd.DataFrame(weighted_stats)\n    \n    return result_df"
    },
    {
        "function_name": "aggregate_time_series",
        "file_name": "time_series.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "period": "str"
        },
        "objectives": [
            "Convert the specified time column \"time_col\" to datetime format.",
            "Aggregate the dataframe by the specified time period (daily, weekly, monthly, etc.).",
            "Fill any missing time periods with forward fill method.",
            "Return the time-aggregated and filled dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_time_series(df, time_col, period):\n    # Step 1: Convert the time column to datetime format\n    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n    \n    # Step 2: Set the time column as the index\n    df.set_index(time_col, inplace=True)\n    \n    # Step 3: Resample the dataframe based on the specified period\n    if period not in ['D', 'W', 'M']:\n        raise ValueError(\"Period must be one of ['D', 'W', 'M']\")\n    resampled_df = df.resample(period).mean()\n    \n    # Step 4: Fill missing time periods with forward fill method\n    resampled_df.fillna(method='ffill', inplace=True)\n    \n    return resampled_df.reset_index()"
    },
    {
        "function_name": "clean_text_data",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "stop_words": "List[str]"
        },
        "objectives": [
            "Verify the existence and string data type of the specified text column.",
            "Remove punctuation from the text data.",
            "Apply tokenization to convert sentences into lists of words.",
            "Remove specified stop words from the tokenized text.",
            "Return the dataframe with the cleaned and tokenized text data."
        ],
        "import_lines": [
            "import pandas as pd",
            "import string",
            "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
        ],
        "function_def": "def clean_text_data(df, text_col, stop_words):\n    # Step 1: Validate the existence and string data type of the text column\n    if text_col not in df.columns:\n        raise ValueError(f\"Text column '{text_col}' is not found in the DataFrame\")\n    if not pd.api.types.is_string_dtype(df[text_col]):\n        raise TypeError(f\"Text column '{text_col}' must be of string type\")\n    \n    # Step 2: Remove punctuation from the text data\n    df[text_col] = df[text_col].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n\n    # Step 3: Apply tokenization to convert sentences into lists of words\n    df[text_col] = df[text_col].apply(lambda x: x.split())\n    \n    # Step 4: Remove specified stop words from the tokenized text\n    stop_words_set = set(stop_words)\n    df[text_col] = df[text_col].apply(lambda x: [word for word in x if word.lower() not in stop_words_set])\n    \n    return df"
    },
    {
        "function_name": "get_top_groups_by_mean",
        "file_name": "group_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "value_col": "str",
            "group_cols": "List[str]",
            "max_num_groups": "int"
        },
        "objectives": [
            "Verify the existence of all specified columns in the DataFrame.",
            "Group the data based on the specified group columns.",
            "Sort the groups based on the mean of the specified value column in descending order.",
            "Return the top 'max_num_groups' groups along with their means."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def get_top_groups_by_mean(df, value_col, group_cols, max_num_groups):\n    # Step 1: Validate the existence of all specified columns\n    if value_col not in df.columns:\n        raise ValueError(f\"Value column '{value_col}' is not found in the DataFrame\")\n    for col in group_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Group column '{col}' is not found in the DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[value_col]):\n            raise TypeError(f\"Value column '{value_col}' must be of numeric type\")\n\n    # Step 2: Group the data based on the specified group columns\n    grouped_df = df.groupby(group_cols)[value_col].mean().reset_index()\n    \n    # Step 3: Sort the groups based on the mean of the specified value column in descending order\n    grouped_df = grouped_df.sort_values(by=value_col, ascending=False)\n    \n    # Step 4: Return the top 'max_num_groups' groups along with their means\n    top_groups_df = grouped_df.head(max_num_groups)\n    \n    return top_groups_df"
    },
    {
        "function_name": "transaction_features",
        "file_name": "transaction_analysis.py",
        "parameters": {
            "transactions": "pandas.DataFrame",
            "id_col": "str",
            "timestamp_col": "str",
            "amount_col": "str"
        },
        "objectives": [
            "Ensure `timestamp_col` is in datetime format and `amount_col` is numeric.",
            "Create time-based features such as day of week, month, and hour from `timestamp_col`.",
            "Group by `id_col` and calculate cumulative sum and rolling sum (window=5) of `amount_col` for each group.",
            "Append these new features to the original DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def transaction_features(transactions, id_col, timestamp_col, amount_col):\n    # Ensure timestamp_col is in datetime format\n    transactions[timestamp_col] = pd.to_datetime(transactions[timestamp_col])\n    \n    # Ensure amount_col is numeric\n    if not pd.api.types.is_numeric_dtype(transactions[amount_col]):\n        raise ValueError(f\"Column '{amount_col}' must be numeric\")\n    \n    # Create time-based features\n    transactions['day_of_week'] = transactions[timestamp_col].dt.dayofweek\n    transactions['month'] = transactions[timestamp_col].dt.month\n    transactions['hour'] = transactions[timestamp_col].dt.hour\n    \n    # Group and calculate cumulative sum and rolling sum\n    transactions = transactions.sort_values(by=[id_col, timestamp_col])\n    transactions['cumulative_sum'] = transactions.groupby(id_col)[amount_col].cumsum()\n    transactions['rolling_sum'] = transactions.groupby(id_col)[amount_col].rolling(window=5).sum().reset_index(level=0, drop=True)\n    \n    return transactions"
    },
    {
        "function_name": "zscore_threshold_summary",
        "file_name": "zscore_summary.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "value_col": "str",
            "threshold": "numeric"
        },
        "objectives": [
            "Validate that `value_col` is numeric.",
            "Generate and append a z-score column based on `value_col`.",
            "Identify rows where z-score exceeds a specified threshold.",
            "Create a summary DataFrame with the count and percentage of values exceeding the threshold for each unique value in a specified column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def zscore_threshold_summary(df, value_col, threshold):\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"Column '{value_col}' must be numeric\")\n    \n    # Calculate z-scores\n    mean_val = df[value_col].mean()\n    std_val = df[value_col].std()\n    df['z_score'] = (df[value_col] - mean_val) / std_val\n    \n    # Identify rows exceeding z-score threshold\n    df['exceeds_threshold'] = (df['z_score'].abs() > threshold).astype(int)\n    \n    # Create summary DataFrame\n    summary = df.groupby('exceeds_threshold').size().reset_index(name='count')\n    summary['percentage'] = (summary['count'] / len(df)) * 100\n    \n    return summary"
    },
    {
        "function_name": "rolling_window_aggregation",
        "file_name": "timeseries_preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "agg_funcs": "dict",
            "rolling_window": "int"
        },
        "objectives": [
            "Convert the specified date column to datetime and set it as the DataFrame index.",
            "Check that the provided aggregation functions are valid; raise an error if not.",
            "Apply a rolling window transformation on the time series data with the specified window size.",
            "Aggregate the rolling windows using the provided functions, and return the transformed DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_window_aggregation(df, date_col, agg_funcs, rolling_window):\n    # Convert date column to datetime and set as index\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.set_index(date_col, inplace=True)\n\n    # Validate aggregation functions\n    valid_funcs = [\"mean\", \"sum\", \"std\", \"var\", \"min\", \"max\"]\n    for func in agg_funcs.values():\n        if func not in valid_funcs:\n            raise ValueError(f\"Invalid aggregation function: {func}\")\n\n    # Apply rolling window transformation\n    rolling_df = df.rolling(window=rolling_window)\n\n    # Aggregate using the specified functions\n    aggregated_df = rolling_df.agg(agg_funcs)\n    \n    return aggregated_df"
    },
    {
        "function_name": "k_fold_assignment",
        "file_name": "cross_validation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "fold_column": "str",
            "num_folds": "int"
        },
        "objectives": [
            "Validate that the specified target column exists.",
            "Implement k-fold cross-validation splitting on the DataFrame.",
            "Assign each row to a fold by creating a new column and populate it with fold numbers.",
            "Return the DataFrame with the additional fold assignment column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import KFold"
        ],
        "function_def": "def k_fold_assignment(df, target_col, fold_column, num_folds):\n    if target_col not in df.columns:\n        raise ValueError(f\"Target column {target_col} is not present\")\n    \n    df = df.copy()\n    \n    kf = KFold(n_splits=num_folds)\n    \n    df[fold_column] = -1\n    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n        df.loc[val_idx, fold_column] = fold\n    \n    return df"
    },
    {
        "function_name": "categorize_target_by_ranges",
        "file_name": "range_categorization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "value_ranges": "dict"
        },
        "objectives": [
            "Ensure the `target_col` is present in the dataframe and is numeric.",
            "For each range in `value_ranges`, create a column indicating whether the value in `target_col` falls within the specified range.",
            "Calculate the proportion of values in `target_col` that fall within each range.",
            "Return a dataframe with the new columns and proportions."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def categorize_target_by_ranges(df, target_col, value_ranges):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n\n    for range_name, (low, high) in value_ranges.items():\n        df[f'{target_col}_{range_name}'] = df[target_col].apply(lambda x: low <= x < high)\n    \n    proportions = {}\n    for range_name in value_ranges.keys():\n        proportions[f'{range_name}_proportion'] = df[f'{target_col}_{range_name}'].mean()\n    \n    proportion_df = pd.DataFrame(proportions, index=[0])\n    result_df = pd.concat([df, proportion_df], axis=1)\n    \n    return result_df"
    },
    {
        "function_name": "linear_regression_prediction",
        "file_name": "regression_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col1": "str",
            "col2": "str"
        },
        "objectives": [
            "Ensure the presence of `col1` and `col2` in the dataframe and validate their data types are numeric.",
            "Calculate the correlation between `col1` and `col2`.",
            "Perform a linear regression using `col1` as the predictor and `col2` as the response variable.",
            "Create a new column in the dataframe with the predicted `col2` values from the linear regression model.",
            "Return the dataframe with the new column and the correlation value as a separate return value."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import LinearRegression"
        ],
        "function_def": "def linear_regression_prediction(df, col1, col2):\n    if col1 not in df.columns:\n        raise ValueError(f\"Column '{col1}' not found in DataFrame\")\n    if col2 not in df.columns:\n        raise ValueError(f\"Column '{col2}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[col1]):\n        raise ValueError(f\"Column '{col1}' must be numeric\")\n    if not pd.api.types.is_numeric_dtype(df[col2]):\n        raise ValueError(f\"Column '{col2}' must be numeric\")\n    \n    correlation = df[[col1, col2]].corr().iloc[0, 1]\n    \n    X = df[[col1]].values.reshape(-1, 1)\n    y = df[col2].values\n    model = LinearRegression()\n    model.fit(X, y)\n    df[f'predicted_{col2}'] = model.predict(X)\n \n    return df, correlation"
    },
    {
        "function_name": "compute_rolling_average",
        "file_name": "rolling_average.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "price_col": "str",
            "window_size": "int"
        },
        "objectives": [
            "Ensure the 'price_col' is numeric. If not, raise a ValueError.",
            "Normalize the 'price_col' by subtracting the mean and dividing by the standard deviation.",
            "Apply a rolling window calculation of the normalized prices to compute the average of every 'window_size' prices.",
            "Add a new column 'Rolling_Avg_Price' to the dataframe with the calculated rolling averages and replace NaN values with the mean of the column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_rolling_average(df, price_col, window_size):\n    if not pd.api.types.is_numeric_dtype(df[price_col]):\n        raise ValueError(f\"Column '{price_col}' must be numeric\")\n        \n    df = df.copy()\n    df[price_col] = (df[price_col] - df[price_col].mean()) / df[price_col].std()\n    \n    df['Rolling_Avg_Price'] = df[price_col].rolling(window=window_size).mean()\n    df['Rolling_Avg_Price'].fillna(df['Rolling_Avg_Price'].mean(), inplace=True)\n    \n    return df"
    },
    {
        "function_name": "group_proportion_sums",
        "file_name": "group_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "operation_col": "str"
        },
        "objectives": [
            "Verify that 'group_col' and 'operation_col' are present and are of appropriate types (category and numeric, respectively).",
            "Group the dataframe by 'group_col' and calculate the group-wise sum of 'operation_col'.",
            "Normalize the group-wise sums so that the sum of all groups equals 1 (proportion of the total).",
            "Return a dictionary with group names as keys and their normalized sums as values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_proportion_sums(df, group_col, operation_col):\n    # Step 1: Validate columns\n    if group_col not in df.columns or not pd.api.types.is_categorical_dtype(df[group_col]):\n        raise ValueError(f\"Column {group_col} is not present or not of categorical type\")\n    if operation_col not in df.columns or not pd.api.types.is_numeric_dtype(df[operation_col]):\n        raise ValueError(f\"Column {operation_col} is not present or not of numeric type\")\n\n    # Step 2: Group by, Sum\n    group_sums = df.groupby(group_col)[operation_col].sum()\n    \n    # Step 3: Normalize sums to proportions\n    total_sum = group_sums.sum()\n    group_proportions = group_sums / total_sum\n\n    # Step 4: Return result\n    return group_proportions.to_dict()"
    },
    {
        "function_name": "date_feature_engineering",
        "file_name": "datetime_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Ensure 'datetime_col' is of datetime type and 'target_col' is numeric.",
            "Extract day of the week, week of the year, and month from 'datetime_col' and create separate columns for each.",
            "For each of these new columns, calculate the average of 'target_col' and return a summary dictionary with these averages.",
            "Return both the modified dataframe and the summary dictionary."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def date_feature_engineering(df, datetime_col, target_col):\n    # Step 1: Validate columns\n    if datetime_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n        raise ValueError(f\"Column {datetime_col} is not present or not of datetime type\")\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column {target_col} is not present or not of numeric type\")\n    \n    # Step 2: Extract date features\n    df['day_of_week'] = df[datetime_col].dt.dayofweek\n    df['week_of_year'] = df[datetime_col].dt.isocalendar().week\n    df['month'] = df[datetime_col].dt.month\n    \n    # Step 3: Calculate averages\n    summary = {\n        'day_of_week_avg': df.groupby('day_of_week')[target_col].mean().to_dict(),\n        'week_of_year_avg': df.groupby('week_of_year')[target_col].mean().to_dict(),\n        'month_avg': df.groupby('month')[target_col].mean().to_dict()\n    }\n    \n    # Step 4: Return modified dataframe and summary\n    return df, summary"
    },
    {
        "function_name": "identify_high_proportion_categories",
        "file_name": "category_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Verify that `category_col` and `value_col` exist in the DataFrame and that `value_col` is numerical.",
            "For each category in `category_col`, calculate the total sum and the proportion of the total sum contributed by each category.",
            "Identify categories where the proportion of the total sum is greater than a specified threshold (e.g., 10%).",
            "Append a new column indicating whether each row belongs to a high-proportion category."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def identify_high_proportion_categories(df, category_col, value_col, threshold=0.10):\n    # Step 1: Ensure columns are present and `value_col` is numerical\n    if category_col not in df.columns or value_col not in df.columns:\n        raise ValueError(f\"Columns {category_col} or {value_col} are not present in the DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"Column {value_col} must be numeric\")\n    \n    # Step 2: Calculate total sum and proportion for each category\n    category_sum = df.groupby(category_col)[value_col].sum()\n    total_sum = category_sum.sum()\n    category_proportion = category_sum / total_sum\n    \n    # Step 3: Identify high-proportion categories\n    high_proportion_categories = category_proportion[category_proportion > threshold].index.tolist()\n    \n    # Step 4: Append new column indicating high-proportion category\n    df['is_high_proportion'] = df[category_col].isin(high_proportion_categories)\n    \n    return df"
    },
    {
        "function_name": "merge_and_concatenate",
        "file_name": "merge_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_merge": "list of str",
            "id_col": "str"
        },
        "objectives": [
            "Validate that `id_col` and all columns in `cols_to_merge` exist in the DataFrame.",
            "Pivot the DataFrame such that `id_col` becomes the index and the `cols_to_merge` are completely merged into one column, keeping track of their original column names.",
            "After pivoting, iterate over each new row to concatenate the values into a single string, maintaining the order.",
            "Return the DataFrame with the merged column and drop the original columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_and_concatenate(df, cols_to_merge, id_col):\n    # Validate columns\n    missing_columns = [col for col in [id_col] + cols_to_merge if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"The following columns are not in the DataFrame: {missing_columns}\")\n\n    # Melt the DataFrame\n    melted_df = pd.melt(df, id_vars=[id_col], value_vars=cols_to_merge, var_name='original_col', value_name='value')\n\n    # Pivot and concatenate values\n    pivoted_df = melted_df.pivot_table(index=id_col, columns='original_col', values='value', aggfunc=lambda x: ' '.join(str(v) for v in x if pd.notna(v)))\n    pivoted_df.columns = [f\"{col}_merged\" for col in pivoted_df.columns]\n    \n    # Reset index to have a flat DataFrame\n    result_df = pivoted_df.reset_index()\n    \n    return result_df"
    },
    {
        "function_name": "detect_rolling_anomalies",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Validate that `date_col` and `value_col` exist in the DataFrame.",
            "Parse `date_col` to datetime and sort the DataFrame based on `date_col`.",
            "Implement a rolling window calculation to compute the average and standard deviation of `value_col` over a window of 7 days.",
            "Detect and annotate anomalies where the value of `value_col` exceeds three standard deviations from the rolling mean."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_rolling_anomalies(df, date_col, value_col):\n    # Validate columns\n    missing_columns = [col for col in [date_col, value_col] if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"The following columns are not in the DataFrame: {missing_columns}\")\n\n    # Parse dates and sort\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(by=date_col)\n\n    # Rolling calculations\n    df['rolling_mean'] = df[value_col].rolling(window=7).mean()\n    df['rolling_std'] = df[value_col].rolling(window=7).std()\n\n    # Detect anomalies\n    df['anomaly'] = (df[value_col] > (df['rolling_mean'] + 3 * df['rolling_std'])) | (df[value_col] < (df['rolling_mean'] - 3 * df['rolling_std']))\n\n    return df"
    },
    {
        "function_name": "encode_and_filter",
        "file_name": "encoding_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list of str",
            "target_col": "str"
        },
        "objectives": [
            "Validate that all `categorical_cols` and `target_col` exist in the DataFrame.",
            "For each categorical column, create dummy/one-hot encoded variables.",
            "Calculate the correlation coefficient of each new variable with respect to the `target_col`.",
            "Keep only those dummy variables with a correlation above 0.3, and return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def encode_and_filter(df, categorical_cols, target_col):\n    # Validate columns\n    missing_columns = [col for col in categorical_cols + [target_col] if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"The following columns are not in the DataFrame: {missing_columns}\")\n\n    # One-hot encoding\n    dummy_df = pd.get_dummies(df[categorical_cols], drop_first=True)\n    \n    # Calculate correlations and filter\n    corr_with_target = dummy_df.corrwith(df[target_col]).abs()\n    high_corr_cols = corr_with_target[corr_with_target > 0.3].index\n\n    # Create result DataFrame\n    result_df = pd.concat([df, dummy_df[high_corr_cols]], axis=1).drop(columns=categorical_cols)\n\n    return result_df"
    },
    {
        "function_name": "compute_composite_score",
        "file_name": "composite_feature_scores.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "features": "list of str",
            "diff_window": "int"
        },
        "objectives": [
            "Validate that all columns in `features` exist in the DataFrame.",
            "For each feature, create a shifted version and calculate the difference over the specified `diff_window`.",
            "Standardize each of the new difference columns.",
            "Aggregate the standardized differences to obtain a composite feature score for each row.",
            "Return the DataFrame including the composite feature score."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_composite_score(df, features, diff_window):\n    # Validate columns\n    missing_columns = [col for col in features if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"The following columns are not in the DataFrame: {missing_columns}\")\n\n    diff_cols = []\n    for feature in features:\n        diff_col_name = f\"{feature}_diff_{diff_window}\"\n        df[diff_col_name] = df[feature].diff(periods=diff_window)\n        diff_cols.append(diff_col_name)\n\n    # Standardize differences\n    standardized_diffs = (df[diff_cols] - df[diff_cols].mean()) / df[diff_cols].std()\n    \n    # Composite score\n    df['composite_feature_score'] = standardized_diffs.sum(axis=1)\n\n    return df"
    },
    {
        "function_name": "conditional_column_creation",
        "file_name": "conditional_columns.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "conditions": "list of (str, str, any)",
            "new_col": "str"
        },
        "objectives": [
            "Validate that each condition element is a 3-tuple in the form (column, operation, value).",
            "Construct and evaluate complex boolean logic from the conditions.",
            "Create a new column `new_col` in the dataframe based on the evaluated boolean logic.",
            "Return the modified dataframe with the new boolean column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def conditional_column_creation(df, conditions, new_col):\n    # Validate conditions\n    for condition in conditions:\n        if not (isinstance(condition, tuple) and len(condition) == 3):\n            raise ValueError(\"Each condition must be a tuple (column, operation, value).\")\n    \n    # Construct and evaluate boolean logic\n    boolean_series = pd.Series(np.ones(len(df), dtype=bool))\n    for col, op, val in conditions:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not in DataFrame\")\n        if op == '==':\n            boolean_series &= (df[col] == val)\n        elif op == '>':\n            boolean_series &= (df[col] > val)\n        elif op == '>=':\n            boolean_series &= (df[col] >= val)\n        elif op == '<':\n            boolean_series &= (df[col] < val)\n        elif op == '<=':\n            boolean_series &= (df[col] <= val)\n        elif op == '!=':\n            boolean_series &= (df[col] != val)\n        else:\n            raise ValueError(f\"Invalid operation '{op}' encountered.\")\n    \n    # Add the new conditional column\n    df[new_col] = boolean_series\n    \n    return df"
    },
    {
        "function_name": "text_analysis_with_sentiment",
        "file_name": "text_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "target_words": "list",
            "sentiment_model": "any"
        },
        "objectives": [
            "Validate that `text_col` is a string type column.",
            "Tokenize the text in the `text_col` and count the occurrences of each word in `target_words`.",
            "Use a pretrained `sentiment_model` to calculate sentiment scores for each row's text.",
            "Append the word count and sentiment scores to the dataframe and return it."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import CountVectorizer"
        ],
        "function_def": "def text_analysis_with_sentiment(df, text_col, target_words, sentiment_model):\n    # Validate text column\n    if text_col not in df.columns or not pd.api.types.is_string_dtype(df[text_col]):\n        raise ValueError(f\"Column '{text_col}' is either missing or not of type string in the DataFrame\")\n    \n    # Tokenize and count occurrences of target words\n    vectorizer = CountVectorizer(vocabulary=target_words)\n    word_counts = vectorizer.transform(df[text_col]).toarray()\n    word_counts_df = pd.DataFrame(word_counts, columns=vectorizer.get_feature_names_out())\n    \n    # Calculate sentiment scores\n    sentiment_scores = df[text_col].apply(sentiment_model.predict)\n    \n    # Append word counts and sentiment scores to the original dataframe\n    df = df.join(word_counts_df)\n    df['sentiment_score'] = sentiment_scores\n    \n    return df"
    },
    {
        "function_name": "group_correlation_pruning",
        "file_name": "correlation_pruning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_groups": "list of lists of str",
            "correlation_threshold": "float",
            "validation_rules": "dict"
        },
        "objectives": [
            "Validate that each sub-list in `col_groups` contains existing columns and follows `validation_rules`.",
            "Calculate correlation matrices for each group of columns.",
            "Prune the columns within each group based on the `correlation_threshold`, keeping only one column from highly correlated pairs.",
            "Return the dataframe with pruned columns and a report on the columns kept/dropped."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_correlation_pruning(df, col_groups, correlation_threshold, validation_rules):\n    # Validate column groups\n    for group in col_groups:\n        for col in group:\n            if col not in df.columns:\n                raise ValueError(f\"Column '{col}' not in DataFrame\")\n            if col in validation_rules and not validation_rules[col](df[col]):\n                raise ValueError(f\"Column '{col}' does not meet validation rule\")\n    \n    # Initialize report\n    report = {\"kept_columns\": [], \"dropped_columns\": []}\n    \n    for group in col_groups:\n        # Calculate correlation matrix\n        corr_matrix = df[group].corr().abs()\n        \n        # DataFrame to keep track of columns to drop\n        to_drop = set()\n        \n        # Iterate over the combinations in the correlation matrix\n        for i in range(len(corr_matrix.columns)):\n            for j in range(i+1, len(corr_matrix.columns)):\n                if corr_matrix.iloc[i, j] > correlation_threshold:\n                    col_to_drop = corr_matrix.columns[j]\n                    if col_to_drop not in to_drop:\n                        to_drop.add(col_to_drop)\n        \n        # Update report and drop columns\n        report[\"kept_columns\"].extend([col for col in group if col not in to_drop])\n        report[\"dropped_columns\"].extend(to_drop)\n        df = df.drop(columns=to_drop)\n    \n    return df, report"
    },
    {
        "function_name": "compute_cov_and_corr",
        "file_name": "statistical_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col1": "str",
            "col2": "str"
        },
        "objectives": [
            "Ensure that \"col1\" and \"col2\" both exist in the DataFrame and are of numerical type.",
            "Compute the covariance between \"col1\" and \"col2\".",
            "Compute the Pearson correlation coefficient between \"col1\" and \"col2\".",
            "Return the computed covariance and correlation coefficient."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_cov_and_corr(df, col1, col2):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"One or both columns do not exist in the DataFrame\")\n\n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise ValueError(\"Both columns must be of numeric type\")\n\n    covariance = df[col1].cov(df[col2])\n    correlation = df[col1].corr(df[col2])\n\n    return covariance, correlation"
    },
    {
        "function_name": "remove_common_words_and_check_empty",
        "file_name": "text_preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_cols": "list",
            "common_words": "list"
        },
        "objectives": [
            "Ensure all columns in \"text_cols\" exist in the DataFrame and are of string type.",
            "For each column in \"text_cols\", remove common words specified in \"common_words\".",
            "Create a binary column for each text column indicating if the resulting text is empty after removing common words.",
            "Return the modified DataFrame and a dictionary indicating the number of empty texts for each text column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_common_words_and_check_empty(df, text_cols, common_words):\n    if not all(col in df.columns for col in text_cols):\n        raise ValueError(\"Some text columns do not exist in the DataFrame\")\n\n    if not all(pd.api.types.is_string_dtype(df[col]) for col in text_cols):\n        raise ValueError(\"All text columns must be of string type\")\n\n    df = df.copy()\n    empty_texts_count = {}\n\n    for col in text_cols:\n        df[col] = df[col].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in common_words]))\n        df[f'{col}_is_empty'] = df[col].str.strip() == ''\n        empty_texts_count[col] = df[f'{col}_is_empty'].sum()\n    \n    return df, empty_texts_count"
    },
    {
        "function_name": "discretize_target_column",
        "file_name": "discretization_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "bins": "list[int]",
            "labels": "list[str]"
        },
        "objectives": [
            "Verify that the target column exists in the DataFrame and is numeric.",
            "Ensure that the length of \"bins\" is one more than the length of \"labels\".",
            "Discretize the values in the target column into the specified bins, assigning the corresponding labels to each bin.",
            "Create a new column named \"<target_col>_discretized\" with the discretized labels and return the DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def discretize_target_column(df, target_col, bins, labels):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n    if len(bins) != len(labels) + 1:\n        raise ValueError(\"Length of bins must be one more than the length of labels\")\n    \n    df = df.copy()\n    df[target_col + '_discretized'] = pd.cut(df[target_col], bins=bins, labels=labels)\n    \n    return df"
    },
    {
        "function_name": "group_standardize_columns",
        "file_name": "standardization_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_standardize": "list[str]",
            "group_col": "str"
        },
        "objectives": [
            "Check that all columns in \"cols_to_standardize\" are present and numeric.",
            "Verify \"group_col\" exists in the DataFrame and is of categorical type.",
            "Compute the mean and standard deviation of each column in \"cols_to_standardize\" within each group defined by \"group_col\".",
            "Standardize each column in \"cols_to_standardize\" within each group using computed mean and standard deviation and return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_standardize_columns(df, cols_to_standardize, group_col):\n    if group_col not in df.columns:\n        raise ValueError(f\"Column '{group_col}' not found in DataFrame\")\n    if not pd.api.types.is_categorical_dtype(df[group_col]):\n        raise ValueError(f\"Column '{group_col}' must be of categorical type\")\n    \n    for col in cols_to_standardize:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n    \n    df = df.copy()\n    group_means = df.groupby(group_col)[cols_to_standardize].transform('mean')\n    group_stds = df.groupby(group_col)[cols_to_standardize].transform('std')\n    \n    for col in cols_to_standardize:\n        df[col] = (df[col] - group_means[col]) / group_stds[col]\n    \n    return df"
    },
    {
        "function_name": "fill_and_cap_values",
        "file_name": "fill_and_cap.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "fill_method": "str",
            "cap_value": "float"
        },
        "objectives": [
            "Identify missing values in the dataframe and fill them using the specified method ('ffill', 'bfill', or 'mean').",
            "For numerical columns, cap the values at a specified threshold.",
            "Identify and flag rows where any column exceeds 1.5 times the interquartile range.",
            "Return the imputed and capped dataframe, along with a DataFrame indicating outlier flags."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def fill_and_cap_values(df, fill_method, cap_value):\n    if fill_method not in ['ffill', 'bfill', 'mean']:\n        raise ValueError(\"fill_method must be 'ffill', 'bfill', or 'mean'\")\n    \n    for col in df.columns:\n        # Step 1: Fill missing values\n        if fill_method == 'mean':\n            df[col] = df[col].fillna(df[col].mean())\n        else:\n            df[col] = df[col].fillna(method=fill_method)\n        \n        # Step 2: Cap numerical values\n        if np.issubdtype(df[col].dtype, np.number):\n            df[col] = df[col].clip(upper=cap_value)\n    \n    # Step 3: Flag outliers\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    outlier_flags = (df > (Q3 + 1.5 * IQR)) | (df < (Q1 - 1.5 * IQR))\n    \n    # Step 4: Return modified dataframe and outlier flags\n    return df, outlier_flags"
    },
    {
        "function_name": "generate_lag_features",
        "file_name": "lag_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "label_col": "str",
            "reference_date": "str"
        },
        "objectives": [
            "Remove future data based on a reference date.",
            "Generate lag features for the label column for different lag periods (e.g., 1, 7, 14 days).",
            "Normalize the lagged features",
            "Return the modified dataframe including lagged features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def generate_lag_features(df, label_col, reference_date):\n    # Step 1: Remove future data\n    df = df[df['date'] <= reference_date].copy()\n    \n    # Step 2: Generate lag features\n    for lag in [1, 7, 14]:\n        df[f'{label_col}_lag_{lag}'] = df[label_col].shift(lag)\n    \n    # Step 3: Normalize the lagged features\n    lag_columns = [f'{label_col}_lag_{lag}' for lag in [1, 7, 14]]\n    scaler = MinMaxScaler()\n    df[lag_columns] = scaler.fit_transform(df[lag_columns])\n    \n    # Step 4: Return modified dataframe\n    return df"
    },
    {
        "function_name": "compute_difference_or_ratio",
        "file_name": "element_wise_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col1": "str",
            "col2": "str",
            "new_col_name": "str"
        },
        "objectives": [
            "Validate both columns exist and are of numeric or datetime types.",
            "Perform element-wise operation (e.g., difference for dates, ratio for numerics) on the specified columns.",
            "Handle any resulting NaN values by providing a default value (e.g., 0 for numerics or min date for datetimes).",
            "Return the dataframe with the new calculated column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_difference_or_ratio(df, col1, col2, new_col_name):\n    # Step 1: Validate columns\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Columns '{col1}' and/or '{col2}' are not present in the dataframe\")\n    \n    if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n        # Step 2: Perform numerical operation\n        df[new_col_name] = df[col1] / df[col2]\n        # Step 4: Handle NaN values\n        df[new_col_name].fillna(0, inplace=True)\n    elif pd.api.types.is_datetime64_any_dtype(df[col1]) and pd.api.types.is_datetime64_any_dtype(df[col2]):\n        # Step 2: Perform datetime operation\n        df[new_col_name] = (df[col1] - df[col2]).dt.days\n        # Step 4: Handle NaN values\n        df[new_col_name].fillna(0, inplace=True)\n    else:\n        raise ValueError(f\"Columns '{col1}' and '{col2}' must be both numeric or both datetime\")\n    \n    return df"
    },
    {
        "function_name": "impute_and_normalize",
        "file_name": "preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols": "List[str]",
            "method": "str"
        },
        "objectives": [
            "Validate the presence of the specified columns in the dataframe and ensure they are of numerical type.",
            "Impute missing values in the specified columns using the specified method (e.g., mean, median, mode).",
            "Detect and remove outliers in the specified columns using the IQR method.",
            "Normalize the remaining values in the specified columns using min-max normalization."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def impute_and_normalize(df, cols, method):\n    # Step 1: Validate the presence of the specified columns and ensure they are numerical\n    for col in cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not of numerical type\")\n    \n    # Step 2: Impute missing values using the specified method\n    if method == 'mean':\n        df[cols] = df[cols].fillna(df[cols].mean())\n    elif method == 'median':\n        df[cols] = df[cols].fillna(df[cols].median())\n    elif method == 'mode':\n        df[cols] = df[cols].fillna(df[cols].mode().iloc[0])\n    else:\n        raise ValueError(\"Imputation method not supported\")\n    \n    # Step 3: Detect and remove outliers using the IQR method\n    for col in cols:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        df = df[~((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR)))]\n\n    # Step 4: Normalize values using min-max normalization\n    df[cols] = (df[cols] - df[cols].min()) / (df[cols].max() - df[cols].min())\n    \n    return df"
    },
    {
        "function_name": "bootstrap_confidence_intervals",
        "file_name": "statistical_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "num_col": "str"
        },
        "objectives": [
            "Ensure the categorical column exists and is of categorical type, and the numerical column exists and is of numerical type.",
            "For each unique value in the categorical column, perform bootstrap resampling of the numerical column and calculate the mean and confidence intervals.",
            "Aggregate these metrics into a new dataframe with the unique values of the categorical column as rows.",
            "Return the aggregated dataframe with bootstrap mean and confidence intervals for each unique value in the categorical column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bootstrap_confidence_intervals(df, cat_col, num_col, n_boot=1000, ci=95):\n    # Step 1: Validate the categorical and numerical columns\n    if cat_col not in df.columns or not pd.api.types.is_categorical_dtype(df[cat_col]):\n        raise ValueError(f\"Column {cat_col} is not present or not of categorical type\")\n    if num_col not in df.columns or not pd.api.types.is_numeric_dtype(df[num_col]):\n        raise ValueError(f\"Column {num_col} is not present or not of numerical type\")\n    \n    results = []\n    \n    # Step 2: Perform bootstrap resampling\n    for cat_value in df[cat_col].unique():\n        sample = df[df[cat_col] == cat_value][num_col].dropna()\n        if len(sample) < 2:\n            raise ValueError(f\"Not enough data for {cat_value} in column {cat_col}\")\n        \n        boot_means = []\n        for _ in range(n_boot):\n            boot_sample = sample.sample(frac=1, replace=True)\n            boot_means.append(boot_sample.mean())\n        \n        # Calculate confidence intervals\n        lower_bound = np.percentile(boot_means, (100 - ci) / 2)\n        upper_bound = np.percentile(boot_means, 100 - (100 - ci) / 2)\n        \n        results.append({\n            cat_col: cat_value,\n            f'{num_col}_mean': np.mean(boot_means),\n            f'{ci}_ci_low': lower_bound,\n            f'{ci}_ci_high': upper_bound\n        })\n    \n    return pd.DataFrame(results)"
    },
    {
        "function_name": "path_distance_and_velocity",
        "file_name": "geospatial_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "path_col": "str",
            "time_col": "str"
        },
        "objectives": [
            "Ensure the path column and time column exist, and the time column is of datetime type.",
            "Parse GPS coordinates from each entry in the path column and unroll the paths into individual points with corresponding timestamps.",
            "Calculate the total distance traveled based on the parsed GPS coordinates for each row using the Haversine formula.",
            "Create a time-series of travel velocity for each row by dividing distance segments by the time delta.",
            "Return the modified dataframe with new columns for total distance and average velocity."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points on the earth (specified in decimal degrees) using the Haversine formula.\n    \"\"\"\n    R = 6371.0 # Radius of Earth in kilometers\n    dlon = np.radians(lon2 - lon1)\n    dlat = np.radians(lat2 - lat1)\n    a = np.sin(dlat / 2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = R * c\n    return distance"
    },
    {
        "function_name": "aggregate_and_merge",
        "file_name": "data_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_by_cols": "list",
            "agg_dict": "dict"
        },
        "objectives": [
            "Group the DataFrame by `group_by_cols` and perform aggregations specified in `agg_dict`.",
            "Flatten the MultiIndex columns resulting from aggregations.",
            "Add aggregate statistic columns back to the original DataFrame.",
            "Return the modified DataFrame with these aggregate statistics included."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_and_merge(df, group_by_cols, agg_dict):\n    grouped_df = df.groupby(group_by_cols).agg(agg_dict)\n    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n    grouped_df = grouped_df.reset_index()\n    \n    df = df.merge(grouped_df, on=group_by_cols, how='left')\n    \n    return df"
    },
    {
        "function_name": "category_percentage_contribution",
        "file_name": "categorical_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Ensure both specified columns exist and are of appropriate types (categorical and numeric respectively).",
            "Compute and add a new column representing the percentage contribution of each category in `cat_col` to the overall sum of `target_col`.",
            "Create a mapping dictionary for each category to its corresponding percentage contribution.",
            "Return the updated dataframe and the mapping dictionary."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def category_percentage_contribution(df, cat_col, target_col):\n    if cat_col not in df.columns or target_col not in df.columns:\n        raise ValueError(f\"Columns '{cat_col}' or '{target_col}' not found in DataFrame\")\n    \n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' is not numeric\")\n    \n    cat_sum = df.groupby(cat_col)[target_col].sum()\n    total_sum = cat_sum.sum()\n    \n    percentage_contrib = (cat_sum / total_sum) * 100\n    \n    df['percentage_contribution'] = df[cat_col].map(percentage_contrib.to_dict())\n    \n    return df, percentage_contrib.to_dict()"
    },
    {
        "function_name": "detect_and_replace_outliers",
        "file_name": "outlier_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "columns_to_replace": "list",
            "window": "int"
        },
        "objectives": [
            "Convert 'date_col' to datetime and set it as the dataframe index.",
            "Detect and replace outliers in 'columns_to_replace' using a rolling window and interquartile range.",
            "Smooth the replaced values using a moving average of specified 'window' size.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def detect_and_replace_outliers(df, date_col, columns_to_replace, window=5):\n    # Step 1: Convert date_col to datetime and set as index\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.set_index(date_col, inplace=True)\n    \n    # Step 2: Detect and replace outliers using IQR\n    for col in columns_to_replace:\n        rolling_window = df[col].rolling(window, center=True)\n        Q1 = rolling_window.quantile(0.25)\n        Q3 = rolling_window.quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outliers = ((df[col] < lower_bound) | (df[col] > upper_bound))\n        \n        df.loc[outliers, col] = np.nan\n    \n    # Step 3: Smooth replaced values using moving average\n    df[columns_to_replace] = df[columns_to_replace].fillna(method='ffill').fillna(method='bfill')\n    df[columns_to_replace] = df[columns_to_replace].rolling(window=window, min_periods=1).mean()\n    \n    return df"
    },
    {
        "function_name": "normalize_and_threshold_groups",
        "file_name": "normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "value_col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Normalize 'value_col' within each group in 'category_col'.",
            "Identify groups where the mean of normalized 'value_col' exceeds the 'threshold'.",
            "Generate summary statistics for these identified groups.",
            "Return the modified dataframe and summary statistics."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def normalize_and_threshold_groups(df, category_col, value_col, threshold):\n    # Step 1: Normalize 'value_col' within each group in 'category_col'\n    df['normalized_value'] = df.groupby(category_col)[value_col].transform(lambda x: (x - x.mean()) / x.std())\n    \n    # Step 2: Identify groups with mean of normalized 'value_col' exceeding 'threshold'\n    group_means = df.groupby(category_col)['normalized_value'].mean().reset_index(name='mean_normalized_value')\n    high_value_groups = group_means[group_means['mean_normalized_value'] > threshold]\n    \n    # Step 3: Generate summary statistics for identified groups\n    summary_stats = high_value_groups.describe()\n    \n    return df, summary_stats"
    },
    {
        "function_name": "remove_outliers",
        "file_name": "outlier_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "outlier_col": "str",
            "method": "str"
        },
        "objectives": [
            "Ensure \"outlier_col\" exists in the dataframe and is of numeric type.",
            "Depending on the chosen \"method\" ('IQR' or 'Z-score'), identify the outliers in the specified column.",
            "If method is 'IQR', compute Q1 and Q3 to determine outliers beyond 1.5 * IQR.",
            "If method is 'Z-score', compute the Z-score to identify outliers beyond a threshold (e.g., |z| > 3).",
            "Return the dataframe excluding the identified outliers."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def remove_outliers(df, outlier_col, method='IQR'):\n    if outlier_col not in df.columns:\n        raise ValueError(f\"Column '{outlier_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[outlier_col]):\n        raise ValueError(f\"Column '{outlier_col}' must be numeric\")\n    if method not in ['IQR', 'Z-score']:\n        raise ValueError(f\"Method '{method}' is not supported. Use 'IQR' or 'Z-score'\")\n\n    if method == 'IQR':\n        Q1 = df[outlier_col].quantile(0.25)\n        Q3 = df[outlier_col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        cleaned_df = df[(df[outlier_col] >= lower_bound) & (df[outlier_col] <= upper_bound)]\n    else:  # Z-score method\n        z_scores = (df[outlier_col] - df[outlier_col].mean()) / df[outlier_col].std()\n        cleaned_df = df[np.abs(z_scores) <= 3]\n\n    return cleaned_df"
    },
    {
        "function_name": "target_encode_categories",
        "file_name": "category_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_cols": "list",
            "target_col": "str"
        },
        "objectives": [
            "For each categorical column in 'cat_cols', encode the categories using Target Encoding.",
            "Compute aggregated statistics like mean and standard deviation of the target column for each category.",
            "Replace the original categorical columns with their corresponding target-encoded values.",
            "Return the modified dataframe with the target-encoded columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def target_encode_categories(df, cat_cols, target_col):\n    df = df.copy()\n    \n    for cat_col in cat_cols:\n        mean_encoding = df.groupby(cat_col)[target_col].mean()\n        std_encoding = df.groupby(cat_col)[target_col].std()\n        \n        encoded_value = df[cat_col].map(mean_encoding)\n        std_value = df[cat_col].map(std_encoding)\n        df[cat_col] = encoded_value.fillna(std_value)  # Replacing with target-encoded values\n\n    return df"
    },
    {
        "function_name": "clean_and_tokenize_text",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_cols": "list"
        },
        "objectives": [
            "For each text column in 'text_cols', perform text cleaning including lowercasing, removing punctuation, and stop-words.",
            "Tokenize the cleaned text into individual words.",
            "Create a new column for each unique word that indicates its count in the rows.",
            "Return the modified dataframe with word count columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import string",
            "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
        ],
        "function_def": "def clean_and_tokenize_text(df, text_cols):\n    df = df.copy()\n\n    for col in text_cols:\n        # Text cleaning\n        df[col] = df[col].str.lower().str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n        df[col] = df[col].apply(lambda x: ' '.join([word for word in x.split() if word not in ENGLISH_STOP_WORDS]))\n\n        # Tokenization and word count\n        word_counts = df[col].str.split().explode().value_counts()\n        for word in word_counts.index:\n            df[f'{col}_{word}_count'] = df[col].apply(lambda x: x.split().count(word))\n        \n    return df"
    },
    {
        "function_name": "encode_top_values",
        "file_name": "encoding_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "k": "int",
            "method": "str"
        },
        "objectives": [
            "Identify the top `k` most frequent values in `target_column`.",
            "Create a new column for each of these top `k` values, encoding whether each row contains one of these top values.",
            "If `method` is 'binary', the new columns should contain binary values indicating presence; if `method` is 'count', they should contain the count of occurrences.",
            "Return the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def encode_top_values(df, target_column, k, method):\n    # Step 1: Identify the top k most frequent values\n    top_values = df[target_column].value_counts().nlargest(k).index\n    \n    # Step 2: Create new columns for each top value\n    for value in top_values:\n        if method == 'binary':\n            df[f'{target_column}_{value}'] = (df[target_column] == value).astype(int)\n        elif method == 'count':\n            df[f'{target_column}_{value}'] = df[target_column].apply(lambda x: 1 if x == value else 0)\n        else:\n            raise ValueError(\"Method should be either 'binary' or 'count'\")\n    \n    return df"
    },
    {
        "function_name": "flag_high_percentile",
        "file_name": "statistical_flags.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_columns": "list",
            "percentile": "float"
        },
        "objectives": [
            "Validate that `percentile` is between 0 and 100.",
            "Calculate the `percentile` value for each column listed in `numerical_columns`.",
            "Create a new column for each original column, containing boolean values indicating if the original value is above the calculated percentile.",
            "Return the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def flag_high_percentile(df, numerical_columns, percentile):\n    if not 0 <= percentile <= 100:\n        raise ValueError(\"Percentile must be between 0 and 100\")\n    \n    df_copy = df.copy()\n    \n    for col in numerical_columns:\n        threshold = df[col].quantile(percentile / 100.0)\n        df_copy[f'{col}_above_{percentile}'] = df[col] > threshold\n    \n    return df_copy"
    },
    {
        "function_name": "scale_columns",
        "file_name": "scaling_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns_to_scale": "list",
            "scale_type": "str"
        },
        "objectives": [
            "Identify and validate scale_type ('minmax' or 'standard').",
            "If `scale_type` is 'minmax', scale each column in `columns_to_scale` to range [0, 1].",
            "If `scale_type` is 'standard', scale each column to have zero mean and unit variance.",
            "Return the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
        ],
        "function_def": "def scale_columns(df, columns_to_scale, scale_type):\n    df_copy = df.copy()\n    \n    if scale_type == 'minmax':\n        scaler = MinMaxScaler()\n        df_copy[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n    elif scale_type == 'standard':\n        scaler = StandardScaler()\n        df_copy[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n    else:\n        raise ValueError(\"Scale type must be either 'minmax' or 'standard'\")\n    \n    return df_copy"
    },
    {
        "function_name": "transform_column",
        "file_name": "transformation_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "methods": "list"
        },
        "objectives": [
            "Validate `methods` contains permissible transformations ('log', 'sqrt', 'square').",
            "For each method in `methods`, create a new column applying the respective transformation to `target_column`.",
            "Handle negative or zero values appropriately (e.g., add a small constant before log transformation).",
            "Return the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def transform_column(df, target_column, methods):\n    df_copy = df.copy()\n    permissible_methods = {'log', 'sqrt', 'square'}\n    \n    for method in methods:\n        if method not in permissible_methods:\n            raise ValueError(f\"Method {method} is not permissible\")\n        \n        if method == 'log':\n            df_copy[f'{target_column}_log'] = np.log(df[target_column] + 1)\n        elif method == 'sqrt':\n            df_copy[f'{target_column}_sqrt'] = np.sqrt(df[target_column])\n        elif method == 'square':\n            df_copy[f'{target_column}_square'] = df[target_column] ** 2\n    \n    return df_copy"
    },
    {
        "function_name": "aggregate_by_category",
        "file_name": "aggregation_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_column": "str",
            "category_column": "str"
        },
        "objectives": [
            "Group the dataframe by `id_column` and `category_column`.",
            "Within each group, calculate aggregated statistics: total count, total sum, average, and standard deviation of all numerical columns.",
            "Flatten the multi-level columns resulting from aggregation into single-level by concatenating the original column name and the aggregation function.",
            "Return the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_by_category(df, id_column, category_column):\n    # Step 1: Group by `id_column` and `category_column`\n    grouped_df = df.groupby([id_column, category_column]).agg(['count', 'sum', 'mean', 'std'])\n    \n    # Step 2: Flatten multi-level columns\n    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n    \n    return grouped_df.reset_index()"
    },
    {
        "function_name": "feature_interaction_with_mappings",
        "file_name": "feature_interaction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_mappings": "dict",
            "target_col": "str"
        },
        "objectives": [
            "Ensure all specified mapped columns and the target column are present in the dataframe.",
            "Apply specified category mappings to designated columns.",
            "Create new feature columns that are interaction terms between each pair of mapped columns.",
            "Train a Ridge regression model using these interaction terms and return the fitted model and interaction features."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.linear_model import Ridge"
        ],
        "function_def": "def feature_interaction_with_mappings(df, col_mappings, target_col):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column {target_col} is not present in the dataframe\")\n\n    for col in col_mappings:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not present in the dataframe\")\n        df[col] = df[col].map(col_mappings[col])\n\n    interaction_terms = []\n    cols = list(col_mappings.keys())\n    \n    for i in range(len(cols)):\n        for j in range(i + 1, len(cols)):\n            new_col = f\"{cols[i]}_x_{cols[j]}\"\n            df[new_col] = df[cols[i]] * df[cols[j]]\n            interaction_terms.append(new_col)\n\n    X = df[interaction_terms]\n    y = df[target_col]\n    \n    model = Ridge()\n    model.fit(X, y)\n\n    return model, interaction_terms"
    },
    {
        "function_name": "stratified_partition",
        "file_name": "partitioning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "partition_cols": "list of str",
            "train_ratio": "float"
        },
        "objectives": [
            "Validate that all columns in 'partition_cols' exist in the dataframe.",
            "Validate that 'train_ratio' is a float between 0 and 1.",
            "Partition the dataframe into training and testing sets based on 'train_ratio'.",
            "Ensure that the partition maintains the original distribution of the 'partition_cols' columns and return both sets."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split"
        ],
        "function_def": "def stratified_partition(df, partition_cols, train_ratio):\n    df = df.copy()\n    \n    for col in partition_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    \n    if not (0 < train_ratio < 1):\n        raise ValueError(f\"train_ratio must be a float between 0 and 1\")\n    \n    train_df, test_df = train_test_split(df, test_size=1-train_ratio, stratify=df[partition_cols])\n    \n    return train_df, test_df"
    },
    {
        "function_name": "merge_with_handling_duplicates",
        "file_name": "merge_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "primary_key": "str",
            "join_column": "str",
            "lookup_df": "pandas.DataFrame"
        },
        "objectives": [
            "Validate the presence of the primary key and join column in the main dataframe.",
            "Ensure that the lookup dataframe has the necessary join column.",
            "Perform a left join between the main dataframe and the lookup dataframe using the join column.",
            "Handle any duplicate columns that may arise due to the join, renaming them appropriately."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_with_handling_duplicates(df, primary_key, join_column, lookup_df):\n    if primary_key not in df.columns:\n        raise ValueError(f\"Primary key '{primary_key}' not found in DataFrame\")\n    if join_column not in df.columns:\n        raise ValueError(f\"Join column '{join_column}' not found in DataFrame\")\n    if join_column not in lookup_df.columns:\n        raise ValueError(f\"Join column '{join_column}' not found in lookup DataFrame\")\n    \n    df = df.copy()\n    merged_df = df.merge(lookup_df, on=join_column, how='left', suffixes=('', '_lookup_dup'))\n    \n    for col in merged_df.columns:\n        if '_lookup_dup' in col:\n            original_col = col.replace('_lookup_dup', '')\n            if original_col in merged_df.columns:\n                merged_df.drop(columns=[original_col], inplace=True)\n                merged_df.rename(columns={col: original_col}, inplace=True)\n    \n    return merged_df"
    },
    {
        "function_name": "filter_non_numeric_columns_by_uniqueness",
        "file_name": "uniqueness_filter.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "non_numeric_columns": "List[str]"
        },
        "objectives": [
            "Confirm that the specified non-numeric columns exist in the DataFrame.",
            "Calculate the number of unique values for each non-numeric column.",
            "Filter out columns with unique values less than a predefined threshold (e.g., 10).",
            "Return the DataFrame with only the columns that have the required level of uniqueness."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_non_numeric_columns_by_uniqueness(df, non_numeric_columns, threshold=10):\n    if not isinstance(threshold, int) or threshold <= 0:\n        raise ValueError(\"Threshold must be a positive integer.\")\n        \n    df = df.copy()\n    \n    valid_columns = [\n        col for col in non_numeric_columns if col in df.columns and \n        not pd.api.types.is_numeric_dtype(df[col]) and \n        df[col].nunique() >= threshold\n    ]\n    \n    return df[valid_columns]"
    },
    {
        "function_name": "backward_fill_grouped_by_id",
        "file_name": "group_fill_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "sequential_col": "str"
        },
        "objectives": [
            "Ensure that the id column and sequential column exist in the DataFrame.",
            "Sort the DataFrame based on the id column and the sequential column sequentially.",
            "Apply backward fill to handle missing values in the other columns for each group identified by the id column.",
            "Return the DataFrame with missing values filled within each group."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def backward_fill_grouped_by_id(df, id_col, sequential_col):\n    if id_col not in df.columns:\n        raise ValueError(f\"ID column '{id_col}' not found in DataFrame\")\n    if sequential_col not in df.columns:\n        raise ValueError(f\"Sequential column '{sequential_col}' not found in DataFrame\")\n    \n    df = df.copy()\n    df.sort_values(by=[id_col, sequential_col], ascending=[True, True], inplace=True)\n    df = df.groupby(id_col).fillna(method='bfill')\n    \n    return df"
    },
    {
        "function_name": "pivot_category_statistics",
        "file_name": "pivot_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "numeric_col": "str"
        },
        "objectives": [
            "Verify the presence and data types of 'category_col' and 'numeric_col'.",
            "Pivot the dataframe to get category-wise statistics (mean, median, std) for the numeric column.",
            "Flatten the multi-level column index resulting from the pivot operation.",
            "Return the pivot table as a dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def pivot_category_statistics(df, category_col, numeric_col):\n    # Step 1: Validate column presence and data types\n    if category_col not in df.columns or not pd.api.types.is_categorical_dtype(df[category_col]):\n        raise ValueError(f\"Column {category_col} is not present or not of categorical type\")\n    if numeric_col not in df.columns or not pd.api.types.is_numeric_dtype(df[numeric_col]):\n        raise ValueError(f\"Column {numeric_col} is not present or not of numeric type\")\n    \n    # Step 2: Pivot table to get category-wise statistics\n    pivot_table = df.pivot_table(index=category_col, values=numeric_col, aggfunc=['mean', 'median', 'std'])\n    \n    # Step 3: Flatten the multi-level column index\n    pivot_table.columns = ['_'.join(col).strip() for col in pivot_table.columns.values]\n    \n    # Step 4: Return pivot table as dataframe\n    return pivot_table.reset_index()"
    },
    {
        "function_name": "create_composite_feature",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "features": "list",
            "new_feature_name": "str",
            "method": "str"
        },
        "objectives": [
            "Ensure all the specified feature columns exist in the DataFrame.",
            "Normalize these feature columns using the specified method, which can be either 'zscore' or 'minmax'.",
            "Create a new feature column that is a composite of the normalized values (e.g., their sum or product based on the method).",
            "Ensure the new feature column does not contain any NaN values and return the DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
        ],
        "function_def": "def create_composite_feature(df, features, new_feature_name, method):\n    if method not in ['zscore', 'minmax', 'sum', 'product']:\n        raise ValueError(\"Invalid method; choose from 'zscore', 'minmax', 'sum', 'product'\")\n    \n    # Step 1: Ensure all specified features exist\n    for feature in features:\n        if feature not in df.columns:\n            raise ValueError(f\"Feature column {feature} does not exist in the DataFrame\")\n    \n    # Step 2: Normalize feature columns\n    if method == 'zscore':\n        scaler = StandardScaler()\n        df[features] = scaler.fit_transform(df[features])\n    elif method == 'minmax':\n        scaler = MinMaxScaler()\n        df[features] = scaler.fit_transform(df[features])\n    \n    # Step 3: Create new composite feature column\n    if method in ['zscore', 'minmax']:\n        df[new_feature_name] = df[features].sum(axis=1) if method == 'sum' else df[features].product(axis=1)\n    elif method == 'sum':\n        df[new_feature_name] = df[features].sum(axis=1)\n    elif method == 'product':\n        df[new_feature_name] = df[features].product(axis=1)\n    \n    # Step 4: Ensure new feature column contains no NaN values\n    if df[new_feature_name].isnull().any():\n        raise ValueError(f\"NaN values found in the new feature column {new_feature_name}\")\n    \n    return df"
    },
    {
        "function_name": "extract_date_features",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_with_dates": "list",
            "format": "str"
        },
        "objectives": [
            "Ensure specified columns exist and can be parsed as datetime with the given format.",
            "Parse the dates and extract features like year, month, day, weekday, hour, and minute.",
            "Append these features as new columns to the DataFrame and avoid NaN values.",
            "Verify new columns are complete and return the DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def extract_date_features(df, cols_with_dates, format):\n    # Step 1: Ensure specified columns exist\n    for col in cols_with_dates:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} does not exist in the DataFrame\")\n    \n    # Step 2: Parse the dates and extract features\n    for col in cols_with_dates:\n        df[col] = pd.to_datetime(df[col], format=format, errors='raise')\n        df[f'{col}_year'] = df[col].dt.year\n        df[f'{col}_month'] = df[col].dt.month\n        df[f'{col}_day'] = df[col].dt.day\n        df[f'{col}_weekday'] = df[col].dt.weekday\n        df[f'{col}_hour'] = df[col].dt.hour\n        df[f'{col}_minute'] = df[col].dt.minute\n    \n    # Step 3: Ensure no NaN values in the new columns\n    new_columns = [f'{col}_{suffix}' for col in cols_with_dates for suffix in ['year', 'month', 'day', 'weekday', 'hour', 'minute']]\n    if df[new_columns].isnull().any().any():\n        raise ValueError(f\"NaN values found in the newly created date feature columns for {cols_with_dates}\")\n    \n    return df"
    },
    {
        "function_name": "mark_and_count_holidays",
        "file_name": "holiday_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "holidays": "list of str (date strings)"
        },
        "objectives": [
            "Validate the presence of the `date_col` in the DataFrame and ensure it is a datetime type.",
            "Ensure `holidays` are converted to datetime objects.",
            "Create a new boolean column `is_holiday` where True if the date in `date_col` is a holiday.",
            "Count the number of holidays if they are on weekdays and append a new integer column `weekdays_holiday_count`."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def mark_and_count_holidays(df, date_col, holidays):\n    if date_col not in df.columns:\n        raise ValueError(f\"Column '{date_col}' not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column '{date_col}' must be of datetime type\")\n    \n    holidays = pd.to_datetime(holidays)\n    df['is_holiday'] = df[date_col].isin(holidays)\n    \n    df['weekdays_holiday_count'] = df.apply(\n        lambda row: sum(\n            (row[date_col].date() == holiday.date()) and (holiday.weekday() < 5)\n            for holiday in holidays\n        ), axis=1)\n    \n    return df"
    },
    {
        "function_name": "filter_high_frequency_binary_columns",
        "file_name": "categorical_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_columns": "list",
            "threshold": "float"
        },
        "objectives": [
            "Ensure that all specified categorical columns are present in the dataframe.",
            "Calculate the mode value for each categorical column.",
            "Convert categorical columns to binary columns using one-hot encoding.",
            "Filter out binary columns where mode value frequency exceeds the given threshold."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_high_frequency_binary_columns(df, cat_columns, threshold):\n    # Step 1: Ensure all categorical columns are present in the dataframe\n    for col in cat_columns:\n        if col not in df.columns or not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not categorical\")\n    \n    # Step 2: Calculate mode value for each categorical column\n    mode_values = {col: df[col].mode()[0] for col in cat_columns}\n    \n    # Step 3: Convert categorical columns to binary columns using one-hot encoding\n    df_one_hot = pd.get_dummies(df, columns=cat_columns)\n    \n    # Step 4: Filter out binary columns where mode value frequency exceeds the given threshold\n    columns_to_keep = []\n    for col, mode_val in mode_values.items():\n        for one_hot_col in df_one_hot.columns:\n            if one_hot_col.startswith(f\"{col}_{mode_val}\") and \\\n               (df_one_hot[one_hot_col].mean() <= threshold):\n                columns_to_keep.append(one_hot_col)\n    \n    filtered_df = df_one_hot[columns_to_keep]\n    \n    return filtered_df"
    },
    {
        "function_name": "resample_with_ewma",
        "file_name": "ewma_resampling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "interval": "str"
        },
        "objectives": [
            "Ensure the specified date column is present and can be parsed as a datetime object.",
            "Set the date column as the index of the dataframe.",
            "Apply an exponential weighted moving average (EWMA) on all numerical columns.",
            "Resample the dataframe at the given interval using the EWMA values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_with_ewma(df, date_col, interval='M'):\n    # Step 1: Ensure the date column is present and can be parsed as datetime\n    if date_col not in df.columns:\n        raise ValueError(f\"Column {date_col} is not present in the dataframe\")\n    df[date_col] = pd.to_datetime(df[date_col])\n    \n    # Step 2: Set the date column as the index\n    df.set_index(date_col, inplace=True)\n    \n    # Step 3: Apply EWMA on all numerical columns\n    for col in df.select_dtypes(include='number').columns:\n        df[col] = df[col].ewm(span=10, adjust=False).mean()\n    \n    # Step 4: Resample the dataframe at the given interval\n    resampled_df = df.resample(interval).mean()\n    \n    return resampled_df"
    },
    {
        "function_name": "compute_year_over_year_growth",
        "file_name": "temporal_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "group_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Convert 'date_col' to datetime and extract the month and year.",
            "Group by extracted month/year and 'group_col' to compute the monthly sum of 'value_col'.",
            "Calculate the year-over-year growth for each group/month.",
            "Return the dataframe with an added 'year_over_year_growth' column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_year_over_year_growth(df, date_col, group_col, value_col):\n    # Step 1: Extract month and year from date column\n    df[date_col] = pd.to_datetime(df[date_col])\n    df['year'] = df[date_col].dt.year\n    df['month'] = df[date_col].dt.month\n    \n    # Step 2: Compute monthly sum of value_col\n    monthly_sum = df.groupby(['year', 'month', group_col])[value_col].sum().reset_index()\n    \n    # Step 3: Calculate year-over-year growth\n    monthly_sum['previous_year'] = monthly_sum['year'] - 1\n    monthly_sum = monthly_sum.merge(monthly_sum, on=['previous_year', 'month', group_col], suffixes=('', '_previous'))\n    \n    monthly_sum['year_over_year_growth'] = ((monthly_sum[value_col] - monthly_sum[value_col + '_previous']) / \n                                            monthly_sum[value_col + '_previous']) * 100\n    \n    # Step 4: Clean up result\n    monthly_sum = monthly_sum[['year', 'month', group_col, 'year_over_year_growth']]\n    df = df.merge(monthly_sum, on=['year', 'month', group_col], how='left')\n    \n    return df"
    },
    {
        "function_name": "mark_rare_categories",
        "file_name": "categorical_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_columns": "List[str]",
            "rare_threshold": "float"
        },
        "objectives": [
            "Validate the presence of each categorical column in the dataframe.",
            "Compute the frequency of each category within the specified columns.",
            "Identify categories that fall below the `rare_threshold` and mark them as 'Rare'.",
            "Replace the identified rare categories with 'Rare' and return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def mark_rare_categories(df, categorical_columns, rare_threshold):\n    if not all(col in df.columns for col in categorical_columns):\n        raise ValueError(\"One or more categorical columns not found in DataFrame\")\n    \n    for col in categorical_columns:\n        category_counts = df[col].value_counts(normalize=True)\n        rare_categories = category_counts[category_counts < rare_threshold].index\n        \n        df[col] = df[col].apply(lambda x: 'Rare' if x in rare_categories else x)\n    \n    return df"
    },
    {
        "function_name": "custom_imputation",
        "file_name": "imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "imputation_methods": "dict"
        },
        "objectives": [
            "Validate that imputation_methods is a dictionary with column names as keys and imputation strategies as values.",
            "Apply mean imputation for columns specified with 'mean' strategy.",
            "Apply median imputation for columns specified with 'median' strategy.",
            "Apply forward-fill imputation for columns specified with 'ffill' strategy.",
            "Return the modified dataframe with imputed values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def custom_imputation(df, imputation_methods):\n    if not isinstance(imputation_methods, dict):\n        raise ValueError(\"imputation_methods must be a dictionary\")\n\n    df = df.copy()\n    \n    for col, method in imputation_methods.items():\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not in the dataframe\")\n        if method == 'mean':\n            df[col].fillna(df[col].mean(), inplace=True)\n        elif method == 'median':\n            df[col].fillna(df[col].median(), inplace=True)\n        elif method == 'ffill':\n            df[col].fillna(method='ffill', inplace=True)\n        else:\n            raise ValueError(f\"Unsupported imputation method {method} for column {col}\")\n    \n    return df"
    },
    {
        "function_name": "deduplicate_and_bin",
        "file_name": "data_cleansing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "key_columns": "List[str]",
            "target_column": "str",
            "bins": "int"
        },
        "objectives": [
            "Check if key_columns and target_column exist in the DataFrame, and are of appropriate types.",
            "Remove any duplicate rows in the DataFrame based on key_columns.",
            "Bin the target_column into the specified number of equal-width bins.",
            "Return the modified DataFrame with binned target column suffixed by '_binned'."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def deduplicate_and_bin(df, key_columns, target_column, bins):\n    # Step 1: Ensure columns exist and are of appropriate types\n    for col in key_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Key column '{col}' is not found in the DataFrame\")\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' is not found in the DataFrame\")\n      \n    # Step 2: Remove duplicate rows based on key_columns\n    df = df.drop_duplicates(subset=key_columns)\n    \n    # Step 3: Bin the target column into specified number of bins\n    df[f'{target_column}_binned'] = pd.cut(df[target_column], bins=bins, labels=False, include_lowest=True)\n    \n    return df"
    },
    {
        "function_name": "filter_and_sort",
        "file_name": "data_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "threshold": "float"
        },
        "objectives": [
            "Ensure target_column exists and is a numeric type.",
            "Remove rows where the target_column values are above the specified threshold.",
            "Sort the DataFrame by the target_column in ascending order.",
            "Return the modified sorted DataFrame with rows filtered by threshold."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_and_sort(df, target_column, threshold):\n    # Step 1: Ensure target_column exists and is a numeric type\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' is not found in the DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_column]):\n        raise ValueError(f\"Target column '{target_column}' must be a numeric type\")\n    \n    # Step 2: Filter rows based on threshold\n    df = df[df[target_column] <= threshold]\n    \n    # Step 3: Sort DataFrame by target_column\n    df = df.sort_values(by=target_column, ascending=True)\n    \n    return df"
    },
    {
        "function_name": "resample_and_aggregate",
        "file_name": "time_series_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "aggregation_funcs": "dict"
        },
        "objectives": [
            "Ensure the date_column exists and is of datetime type.",
            "Set the date_column as the DataFrame index, resampling to a monthly frequency.",
            "Aggregate the numeric columns using the provided aggregation functions.",
            "Return the resampled and aggregated DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_aggregate(df, date_column, aggregation_funcs):\n    # Step 1: Ensure the date_column exists and is of datetime type\n    if date_column not in df.columns:\n        raise ValueError(f\"Date column '{date_column}' is not found in the DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[date_column]):\n        raise ValueError(f\"Date column '{date_column}' must be of datetime type\")\n    \n    # Step 2: Set the date_column as index and resample to monthly frequency\n    df = df.set_index(date_column).resample('M')\n    \n    # Step 3: Aggregate numeric columns using provided aggregation functions\n    df = df.agg(aggregation_funcs)\n    \n    return df"
    },
    {
        "function_name": "split_and_bin_column",
        "file_name": "split_and_bin.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_to_split": "str",
            "max_percent": "float",
            "new_col_prefix": "str"
        },
        "objectives": [
            "Ensure the specified column exists and is of type 'object' or 'string'.",
            "Split the column into unique values until the number of unique values is less than or equal to max_percent of the total number of rows in df.",
            "For each unique value, create a new column with the prefix new_col_prefix and set 1 if the row contains the value and 0 otherwise.",
            "Return the modified dataframe with the new binary columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_and_bin_column(df, col_to_split, max_percent, new_col_prefix):\n    # Step 1: Validate column presence and type\n    if col_to_split not in df.columns or not pd.api.types.is_string_dtype(df[col_to_split]):\n        raise ValueError(f\"Column '{col_to_split}' is not present or not a string/object type\")\n\n    # Step 2: Split column into unique values until they are <= max_percent of rows\n    unique_values = df[col_to_split].unique()\n    max_unique_values = int(max_percent * len(df))\n    if len(unique_values) > max_unique_values:\n        unique_values = unique_values[:max_unique_values]\n\n    # Step 3: Create binary columns for each unique value\n    for val in unique_values:\n        new_col_name = f\"{new_col_prefix}_{val}\"\n        df[new_col_name] = df[col_to_split].apply(lambda x: 1 if x == val else 0)\n\n    return df"
    },
    {
        "function_name": "impute_with_rolling_mean",
        "file_name": "rolling_mean_imputer.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "fill_columns": "list of str",
            "rolling_window": "int",
            "min_periods": "int"
        },
        "objectives": [
            "Apply a rolling mean to the specified 'fill_columns' with a defined 'rolling_window' and 'min_periods'.",
            "Fill missing values in the original columns using the rolling mean results.",
            "Calculate the first order derivative (rate of change) for each of the rolling mean columns.",
            "Return the modified dataframe with the rolling mean, rate of change columns, and imputed missing values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_with_rolling_mean(df, fill_columns, rolling_window, min_periods):\n    rolling_means = df[fill_columns].rolling(window=rolling_window, min_periods=min_periods).mean()\n    df[fill_columns] = df[fill_columns].fillna(rolling_means)\n    \n    for col in fill_columns:\n        df[f'{col}_rolling_mean'] = rolling_means[col]\n        df[f'{col}_rate_of_change'] = rolling_means[col].diff()\n    \n    return df"
    },
    {
        "function_name": "discretize_and_correlate",
        "file_name": "target_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "feature_cols": "list of str",
            "bins": "int"
        },
        "objectives": [
            "Discretize the 'target_col' into the specified number of bins.",
            "Encode the discretized target variable using one-hot encoding.",
            "Calculate the correlation coefficient matrix between the encoded target columns and 'feature_cols'.",
            "Return a dataframe with the correlation values and corresponding feature names."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def discretize_and_correlate(df, target_col, feature_cols, bins):\n    df['binned_target'] = pd.cut(df[target_col], bins=bins, labels=False)\n    encoded_target = pd.get_dummies(df['binned_target'], prefix='target_bin')\n\n    correlation_data = {'feature': [], 'correlation': []}\n    for feature in feature_cols:\n        for col in encoded_target.columns:\n            corr_value = np.corrcoef(df[feature].values, encoded_target[col].values)[0, 1]\n            correlation_data['feature'].append(f'{feature}_vs_{col}')\n            correlation_data['correlation'].append(corr_value)\n    \n    correlation_df = pd.DataFrame(correlation_data)\n    return correlation_df"
    },
    {
        "function_name": "generate_n_grams",
        "file_name": "text_preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "n_grams": "int",
            "min_freq": "int"
        },
        "objectives": [
            "Tokenize the text data in 'text_col'.",
            "Generate n-grams (up to the specified 'n_grams') from the tokenized text.",
            "Filter out n-grams that do not meet the minimum frequency count 'min_freq'.",
            "Return a dataframe with the filtered n-grams and their corresponding frequencies."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import CountVectorizer"
        ],
        "function_def": "def generate_n_grams(df, text_col, n_grams, min_freq):\n    vectorizer = CountVectorizer(ngram_range=(1, n_grams), min_df=min_freq)\n    n_gram_matrix = vectorizer.fit_transform(df[text_col])\n    n_gram_freq = n_gram_matrix.sum(axis=0).A1\n    n_grams_df = pd.DataFrame({'n_gram': vectorizer.get_feature_names_out(), 'frequency': n_gram_freq})\n    \n    return n_grams_df"
    },
    {
        "function_name": "find_highly_correlated_columns",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "min_corr": "float"
        },
        "objectives": [
            "Validate that all columns are numeric.",
            "Calculate the Pearson correlation matrix for the dataframe.",
            "Identify pairs of columns whose correlation exceeds `min_corr`.",
            "Return a list of tuples containing such highly correlated column pairs."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def find_highly_correlated_columns(df, min_corr):\n    # Validate that all columns are numeric\n    if not all(pd.api.types.is_numeric_dtype(df[col]) for col in df.columns):\n        raise ValueError(\"All columns must be numeric\")\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Identify pairs of columns with correlation exceeding min_corr\n    correlated_pairs = []\n    for col1 in corr_matrix:\n        for col2 in corr_matrix:\n            if col1 != col2 and abs(corr_matrix.at[col1, col2]) > min_corr:\n                correlated_pairs.append((col1, col2))\n\n    # Remove duplicate pairs (col1, col2) and (col2, col1)\n    correlated_pairs = list(set(tuple(sorted(pair)) for pair in correlated_pairs))\n\n    return correlated_pairs"
    },
    {
        "function_name": "compute_ewm_difference",
        "file_name": "ewm_difference.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col1": "str",
            "col2": "str",
            "window_size": "int"
        },
        "objectives": [
            "Validate that 'col1' and 'col2' exist in the DataFrame and are numeric.",
            "Calculate the difference between the values in 'col1' and 'col2' and store it in a new column 'diff'.",
            "Compute the exponentially weighted average of 'diff' using the specified 'window_size'.",
            "Replace NaN values in the exponentially weighted average column with the mean of the column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_ewm_difference(df, col1, col2, window_size):\n    # Step 1: Validate columns\n    required_columns = [col1, col2]\n    for col in required_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' must be present in the DataFrame.\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric.\")\n    \n    df = df.copy()\n    \n    # Step 2: Compute difference\n    df['diff'] = df[col1] - df[col2]\n    \n    # Step 3: Compute exponentially weighted moving average\n    df['ewm_diff'] = df['diff'].ewm(span=window_size, adjust=False).mean()\n    \n    # Step 4: Replace NaN values\n    df['ewm_diff'].fillna(df['ewm_diff'].mean(), inplace=True)\n    \n    return df"
    },
    {
        "function_name": "encode_and_calculate_mean",
        "file_name": "category_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "target_col": "str",
            "encode_map": "dict"
        },
        "objectives": [
            "Validate that 'cat_col' exists and is of object type, and 'target_col' exists and is numeric in the DataFrame.",
            "Encode 'cat_col' using the provided 'encode_map'.",
            "Group the DataFrame by the encoded category column and calculate the mean of 'target_col' for each group.",
            "Return the DataFrame with the encoded column and a new column 'target_mean' containing the calculated means."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def encode_and_calculate_mean(df, cat_col, target_col, encode_map):\n    # Step 1: Validate columns\n    if cat_col not in df.columns or not pd.api.types.is_object_dtype(df[cat_col]):\n        raise ValueError(f\"Column '{cat_col}' must be present and of object type in DataFrame\")\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be present and of numeric type in DataFrame\")\n    \n    df = df.copy()\n    \n    # Step 2: Encode the categorical column\n    df[cat_col] = df[cat_col].map(encode_map)\n    \n    # Step 3: Calculate mean of target column for each group\n    target_mean_df = df.groupby(cat_col)[target_col].mean().reset_index(name='target_mean')\n    \n    # Step 4: Merge with original DataFrame\n    df = pd.merge(df, target_mean_df, on=cat_col, how='left')\n    \n    return df"
    },
    {
        "function_name": "category_proportion_by_bins",
        "file_name": "categorical_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numeric_col": "str",
            "cat_col": "str",
            "n_bins": "int"
        },
        "objectives": [
            "Ensure both `numeric_col` and `cat_col` exist in the dataframe and are the correct type (numeric for `numeric_col` and categorical for `cat_col`).",
            "Divide the `numeric_col` into `n_bins` bins using equal-width binning.",
            "Calculate the proportion of each category within each bin.",
            "Return a dataframe showing the bin ranges, category, and respective proportions."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def category_proportion_by_bins(df, numeric_col, cat_col, n_bins):\n    # Step 1: Validate the existence and type of the specified columns\n    if numeric_col not in df.columns or cat_col not in df.columns:\n        raise ValueError(\"Specified columns are not found in the DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[numeric_col]):\n        raise TypeError(f\"Numeric column '{numeric_col}' must be of numeric type\")\n    if not pd.api.types.is_categorical_dtype(df[cat_col]):\n        df[cat_col] = df[cat_col].astype('category')\n    \n    # Step 2: Divide the numeric column into n_bins using equal-width binning\n    df['numeric_bins'] = pd.cut(df[numeric_col], bins=n_bins)\n    \n    # Step 3: Calculate the proportion of each category within each bin\n    bin_cat_counts = df.groupby(['numeric_bins', cat_col]).size()\n    bin_counts = df.groupby('numeric_bins').size()\n    proportions = bin_cat_counts / bin_counts\n    \n    # Step 4: Return a DataFrame with the bin ranges, category, and respective proportions\n    prop_df = proportions.reset_index(name='proportion')\n    return prop_df"
    },
    {
        "function_name": "one_hot_encode_and_split",
        "file_name": "encoding_and_splitting.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Ensure that the target column exists and extract its unique values.",
            "One-hot encode the target column.",
            "Perform a 70-30 train-test split and return both sets."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split"
        ],
        "function_def": "def one_hot_encode_and_split(df, target_col):\n    # Ensure target_col exists\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n\n    # One-hot encode the target column\n    df = pd.get_dummies(df, columns=[target_col])\n\n    # Perform 70-30 train-test split\n    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n\n    return train_df, test_df"
    },
    {
        "function_name": "normalize_grouped_statistics",
        "file_name": "statistics_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "grouping_col": "str",
            "value_col": "str"
        },
        "objectives": [
            "Check if the grouping column and value column exist in the dataframe.",
            "Group the dataframe by the grouping column and calculate the sum, mean, and standard deviation of the value column.",
            "Normalize these statistics to the range [0, 1].",
            "Return a dataframe containing the normalized statistics."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def normalize_grouped_statistics(df, grouping_col, value_col):\n    # Check if columns exist\n    if grouping_col not in df.columns or value_col not in df.columns:\n        raise ValueError(f\"Columns '{grouping_col}' or '{value_col}' not found in DataFrame\")\n\n    # Group by grouping_col and calculate statistics\n    grouped_stats = df.groupby(grouping_col)[value_col].agg(['sum', 'mean', 'std']).reset_index()\n\n    # Normalize the statistics\n    scaler = MinMaxScaler()\n    grouped_stats[['sum', 'mean', 'std']] = scaler.fit_transform(grouped_stats[['sum', 'mean', 'std']])\n\n    return grouped_stats"
    },
    {
        "function_name": "compute_string_lengths",
        "file_name": "text_preprocessing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "string_cols": "list"
        },
        "objectives": [
            "Validate the presence and type of each column in `string_cols`.",
            "Compute length of words in each string column.",
            "Create new columns with these lengths.",
            "Return the DataFrame with the new length columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_string_lengths(df, string_cols):\n    # Validate presence and type of each column in string_cols\n    for col in string_cols:\n        if col not in df.columns or not pd.api.types.is_string_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' not found or is not of string type in DataFrame\")\n    \n    # Compute length of each string and create new columns\n    for col in string_cols:\n        df[col + '_len'] = df[col].str.len()\n    \n    return df"
    },
    {
        "function_name": "compute_interactions",
        "file_name": "interaction_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "interaction_columns": "list",
            "method": "str"
        },
        "objectives": [
            "Validate the presence of `interaction_columns` in the DataFrame and ensure they are numeric.",
            "Compute pairwise interactions between all columns in `interaction_columns` using the specified `method` ('sum', 'multiply', etc.).",
            "Create new columns to store each interaction result.",
            "Return the modified DataFrame with interaction columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_interactions(df, interaction_columns, method):\n    # Validate presence and type of each column in interaction_columns\n    for col in interaction_columns:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' not found or is not numeric in DataFrame\")\n    \n    # Compute pairwise interactions based on specified method\n    if method not in ['sum', 'multiply']:\n        raise ValueError(f\"Method '{method}' not supported. Use 'sum' or 'multiply'.\")\n\n    for i in range(len(interaction_columns)):\n        for j in range(i + 1, len(interaction_columns)):\n            col1, col2 = interaction_columns[i], interaction_columns[j]\n            if method == 'sum':\n                df[f'{col1}_{col2}_sum'] = df[col1] + df[col2]\n            elif method == 'multiply':\n                df[f'{col1}_{col2}_multiply'] = df[col1] * df[col2]\n    \n    return df"
    },
    {
        "function_name": "create_pivot_table",
        "file_name": "pivot_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_pivot": "list",
            "agg_func": "str"
        },
        "objectives": [
            "Ensure the `cols_to_pivot` exist in the DataFrame and are categorical.",
            "Create a pivot table using the specified aggregation function (agg_func).",
            "Convert the pivot table back into a DataFrame.",
            "Return the pivoted DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_pivot_table(df, cols_to_pivot, agg_func):\n    # Validate presence of cols_to_pivot and ensure they are categorical\n    for col in cols_to_pivot:\n        if col not in df.columns or not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' not found or is not categorical in DataFrame\")\n    \n    # Create pivot table\n    pivot_df = df.pivot_table(index=cols_to_pivot[0], columns=cols_to_pivot[1:], aggfunc=agg_func, margins=True)\n    \n    # Convert pivot table to DataFrame\n    pivot_df = pivot_df.reset_index()\n    \n    return pivot_df"
    },
    {
        "function_name": "create_binary_threshold",
        "file_name": "threshold_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Ensure `target_col` exists and is numeric.",
            "Create a binary column indicating if the value in `target_col` exceeds `threshold`.",
            "Group by this binary column and compute statistics (mean, sum) for other numerical columns.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_binary_threshold(df, target_col, threshold):\n    # Ensure target_col exists and is numeric\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' not found or is not numeric in DataFrame\")\n    \n    # Create binary column\n    df[f'{target_col}_binary'] = (df[target_col] > threshold).astype(int)\n    \n    # Group by the binary column and compute statistics\n    grouped_df = df.groupby(f'{target_col}_binary').agg(['mean', 'sum'])\n    \n    # Flatten the MultiIndex columns for clarity\n    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n    grouped_df = grouped_df.reset_index()\n    \n    return grouped_df"
    },
    {
        "function_name": "group_based_z_score",
        "file_name": "group_stats.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Validate that both `group_col` and `target_col` are present in the DataFrame and the target column is numeric.",
            "Compute the z-score of the `target_col` within each group defined by `group_col`.",
            "Replace the original `target_col` values with their corresponding z-scores and store original values in a backup column.",
            "Return the DataFrame with z-scored target values and the backup column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import zscore"
        ],
        "function_def": "def group_based_z_score(df, group_col, target_col):\n    if group_col not in df.columns or target_col not in df.columns:\n        raise ValueError(f\"Either '{group_col}' or '{target_col}' column not found in the DataFrame\")\n    \n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"The column '{target_col}' must be numeric\")\n    \n    df[target_col + '_backup'] = df[target_col]\n    \n    df[target_col] = df.groupby(group_col)[target_col].transform(lambda x: zscore(x, ddof=1))\n    \n    return df"
    },
    {
        "function_name": "handle_outliers",
        "file_name": "outlier_management.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_name": "str",
            "outlier_method": "str",
            "threshold": "float"
        },
        "objectives": [
            "Validate if the specified column `col_name` is present and numeric in the DataFrame.",
            "Detect outliers in `col_name` using the specified method, either 'z_score' or 'iqr'.",
            "Replace outliers with a calculated threshold value depending on the method.",
            "Return the DataFrame with outliers handled."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def handle_outliers(df, col_name, outlier_method, threshold=3.0):\n    if col_name not in df.columns:\n        raise ValueError(f\"Column '{col_name}' not found in the DataFrame\")\n    \n    if not pd.api.types.is_numeric_dtype(df[col_name]):\n        raise ValueError(f\"The column '{col_name}' must be numeric\")\n    \n    col_data = df[col_name]\n    \n    if outlier_method == 'z_score':\n        mean = col_data.mean()\n        std_dev = col_data.std()\n        outliers = (col_data - mean).abs() > (threshold * std_dev)\n    elif outlier_method == 'iqr':\n        Q1 = col_data.quantile(0.25)\n        Q3 = col_data.quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - (threshold * IQR)\n        upper_bound = Q3 + (threshold * IQR)\n        outliers = (col_data < lower_bound) | (col_data > upper_bound)\n    else:\n        raise ValueError(\"outlier_method must be either 'z_score' or 'iqr'\")\n    \n    df.loc[outliers, col_name] = np.nan\n    df[col_name].fillna(df[col_name].median(), inplace=True)\n    \n    return df"
    },
    {
        "function_name": "categorical_cleanup",
        "file_name": "category_management.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str"
        },
        "objectives": [
            "Verify that `category_col` is categorical.",
            "Calculate the frequency of each category.",
            "Remove categories comprising less than 5% of the total entries, labeling them as 'Other'.",
            "Return the modified DataFrame and a dictionary of the original category frequencies."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def categorical_cleanup(df, category_col):\n    if category_col not in df.columns:\n        raise ValueError(f\"The column '{category_col}' is not found in the DataFrame\")\n    \n    if not pd.api.types.is_categorical_dtype(df[category_col]):\n        raise ValueError(f\"The column '{category_col}' must be categorical\")\n    \n    category_freq = df[category_col].value_counts(normalize=True)\n    rare_categories = category_freq[category_freq < 0.05].index\n    \n    df[category_col] = df[category_col].apply(lambda x: 'Other' if x in rare_categories else x)\n    \n    return df, category_freq.to_dict()"
    },
    {
        "function_name": "impute_and_reduce",
        "file_name": "knn_pca_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_cols": "list",
            "target_col": "str"
        },
        "objectives": [
            "Impute missing values in `numerical_cols` using K-Nearest Neighbors (KNN) algorithm.",
            "Standardize `numerical_cols` (zero mean, unit variance).",
            "Perform Principal Component Analysis (PCA) on `numerical_cols` to reduce dimensionality to 2 components while preserving variance.",
            "Return the transformed DataFrame and an array of explained variances by each principal component."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.impute import KNNImputer",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.decomposition import PCA"
        ],
        "function_def": "def impute_and_reduce(df, numerical_cols, target_col):\n    df = df.copy()\n    imputer = KNNImputer(n_neighbors=5)\n    scaler = StandardScaler()\n    pca = PCA(n_components=2)\n\n    df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n    pca_result = pca.fit_transform(df[numerical_cols])\n    \n    pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n    df.reset_index(drop=True, inplace=True)\n    pca_df.reset_index(drop=True, inplace=True)\n    df = pd.concat([df, pca_df], axis=1)\n    \n    explained_variance = pca.explained_variance_ratio_\n    return df, explained_variance"
    },
    {
        "function_name": "rolling_window_diff",
        "file_name": "sequence_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "sequence_col": "str",
            "window_size": "int"
        },
        "objectives": [
            "Generate rolling window statistics (mean, std, min, max) for `sequence_col` with the specified `window_size`.",
            "Identify and impute rolling window statistics at missing intervals.",
            "Perform a Differencing technique to stabilize the mean, isolating trend from `sequence_col`.",
            "Return the modified DataFrame with rolling statistics and difference series."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def rolling_window_diff(df, sequence_col, window_size):\n    df = df.copy()\n\n    df[f'{sequence_col}_rolling_mean'] = df[sequence_col].rolling(window=window_size).mean()\n    df[f'{sequence_col}_rolling_std'] = df[sequence_col].rolling(window=window_size).std()\n    df[f'{sequence_col}_rolling_min'] = df[sequence_col].rolling(window=window_size).min()\n    df[f'{sequence_col}_rolling_max'] = df[sequence_col].rolling(window=window_size).max()\n\n    # Impute missing rolling statistics\n    for col in [f'{sequence_col}_rolling_mean', f'{sequence_col}_rolling_std', f'{sequence_col}_rolling_min', f'{sequence_col}_rolling_max']:\n        df[col].fillna(df[col].mean(), inplace=True)\n\n    # Differencing to remove trend\n    df[f'{sequence_col}_diff'] = df[sequence_col].diff().fillna(0)\n    \n    return df"
    },
    {
        "function_name": "scale_and_filter",
        "file_name": "scaling_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list of str",
            "lower_bound": "float",
            "upper_bound": "float"
        },
        "objectives": [
            "Scale the specified columns to fall within the provided lower and upper bounds.",
            "Identify and remove any rows where scaled values fall outside the range [0, 1].",
            "Compute the variance of each scaled column.",
            "Return the modified DataFrame with variance information."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def scale_and_filter(df, columns, lower_bound, upper_bound):\n    df = df.copy()\n    \n    # Step 1: Scale specified columns to the provided bounds\n    scaler = MinMaxScaler(feature_range=(lower_bound, upper_bound))\n    df[columns] = scaler.fit_transform(df[columns])\n    \n    # Step 2: Remove rows where scaled values fall outside the range [0, 1]\n    df = df[(df[columns] >= 0) & (df[columns] <= 1)].dropna()\n    \n    # Step 3: Compute the variance of each scaled column\n    variance_info = df[columns].var().to_dict()\n    \n    return df, variance_info"
    },
    {
        "function_name": "polynomial_and_linear_regression",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_cols": "list",
            "target_col": "str"
        },
        "objectives": [
            "Confirm that all columns in feature_cols and the target_col exist in df.",
            "Create polynomial features for the columns in feature_cols.",
            "Fit a linear regression model using the polynomial features to predict target_col.",
            "Return the coefficients of the linear regression model."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import PolynomialFeatures",
            "from sklearn.linear_model import LinearRegression"
        ],
        "function_def": "def polynomial_and_linear_regression(df, feature_cols, target_col):\n    for col in feature_cols + [target_col]:\n        if col not in df.columns:\n            raise ValueError(f\"The column '{col}' specified is not present in the dataframe\")\n    \n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    X_poly = poly.fit_transform(df[feature_cols])\n    \n    lin_reg = LinearRegression()\n    lin_reg.fit(X_poly, df[target_col])\n    \n    return lin_reg.coef_"
    },
    {
        "function_name": "pivot_and_fill_na",
        "file_name": "pivot_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "pivot_index": "list",
            "pivot_columns": "list",
            "pivot_values": "str"
        },
        "objectives": [
            "Confirm that the columns in pivot_index, pivot_columns, and pivot_values exist in df.",
            "Pivot the dataframe using given parameters, ensuring no duplicates in the pivot_table.",
            "Fill missing values in the pivoted table with a specified filler.",
            "Return the pivoted dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def pivot_and_fill_na(df, pivot_index, pivot_columns, pivot_values, filler=0):\n    for col in pivot_index + pivot_columns + [pivot_values]:\n        if col not in df.columns:\n            raise ValueError(f\"The column '{col}' specified is not present in the dataframe\")\n    \n    pivot_table = df.pivot_table(index=pivot_index, columns=pivot_columns, values=pivot_values, aggfunc='first')\n    pivot_table.fillna(filler, inplace=True)\n    \n    return pivot_table.reset_index()"
    },
    {
        "function_name": "bin_dataframes_columns",
        "file_name": "binarization_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "bin_edges": "dict"
        },
        "objectives": [
            "Confirm that the keys in bin_edges exist as columns in df.",
            "Bin the values in each specified column according to the corresponding bin edges.",
            "Create a new column for each specified column indicating the bin label.",
            "Return the modified dataframe with the new bin columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def bin_dataframes_columns(df, bin_edges):\n    for col in bin_edges.keys():\n        if col not in df.columns:\n            raise ValueError(f\"The column '{col}' specified in bin edges is not present in the dataframe\")\n        \n        bins, labels = bin_edges[col]\n        df[f'{col}_bin'] = pd.cut(df[col], bins=bins, labels=labels, include_lowest=True)\n    \n    return df"
    },
    {
        "function_name": "aggregate_weekly",
        "file_name": "temporal_aggregation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "group_col": "str",
            "agg_funcs": "dict"
        },
        "objectives": [
            "Validate that the specified date_col is indeed a date column and the group_col exists.",
            "Group the DataFrame by group_col and resample date_col to a weekly frequency.",
            "Apply specified aggregation functions from agg_funcs to the resampled DataFrame.",
            "Return the resampled and aggregated DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def aggregate_weekly(df, date_col, group_col, agg_funcs):\n    # Step 1: Validate date column and group column\n    if date_col not in df.columns or pd.to_datetime(df[date_col], errors='coerce').isnull().all():\n        raise ValueError(f\"Column {date_col} must be present and of datetime type.\")\n    if group_col not in df.columns:\n        raise ValueError(f\"Column {group_col} must be present in DataFrame.\")\n    \n    # Step 2: Convert the date column to datetime if not already done\n    df[date_col] = pd.to_datetime(df[date_col])\n    \n    # Step 3: Group by the group column and resample the date column to weekly frequency\n    grouped = df.set_index(date_col).groupby(group_col).resample('W').agg(agg_funcs).reset_index()\n    \n    return grouped"
    },
    {
        "function_name": "transform_categories",
        "file_name": "category_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "freq_threshold": "int",
            "one_hot": "bool"
        },
        "objectives": [
            "Ensure category_col exists and is of categorical type.",
            "Replace categories in category_col that appear less than freq_threshold times with 'Other'.",
            "If one_hot is True, perform one-hot encoding on modified category_col.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def transform_categories(df, category_col, freq_threshold, one_hot):\n    # Step 1: Ensure category_col exists and is categorical\n    if category_col not in df.columns:\n        raise ValueError(f\"Column {category_col} does not exist in the DataFrame.\")\n    if not pd.api.types.is_categorical_dtype(df[category_col]):\n        df[category_col] = df[category_col].astype('category')\n    \n    # Step 2: Replace rare categories with 'Other'\n    freq = df[category_col].value_counts()\n    to_replace = freq[freq < freq_threshold].index\n    df[category_col] = df[category_col].cat.add_categories('Other').replace(to_replace, 'Other')\n    \n    # Step 3: One-hot encoding if one_hot is True\n    if one_hot:\n        df = pd.get_dummies(df, columns=[category_col], prefix=category_col)\n    \n    return df"
    },
    {
        "function_name": "train_test_split_and_predict",
        "file_name": "regression_model.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "predictor_cols": "list",
            "test_size": "float"
        },
        "objectives": [
            "Ensure all predictor_cols exist in the DataFrame.",
            "Split the DataFrame into train and test sets using test_size as the proportion for testing.",
            "Fit a linear regression model using training set and predict on the test set.",
            "Return the predicted values for the test set and the trained model."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.linear_model import LinearRegression"
        ],
        "function_def": "def train_test_split_and_predict(df, target_col, predictor_cols, test_size):\n    X = df[predictor_cols]\n    y = df[target_col]\n\n    # Step 1: Ensure all predictor columns exist\n    for col in predictor_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Predictor column {col} does not exist in the DataFrame\")\n    \n    # Step 2: Split the DataFrame\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    # Step 3: Fit a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Step 4: Predict on the test set\n    y_pred = model.predict(X_test)\n    \n    return y_pred, model"
    },
    {
        "function_name": "impute_and_clean",
        "file_name": "data_cleaning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_cols": "list",
            "method": "str"
        },
        "objectives": [
            "Ensure all specified feature columns exist in the DataFrame.",
            "Impute missing values in feature columns using the specified method ('mean', 'median', 'mode').",
            "Check and handle any remaining NaN values by removing the affected rows.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_and_clean(df, feature_cols, method):\n    # Step 1: Ensure all specified feature columns exist\n    for col in feature_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Feature column {col} does not exist in the DataFrame\")\n\n    # Step 2: Impute missing values using the specified method\n    if method == 'mean':\n        df[feature_cols] = df[feature_cols].fillna(df[feature_cols].mean())\n    elif method == 'median':\n        df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())\n    elif method == 'mode':\n        df[feature_cols] = df[feature_cols].fillna(df[feature_cols].mode().iloc[0])\n    else:\n        raise ValueError(\"Method must be one of 'mean', 'median', or 'mode'\")\n\n    # Step 3: Handle remaining NaN values by removing rows with NaNs\n    df.dropna(subset=feature_cols, inplace=True)\n    \n    return df"
    },
    {
        "function_name": "temporal_resampling_and_rolling_stats",
        "file_name": "temporal_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "freq": "str"
        },
        "objectives": [
            "Convert the specified time column into a datetime object.",
            "Resample the data based on a specified frequency (e.g., daily, weekly, monthly).",
            "Forward fill missing values created during resampling to maintain continuity.",
            "Calculate the rolling mean and standard deviation for a window of the last 7 periods and add these as new columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def temporal_resampling_and_rolling_stats(df, time_col, freq):\n    if time_col not in df.columns:\n        raise ValueError(f\"Column {time_col} is not in the dataframe.\")\n    \n    df = df.copy()\n    df[time_col] = pd.to_datetime(df[time_col])\n    \n    df.set_index(time_col, inplace=True)\n    df = df.resample(freq).ffill()\n    \n    rolling_mean = df.rolling(window=7).mean()\n    rolling_std = df.rolling(window=7).std()\n    \n    df['rolling_mean'] = rolling_mean\n    df['rolling_std'] = rolling_std\n    \n    df.reset_index(inplace=True)\n    \n    return df"
    },
    {
        "function_name": "sliding_window_summarization",
        "file_name": "window_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int",
            "step_size": "int"
        },
        "objectives": [
            "Create windows of a specified size over the DataFrame using a sliding window approach.",
            "Ensure each window shifts by a specified step size.",
            "For each window, compute summary statistics (mean and standard deviation) for all numeric columns.",
            "Return a new DataFrame where each row represents the summary statistics of a window."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def sliding_window_summarization(df, window_size, step_size):\n    if window_size <= 0:\n        raise ValueError(\"Window size must be greater than 0.\")\n    if step_size <= 0:\n        raise ValueError(\"Step size must be greater than 0.\")\n    if df.shape[0] < window_size:\n        raise ValueError(\"DataFrame has fewer rows than the window size.\")\n    \n    windows = [\n        df.iloc[i:i + window_size] \n        for i in range(0, df.shape[0] - window_size + 1, step_size)\n    ]\n    \n    summary_data = {\n        f\"{col}_mean\": [window[col].mean() for window in windows] for col in df.select_dtypes(include='number').columns\n    }\n    summary_data.update({\n        f\"{col}_std\": [window[col].std() for window in windows] for col in df.select_dtypes(include='number').columns\n    })\n    \n    return pd.DataFrame(summary_data)"
    },
    {
        "function_name": "conditional_mean_imputation",
        "file_name": "imputation_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "conditioning_col": "str",
            "cols_to_impute": "list[str]"
        },
        "objectives": [
            "Verify if the conditioning column and columns to impute are present.",
            "Check the distribution of the conditioning column is balanced.",
            "For each column to be imputed, apply conditional imputation based on groups in the conditioning column using the mean of each group.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def conditional_mean_imputation(df, conditioning_col, cols_to_impute):\n    if conditioning_col not in df.columns:\n        raise ValueError(f\"Column {conditioning_col} not found in DataFrame.\")\n    \n    for col in cols_to_impute:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} not found in DataFrame.\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} must be numeric.\")\n    \n    value_counts = df[conditioning_col].value_counts()\n    if value_counts.min() < 10:\n        raise ValueError(\"The conditioning column is imbalanced; each group needs at least 10 samples.\")\n    \n    df = df.copy()\n    for col in cols_to_impute:\n        means = df.groupby(conditioning_col)[col].transform('mean')\n        df[col].fillna(means, inplace=True)\n    \n    return df"
    },
    {
        "function_name": "kmeans_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_cols": "list",
            "k": "int"
        },
        "objectives": [
            "Ensure that each column in `numerical_cols` exists in the dataframe.",
            "Use K-means clustering to create `k` clusters based on the numerical columns.",
            "Add a new column to the dataframe for the cluster labels.",
            "For each cluster, calculate the centroids and return them along with the transformed dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def kmeans_clustering(df, numerical_cols, k):\n    for col in numerical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Numerical column {col} is not in the DataFrame\")\n    \n    kmeans = KMeans(n_clusters=k)\n    df['cluster'] = kmeans.fit_predict(df[numerical_cols])\n\n    centroids = pd.DataFrame(kmeans.cluster_centers_, columns=numerical_cols)\n    \n    return df, centroids"
    },
    {
        "function_name": "text_topic_modeling",
        "file_name": "topic_modeling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "n_topics": "int"
        },
        "objectives": [
            "Ensure that the `text_col` exists within the dataframe.",
            "Apply TF-IDF vectorization to the text column to convert text into numeric form.",
            "Perform Latent Dirichlet Allocation (LDA) to extract `n_topics` topics.",
            "Return the dataframe with an appended column indicating the dominant topic for each text entry."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.decomposition import LatentDirichletAllocation"
        ],
        "function_def": "def text_topic_modeling(df, text_col, n_topics):\n    if text_col not in df.columns:\n        raise ValueError(f\"Text column {text_col} is not in the DataFrame\")\n\n    # TF-IDF Vectorization\n    tfidf = TfidfVectorizer(max_features=1000)\n    tfidf_matrix = tfidf.fit_transform(df[text_col])\n\n    # LDA Topic Modeling\n    lda = LatentDirichletAllocation(n_components=n_topics)\n    lda_matrix = lda.fit_transform(tfidf_matrix)\n    \n    # Assign dominant topic\n    df['dominant_topic'] = lda_matrix.argmax(axis=1)\n    \n    return df"
    },
    {
        "function_name": "extract_regex_substrings",
        "file_name": "string_extraction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_name": "str",
            "regex_pattern": "str"
        },
        "objectives": [
            "Validate the existence of `col_name` in the dataframe.",
            "Check if `regex_pattern` is a valid regex string.",
            "Extract substrings from `col_name` that match the given regex pattern.",
            "Store these substrings in a new column and handle cases where no match is found by inserting NaNs."
        ],
        "import_lines": [
            "import pandas as pd",
            "import re"
        ],
        "function_def": "def extract_regex_substrings(df, col_name, regex_pattern):\n    if col_name not in df.columns:\n        raise ValueError(f\"Column '{col_name}' not found in DataFrame\")\n    \n    try:\n        re.compile(regex_pattern)\n    except re.error:\n        raise ValueError(f\"'{regex_pattern}' is not a valid regex pattern\")\n\n    df[f'extracted_{col_name}'] = df[col_name].apply(lambda x: re.search(regex_pattern, x).group() if pd.notnull(x) and re.search(regex_pattern, x) else None)\n\n    return df"
    },
    {
        "function_name": "bin_numerical_columns",
        "file_name": "binning_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_cols": "list",
            "bin_edges": "dict"
        },
        "objectives": [
            "Validate that all `numerical_cols` exist and are numeric in the dataframe.",
            "Validate that `bin_edges` contains appropriate binning criteria for each column.",
            "Bin each numerical column as per the corresponding `bin_edges`.",
            "Add new columns containing the binned values as categorical data in the dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_numerical_columns(df, numerical_cols, bin_edges):\n    for col in numerical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n        if col not in bin_edges or not isinstance(bin_edges[col], list):\n            raise ValueError(f\"Invalid bin edges for column '{col}'\")\n    \n    for col in numerical_cols:\n        bins = bin_edges[col]\n        labels = [f'bin_{i}' for i in range(len(bins) - 1)]\n        df[f'binned_{col}'] = pd.cut(df[col], bins=bins, labels=labels)\n\n    return df"
    },
    {
        "function_name": "replace_low_frequency_values",
        "file_name": "frequency_replacement.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "min_freq": "int",
            "new_col_name": "str"
        },
        "objectives": [
            "Ensure 'target_col' exists in the DataFrame.",
            "Replace low-frequency values in 'target_col' (less than 'min_freq') with \"Other\".",
            "Count the frequencies of each unique value in the modified 'target_col'.",
            "Create a new column named 'new_col_name' with replaced values and return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def replace_low_frequency_values(df, target_col, min_freq, new_col_name):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    \n    value_counts = df[target_col].value_counts()\n    low_freq_values = value_counts[value_counts < min_freq].index\n    \n    df[new_col_name] = df[target_col].apply(lambda x: 'Other' if x in low_freq_values else x)\n    \n    return df"
    },
    {
        "function_name": "feature_extraction_from_categories",
        "file_name": "feature_extraction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "value_col": "str",
            "new_col_prefix": "str"
        },
        "objectives": [
            "Confirm 'category_col' and 'value_col' exist in DataFrame.",
            "Extract and rank categories based on the sum, mean, and median of 'value_col'.",
            "Create new binary columns indicating if each row's category is in the top-10 of each ranking metric.",
            "Concatenate these binary columns with original DataFrame and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def feature_extraction_from_categories(df, category_col, value_col, new_col_prefix):\n    if category_col not in df.columns or value_col not in df.columns:\n        raise ValueError(f\"Columns '{category_col}' or '{value_col}' not found in DataFrame\")\n\n    sum_ranked = df.groupby(category_col)[value_col].sum().nlargest(10).index\n    mean_ranked = df.groupby(category_col)[value_col].mean().nlargest(10).index\n    median_ranked = df.groupby(category_col)[value_col].median().nlargest(10).index\n\n    df[f'{new_col_prefix}_top10_sum'] = df[category_col].apply(lambda x: 1 if x in sum_ranked else 0)\n    df[f'{new_col_prefix}_top10_mean'] = df[category_col].apply(lambda x: 1 if x in mean_ranked else 0)\n    df[f'{new_col_prefix}_top10_median'] = df[category_col].apply(lambda x: 1 if x in median_ranked else 0)\n    \n    return df"
    },
    {
        "function_name": "scale_numeric_features",
        "file_name": "feature_scaling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numeric_cols": "list",
            "method": "str"
        },
        "objectives": [
            "Verify all 'numeric_cols' exist in the DataFrame and are numeric.",
            "Normalize or standardize the specified columns depending on the 'method' parameter.",
            "If 'method' is \"min-max\", scale features to the range [0, 1].",
            "If 'method' is \"z-score\", scale features to have mean 0 and variance 1.",
            "Return the modified DataFrame with scaled columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
        ],
        "function_def": "def scale_numeric_features(df, numeric_cols, method):\n    df = df.copy()\n\n    if not all(col in df.columns for col in numeric_cols):\n        raise ValueError(f\"Not all specified columns found in DataFrame\")\n    \n    if not all(pd.api.types.is_numeric_dtype(df[col]) for col in numeric_cols):\n        raise ValueError(f\"All specified columns must be numeric\")\n    \n    if method == \"min-max\":\n        scaler = MinMaxScaler()\n    elif method == \"z-score\":\n        scaler = StandardScaler()\n    else:\n        raise ValueError(f\"Invalid method '{method}'. Use 'min-max' or 'z-score'.\")\n    \n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    \n    return df"
    },
    {
        "function_name": "split_and_clean_data",
        "file_name": "data_splitting.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "feature_cols": "List[str]"
        },
        "objectives": [
            "Remove rows containing missing values in `target_col`.",
            "Fill missing values in `feature_cols` using forward fill followed by backward fill.",
            "Split the dataframe into training and testing sets with an 80/20 ratio.",
            "Return the training and testing datasets as separate dataframes."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def split_and_clean_data(df, target_col, feature_cols):\n    df = df.copy()\n    df = df.dropna(subset=[target_col])\n    \n    for col in feature_cols:\n        df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n\n    train_df = df.sample(frac=0.8, random_state=42)\n    test_df = df.drop(train_df.index)\n    \n    return train_df, test_df"
    },
    {
        "function_name": "conditional_encoding",
        "file_name": "conditional_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Conditionally encode the values in `category_col` by their frequency.",
            "Perform feature scaling on the `target_col` within each encoded group.",
            "Create dummy variables for the encoded version of `category_col`.",
            "Return the dataframe with the target column scaled and dummy variables added."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def conditional_encoding(df, category_col, target_col):\n    df = df.copy()\n    \n    # Encode category column by frequency\n    freq_encoding = df[category_col].value_counts().to_dict()\n    df[f'{category_col}_encoded'] = df[category_col].map(freq_encoding)\n    \n    # Scale target column within each encoded group\n    scaler = StandardScaler()\n    df[target_col] = df.groupby(f'{category_col}_encoded')[target_col].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten())\n    \n    # Create dummy variables for the encoded category column\n    dummies = pd.get_dummies(df[f'{category_col}_encoded'], prefix=category_col)\n    df = pd.concat([df, dummies], axis=1)\n    \n    return df"
    },
    {
        "function_name": "tfidf_svd",
        "file_name": "text_dimensionality_reduction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str"
        },
        "objectives": [
            "Implement TF-IDF vectorization on the `text_col`.",
            "Reduce the dimensionality of the TF-IDF matrix using Singular Value Decomposition (SVD) to 2 components.",
            "Append the two principal components as new columns in the original dataframe.",
            "Return the modified dataframe with the 2 new SVD columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.decomposition import TruncatedSVD"
        ],
        "function_def": "def tfidf_svd(df, text_col):\n    df = df.copy()\n    \n    # TF-IDF Vectorization\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(df[text_col])\n    \n    # Reduce dimensionality using SVD\n    svd = TruncatedSVD(n_components=2)\n    svd_matrix = svd.fit_transform(tfidf_matrix)\n    \n    df['svd_component_1'] = svd_matrix[:, 0]\n    df['svd_component_2'] = svd_matrix[:, 1]\n    \n    return df"
    },
    {
        "function_name": "map_day_of_week",
        "file_name": "time_series_mapping.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "freq_dict": "dict"
        },
        "objectives": [
            "Ensure `date_col` is of datetime type, if not, convert to datetime.",
            "Create a new column that indicates the day of the week derived from `date_col`.",
            "Map these days of the week to values provided in `freq_dict` and create a new column `day_mapped`.",
            "Replace any unmapped day of the week with a default value 'unknown'."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def map_day_of_week(df, date_col, freq_dict):\n    # Convert date_col to datetime if it's not already\n    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        df[date_col] = pd.to_datetime(df[date_col])\n    \n    # Extract day of the week\n    df['day_of_week'] = df[date_col].dt.day_name()\n    \n    # Map day of the week to custom values\n    df['day_mapped'] = df['day_of_week'].map(freq_dict).fillna('unknown')\n    \n    return df"
    },
    {
        "function_name": "add_holiday_indicator",
        "file_name": "holiday_detection.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str",
            "holidays": "list"
        },
        "objectives": [
            "Ensure 'datetime_col' exists in the DataFrame and is of datetime type.",
            "Identify the 'date_part' (Day, Month, Year, etc.) from 'datetime_col'.",
            "Check if each date in 'datetime_col' is a holiday (as given in the list of 'holidays').",
            "Add a new boolean column 'is_holiday' that marks True if the date is a holiday, otherwise False."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_holiday_indicator(df, datetime_col, holidays):\n    if datetime_col not in df.columns:\n        raise ValueError(f\"Column '{datetime_col}' not found in DataFrame\")\n    \n    df = df.copy()\n    df[datetime_col] = pd.to_datetime(df[datetime_col])\n    holidays = pd.to_datetime(holidays)\n    \n    df['is_holiday'] = df[datetime_col].isin(holidays)\n    \n    return df"
    },
    {
        "function_name": "threshold_flags",
        "file_name": "scaling_and_flags.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "List[str]",
            "upper_threshold": "float",
            "lower_threshold": "float"
        },
        "objectives": [
            "Check if the specified columns exist and are numerical.",
            "For each column, scale it using min-max normalization.",
            "Create a new column for each input column with boolean flags indicating whether the value exceeds the upper threshold or falls below the lower threshold.",
            "Return the modified dataframe with these new boolean flag columns for each input column."
        ],
        "import_lines": [
            "import pandas as pd",
            "from typing import List"
        ],
        "function_def": "def threshold_flags(df, columns, upper_threshold, lower_threshold):\n    # Step 1: Ensure column presence and type checks\n    for col in columns:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not numerical\")\n    \n    # Step 2: Scale the columns using min-max normalization\n    for col in columns:\n        min_val = df[col].min()\n        max_val = df[col].max()\n        df[col + '_scaled'] = (df[col] - min_val) / (max_val - min_val)\n    \n    # Step 3: Create boolean flag columns for thresholds\n    for col in columns:\n        df[col + '_upper_flag'] = df[col + '_scaled'] > upper_threshold\n        df[col + '_lower_flag'] = df[col + '_scaled'] < lower_threshold\n    \n    return df"
    },
    {
        "function_name": "filter_and_summarize_datetime_columns",
        "file_name": "datetime_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_cols": "list",
            "operation_type": "str",
            "threshold": "dict"
        },
        "objectives": [
            "Validate that all specified datetime columns exist in the DataFrame and are of datetime type.",
            "Based on the provided `operation_type` (either \"count\" or \"range\"), perform different operations:",
            "For \"count\": Count the number of occurrences for each datetime column and filter out the rows which are below a certain count specified in `threshold`.",
            "For \"range\": Ensure the datetime values in each column fall within a specific range provided in `threshold`.",
            "Create a summary report of the filtered results, showing the column names, operation performed, and the number of remaining rows.",
            "Return the filtered DataFrame along with the summary report as a tuple."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_and_summarize_datetime_columns(df, datetime_cols, operation_type, threshold):\n    # Step 1: Validate datetime columns\n    for col in datetime_cols:\n        if col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not of datetime type\")\n    \n    if operation_type not in [\"count\", \"range\"]:\n        raise ValueError(\"Operation type must be either 'count' or 'range'\")\n    \n    summary_report = []\n\n    if operation_type == \"count\":\n        # Step 2: Count occurrences and apply threshold\n        for col in datetime_cols:\n            counts = df[col].value_counts()\n            df = df[df[col].apply(lambda x: counts[x] >= threshold[col])]\n            summary_report.append({\"column\": col, \"operation\": \"count\", \"remaining_rows\": len(df)})\n    \n    elif operation_type == \"range\":\n        # Step 3: Filter based on datetime range\n        for col in datetime_cols:\n            start, end = threshold[col]\n            df = df[(df[col] >= start) & (df[col] <= end)]\n            summary_report.append({\"column\": col, \"operation\": \"range\", \"remaining_rows\": len(df)})\n\n    # Step 4: Return filtered DataFrame and summary report\n    return df, pd.DataFrame(summary_report)"
    },
    {
        "function_name": "rolling_statistics",
        "file_name": "rolling_statistics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_by_col": "str",
            "agg_col": "str",
            "window_size": "int"
        },
        "objectives": [
            "Validate that `group_by_col` and `agg_col` are present in the DataFrame.",
            "Group the DataFrame by `group_by_col` and sort the groups by index.",
            "Compute the rolling mean and rolling standard deviation for the `agg_col` within each group using the specified `window_size`.",
            "Create a new DataFrame that contains the original columns along with rolling mean and rolling standard deviation columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def rolling_statistics(df, group_by_col, agg_col, window_size):\n    if group_by_col not in df.columns:\n        raise ValueError(f\"Column '{group_by_col}' not found in the DataFrame\")\n    if agg_col not in df.columns:\n        raise ValueError(f\"Column '{agg_col}' not found in the DataFrame\")\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"Window size must be a positive integer\")\n    \n    # Step 2: Group by and sort\n    df = df.sort_values(by=[group_by_col]).set_index(group_by_col)\n    \n    # Step 3: Compute rolling statistics\n    df[f'{agg_col}_rolling_mean'] = df.groupby(group_by_col)[agg_col].rolling(window=window_size).mean().reset_index(level=0, drop=True)\n    df[f'{agg_col}_rolling_std'] = df.groupby(group_by_col)[agg_col].rolling(window=window_size).std().reset_index(level=0, drop=True)\n    \n    # Step 4: Reset the index and return\n    df = df.reset_index()\n    return df"
    },
    {
        "function_name": "value_discretization",
        "file_name": "value_discretization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "value_col": "str",
            "bin_edges": "list"
        },
        "objectives": [
            "Validate that the specified value column exists in the DataFrame.",
            "Discretize the continuous values in the specified column into bins based on the provided `bin_edges`.",
            "Compute the frequency and cumulative frequency of each bin.",
            "Create a new DataFrame that contains the original column values, bin labels, frequencies, and cumulative frequencies, and return this DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def value_discretization(df, value_col, bin_edges):\n    if value_col not in df.columns:\n        raise ValueError(f\"Column '{value_col}' not found in the DataFrame\")\n    if not all(isinstance(edge, (int, float)) for edge in bin_edges):\n        raise ValueError(\"All bin edges must be int or float\")\n\n    # Step 2: Discretize values in bins\n    df['bins'] = pd.cut(df[value_col], bins=bin_edges, include_lowest=True)\n    \n    # Step 3: Compute frequency and cumulative frequency\n    frequency = df['bins'].value_counts().sort_index().reset_index()\n    frequency.columns = ['bin', 'frequency']\n    frequency['cumulative_frequency'] = frequency['frequency'].cumsum()\n    \n    # Step 4: Merge with original DataFrame\n    result = pd.merge(df, frequency, left_on='bins', right_on='bin', how='left').drop(columns=['bins'])\n    return result"
    },
    {
        "function_name": "z_score_per_segment",
        "file_name": "segmentation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "segment_col": "str",
            "numerical_cols": "list"
        },
        "objectives": [
            "Validate that specified segment column and numerical columns exist in the dataframe.",
            "Segment the dataframe based on unique values of the segment column.",
            "Calculate z-scores for the numerical columns within each segment.",
            "Return the dataframe with z-scores added."
        ],
        "import_lines": [
            "import pandas as pd",
            "import scipy.stats as stats"
        ],
        "function_def": "def z_score_per_segment(df, segment_col, numerical_cols):\n    # Step 1: Validate columns\n    for col in [segment_col] + numerical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not present in the dataframe\")\n    \n    segmented_df = df.copy()\n\n    # Step 2: Calculate z-scores within each segment\n    for segment_value in segmented_df[segment_col].unique():\n        segment = segmented_df[segmented_df[segment_col] == segment_value]\n        for col in numerical_cols:\n            z_scores = stats.zscore(segment[col])\n            segmented_df.loc[segmented_df[segment_col] == segment_value, f'z_{col}'] = z_scores\n    \n    return segmented_df"
    },
    {
        "function_name": "detect_time_series_anomalies",
        "file_name": "time_series_anomalies.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_series_column": "str",
            "window_size": "int"
        },
        "objectives": [
            "Validate the presence of `time_series_column` and ensure it's of a numeric type.",
            "Apply a rolling window of size `window_size` to compute moving averages.",
            "Identify and mark anomalies where the difference between raw data and moving averages exceeds 2 standard deviations.",
            "Return the DataFrame with an additional column marking anomalies."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def detect_time_series_anomalies(df, time_series_column, window_size):\n    # Step 1: Ensure time_series_column exists and is of numeric type\n    if time_series_column not in df.columns:\n        raise ValueError(f\"Column '{time_series_column}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[time_series_column]):\n        raise ValueError(f\"Column '{time_series_column}' must be of numeric type\")\n    \n    # Step 2: Apply rolling window to compute moving averages\n    df['moving_avg'] = df[time_series_column].rolling(window=window_size, min_periods=1).mean()\n    \n    # Step 3: Detect anomalies\n    df['difference'] = df[time_series_column] - df['moving_avg']\n    std_deviation = df['difference'].std()\n    df['anomaly'] = (df['difference'].abs() > 2 * std_deviation)\n    \n    # Drop helper columns\n    df = df.drop(columns=['difference'])\n    \n    return df"
    },
    {
        "function_name": "apply_transformations",
        "file_name": "column_transformations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list",
            "method": "str (possible values"
        },
        "objectives": [
            "Validate the presence of columns and ensure they are numeric.",
            "Apply the specified transformation (logarithm or square root) to each column.",
            "Handle invalid transformations (i.e., log of non-positive values) gracefully by setting those to NaN.",
            "Return the updated DataFrame with transformed values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def apply_transformations(df, columns, method):\n    if method not in ['log', 'sqrt']:\n        raise ValueError(\"Method must be either 'log' or 'sqrt'\")\n    \n    # Step 1: Validate columns and their types\n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n    \n    # Step 2: Apply the specified transformation\n    df = df.copy()\n    for col in columns:\n        if method == 'log':\n            df[col] = df[col].apply(lambda x: np.log(x) if x > 0 else np.nan)\n        elif method == 'sqrt':\n            df[col] = df[col].apply(lambda x: np.sqrt(x) if x >= 0 else np.nan)\n    \n    return df"
    },
    {
        "function_name": "extract_date_features",
        "file_name": "date_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str"
        },
        "objectives": [
            "Validate if `date_col` contains date-like objects.",
            "Extract year, month, and day as separate columns.",
            "Create a column representing the difference in days from a fixed reference date.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def extract_date_features(df, date_col):\n    # Step 1: Validate if date_col contains date-like objects\n    try:\n        df[date_col] = pd.to_datetime(df[date_col])\n    except Exception as e:\n        raise ValueError(f\"The column '{date_col}' cannot be converted to datetime.\\nError: {e}\")\n\n    # Step 2: Extract year, month, and day as separate columns\n    df[f'{date_col}_year'] = df[date_col].dt.year\n    df[f'{date_col}_month'] = df[date_col].dt.month\n    df[f'{date_col}_day'] = df[date_col].dt.day\n\n    # Step 3: Create a column for the difference in days from a fixed reference date\n    reference_date = pd.Timestamp('2000-01-01')\n    df[f'{date_col}_days_from_ref'] = (df[date_col] - reference_date).dt.days\n    \n    return df"
    },
    {
        "function_name": "reduce_cardinality",
        "file_name": "categorical_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str"
        },
        "objectives": [
            "Detect categorical columns with high cardinality.",
            "Merge less frequent categories into a single 'Other' category.",
            "Encode the reduced cardinality categories with integers.",
            "Return the modified DataFrame with encoded categorical values."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "function_def": "def reduce_cardinality(df, category_col):\n    # Step 1: Detect high cardinality\n    total_categories = df[category_col].nunique()\n    if total_categories > 10:\n        # Step 2: Merge less frequent categories into 'Other'\n        category_counts = df[category_col].value_counts()\n        threshold = category_counts.quantile(0.9)\n        common_categories = category_counts[category_counts >= threshold].index\n        df[category_col] = df[category_col].apply(lambda x: x if x in common_categories else 'Other')\n    \n    # Step 3: Encode categories\n    encoder = LabelEncoder()\n    df[f'{category_col}_encoded'] = encoder.fit_transform(df[category_col])\n    \n    return df"
    },
    {
        "function_name": "weighted_sum_and_normalize",
        "file_name": "feature_engineering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "List[str]",
            "new_col_name": "str"
        },
        "objectives": [
            "Ensure each column in `columns` exists in the dataframe and is of numeric type.",
            "Calculate a weighted sum of the specified `columns` using their variances as weights.",
            "Normalize the weighted sum to a range of 0 to 1.",
            "Return a new dataframe with the original columns plus an additional column containing the normalized weighted sum."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def weighted_sum_and_normalize(df, columns, new_col_name=\"weighted_sum\"):\n    df = df.copy()\n\n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n\n    variances = [df[col].var() for col in columns]\n    total_variance = sum(variances)\n    weights = [var / total_variance for var in variances]\n    \n    weighted_sum = sum(df[col] * weight for col, weight in zip(columns, weights))\n    normalized_weighted_sum = (weighted_sum - weighted_sum.min()) / (weighted_sum.max() - weighted_sum.min())\n\n    df[new_col_name] = normalized_weighted_sum\n    \n    return df"
    },
    {
        "function_name": "correlation_and_product",
        "file_name": "correlation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col1": "str",
            "col2": "str"
        },
        "objectives": [
            "Validate that both columns exist and are numerical.",
            "Calculate the Pearson correlation coefficient between the two columns.",
            "Create a new column that is the product of these two columns after normalizing them.",
            "Return the modified dataframe and the correlation coefficient."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def correlation_and_product(df, col1, col2):\n    # Step 1: Validate columns\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Column {col1} or {col2} is not present in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise ValueError(f\"Column {col1} or {col2} is not numeric\")\n    \n    # Step 2: Calculate Pearson correlation\n    correlation = df[[col1, col2]].corr().iloc[0, 1]\n    \n    # Step 3: Normalize columns\n    df[f'{col1}_norm'] = (df[col1] - df[col1].mean()) / df[col1].std()\n    df[f'{col2}_norm'] = (df[col2] - df[col2].mean()) / df[col2].std()\n    \n    # Step 4: Product of normalized columns\n    df['product'] = df[f'{col1}_norm'] * df[f'{col2}_norm']\n    \n    return df, correlation"
    },
    {
        "function_name": "bucketize_and_stats",
        "file_name": "bucketing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "bucket_col": "str",
            "buckets": "int"
        },
        "objectives": [
            "Validate the presence of the bucket column and ensure it is numerical.",
            "Create equal-sized buckets of the specified column.",
            "Calculate the mean and standard deviation within each bucket.",
            "Return the modified dataframe with a column indicating the bucket each row belongs to, and a dataframe containing bucket means and standard deviations."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def bucketize_and_stats(df, bucket_col, buckets):\n    # Step 1: Validate column\n    if bucket_col not in df.columns:\n        raise ValueError(f\"Column {bucket_col} is not present in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[bucket_col]):\n        raise ValueError(f\"Column {bucket_col} is not numeric\")\n    \n    # Step 2 & 3: Create buckets and calculate stats\n    df['bucket'] = pd.qcut(df[bucket_col], buckets, labels=False)\n    bucket_stats = df.groupby('bucket')[bucket_col].agg(['mean', 'std']).reset_index()\n    \n    return df, bucket_stats"
    },
    {
        "function_name": "flag_high_variance",
        "file_name": "variance_flagger.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "window_size": "int"
        },
        "objectives": [
            "Validate the type of input columns to ensure the target column is numeric.",
            "Compute a rolling mean of the target column using a specified window size.",
            "Identify rows where the rolling mean exceeds a dynamically computed standard deviation threshold.",
            "For these identified rows, flag and create a new column indicating 'high_variance'."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def flag_high_variance(df, target_col, window_size):\n    if target_col not in df.columns or not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(\"Target column is not found or is not numeric\")\n\n    df[f'{target_col}_rolling_mean'] = df[target_col].rolling(window=window_size).mean()\n    std_dev = df[target_col].std()\n\n    high_variance_condition = df[f'{target_col}_rolling_mean'] > 2 * std_dev\n    df['high_variance'] = high_variance_condition\n\n    return df"
    },
    {
        "function_name": "normalize_numeric_columns",
        "file_name": "numeric_normalizer.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "List[str]"
        },
        "objectives": [
            "Ensure all specified columns exist in the DataFrame and are of numeric type.",
            "Normalize each specified column to have mean = 0 and standard deviation = 1.",
            "Handle missing values by imputing with the column's mean after normalization.",
            "Return the DataFrame with normalized and imputed values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def normalize_numeric_columns(df, columns):\n    for col in columns:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column: {col} is not numerical or does not exist\")\n\n    df = df.copy()\n    \n    for col in columns:\n        mean = df[col].mean()\n        std = df[col].std()\n        \n        df[col] = (df[col] - mean) / std\n        df[col].fillna(0, inplace=True)\n    \n    return df"
    },
    {
        "function_name": "combine_sparse_categories",
        "file_name": "category_combiner.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "min_count": "int"
        },
        "objectives": [
            "Validate that the category column exists and is of object (string) type.",
            "Compute the frequency of each category in the column.",
            "Identify categories that appear fewer times than the specified minimum count.",
            "Replace those sparse categories with the string 'Other' and return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def combine_sparse_categories(df, category_col, min_count):\n    if category_col not in df.columns or df[category_col].dtype != 'object':\n        raise ValueError(\"The provided category column does not exist or is not of object type\")\n\n    category_counts = df[category_col].value_counts()\n    sparse_categories = category_counts[category_counts < min_count].index\n\n    df[category_col] = df[category_col].apply(\n        lambda x: 'Other' if x in sparse_categories else x\n    )\n    \n    return df"
    },
    {
        "function_name": "add_ewma_and_roc_features",
        "file_name": "time_series_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_column": "str",
            "feature_columns": "list of str"
        },
        "objectives": [
            "Convert the specified time_column to datetime if it is not already, and set it as the dataframe's index.",
            "For each feature column in \"feature_columns\", compute the exponentially weighted moving average (EWMA) with a specified span.",
            "Compute the rate-of-change of these EWMA columns over a specified period.",
            "Return the dataframe with added columns for both EWMA and rate-of-change computations."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_ewma_and_roc_features(df, time_column, feature_columns, span=10, period=5):\n    # Convert time_column to datetime and set as index\n    if not pd.api.types.is_datetime64_any_dtype(df[time_column]):\n        df[time_column] = pd.to_datetime(df[time_column])\n    \n    df = df.set_index(time_column)\n    \n    for feature in feature_columns:\n        ewma_col_name = f\"{feature}_ewma_{span}\"\n        roc_col_name = f\"{feature}_roc_{period}\"\n\n        # Compute EWMA\n        df[ewma_col_name] = df[feature].ewm(span=span).mean()\n        \n        # Compute Rate of Change (ROC)\n        df[roc_col_name] = df[ewma_col_name].pct_change(periods=period) * 100\n        \n    return df"
    },
    {
        "function_name": "smooth_and_segment_features",
        "file_name": "feature_enhancement.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list of str"
        },
        "objectives": [
            "For each column in \"columns\", apply an interpolating spline fit to smooth out the noise in the data.",
            "Compute the first and second derivatives of the smoothed data as new columns.",
            "Segment the smoothed data into windows of a specified size and compute the mean and variance for each window.",
            "Return the enhanced dataframe with the smoothed, derivative, and window statistics columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.interpolate import UnivariateSpline"
        ],
        "function_def": "def smooth_and_segment_features(df, columns, window_size=10):\n    for col in columns:\n        spline = UnivariateSpline(df.index, df[col], s=1)\n        \n        # Smoothed data\n        smoothed_col = f\"{col}_smoothed\"\n        df[smoothed_col] = spline(df.index)\n        \n        # First derivative\n        df[f\"{col}_first_derivative\"] = spline.derivative()(df.index)\n        \n        # Second derivative\n        df[f\"{col}_second_derivative\"] = spline.derivative(n=2)(df.index)\n        \n        # Windowed statistics\n        smoothed_values = df[smoothed_col].values\n        window_means = []\n        window_vars = []\n        \n        for start in range(0, len(smoothed_values), window_size):\n            window = smoothed_values[start:start + window_size]\n            window_means.append(window.mean())\n            window_vars.append(window.var())\n        \n        df[f\"{col}_window_mean\"] = np.repeat(window_means, window_size)[:len(df)]\n        df[f\"{col}_window_var\"] = np.repeat(window_vars, window_size)[:len(df)]\n    \n    return df"
    },
    {
        "function_name": "categorical_transform",
        "file_name": "category_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categories": "list of str"
        },
        "objectives": [
            "For each categorical column, compute the frequency count of each category.",
            "Create new columns representing the proportion of each category in relation to the total count.",
            "Encode the columns using target encoding based on their effect on a specified 'target' column.",
            "Return the transformed dataframe with frequency counts, proportions, and target-encoded columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def categorical_transform(df, categories, target):\n    for cat in categories:\n        freq_col = f\"{cat}_freq\"\n        prop_col = f\"{cat}_prop\"\n        target_enc_col = f\"{cat}_target_enc\"\n        \n        freq_counts = df[cat].value_counts()\n        total_counts = freq_counts.sum()\n        \n        df[freq_col] = df[cat].map(freq_counts)\n        df[prop_col] = df[freq_col] / total_counts\n        \n        target_means = df.groupby(cat)[target].mean()\n        df[target_enc_col] = df[cat].map(target_means)\n        \n    return df"
    },
    {
        "function_name": "grouped_cumulative_features",
        "file_name": "group_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_by_column": "str",
            "target_column": "str"
        },
        "objectives": [
            "Group the dataframe by \"group_by_column\" and calculate cumulative sums for the \"target_column\" within each group.",
            "Compute the expanding mean and expanding standard deviation of the \"target_column\" for each group.",
            "Identify and separate the top-k largest sequences within each group based on the cumulative sum.",
            "Return the transformed dataframe containing the cumulative sum, expanding mean, expanding standard deviation, and top-k sequence columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def grouped_cumulative_features(df, group_by_column, target_column, k=3):\n    df = df.sort_values(by=[group_by_column, target_column]).reset_index(drop=True)\n    \n    grouped = df.groupby(group_by_column)\n    \n    df[f\"{target_column}_cumsum\"] = grouped[target_column].cumsum()\n    df[f\"{target_column}_expanding_mean\"] = grouped[target_column].expanding().mean().reset_index(level=0, drop=True)\n    df[f\"{target_column}_expanding_std\"] = grouped[target_column].expanding().std().reset_index(level=0, drop=True)\n    \n    top_k_indices = grouped[f\"{target_column}_cumsum\"].nlargest(k).index.get_level_values(1)\n    df[f\"{target_column}_top_k\"] = 0\n    df.loc[top_k_indices, f\"{target_column}_top_k\"] = 1\n    \n    return df"
    },
    {
        "function_name": "correlate_one_hot",
        "file_name": "categorical_transformations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str"
        },
        "objectives": [
            "Validate the presence of the specified categorical column in the DataFrame.",
            "Perform one-hot encoding on the categorical column.",
            "Compute the pairwise correlation matrix of the one-hot encoded columns.",
            "Return both the modified DataFrame with the one-hot encoded columns and the correlation matrix."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def correlate_one_hot(df, cat_col):\n    # Step 1: Validate column\n    if cat_col not in df.columns:\n        raise ValueError(f\"Column '{cat_col}' not found in DataFrame\")\n    if not pd.api.types.is_categorical_dtype(df[cat_col]) and not pd.api.types.is_object_dtype(df[cat_col]):\n        raise ValueError(f\"Column '{cat_col}' must be categorical\")\n\n    # Step 2: Perform one-hot encoding\n    df = pd.get_dummies(df, columns=[cat_col], drop_first=True)\n\n    # Step 3: Compute correlation matrix\n    correlation_matrix = df.corr()\n\n    return df, correlation_matrix"
    },
    {
        "function_name": "normalize_product",
        "file_name": "feature_interactions.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_a": "str",
            "col_b": "str"
        },
        "objectives": [
            "Validate the presence of both specified columns and ensure they are numeric.",
            "Compute the pointwise product of the two columns.",
            "Normalize the result to have a mean of 0 and standard deviation of 1.",
            "Return the modified DataFrame with the new normalized product column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def normalize_product(df, col_a, col_b):\n    # Step 1: Validate columns\n    if col_a not in df.columns:\n        raise ValueError(f\"Column '{col_a}' not found in DataFrame\")\n    if col_b not in df.columns:\n        raise ValueError(f\"Column '{col_b}' not found in DataFrame\")\n    if not (pd.api.types.is_numeric_dtype(df[col_a]) and pd.api.types.is_numeric_dtype(df[col_b])):\n        raise ValueError(f\"Both columns '{col_a}' and '{col_b}' must be numeric\")\n\n    # Step 2: Compute pointwise product\n    product_col = df[col_a] * df[col_b]\n\n    # Step 3: Normalize the product column\n    mean_product = product_col.mean()\n    std_product = product_col.std()\n    df[\"normalized_product\"] = (product_col - mean_product) / std_product\n\n    return df"
    },
    {
        "function_name": "create_aggregate_window_features",
        "file_name": "window_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "date_col": "str",
            "feature_cols": "list"
        },
        "objectives": [
            "Ensure the presence and correct data types of `id_col`, `date_col`, and `feature_cols`.",
            "Sort the DataFrame by `id_col` and `date_col`.",
            "Create trailing and forward sum features for the specified `feature_cols`.",
            "Return the DataFrame with the new sum features for each `id_col`."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_aggregate_window_features(df, id_col, date_col, feature_cols):\n    for col in [id_col, date_col] + feature_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column '{date_col}' must be of datetime type\")\n    \n    df = df.sort_values(by=[id_col, date_col])\n    \n    for col in feature_cols:\n        df[f'{col}_trailing_sum'] = df.groupby(id_col)[col].transform(lambda x: x.rolling(window=3, min_periods=1).sum())\n        df[f'{col}_forward_sum'] = df.groupby(id_col)[col].transform(lambda x: x.rolling(window=3, min_periods=1).sum().shift(-2))\n    \n    return df"
    },
    {
        "function_name": "conditional_count_features",
        "file_name": "conditional_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "condition_col": "str",
            "condition_val": "Any",
            "new_col_prefix": "str"
        },
        "objectives": [
            "Ensure the presence and correct type of `condition_col`.",
            "Filter rows based on whether `condition_col` is equal to `condition_val`.",
            "For the filtered rows, create count features based on different columns and store them with a specified prefix.",
            "Return the DataFrame augmented with these new features only for the filtered rows."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def conditional_count_features(df, condition_col, condition_val, new_col_prefix):\n    if condition_col not in df.columns:\n        raise ValueError(f\"Column '{condition_col}' not found in DataFrame\")\n    \n    filtered_df = df[df[condition_col] == condition_val].copy()\n    \n    count_cols = filtered_df.columns.difference([condition_col]).tolist()\n    for col in count_cols:\n        filtered_df[f'{new_col_prefix}_{col}_count'] = filtered_df.groupby(col)[col].transform('count')\n    \n    df = df.merge(filtered_df, how='left', on=condition_col, suffixes=('', '_filtered'))\n    \n    return df"
    },
    {
        "function_name": "handle_outliers",
        "file_name": "outlier_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "columns": "list",
            "strategy": "str"
        },
        "objectives": [
            "Validate the presence of specified `columns` and ensure they are of the appropriate types.",
            "Handle outliers by applying the specified `strategy` ('remove' or 'cap').",
            "If 'remove' strategy is chosen, drop rows containing outliers across specified columns.",
            "If 'cap' strategy is chosen, cap values outside 1.5 times Interquartile Range (IQR) at the boundaries.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def handle_outliers(df, columns, strategy):\n    if strategy not in ['remove', 'cap']:\n        raise ValueError(\"Strategy must be 'remove' or 'cap'\")\n    \n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        \n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric type\")\n    \n    for col in columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        if strategy == 'remove':\n            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n        elif strategy == 'cap':\n            df[col] = df[col].clip(lower_bound, upper_bound)\n    \n    return df"
    },
    {
        "function_name": "bin_numerical_columns",
        "file_name": "discretization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "enc_cols": "list of str",
            "n_bins": "int",
            "strategy": "str"
        },
        "objectives": [
            "Validate that all columns in enc_cols exist in the DataFrame.",
            "Use KBinsDiscretizer to bin the numerical columns in enc_cols into n_bins with the specified strategy ('uniform', 'quantile', 'kmeans').",
            "Replace the original numerical columns with their corresponding bin labels.",
            "Return the transformed DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import KBinsDiscretizer"
        ],
        "function_def": "def bin_numerical_columns(df, enc_cols, n_bins, strategy):\n    df = df.copy()\n    \n    for col in enc_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    \n    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n    df[enc_cols] = discretizer.fit_transform(df[enc_cols])\n    \n    return df"
    },
    {
        "function_name": "compute_rolling_window_avg",
        "file_name": "rolling_window.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "timestamp_col": "str",
            "target_col": "str",
            "window_size": "int"
        },
        "objectives": [
            "Validate that the id_col, timestamp_col, and target_col exist in the DataFrame.",
            "Ensure the timestamp_col is of datetime type and sort the DataFrame based on this column.",
            "Group the DataFrame by id_col and calculate the rolling window average of target_col with the specified window_size.",
            "Return the DataFrame with the new rolling average column included."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def compute_rolling_window_avg(df, id_col, timestamp_col, target_col, window_size):\n    if id_col not in df.columns or timestamp_col not in df.columns or target_col not in df.columns:\n        raise ValueError(f\"Columns {id_col}, {timestamp_col}, and {target_col} must be present in DataFrame\")\n\n    df = df.copy()\n    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n    df.sort_values(by=[id_col, timestamp_col], inplace=True)\n    \n    grouped = df.groupby(id_col)\n    df[f\"rolling_avg_{target_col}\"] = grouped[target_col].rolling(window=window_size).mean().reset_index(level=0, drop=True)\n    \n    return df"
    },
    {
        "function_name": "preprocess_categorical_and_continuous",
        "file_name": "transformations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list of str",
            "continuous_cols": "list of str"
        },
        "objectives": [
            "Validate that all columns in `categorical_cols` and `continuous_cols` exist in the DataFrame and are of the correct types.",
            "Convert all categorical columns into one-hot encoded columns.",
            "Scale all continuous columns to a range between 0 and 1 using Min-Max scaling.",
            "Concatenate the transformed columns and return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def preprocess_categorical_and_continuous(df, categorical_cols, continuous_cols):\n    for col in categorical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Categorical column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_categorical_dtype(df[col]) and not pd.api.types.is_object_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be categorical\")\n\n    for col in continuous_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Continuous column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n\n    df = df.copy()\n    \n    # Convert categorical columns to one-hot encoding\n    df = pd.get_dummies(df, columns=categorical_cols)\n\n    # Scale continuous columns\n    scaler = MinMaxScaler()\n    df[continuous_cols] = scaler.fit_transform(df[continuous_cols])\n\n    return df"
    },
    {
        "function_name": "chi_square_analysis",
        "file_name": "categorical_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "categorical_cols": "List[str]"
        },
        "objectives": [
            "Verify the specified target and categorical columns exist in the DataFrame.",
            "Compute the Chi-square statistic for each categorical column against the target column.",
            "Convert the Chi-square p-values to a standardized scale (e.g., log scale).",
            "Return a DataFrame with the original category columns and their corresponding transformed p-values."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from scipy.stats import chi2_contingency"
        ],
        "function_def": "def chi_square_analysis(df, target_col, categorical_cols):\n    if target_col not in df.columns:\n        raise ValueError(f\"Target column {target_col} is not present in the DataFrame\")\n    for col in categorical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Categorical column {col} is not present in the DataFrame\")\n\n    chi2_df = pd.DataFrame(index=categorical_cols, columns=['p_value_transformed'])\n\n    for col in categorical_cols:\n        contingency_table = pd.crosstab(df[col], df[target_col])\n        chi2, p, dof, expected = chi2_contingency(contingency_table)\n        chi2_df.at[col, 'p_value_transformed'] = -np.log(p)\n    \n    return chi2_df"
    },
    {
        "function_name": "remove_outliers",
        "file_name": "data_cleaning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Validate if the target column exists in the DataFrame.",
            "Identify outliers in the target column using the specified threshold.",
            "Remove entire rows containing outliers from the DataFrame, based on the target column.",
            "Return the cleaned DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_outliers(df, target_col, threshold):\n    if target_col not in df.columns:\n        raise ValueError(f\"Target column {target_col} is not present in the DataFrame\")\n    \n    df = df.copy()\n    mean = df[target_col].mean()\n    std_dev = df[target_col].std()\n    lower_bound = mean - threshold * std_dev\n    upper_bound = mean + threshold * std_dev\n    \n    cleaned_df = df[(df[target_col] >= lower_bound) & (df[target_col] <= upper_bound)]\n    \n    return cleaned_df"
    },
    {
        "function_name": "analyze_and_normalize_text",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_cols": "list of str",
            "min_word_length": "int"
        },
        "objectives": [
            "Validate that all specified text columns exist in the DataFrame and are of string type.",
            "Calculate the total number of words in each text column that meet or exceed the minimum word length.",
            "Normalize each text column by lowercasing all text and removing punctuation.",
            "Return a DataFrame where each row represents a column and contains the calculated word counts and a sample of the processed text column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import string",
            "import numpy as np"
        ],
        "function_def": "def analyze_and_normalize_text(df, text_cols, min_word_length):\n    # Step 1: Validate text columns\n    for col in text_cols:\n        if col not in df.columns or not pd.api.types.is_string_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not of string type\")\n    \n    results = []\n\n    for col in text_cols:\n        # Step 2: Calculate word counts meeting the min_word_length\n        word_counts = df[col].apply(lambda x: len([word for word in str(x).split() if len(word) >= min_word_length]))\n        \n        # Step 3: Normalize text (lowercase and remove punctuation)\n        cleaned_texts = df[col].apply(lambda x: str(x).lower().translate(str.maketrans('', '', string.punctuation)))\n        \n        # Append results to the results list\n        results.append({\n            \"column\": col,\n            \"word_counts\": word_counts.sum(),\n            \"sample_text\": cleaned_texts.sample(1).values[0]\n        })\n    \n    # Convert results to DataFrame\n    summary_df = pd.DataFrame(results)\n    return summary_df"
    },
    {
        "function_name": "impute_missing_values_grouped",
        "file_name": "missing_value_imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "impute_strategy": "str",
            "group_cols": "list of str"
        },
        "objectives": [
            "Validate that the specified `target_col` and `group_cols` exist in the DataFrame and are of the correct type.",
            "Implement different imputation strategies (\"mean\", \"median\", \"mode\") for missing values in `target_col`, grouped by `group_cols`.",
            "Raise error if the specified imputation strategy is not recognized.",
            "Return the DataFrame with the missing values imputed."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_missing_values_grouped(df, target_col, impute_strategy, group_cols):\n    # Step 1: Validate columns\n    if target_col not in df.columns:\n        raise ValueError(f\"Target column {target_col} is not present\")\n    \n    for col in group_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Group column {col} is not present\")\n    \n    valid_strategies = [\"mean\", \"median\", \"mode\"]\n    if impute_strategy not in valid_strategies:\n        raise ValueError(f\"Impute strategy must be one of {valid_strategies}\")\n\n    # Step 2: Impute missing values\n    df = df.copy()\n    \n    if impute_strategy == \"mean\":\n        impute_func = lambda x: x.fillna(x.mean())\n    elif impute_strategy == \"median\":\n        impute_func = lambda x: x.fillna(x.median())\n    elif impute_strategy == \"mode\":\n        impute_func = lambda x: x.fillna(x.mode()[0])\n\n    df[target_col] = df.groupby(group_cols)[target_col].transform(impute_func)\n    \n    return df"
    },
    {
        "function_name": "group_and_compute_operation",
        "file_name": "group_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "id_col": "str",
            "value_col": "str",
            "operation": "str"
        },
        "objectives": [
            "Validate the specified `id_col` and `value_col` exist in the DataFrame and are of numeric type.",
            "Based on the given `operation` (either \"sum\", \"mean\", \"max\", \"min\"), compute the operation for each unique ID in `id_col` on the values in `value_col`.",
            "Group the DataFrame by `id_col` and create a new DataFrame with the computed values for each ID.",
            "Return the new DataFrame with the grouped results."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def group_and_compute_operation(df, id_col, value_col, operation):\n    # Step 1: Validate columns\n    if id_col not in df.columns or not pd.api.types.is_numeric_dtype(df[id_col]):\n        raise ValueError(f\"ID col {id_col} is not present or not of numeric type\")\n    if value_col not in df.columns or not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"Value col {value_col} is not present or not of numeric type\")\n    \n    valid_operations = [\"sum\", \"mean\", \"max\", \"min\"]\n    if operation not in valid_operations:\n        raise ValueError(f\"Operation must be one of {valid_operations}\")\n    \n    # Step 2: Perform the specified operation\n    if operation == \"sum\":\n        grouped_df = df.groupby(id_col)[value_col].sum().reset_index()\n    elif operation == \"mean\":\n        grouped_df = df.groupby(id_col)[value_col].mean().reset_index()\n    elif operation == \"max\":\n        grouped_df = df.groupby(id_col)[value_col].max().reset_index()\n    elif operation == \"min\":\n        grouped_df = df.groupby(id_col)[value_col].min().reset_index()\n    \n    return grouped_df"
    },
    {
        "function_name": "category_based_scaling",
        "file_name": "scaling_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "num_cols": "list of str",
            "scaler_type": "str"
        },
        "objectives": [
            "Validate the specified `category_col` and numeric columns exist in the DataFrame.",
            "For each unique category in `category_col`, apply a specified scaling method (\"standard\", \"minmax\") to the numeric columns.",
            "Raise error for unsupported scaler types.",
            "Return the DataFrame with scaled values along with metadata indicating which scaling was applied to which categories."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
        ],
        "function_def": "def category_based_scaling(df, category_col, num_cols, scaler_type):\n    # Step 1: Validate columns\n    if category_col not in df.columns:\n        raise ValueError(\"Category column not found in DataFrame\")\n    \n    for col in num_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Numeric column {col} not found or not numeric in DataFrame\")\n    \n    # Step 2: Validate scaler type\n    if scaler_type not in [\"standard\", \"minmax\"]:\n        raise ValueError(\"Invalid scaler type. Use 'standard' or 'minmax'\")\n\n    # Step 3: Apply scaling per category\n    df = df.copy()\n    metadata = []\n\n    for category in df[category_col].unique():\n        category_mask = df[category_col] == category\n        if scaler_type == \"standard\":\n            scaler = StandardScaler()\n        else:\n            scaler = MinMaxScaler()\n        \n        scaled_values = scaler.fit_transform(df.loc[category_mask, num_cols])\n        df.loc[category_mask, num_cols] = scaled_values\n\n        # Step 4: Store metadata\n        metadata.append({\n            \"category\": category,\n            \"scaler\": scaler_type,\n            \"scaled_columns\": num_cols\n        })\n    \n    return df, pd.DataFrame(metadata)"
    },
    {
        "function_name": "numerical_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_cols": "list",
            "categories": "int"
        },
        "objectives": [
            "Ensure the numerical_cols exist and contain numeric data types.",
            "Discretize the numerical columns into specified number of categories using k-means clustering.",
            "Assign the cluster labels to each corresponding row.",
            "Return the modified dataframe with new columns for the clustered labels."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def numerical_clustering(df, numerical_cols, categories):\n    for col in numerical_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must exist and be of numeric type\")\n            \n    df = df.copy()\n    for col in numerical_cols:\n        kmeans = KMeans(n_clusters=categories, random_state=42)\n        df[f'{col}_cluster'] = kmeans.fit_predict(df[[col]])\n        \n    return df"
    },
    {
        "function_name": "event_occurrences_by_date",
        "file_name": "event_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "event_cols": "list"
        },
        "objectives": [
            "Ensure the date_col exists and is of datetime type.",
            "Generate a new column for each event column that marks any row as True if it matches a specified date in date_col.",
            "Sum the occurrences of each event per unique date.",
            "Return the modified dataframe with new columns for event flags and a summary dataframe with date-wise event counts."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def event_occurrences_by_date(df, date_col, event_cols):\n    if date_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column '{date_col}' must exist and be of datetime type\")\n        \n    for col in event_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' must exist in the dataframe\")\n    \n    df = df.copy()\n    \n    for col in event_cols:\n        df[f'{col}_flag'] = df[col].notna() & (df[date_col].notnull())\n    \n    summary_df = df.groupby(date_col)[[f'{col}_flag' for col in event_cols]].sum().reset_index()\n    \n    return df, summary_df"
    },
    {
        "function_name": "sales_enrichment",
        "file_name": "sales_analysis.py",
        "parameters": {
            "sales_df": "pandas.DataFrame",
            "cust_df": "pandas.DataFrame",
            "prod_df": "pandas.DataFrame"
        },
        "objectives": [
            "Ensure the sales_df has 'customer_id' and 'product_id' columns.",
            "Merge sales_df with cust_df and prod_df to gather customer and product information.",
            "Calculate total sales for each customer and each product line.",
            "Return a modified sales_df enriched with customer and product data along with summary dataframes for total sales per customer and product."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def sales_enrichment(sales_df, cust_df, prod_df):\n    if 'customer_id' not in sales_df.columns or 'product_id' not in sales_df.columns:\n        raise ValueError(\"sales_df must contain 'customer_id' and 'product_id' columns\")\n\n    sales_df = sales_df.copy()\n    sales_df = sales_df.merge(cust_df, on='customer_id', how='left')\n    sales_df = sales_df.merge(prod_df, on='product_id', how='left')\n    \n    customer_sales_summary = sales_df.groupby('customer_id')['sales_amount'].sum().reset_index()\n    product_sales_summary = sales_df.groupby('product_id')['sales_amount'].sum().reset_index()\n    \n    return sales_df, customer_sales_summary, product_sales_summary"
    },
    {
        "function_name": "hierarchical_cumulative_metrics",
        "file_name": "hierarchical_features.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "hierarchy_cols": "list of str"
        },
        "objectives": [
            "Validate that all columns in `hierarchy_cols` exist in the DataFrame and are categorical.",
            "Group the DataFrame by hierarchical levels specified in `hierarchy_cols`.",
            "For each group, calculate cumulative sum, mean, and max of numeric columns.",
            "Return a modified DataFrame including the computed cumulative metrics for each hierarchical level."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def hierarchical_cumulative_metrics(df, hierarchy_cols):\n    for col in hierarchy_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column `{col}` is not in the DataFrame\")\n        if not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column `{col}` must be categorical\")\n    \n    # Group by hierarchical levels\n    grouped = df.groupby(hierarchy_cols)\n    \n    # Calculating cumulative metrics\n    cumulative_df = grouped.cumsum()\n    mean_df = grouped.transform('mean')\n    max_df = grouped.transform('max')\n    \n    cumulative_df.columns = [f'{col}_cumsum' for col in cumulative_df.columns]\n    mean_df.columns = [f'{col}_mean' for col in mean_df.columns]\n    max_df.columns = [f'{col}_max' for col in max_df.columns]\n    \n    result_df = pd.concat([df, cumulative_df, mean_df, max_df], axis=1)\n    \n    return result_df"
    },
    {
        "function_name": "remove_duplicate_rows",
        "file_name": "duplicate_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols": "list"
        },
        "objectives": [
            "Check for duplicate rows in the dataframe.",
            "Drop rows that are fully duplicated across the specified `cols`.",
            "Create a log of dropped rows and store it in a separate dataframe.",
            "Return the cleaned dataframe and the log of dropped rows."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_duplicate_rows(df, cols):\n    # Identify duplicated rows\n    duplicate_rows = df[df.duplicated(subset=cols, keep=False)]\n    \n    # Drop duplicated rows\n    df_cleaned = df.drop_duplicates(subset=cols)\n    \n    return df_cleaned, duplicate_rows"
    },
    {
        "function_name": "resample_and_interpolate",
        "file_name": "time_series_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "ts_col": "str"
        },
        "objectives": [
            "Check if `ts_col` is of datetime type; convert if not.",
            "Resample the dataframe to a daily frequency, filling in missing dates.",
            "For any missing numerical data after resampling, interpolate values linearly.",
            "Return the resampled dataframe with interpolated values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_interpolate(df, ts_col):\n    if not pd.api.types.is_datetime64_any_dtype(df[ts_col]):\n        df[ts_col] = pd.to_datetime(df[ts_col])\n    \n    # Set the datetime column as index\n    df.set_index(ts_col, inplace=True)\n\n    # Resample to daily frequency\n    df_resampled = df.resample('D').asfreq()\n\n    # Interpolate missing values\n    df_interpolated = df_resampled.interpolate(method='linear')\n    \n    return df_interpolated.reset_index()"
    },
    {
        "function_name": "combine_columns_with_strategy",
        "file_name": "combiner.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_list": "list",
            "new_col_prefix": "str",
            "strategy": "str"
        },
        "objectives": [
            "Verify all columns in `col_list` exist in the dataframe.",
            "Combine the specified columns based on the given strategy ('mean', 'max', 'min', 'sum').",
            "Create a new column for each strategy result with a specified prefix and the strategy name.",
            "Return the dataframe with the newly created columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def combine_columns_with_strategy(df, col_list, new_col_prefix, strategy):\n    # Step 1: Verify all columns in `col_list` exist in `df`\n    for col in col_list:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is not present in the dataframe\")\n        \n    # Step 2: Combine columns based on the given strategy\n    if strategy == 'mean':\n        combined_col = df[col_list].mean(axis=1)\n    elif strategy == 'max':\n        combined_col = df[col_list].max(axis=1)\n    elif strategy == 'min':\n        combined_col = df[col_list].min(axis=1)\n    elif strategy == 'sum':\n        combined_col = df[col_list].sum(axis=1)\n    else:\n        raise ValueError(\"Invalid strategy. Use 'mean', 'max', 'min', or 'sum'.\")\n    \n    # Step 3: Create a new column with the combined results\n    new_col_name = f\"{new_col_prefix}_{strategy}\"\n    df[new_col_name] = combined_col\n    \n    return df"
    },
    {
        "function_name": "weighted_sum_of_scaled_cols",
        "file_name": "scaling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numerical_cols": "list",
            "new_col_name": "str"
        },
        "objectives": [
            "Ensure specified columns exist in `df` and are numerical.",
            "Scale each numerical column to range [0, 1] using min-max scaling.",
            "Compute a weighted sum of the scaled values based on column variances as weights.",
            "Return the dataframe with the new weighted sum column."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def weighted_sum_of_scaled_cols(df, numerical_cols, new_col_name):\n    # Step 1: Validate existence and type of numerical columns\n    for col in numerical_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column {col} is not present or not of numeric type\")\n    \n    # Step 2: Min-max scale each numerical column to range [0, 1]\n    scaled_cols = (df[numerical_cols] - df[numerical_cols].min()) / (df[numerical_cols].max() - df[numerical_cols].min())\n    \n    # Step 3: Compute a weighted sum of scaled values based on variances as weights\n    variances = df[numerical_cols].var()\n    weights = variances / variances.sum()\n    weighted_sum = scaled_cols.dot(weights)\n    \n    # Step 4: Return the dataframe with the new weighted sum column\n    df[new_col_name] = weighted_sum\n    return df"
    },
    {
        "function_name": "bin_and_analyze",
        "file_name": "binned_stat_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "n_bins": "int",
            "bin_col": "str"
        },
        "objectives": [
            "Ensure 'target_col' exists and is numeric.",
            "Bin the target variable into 'n_bins' equal-width intervals.",
            "Compute the mean and variance for each bin.",
            "Return a dataframe with a new column for bin labels and two new columns for the mean and variance of each bin."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_and_analyze(df, target_col, n_bins, bin_col):\n    # Step 1: Ensure 'target_col' exists and is numeric\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n    \n    # Step 2: Bin the target variable\n    df[bin_col], bins = pd.cut(df[target_col], bins=n_bins, labels=False, retbins=True)\n    \n    # Step 3: Compute mean and variance for each bin\n    bin_stats = df.groupby(bin_col)[target_col].agg(['mean', 'var']).reset_index()\n    bin_stats.columns = [bin_col, f'{target_col}_mean', f'{target_col}_variance']\n    \n    # Merge stats with original dataframe\n    df = df.merge(bin_stats, on=bin_col, how='left')\n    \n    return df"
    },
    {
        "function_name": "filter_groups_by_mean_threshold",
        "file_name": "group_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "group_col": "str",
            "agg_col": "str",
            "threshold": "float"
        },
        "objectives": [
            "Validate that group_col and agg_col exist and are of appropriate types.",
            "Group the dataframe by group_col and calculate the mean and standard deviation of agg_col for each group.",
            "Filter groups where the mean of agg_col exceeds the specified threshold.",
            "Return the modified dataframe with an additional 'filtered' column indicating whether each row passes the filter."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_groups_by_mean_threshold(df, group_col, agg_col, threshold):\n    # Step 1: Validate columns\n    if group_col not in df.columns or agg_col not in df.columns:\n        raise ValueError(f\"Column {group_col} or {agg_col} is not present in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[agg_col]):\n        raise ValueError(f\"Column {agg_col} is not numeric\")\n    \n    # Step 2: Calculate group statistics\n    group_stats = df.groupby(group_col)[agg_col].agg(['mean', 'std']).reset_index()\n    \n    # Step 3: Filter groups by mean\n    group_stats['filtered'] = group_stats['mean'] > threshold\n    df = pd.merge(df, group_stats[[group_col, 'filtered']], on=group_col)\n    \n    return df"
    },
    {
        "function_name": "flag_time_gaps",
        "file_name": "time_gaps.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str",
            "interval": "str"
        },
        "objectives": [
            "Validate that datetime_col exists and is of appropriate type.",
            "Ensure that datetime_col is in datetime format.",
            "Calculate the difference in time between consecutive rows based on datetime_col.",
            "Create a flag column to indicate if the time gap exceeds the specified interval.",
            "Return the modified dataframe with the new columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def flag_time_gaps(df, datetime_col, interval):\n    # Step 1: Validate column\n    if datetime_col not in df.columns:\n        raise ValueError(f\"Column {datetime_col} is not present in DataFrame\")\n    \n    # Step 2: Ensure datetime format\n    df[datetime_col] = pd.to_datetime(df[datetime_col])\n    \n    # Step 3: Calculate time differences\n    df['time_diff'] = df[datetime_col].diff().dt.total_seconds()\n    \n    # Step 4: Create flag for time gaps\n    interval_seconds = pd.Timedelta(interval).total_seconds()\n    df['gap_flag'] = df['time_diff'] > interval_seconds\n    \n    return df"
    },
    {
        "function_name": "balance_classes",
        "file_name": "class_balancing.py",
        "parameters": {
            "`df`": "pandas.DataFrame",
            "`target_col`": "str",
            "`class_weight_column`": "str"
        },
        "objectives": [
            "Validate the target column and class weight column exist and are categorical.",
            "Calculate class distribution from the target column.",
            "Apply inverse class frequencies to create weights for each class in the class weight column.",
            "Ensure balanced class distribution by oversampling minority classes based on calculated weights.",
            "Return the resampled dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.utils import resample"
        ],
        "function_def": "def balance_classes(df, target_col, class_weight_column):\n    # Step 1: Validate columns\n    if target_col not in df.columns or not pd.api.types.is_categorical_dtype(df[target_col]):\n        raise ValueError(f\"Target column '{target_col}' not found or not categorical\")\n    \n    if class_weight_column not in df.columns or not pd.api.types.is_categorical_dtype(df[class_weight_column]):\n        raise ValueError(f\"Class weight column '{class_weight_column}' not found or not categorical\")\n    \n    # Step 2: Calculate class distribution\n    class_counts = df[target_col].value_counts()\n    total = class_counts.sum()\n    class_weights = total / class_counts\n    \n    # Step 3: Apply weights\n    df[class_weight_column] = df[target_col].map(class_weights)\n    \n    # Step 4: Resample\n    resampled_df = df.sample(frac=1, replace=True, weights=df[class_weight_column])\n    \n    return resampled_df.reset_index(drop=True)"
    },
    {
        "function_name": "expand_categorical",
        "file_name": "categorical_expansion.py",
        "parameters": {
            "`df`": "pandas.DataFrame",
            "`categorical_cols`": "List[str]",
            "`prefix_sep`": "str"
        },
        "objectives": [
            "Validate specified columns exist and are categorical.",
            "Create dummy/one-hot encoded columns for the specified categorical columns.",
            "Use specified prefix separator in dummy column names.",
            "Return the expanded dataframe with original categorical columns dropped and new dummy columns added."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def expand_categorical(df, categorical_cols, prefix_sep):\n    # Step 1: Validate columns\n    for col in categorical_cols:\n        if col not in df.columns or not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' either not found or not categorical\")\n\n    # Step 2: Create dummies with specified prefix separator\n    df = pd.get_dummies(df, columns=categorical_cols, prefix_sep=prefix_sep)\n    \n    return df"
    },
    {
        "function_name": "calculate_normalized_kurtosis_skewness",
        "file_name": "stat_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col": "str",
            "method": "str (valid values"
        },
        "objectives": [
            "Validate that `col` is numeric.",
            "Normalize column `col` to have mean 0 and variance 1.",
            "Calculate the kurtosis or skewness of the normalized column based on `method`.",
            "Return the calculated value."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import kurtosis, skew"
        ],
        "function_def": "def calculate_normalized_kurtosis_skewness(df, col, method='kurtosis'):\n    if col not in df.columns:\n        raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[col]):\n        raise ValueError(f\"Column '{col}' must be numeric\")\n    \n    # Step 1: Normalize the column\n    norm_col = (df[col] - df[col].mean()) / df[col].std()\n    \n    # Step 2: Calculate kurtosis or skewness\n    if method == 'kurtosis':\n        result = kurtosis(norm_col)\n    elif method == 'skewness':\n        result = skew(norm_col)\n    else:\n        raise ValueError(\"Invalid method. Choose 'kurtosis' or 'skewness'\")\n    \n    return result"
    },
    {
        "function_name": "remove_highly_missing_columns",
        "file_name": "missing_data_handling.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "max_missing": "float"
        },
        "objectives": [
            "Check if the dataframe is valid and not empty.",
            "For each column in the dataframe, calculate the percentage of missing values.",
            "Identify columns with missing values exceeding the `max_missing` threshold and keep their names.",
            "Return a new dataframe excluding columns that exceed the missing threshold."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def remove_highly_missing_columns(df, max_missing):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Calculate the percentage of missing values for each column\n    missing_percentage = df.isnull().mean() * 100\n    \n    # Identify columns to drop\n    columns_to_drop = missing_percentage[missing_percentage > max_missing].index\n    \n    # Drop columns and return new dataframe"
    },
    {
        "function_name": "shift_columns",
        "file_name": "shift_utilities.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_shift": "list",
            "periods": "int",
            "fill_value": "float or None"
        },
        "objectives": [
            "Check if columns specified in `cols_to_shift` exist in the DataFrame.",
            "Shift the specified columns by the given number of periods, filling any resulting NaNs with `fill_value`.",
            "Ensure the fill value is applied correctly, without altering existing NaNs.",
            "Return the DataFrame with shifted columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def shift_columns(df, cols_to_shift, periods, fill_value):\n    for col in cols_to_shift:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} not found in DataFrame\")\n    \n    for col in cols_to_shift:\n        df[col] = df[col].shift(periods)\n        if fill_value is not None:\n            df[col].fillna(fill_value, inplace=True)\n    \n    return df"
    },
    {
        "function_name": "resample_and_fill",
        "file_name": "timeseries_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "time_col": "str",
            "period": "str"
        },
        "objectives": [
            "Validate that `time_col` is a datetime column in the dataframe.",
            "Resample the dataframe based on the specified period (e.g., 'D' for day, 'W' for week).",
            "Forward fill and backward fill any missing values to ensure continuity in the timeseries data.",
            "Return the resampled dataframe with missing values filled."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def resample_and_fill(df, time_col, period):\n    if time_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n        raise ValueError(f\"Column '{time_col}' must be present and of datetime type in the DataFrame\")\n\n    df.set_index(time_col, inplace=True)\n    resampled_df = df.resample(period).asfreq()\n\n    resampled_df.fillna(method='ffill', inplace=True)\n    resampled_df.fillna(method='bfill', inplace=True)\n\n    resampled_df.reset_index(inplace=True)\n    return resampled_df"
    },
    {
        "function_name": "cluster_data",
        "file_name": "clustering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_columns": "list of str",
            "n_clusters": "int"
        },
        "objectives": [
            "Ensure all \"target_columns\" exist and are numeric.",
            "Normalize the columns using Min-Max scaling.",
            "Apply K-means clustering to group the rows into \"n_clusters\".",
            "Return the DataFrame with an additional column indicating the cluster each row belongs to."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def cluster_data(df, target_columns, n_clusters):\n    # Ensure columns exist and are numeric\n    for col in target_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Target column '{col}' is not found in the DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Target column '{col}' must be numeric\")\n\n    # Normalize using Min-Max scaling\n    scaler = MinMaxScaler()\n    df[target_columns] = scaler.fit_transform(df[target_columns])\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    df['cluster'] = kmeans.fit_predict(df[target_columns])\n\n    return df"
    },
    {
        "function_name": "groupby_metrics",
        "file_name": "group_metrics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "groupby_columns": "list of str",
            "metrics_dict": "dict"
        },
        "objectives": [
            "Ensure all \"groupby_columns\" exist and are appropriate for grouping.",
            "Group the DataFrame by \"groupby_columns\".",
            "Compute specified metrics for each group according to \"metrics_dict\".",
            "Return a new DataFrame that consolidates these grouped metrics."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def groupby_metrics(df, groupby_columns, metrics_dict):\n    # Ensure columns for grouping exist\n    for col in groupby_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Group by column '{col}' is not found in the DataFrame\")\n    \n    # Group and compute metrics\n    grouped = df.groupby(groupby_columns).agg(metrics_dict)\n    \n    return grouped.reset_index()"
    },
    {
        "function_name": "filter_and_clean_cats",
        "file_name": "cat_filter.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col_name": "str",
            "threshold": "float",
            "min_pct": "float"
        },
        "objectives": [
            "Filter the DataFrame to only include rows where \"col_name\" values exceed \"threshold\".",
            "Identify categorical features in the filtered DataFrame.",
            "Drop categories that represent less than \"min_pct\" of the total rows.",
            "Return the filtered, cleaned DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_and_clean_cats(df, col_name, threshold, min_pct):\n    # Filter rows by threshold\n    filtered_df = df[df[col_name] > threshold]\n    \n    # Identify categorical columns\n    cat_columns = filtered_df.select_dtypes(include=['object', 'category']).columns\n    \n    # Drop rare categories\n    for col in cat_columns:\n        value_counts = filtered_df[col].value_counts(normalize=True)\n        rare_categories = value_counts[value_counts < min_pct].index\n        filtered_df = filtered_df[~filtered_df[col].isin(rare_categories)]\n    \n    return filtered_df"
    },
    {
        "function_name": "monthly_difference",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "num_col": "str"
        },
        "objectives": [
            "Ensure the date column exists and is of datetime type and the numerical column exists and is of numerical type.",
            "Set the date column as the DataFrame index.",
            "Resample the DataFrame to a monthly frequency and aggregate the numerical column using a sum.",
            "Compute the difference in the sum of the numerical column between consecutive months and return this modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def monthly_difference(df, date_col, num_col):\n    # Step 1: Ensure columns exist and are of correct type\n    if date_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n        raise ValueError(f\"Column {date_col} is not in the dataframe or is not of datetime type\")\n    if num_col not in df.columns or not pd.api.types.is_numeric_dtype(df[num_col]):\n        raise ValueError(f\"Column {num_col} is not in the dataframe or is not of numeric type\")\n    \n    # Step 2: Set date column as index\n    df.set_index(date_col, inplace=True)\n    \n    # Step 3: Resample to monthly frequency and aggregate using sum\n    monthly_sum_df = df.resample('M').sum()\n    \n    # Step 4: Compute difference between consecutive months\n    monthly_sum_df['monthly_diff'] = monthly_sum_df[num_col].diff()\n    \n    return monthly_sum_df[['monthly_diff', num_col]]"
    },
    {
        "function_name": "standardize_and_correlate",
        "file_name": "data_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "num_col_1": "str",
            "num_col_2": "str"
        },
        "objectives": [
            "Ensure the numerical columns exist and are of numeric type.",
            "Standardize the numerical columns (zero mean, unit variance).",
            "Compute the correlation coefficient between the two standardized columns.",
            "Return the correlation coefficient and the standardized DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def standardize_and_correlate(df, num_col_1, num_col_2):\n    # Step 1: Validate numerical columns\n    if num_col_1 not in df.columns or not pd.api.types.is_numeric_dtype(df[num_col_1]):\n        raise ValueError(f\"Column '{num_col_1}' is not a numeric column in the DataFrame\")\n    if num_col_2 not in df.columns or not pd.api.types.is_numeric_dtype(df[num_col_2]):\n        raise ValueError(f\"Column '{num_col_2}' is not a numeric column in the DataFrame\")\n    \n    # Step 2: Standardize the numerical columns\n    scaler = StandardScaler()\n    df[[num_col_1, num_col_2]] = scaler.fit_transform(df[[num_col_1, num_col_2]])\n    \n    # Step 3: Compute correlation coefficient\n    correlation_coefficient = df[num_col_1].corr(df[num_col_2])\n    \n    return correlation_coefficient, df"
    },
    {
        "function_name": "stratified_sampling",
        "file_name": "sampling_methods.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "strat_col": "str",
            "sample_size": "int"
        },
        "objectives": [
            "Ensure the stratification column exists; if it is numeric, convert it to categorical based on unique values.",
            "Stratify the DataFrame based on the specified column.",
            "For each stratum, perform stratified sampling of the given sample size.",
            "Return a concatenated DataFrame of the stratified samples."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def stratified_sampling(df, strat_col, sample_size):\n    # Step 1: Validate stratification column\n    if strat_col not in df.columns:\n        raise ValueError(f\"Stratification column '{strat_col}' does not exist in the DataFrame\")\n    if pd.api.types.is_numeric_dtype(df[strat_col]):\n        df[strat_col] = df[strat_col].astype('category')\n    \n    # Step 2: Perform stratified sampling\n    unique_strata = df[strat_col].unique()\n    stratified_samples = []\n    \n    for stratum in unique_strata:\n        stratum_df = df[df[strat_col] == stratum]\n        if len(stratum_df) < sample_size:\n            raise ValueError(f\"Not enough samples in stratum {stratum} for the required sample size\")\n        stratified_sample = stratum_df.sample(n=sample_size, random_state=42)\n        stratified_samples.append(stratified_sample)\n    \n    return pd.concat(stratified_samples)"
    },
    {
        "function_name": "transform_time_series",
        "file_name": "time_series_transformations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list of str",
            "continuous_cols": "list of str",
            "time_col": "str"
        },
        "objectives": [
            "Ensure that all columns in `categorical_cols` and `continuous_cols` exist in the DataFrame and validate their types.",
            "Parse the `time_col` as datetime and create new columns: day, month, year, day_of_week.",
            "Perform z-score normalization on `continuous_cols`.",
            "Return the transformed DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def transform_time_series(df, categorical_cols, continuous_cols, time_col):\n    # Step 1: Validate column existence and types\n    for col in categorical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Categorical column '{col}' not found in DataFrame\")\n        if not isinstance(df[col].dtype, (pd.CategoricalDtype, object)):\n            raise ValueError(f\"Column '{col}' must be categorical\")\n\n    for col in continuous_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Continuous column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n\n    if time_col not in df.columns:\n        raise ValueError(f\"Time column '{time_col}' not found in DataFrame\")\n    \n    df = df.copy()\n\n    # Step 2: Parse date column and create new time-related columns\n    df[time_col] = pd.to_datetime(df[time_col])\n    df['day'] = df[time_col].dt.day\n    df['month'] = df[time_col].dt.month\n    df['year'] = df[time_col].dt.year\n    df['day_of_week'] = df[time_col].dt.dayofweek\n\n    # Step 3: Z-score normalization\n    df[continuous_cols] = df[continuous_cols].apply(lambda x: (x - x.mean()) / x.std())\n\n    return df"
    },
    {
        "function_name": "clean_and_scale",
        "file_name": "cleaning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "feature_cols": "list of str",
            "outlier_factor": "float"
        },
        "objectives": [
            "Ensure all columns in `feature_cols` exist in the DataFrame and are numerical. Validate existence and type of `target_col`.",
            "For each feature column, use the Z-score to identify and remove outliers that are beyond the specified `outlier_factor`.",
            "Perform min-max scaling on the feature columns.",
            "Return the cleaned and scaled DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def clean_and_scale(df, target_col, feature_cols, outlier_factor):\n    # Step 1: Validate column existence and types\n    if target_col not in df.columns:\n        raise ValueError(f\"Target column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n\n    for col in feature_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Feature column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n\n    df = df.copy()\n\n    # Step 2: Identify and remove outliers\n    for col in feature_cols:\n        z_scores = (df[col] - df[col].mean()) / df[col].std()\n        df = df[np.abs(z_scores) <= outlier_factor]\n    \n    # Step 3: Min-max scaling\n    min_max_scaled = lambda x: (x - x.min()) / (x.max() - x.min())\n    df[feature_cols] = df[feature_cols].apply(min_max_scaled)\n\n    return df"
    },
    {
        "function_name": "filter_by_thresholds",
        "file_name": "threshold_filtering.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "thresholds": "dict"
        },
        "objectives": [
            "Ensure 'target_col' exists and is numeric.",
            "Validate that 'thresholds' is a dictionary with keys as column names and values as numeric threshold values.",
            "Filter out rows where any column in 'thresholds' does not meet its specified threshold value.",
            "Summarize and return the count of removed rows and the filtered DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def filter_by_thresholds(df, target_col, thresholds):\n    if target_col not in df.columns:\n        raise ValueError(f\"Column '{target_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' must be numeric\")\n    if not isinstance(thresholds, dict):\n        raise ValueError(\"Thresholds must be a dictionary with column names as keys and threshold values as values\")\n    \n    original_count = len(df)\n    for col, threshold in thresholds.items():\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n\n        df = df[df[col] > threshold]\n    \n    removed_count = original_count - len(df)\n    return {\"removed_count\": removed_count, \"filtered_df\": df}"
    },
    {
        "function_name": "anova_f_score",
        "file_name": "anova.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_col": "str",
            "target_col": "str"
        },
        "objectives": [
            "Verify that `categorical_col` and `target_col` exist and are string and numeric types respectively.",
            "Encode the `categorical_col` using one-hot encoding.",
            "Perform ANOVA F-test between the target column and each one-hot encoded column to compute F-scores.",
            "Return a DataFrame with F-scores for each one-hot encoded variable."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import OneHotEncoder",
            "from sklearn.feature_selection import f_classif"
        ],
        "function_def": "def anova_f_score(df, categorical_col, target_col):\n    if categorical_col not in df.columns:\n        raise ValueError(f\"Column {categorical_col} not found in DataFrame\")\n    if target_col not in df.columns:\n        raise ValueError(f\"Column {target_col} not found in DataFrame\")\n    if not pd.api.types.is_string_dtype(df[categorical_col]):\n        raise ValueError(f\"Column {categorical_col} must be of string type\")\n    if not pd.api.types.is_numeric_dtype(df[target_col]):\n        raise ValueError(f\"Column {target_col} must be of numeric type\")\n    \n    one_hot_enc = OneHotEncoder()\n    one_hot_encoded = one_hot_enc.fit_transform(df[[categorical_col]]).toarray()\n    one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_enc.get_feature_names_out([categorical_col]))\n    \n    f_scores, _ = f_classif(one_hot_df, df[target_col])\n    f_score_df = pd.DataFrame({'Variable': one_hot_df.columns, 'F-Score': f_scores})\n    \n    return f_score_df"
    },
    {
        "function_name": "bin_and_encode_numeric_features",
        "file_name": "binned_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numeric_threshold": "float"
        },
        "objectives": [
            "Identify columns in the dataframe \"df\" which contain numeric values.",
            "For each numeric column, bin the data based on a specific threshold using quantile-based binning (e.g., bin sizes of equal number of data points).",
            "Encode the binned columns using one-hot encoding.",
            "Return the modified DataFrame with binned and encoded columns."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_and_encode_numeric_features(df, numeric_threshold):\n    numeric_cols = df.select_dtypes(include='number').columns\n    new_df = df.copy()\n\n    for col in numeric_cols:\n        bins = np.linspace(0, 1, numeric_threshold + 1)\n        quantile_bins = np.quantile(new_df[col], bins)\n        new_df[col + '_binned'] = pd.cut(new_df[col], bins=quantile_bins, include_lowest=True)\n        dummy_df = pd.get_dummies(new_df[col + '_binned'], prefix=col + '_binned')\n        new_df = pd.concat([new_df, dummy_df], axis=1)\n        new_df.drop(columns=[col + '_binned'], inplace=True)\n\n    return new_df"
    },
    {
        "function_name": "assign_quantile_labels",
        "file_name": "quantile_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "quantile_ranges": "list",
            "col_to_quantile": "str"
        },
        "objectives": [
            "Ensure the specified column exists and is numerical.",
            "Divide the data column into quantile ranges.",
            "Create a new column that assigns a unique label to each quantile range.",
            "Return the modified DataFrame with the new quantile labels."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def assign_quantile_labels(df, quantile_ranges, col_to_quantile):\n    # Step 1: Ensure specified column exists and is numerical\n    if col_to_quantile not in df.columns:\n        raise ValueError(f\"Column {col_to_quantile} does not exist in DataFrame\")\n    \n    if not np.issubdtype(df[col_to_quantile].dtype, np.number):\n        raise ValueError(f\"Column {col_to_quantile} is not numerical\")\n\n    # Step 2: Divide the column into quantile ranges\n    quantiles = df[col_to_quantile].quantile(quantile_ranges).values\n    \n    # Step 3: Create a new column for quantile labels\n    df['quantile_label'] = pd.cut(df[col_to_quantile], bins=quantiles, labels=['Q1', 'Q2', 'Q3', 'Q4'], include_lowest=True)\n    \n    return df"
    },
    {
        "function_name": "validate_foreign_keys",
        "file_name": "key_validation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "primary_key": "str",
            "fk_dictionary": "dict"
        },
        "objectives": [
            "Ensure the primary key column and all foreign key columns exist in the DataFrame.",
            "Validate foreign key constraints by checking if all foreign key values exist in the corresponding primary key column.",
            "Remove rows that violate foreign key constraints.",
            "Return the cleaned DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def validate_foreign_keys(df, primary_key, fk_dictionary):\n    # Step 1: Ensure primary key column and foreign key columns exist\n    if primary_key not in df.columns:\n        raise ValueError(f\"Primary key column {primary_key} does not exist in DataFrame\")\n\n    for fk_col, fk_table in fk_dictionary.items():\n        if fk_col not in df.columns:\n            raise ValueError(f\"Foreign key column {fk_col} does not exist in DataFrame\")\n\n    # Step 2: Validate foreign key constraints\n    for fk_col, fk_table in fk_dictionary.items():\n        if not df[fk_col].isin(fk_table[primary_key]).all():\n            print(f\"Foreign key constraint violated on column {fk_col}\")\n\n    # Step 3: Remove rows that violate foreign key constraints\n    for fk_col, fk_table in fk_dictionary.items():\n        valid_rows = df[fk_col].isin(fk_table[primary_key])\n        df = df[valid_rows]\n\n    return df"
    },
    {
        "function_name": "scale_normalize_and_summarize",
        "file_name": "scaling_and_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "numeric_cols": "list of str",
            "strategy": "str"
        },
        "objectives": [
            "Scale the specified numeric columns using different strategies like Min-Max, Standard, or Robust scaling.",
            "Normalize the scaled data to have unit norm.",
            "Create a summary statistics table for the original and scaled data.",
            "Save the summary statistics table as a CSV file."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer"
        ],
        "function_def": "def scale_normalize_and_summarize(df, numeric_cols, strategy):\n    df = df.copy()\n    \n    # Step 1: Scaling the specified numeric columns\n    if strategy == 'minmax':\n        scaler = MinMaxScaler()\n    elif strategy == 'standard':\n        scaler = StandardScaler()\n    elif strategy == 'robust':\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Strategy not recognized. Use 'minmax', 'standard', or 'robust'.\")\n    \n    df_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_cols]), columns=numeric_cols, index=df.index)\n    \n    # Step 2: Normalizing the data to have unit norm\n    normalizer = Normalizer()\n    df_normalized = pd.DataFrame(normalizer.fit_transform(df_scaled), columns=numeric_cols, index=df.index)\n    \n    # Step 3: Create a summary statistics table\n    summary_original = df[numeric_cols].describe().T\n    summary_scaled = df_scaled.describe().T\n    summary_normalized = df_normalized.describe().T\n    \n    summary = pd.concat([summary_original, summary_scaled, summary_normalized], keys=['Original', 'Scaled', 'Normalized'], axis=1)\n    \n    # Step 4: Save the summary statistics table as CSV\n    summary.to_csv('summary_statistics.csv')\n    \n    return df_normalized"
    },
    {
        "function_name": "text_ngrams_generation",
        "file_name": "text_ngrams.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "stop_words": "list"
        },
        "objectives": [
            "Remove stop words from the text in the specified column.",
            "Perform stemming on the filtered words.",
            "Generate n-grams (bigrams and trigrams) from the stemmed words.",
            "Add these n-grams as new columns in the DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "from nltk.corpus import stopwords",
            "from nltk.stem import PorterStemmer",
            "from sklearn.feature_extraction.text import CountVectorizer"
        ],
        "function_def": "def text_ngrams_generation(df, text_col, stop_words):\n    df = df.copy()\n    \n    # Step 1: Remove stop words\n    df[text_col] = df[text_col].apply(lambda x: \" \".join([word for word in x.split() if word.lower() not in stop_words]))\n    \n    # Step 2: Perform stemming\n    stemmer = PorterStemmer()\n    df[text_col] = df[text_col].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n    \n    # Step 3: Generate n-grams (bigrams and trigrams)\n    vectorizer = CountVectorizer(ngram_range=(2, 3))\n    ngrams = vectorizer.fit_transform(df[text_col])\n    ngrams_df = pd.DataFrame(ngrams.toarray(), columns=vectorizer.get_feature_names_out(), index=df.index)\n    \n    # Step 4: Add n-grams to the DataFrame\n    df = pd.concat([df, ngrams_df], axis=1)\n    \n    return df"
    },
    {
        "function_name": "merge_on_columns",
        "file_name": "merge_utils.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "primary_col": "str",
            "foreign_col": "str"
        },
        "objectives": [
            "Ensure the presence of primary_col in the dataframe and foreign_col in a separate auxiliary dataset.",
            "Perform a left join on the dataframe using primary_col and foreign_col.",
            "Validate and handle any null values resulting from the join.",
            "Return the merged dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_on_columns(df, aux_df, primary_col, foreign_col):\n    if primary_col not in df.columns:\n        raise ValueError(f\"Column {primary_col} not found in DataFrame\")\n    if foreign_col not in aux_df.columns:\n        raise ValueError(f\"Column {foreign_col} not found in auxiliary DataFrame\")\n    \n    merged_df = df.merge(aux_df, left_on=primary_col, right_on=foreign_col, how='left')\n    \n    # Handle null values post-merge\n    merged_df.fillna(value={'foreign_col': 'Unknown'}, inplace=True)\n    \n    return merged_df"
    },
    {
        "function_name": "time_series_enhancement",
        "file_name": "timeseries_enhancement.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_col": "str",
            "numeric_col": "str"
        },
        "objectives": [
            "Parse `date_col` into datetime and set it as the DataFrame index.",
            "Resample the DataFrame to daily frequency and interpolate missing values using spline interpolation.",
            "Calculate autocorrelation of `numeric_col` over lags of 1 to 7 days.",
            "Create a cyclical time feature for the day of the year and return the modified DataFrame with these features."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def time_series_enhancement(df, date_col, numeric_col):\n    # Step 1: Parse date_col into datetime and set as index\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.set_index(date_col, inplace=True)\n    \n    # Step 2: Resample to daily frequency and interpolate missing values using spline interpolation\n    df = df.resample('D').asfreq()\n    df[numeric_col] = df[numeric_col].interpolate(method='spline', order=2)\n    \n    # Step 3: Calculate autocorrelation of numeric_col over lags 1 to 7 days\n    autocorrs = {f'{numeric_col}_autocorr_lag{lag}': df[numeric_col].autocorr(lag) for lag in range(1, 8)}\n    for key, value in autocorrs.items():\n        df[key] = value\n    \n    # Step 4: Create a cyclical time feature for day of the year\n    day_of_year = df.index.dayofyear\n    df['day_of_year_sin'] = np.sin(2 * np.pi * day_of_year / 365)\n    df['day_of_year_cos'] = np.cos(2 * np.pi * day_of_year / 365)\n    \n    return df"
    },
    {
        "function_name": "aggregate_and_weigh_statistics",
        "file_name": "group_statistics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "group_cols": "list"
        },
        "objectives": [
            "For each combination of values from `group_cols`, calculate the aggregated mean, median, and standard deviation of the `target_col`.",
            "Perform weighted averaging of these statistics based on the inverse of the group size.",
            "Normalize the resulting statistics to unit mean and variance.",
            "Return the modified DataFrame with aggregated, weighted, and normalized statistics."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import StandardScaler"
        ],
        "function_def": "def aggregate_and_weigh_statistics(df, target_col, group_cols):\n    # Step 1: Calculate aggregated mean, median, and std for each group combination\n    group_stats = df.groupby(group_cols)[target_col].agg(['mean', 'median', 'std']).reset_index()\n    \n    # Step 2: Calculate the group size\n    group_sizes = df.groupby(group_cols).size().reset_index(name='size')\n    group_stats = group_stats.merge(group_sizes, on=group_cols)\n    \n    # Step 3: Perform weighted averaging\n    group_stats['weighted_mean'] = group_stats['mean'] / group_stats['size']\n    group_stats['weighted_median'] = group_stats['median'] / group_stats['size']\n    group_stats['weighted_std'] = group_stats['std'] / group_stats['size']\n    \n    # Step 4: Normalize the statistics to unit mean and variance\n    scaler = StandardScaler()\n    stats_to_normalize = ['weighted_mean', 'weighted_median', 'weighted_std']\n    group_stats[stats_to_normalize] = scaler.fit_transform(group_stats[stats_to_normalize])\n    \n    # Merge back to original dataframe\n    df = df.merge(group_stats[group_cols + stats_to_normalize], on=group_cols, how='left')\n    \n    return df"
    },
    {
        "function_name": "generate_time_diff_and_cumulative_count",
        "file_name": "time_diff_and_count.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "entity_col": "str",
            "datetime_col": "str"
        },
        "objectives": [
            "Validate presence and correct data types of `entity_col` (numeric or string) and `datetime_col` (datetime).",
            "Sort dataframe by `entity_col` first and then by `datetime_col` within each entity group.",
            "Compute and add new columns: time difference (in days) from the previous event for each entity, and the cumulative count of events per entity.",
            "Return the dataframe with the new columns added."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def generate_time_diff_and_cumulative_count(df, entity_col, datetime_col):\n    # Validate columns\n    if entity_col not in df.columns or datetime_col not in df.columns:\n        raise ValueError(f\"Columns '{entity_col}' and '{datetime_col}' must be present\")\n    if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n        raise ValueError(f\"Column '{datetime_col}' must be of datetime type\")\n\n    df = df.copy()\n\n    # Sort dataframe\n    df.sort_values(by=[entity_col, datetime_col], inplace=True)\n\n    # Compute time differences\n    df['time_diff_days'] = df.groupby(entity_col)[datetime_col].diff().dt.days\n    \n    # Cumulative count of events per entity\n    df['event_count'] = df.groupby(entity_col).cumcount() + 1\n\n    return df"
    },
    {
        "function_name": "apply_rolling_statistics",
        "file_name": "rolling_statistics.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int",
            "min_periods": "int"
        },
        "objectives": [
            "Validate that the dataframe is not empty and has at least one numeric or datetime column.",
            "For each numeric column, compute a rolling mean with the given `window_size` and `min_periods`.",
            "For each datetime column, compute a rolling count of non-null values within the window.",
            "Add the new columns with rolling statistics to the dataframe and return it."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def apply_rolling_statistics(df, window_size, min_periods):\n    if df.empty:\n        raise ValueError(\"Input dataframe is empty\")\n    numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n    datetime_columns = df.select_dtypes(include=['datetime']).columns.tolist()\n    if not numeric_columns and not datetime_columns:\n        raise ValueError(\"Dataframe must have at least one numeric or datetime column\")\n\n    df = df.copy()\n\n    # Compute rolling mean for numeric columns\n    for col in numeric_columns:\n        df[f'{col}_rolling_mean_{window_size}'] = df[col].rolling(window=window_size, min_periods=min_periods).mean()\n\n    # Compute rolling count of non-null values for datetime columns\n    for col in datetime_columns:\n        df[f'{col}_rolling_count_{window_size}'] = df[col].rolling(window=window_size, min_periods=min_periods).count()\n\n    return df"
    },
    {
        "function_name": "stratified_train_test_split",
        "file_name": "splitting_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "train_size": "float",
            "target_col": "str"
        },
        "objectives": [
            "Validate that the target column exists",
            "Ensure the training size is a valid fraction (0 < train_size < 1)",
            "Split the DataFrame into training and test sets based on the specified training size",
            "Preserve the class distribution of the target column in both training and test sets using stratified splitting"
        ],
        "import_lines": [
            "from sklearn.model_selection import train_test_split",
            "import pandas as pd"
        ],
        "function_def": "def stratified_train_test_split(df, train_size, target_col):\n    if target_col not in df.columns:\n        raise ValueError(f\"Target column '{target_col}' not present in DataFrame\")\n        \n    if not (0 < train_size < 1):\n        raise ValueError(\"Train size must be a float between 0 and 1\")\n    \n    y = df[target_col]\n    X_train, X_test, y_train, y_test = train_test_split(df, y, train_size=train_size, stratify=y)\n    \n    train_df = X_train.copy()\n    train_df[target_col] = y_train\n    \n    test_df = X_test.copy()\n    test_df[target_col] = y_test\n    \n    return train_df, test_df"
    },
    {
        "function_name": "count_keywords",
        "file_name": "keyword_counter.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_col": "str",
            "keyword_list": "list"
        },
        "objectives": [
            "Ensure 'text_col' exists in the DataFrame.",
            "Perform case-insensitive search for each keyword in 'keyword_list' within the 'text_col'.",
            "Count the occurrences of each keyword in the 'text_col' for each row.",
            "Return a DataFrame with original columns plus additional columns for each count of keywords."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def count_keywords(df, text_col, keyword_list):\n    if text_col not in df.columns:\n        raise ValueError(f\"Column '{text_col}' not found in DataFrame\")\n    \n    df = df.copy()\n    \n    for keyword in keyword_list:\n        col_name = f'count_{keyword}'\n        df[col_name] = df[text_col].str.lower().str.count(keyword.lower())\n    \n    return df"
    },
    {
        "function_name": "one_hot_encode_top_n",
        "file_name": "one_hot_encoder.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list",
            "top_n": "int"
        },
        "objectives": [
            "Ensure all specified 'categorical_cols' exist.",
            "For each categorical column, identify the top \"n\" most frequent categories.",
            "Create one-hot encoded columns for these top \"n\" categories.",
            "Return the modified DataFrame with one-hot encoded columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def one_hot_encode_top_n(df, categorical_cols, top_n):\n    for col in categorical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    \n    df = df.copy()\n    \n    for col in categorical_cols:\n        top_categories = df[col].value_counts().nlargest(top_n).index\n        for category in top_categories:\n            df[f\"{col}_{category}\"] = (df[col] == category).astype(int)\n    \n    return df"
    },
    {
        "function_name": "calculate_ewma_ewmsd",
        "file_name": "ewma_calculator.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "window_size": "int",
            "numeric_cols": "list"
        },
        "objectives": [
            "Ensure all specified 'numeric_cols' exist and are numeric types.",
            "Calculate the exponentially weighted moving average (EWMA) for each numeric column based on the 'window_size'.",
            "Calculate the exponentially weighted moving standard deviation (EWMSD) for each numeric column.",
            "Return a DataFrame with the original columns plus additional columns for EWMA and EWMSD."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_ewma_ewmsd(df, window_size, numeric_cols):\n    for col in numeric_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' not found or is not numeric in DataFrame\")\n    \n    df = df.copy()\n    \n    for col in numeric_cols:\n        df[f\"{col}_ewma\"] = df[col].ewm(span=window_size, adjust=False).mean()\n        df[f\"{col}_ewmsd\"] = df[col].ewm(span=window_size, adjust=False).std()\n    \n    return df"
    },
    {
        "function_name": "top_n_within_category",
        "file_name": "ranking_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "num_col": "str",
            "n": "int"
        },
        "objectives": [
            "Validate that the specified categorical and numeric columns exist and are of appropriate types.",
            "Calculate the rank of each numeric value within each category.",
            "Extract the top-N rows within each category based on the rank and sort them.",
            "Return the dataframe containing the top-N ranked rows for each category."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def top_n_within_category(df, cat_col, num_col, n):\n    if cat_col not in df.columns:\n        raise ValueError(f\"Column '{cat_col}' is not found in the DataFrame\")\n    if num_col not in df.columns or not np.issubdtype(df[num_col].dtype, np.number):\n        raise ValueError(f\"Column '{num_col}' is not found in the DataFrame or is not numeric\")\n\n    df['rank'] = df.groupby(cat_col)[num_col].rank(method='first', ascending=False)\n    top_n_df = df[df['rank'] <= n].sort_values([cat_col, 'rank']).reset_index(drop=True)\n    top_n_df = top_n_df.drop(columns=['rank'])\n\n    return top_n_df"
    },
    {
        "function_name": "impute_groupby_statistic",
        "file_name": "imputation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "value_col": "str",
            "method": "str"
        },
        "objectives": [
            "Validate the specified columns and ensure `method` is one of \"mean\", \"median\".",
            "Group by `category_col` and compute the specified statistic (`method`) for each group.",
            "Replace the original values in `value_col` with the computed statistic for their respective groups.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def impute_groupby_statistic(df, category_col, value_col, method):\n    # Step 1: Validate columns\n    if category_col not in df.columns:\n        raise ValueError(f\"Category column '{category_col}' must be present\")\n    if value_col not in df.columns or not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"Value column '{value_col}' must be present and numeric\")\n    \n    # Step 2: Validate method\n    if method not in [\"mean\", \"median\"]:\n        raise ValueError(f\"Method must be either 'mean' or 'median'\")\n    \n    df = df.copy()\n    \n    # Step 3: Compute the statistic for each group\n    if method == \"mean\":\n        imputed_values = df.groupby(category_col)[value_col].transform('mean')\n    else:\n        imputed_values = df.groupby(category_col)[value_col].transform('median')\n    \n    # Step 4: Replace original values with imputed values\n    df[value_col] = imputed_values\n    \n    return df"
    },
    {
        "function_name": "groupby_aggregate_and_diff",
        "file_name": "groupby_aggregator.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "groupby_cols": "list of str",
            "agg_cols": "dict (keys are column names, values are aggregation functions)",
            "diff_cols": "list of str"
        },
        "objectives": [
            "Group the dataframe by 'groupby_cols' and apply aggregation operations specified in 'agg_cols'.",
            "For specified 'diff_cols', compute the difference between consecutive rows within each group.",
            "Append these differences as new columns with suffix '_diff' to the dataframe.",
            "Return the aggregated dataframe with the added difference columns."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def groupby_aggregate_and_diff(df, groupby_cols, agg_cols, diff_cols):\n    # Group by and aggregate\n    grouped_df = df.groupby(groupby_cols).agg(agg_cols).reset_index()\n    \n    # Compute differences for specified columns\n    for col in diff_cols:\n        grouped_df[f'{col}_diff'] = grouped_df.groupby(groupby_cols)[col].diff()\n    \n    return grouped_df"
    },
    {
        "function_name": "analyze_threshold_violations",
        "file_name": "threshold_analysis.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "threshold": "int",
            "columns_to_analyze": "list"
        },
        "objectives": [
            "Validate that all columns in `columns_to_analyze` are present in the DataFrame and are numeric.",
            "Identify rows where any of the specified columns have values below the threshold.",
            "Calculate the proportion of such rows for each specified column.",
            "Return a DataFrame summarizing the proportions of rows below the threshold for each column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def analyze_threshold_violations(df, threshold, columns_to_analyze):\n    # Step 1: Validate columns\n    for col in columns_to_analyze:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n    \n    # Step 2: Identify rows below threshold\n    below_threshold = df[columns_to_analyze] < threshold\n    \n    # Step 3: Calculate proportions\n    proportions = below_threshold.mean()\n    \n    # Step 4: Create summary DataFrame\n    summary_df = pd.DataFrame(proportions, columns=['proportion_below_threshold'])\n    \n    return summary_df"
    },
    {
        "function_name": "add_normalized_interaction_term",
        "file_name": "interaction_terms.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col1": "str",
            "col2": "str",
            "new_col_name": "str"
        },
        "objectives": [
            "Validate that both columns `col1` and `col2` exist in the DataFrame and are numeric.",
            "Calculate the correlation between `col1` and `col2`.",
            "Create a new column in the DataFrame with `new_col_name` containing the product of `col1` and `col2`.",
            "Normalize the newly created column values to the range [0, 1] using MinMaxScaler."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler"
        ],
        "function_def": "def add_normalized_interaction_term(df, col1, col2, new_col_name):\n    # Step 1: Validate columns\n    for col in [col1, col2]:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be numeric\")\n    \n    # Step 2: Calculate correlation (not necessary for other steps but can be useful context)\n    correlation = df[[col1, col2]].corr().iloc[0, 1]\n    print(f\"Correlation between {col1} and {col2}: {correlation}\")\n    \n    # Step 3: Create interaction term\n    df[new_col_name] = df[col1] * df[col2]\n    \n    # Step 4: Normalize interaction term\n    scaler = MinMaxScaler()\n    df[new_col_name] = scaler.fit_transform(df[[new_col_name]])\n    \n    return df"
    },
    {
        "function_name": "merge_low_frequency_categories",
        "file_name": "category_merging.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "categorical_cols": "list",
            "min_freq": "int"
        },
        "objectives": [
            "Validate that all columns in `categorical_cols` exist in the DataFrame and are of categorical type.",
            "For each categorical column, identify categories with a frequency lower than `min_freq`.",
            "Merge these low-frequency categories into a single category named 'Other'.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def merge_low_frequency_categories(df, categorical_cols, min_freq):\n    # Step 1: Validate columns\n    for col in categorical_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if not pd.api.types.is_categorical_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must be of categorical type\")\n    \n    # Step 2: Identify low-frequency categories and merge\n    df = df.copy()\n    \n    for col in categorical_cols:\n        freq = df[col].value_counts()\n        low_freq_categories = freq[freq < min_freq].index\n        \n        # Step 3: Replace low-frequency categories with 'Other'\n        df[col] = df[col].apply(lambda x: 'Other' if x in low_freq_categories else x)\n        \n        # Step 4: Convert back to categorical type\n        df[col] = df[col].astype('category')\n    \n    return df"
    },
    {
        "function_name": "generate_flat_pivot_table",
        "file_name": "pivot_table_generation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "pivot_index": "str",
            "pivot_columns": "list",
            "values": "str",
            "aggfunc": "str"
        },
        "objectives": [
            "Validate that the pivot_index and pivot_columns exist in the DataFrame.",
            "Validate that the values column exists in the DataFrame and is numeric.",
            "Generate a pivot table with the given index, columns, values, and aggregation function.",
            "Flatten the multi-level column index in the resulting pivot table."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def generate_flat_pivot_table(df, pivot_index, pivot_columns, values, aggfunc):\n    # Step 1: Validate columns\n    for col in pivot_columns + [pivot_index, values]:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        if col == values and not pd.api.types.is_numeric_dtype(df[values]):\n            raise ValueError(f\"Values column '{values}' must be numeric\")\n    \n    # Step 2: Generate pivot table\n    pivot_table = pd.pivot_table(df, index=pivot_index, columns=pivot_columns, values=values, aggfunc=aggfunc)\n    \n    # Step 3: Flatten the multi-level column index\n    pivot_table.columns = ['_'.join(map(str, col)).strip() for col in pivot_table.columns.values]\n    pivot_table.reset_index(inplace=True)\n    \n    return pivot_table"
    },
    {
        "function_name": "perform_operation",
        "file_name": "arithmetic_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "primary_col": "str",
            "secondary_col": "str",
            "operation": "str"
        },
        "objectives": [
            "Ensure both specified columns exist and are numeric.",
            "Perform the specified arithmetic operation (addition, subtraction, multiplication, division) between the two columns.",
            "Create a new column named 'operation_result' to store the result of the operation.",
            "Return the modified DataFrame with the new operation result column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def perform_operation(df, primary_col, secondary_col, operation):\n    if primary_col not in df.columns or secondary_col not in df.columns:\n        raise ValueError(f\"One or both columns '{primary_col}', '{secondary_col}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[primary_col]) or not pd.api.types.is_numeric_dtype(df[secondary_col]):\n        raise ValueError(f\"Both columns '{primary_col}' and '{secondary_col}' must be numeric\")\n\n    if operation == 'add':\n        df['operation_result'] = df[primary_col] + df[secondary_col]\n    elif operation == 'subtract':\n        df['operation_result'] = df[primary_col] - df[secondary_col]\n    elif operation == 'multiply':\n        df['operation_result'] = df[primary_col] * df[secondary_col]\n    elif operation == 'divide':\n        df['operation_result'] = df[primary_col] / df[secondary_col]\n    else:\n        raise ValueError(f\"Invalid operation '{operation}'. Supported operations: 'add', 'subtract', 'multiply', 'divide'\")\n    \n    return df"
    },
    {
        "function_name": "weighted_encode",
        "file_name": "weighted_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str",
            "encoded_cols": "list"
        },
        "objectives": [
            "For each column in \"encoded_cols\", compute the correlation with the \"target_col\".",
            "Identify the column from \"encoded_cols\" with the highest absolute correlation to \"target_col\".",
            "Implement a weighted encoding for the identified column based on frequency and correlation.",
            "Return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def weighted_encode(df, target_col, encoded_cols):\n    # Step 1: Compute correlations with the target column\n    correlations = {}\n    for col in encoded_cols:\n        correlations[col] = df[col].corr(df[target_col])\n    \n    # Step 2: Identify column with highest absolute correlation\n    best_col = max(correlations, key=lambda col: abs(correlations[col]))\n    best_correlation = correlations[best_col]\n    \n    # Step 3: Implement weighted encoding\n    freq = df[best_col].value_counts(normalize=True)\n    encoding_map = {k: v * best_correlation for k, v in freq.items()}\n    \n    df[best_col] = df[best_col].map(encoding_map)\n    \n    return df"
    },
    {
        "function_name": "add_cluster_labels",
        "file_name": "clustering_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "num_clusters": "int",
            "clustering_columns": "list"
        },
        "objectives": [
            "Validate \"num_clusters\" to ensure it is a positive integer.",
            "Perform k-means clustering on the specified \"clustering_columns\".",
            "Assign each row in the dataframe a cluster label based on the k-means clustering result.",
            "Append the cluster labels to the original dataframe and return the modified dataframe."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def add_cluster_labels(df, num_clusters, clustering_columns):\n    if not isinstance(num_clusters, int) or num_clusters <= 0:\n        raise ValueError(\"'num_clusters' must be a positive integer\")\n\n    kmeans = KMeans(n_clusters=num_clusters)\n    cluster_labels = kmeans.fit_predict(df[clustering_columns])\n    \n    df['cluster_label'] = cluster_labels\n    \n    return df"
    },
    {
        "function_name": "calculate_date_difference",
        "file_name": "date_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "date_column": "str",
            "reference_date": "str"
        },
        "objectives": [
            "Validate that the date_column exists and contains dates.",
            "Parse the reference_date and ensure it's in a recognizable date format.",
            "Calculate date differences (in days) between the dates in the date column and the reference date.",
            "Add this information as a new column in the DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def calculate_date_difference(df, date_column, reference_date):\n    # Step 1: Ensure date_column exists and contains date values\n    if date_column not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[date_column]):\n        raise ValueError(f\"Column {date_column} is either not present or not of date type\")\n    \n    # Step 2: Parse reference_date\n    try:\n        ref_date = pd.to_datetime(reference_date)\n    except ValueError:\n        raise ValueError(f\"Reference date {reference_date} is not in a recognizable date format\")\n    \n    # Step 3: Calculate date differences\n    df['date_difference_days'] = (df[date_column] - ref_date).dt.days\n    \n    return df"
    },
    {
        "function_name": "apply_column_transformations",
        "file_name": "transformation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "column_specs": "dict"
        },
        "objectives": [
            "Validate that column_specs keys are present in the DataFrame.",
            "Apply specified transformations (such as scaling, normalization, or user-defined transformations) based on the column_specs configuration.",
            "Ensure no NaN or inf values are generated after transformation.",
            "Return the transformed DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
        ],
        "function_def": "def apply_column_transformations(df, column_specs):\n    # Step 1: Ensure columns in column_specs are present in the DataFrame\n    for col in column_specs:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} specified in column_specs is not present in the DataFrame\")\n    \n    # Step 2: Apply transformations\n    for col, transform in column_specs.items():\n        if transform == 'scale':\n            scaler = StandardScaler()\n            df[col] = scaler.fit_transform(df[[col]])\n        elif transform == 'normalize':\n            scaler = MinMaxScaler()\n            df[col] = scaler.fit_transform(df[[col]])\n        elif callable(transform):\n            df[col] = df[col].apply(transform)\n        else:\n            raise ValueError(f\"Invalid transformation specified for column {col}\")\n    \n    # Step 3: Ensure no NaN or inf values are generated\n    if df.isnull().any().any() or np.isinf(df).any().any():\n        raise ValueError(\"Transformation resulted in NaN or inf values\")\n    \n    return df"
    },
    {
        "function_name": "bin_features_and_replace_labels",
        "file_name": "feature_binning.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "feature_col": "str",
            "label_col": "str"
        },
        "objectives": [
            "Ensure 'feature_col' and 'label_col' are present in the DataFrame. If either is missing, raise a ValueError.",
            "Bin the values in 'feature_col' into quartiles and create a new column 'feature_col_bin' representing these bins.",
            "Calculate the mean of 'label_col' for each bin and create a mapping of bin to mean value.",
            "Replace the values in 'label_col' with their respective bin's mean from the mapping.",
            "Return the modified DataFrame and the bin-to-mean mapping."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def bin_features_and_replace_labels(df, feature_col, label_col):\n    # Step 1: Ensure required columns are present\n    if feature_col not in df.columns or label_col not in df.columns:\n        raise ValueError(f\"Columns '{feature_col}' and/or '{label_col}' not found in DataFrame\")\n    \n    df = df.copy()\n\n    # Step 2: Bin feature_col into quartiles\n    df[f\"{feature_col}_bin\"] = pd.qcut(df[feature_col], 4, labels=False)\n\n    # Step 3: Calculate mean of label_col for each bin\n    bin_means = df.groupby(f\"{feature_col}_bin\")[label_col].mean().to_dict()\n\n    # Step 4: Replace label_col values with bin means\n    df[label_col] = df[f\"{feature_col}_bin\"].map(bin_means)\n\n    return df, bin_means"
    },
    {
        "function_name": "add_term_presence_column",
        "file_name": "text_processing.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "text_column": "str",
            "new_column_name": "str",
            "terms_list": "list"
        },
        "objectives": [
            "Ensure the `text_column` exists in the DataFrame and is of object type.",
            "Tokenize the text in the `text_column` into words.",
            "Create a new column with binary values indicating the presence of any term from `terms_list` in each row of the `text_column`.",
            "Return the DataFrame with the new binary indicator column."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def add_term_presence_column(df, text_column, new_column_name, terms_list):\n    if text_column not in df.columns:\n        raise ValueError(f\"Text column '{text_column}' not found in DataFrame\")\n    if not pd.api.types.is_object_dtype(df[text_column]):\n        raise ValueError(f\"Text column '{text_column}' must be of object type\")\n\n    # Step 2: Tokenize text and check for term presence\n    df[new_column_name] = df[text_column].apply(\n        lambda text: any(term in text.split() for term in terms_list)\n    )\n\n    return df"
    },
    {
        "function_name": "create_lagged_features",
        "file_name": "time_series_lagging.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_column": "str",
            "lag_periods": "int"
        },
        "objectives": [
            "Ensure the `target_column` exists in the DataFrame and is numeric.",
            "Create lagged versions of the `target_column`, shifting the values by the specified `lag_periods`.",
            "Handle missing values caused by the lagging by removing them (i.e., dropping rows containing NaNs).",
            "Return the DataFrame with the lagged column and without any NaNs."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def create_lagged_features(df, target_column, lag_periods):\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[target_column]):\n        raise ValueError(f\"Target column '{target_column}' must be numeric\")\n\n    # Step 2: Create lagged versions of target_column\n    for lag in range(1, lag_periods + 1):\n        df[f'{target_column}_lag_{lag}'] = df[target_column].shift(lag)\n    \n    # Step 3: Drop rows with NaN values due to lagging\n    df = df.dropna().reset_index(drop=True)\n    \n    return df"
    },
    {
        "function_name": "stratified_sampling",
        "file_name": "sampling_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "sampled_col": "str",
            "stratify_col": "str",
            "train_size": "float"
        },
        "objectives": [
            "Ensure `sampled_col` and `stratify_col` are in the dataframe and are categorical.",
            "Verify that `train_size` is between 0 and 1.",
            "Perform stratified sampling based on `stratify_col` to assign the samples to train and test datasets.",
            "Return two DataFrames: train and test datasets."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split"
        ],
        "function_def": "def stratified_sampling(df, sampled_col, stratify_col, train_size):\n    if sampled_col not in df.columns or not pd.api.types.is_numeric_dtype(df[sampled_col]):\n        raise ValueError(f\"Column '{sampled_col}' not found or is not numeric in DataFrame\")\n    if stratify_col not in df.columns or not pd.api.types.is_categorical_dtype(df[stratify_col]):\n        raise ValueError(f\"Column '{stratify_col}' not found or is not categorical in DataFrame\")\n    if not (0 < train_size < 1):\n        raise ValueError(\"train_size must be between 0 and 1\")\n\n    df = df.copy()\n    \n    train_df, test_df = train_test_split(df, train_size=train_size, stratify=df[stratify_col], random_state=42)\n    \n    return train_df, test_df"
    },
    {
        "function_name": "handle_large_values",
        "file_name": "value_operations.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "col": "str",
            "threshold": "float",
            "method": "str"
        },
        "objectives": [
            "Ensure `col` exists and is numeric.",
            "Verify that the `threshold` is a non-negative number.",
            "Depending on `method` ('clip', 'drop'), either clip the values exceeding the threshold or remove the rows with values exceeding the threshold.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def handle_large_values(df, col, threshold, method):\n    if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n        raise ValueError(f\"Column '{col}' not found or is not numeric in DataFrame\")\n    if threshold < 0:\n        raise ValueError(\"Threshold must be a non-negative number\")\n    if method not in ['clip', 'drop']:\n        raise ValueError(\"Method must be either 'clip' or 'drop'\")\n    \n    df = df.copy()\n    \n    if method == 'clip':\n        df[col] = df[col].clip(upper=threshold)\n    else:\n        df = df[df[col] <= threshold]\n    \n    return df"
    },
    {
        "function_name": "aggregate_scaled_numerics",
        "file_name": "scale_and_aggregate.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cat_col": "str",
            "num_col": "str",
            "scale_method": "str (valid values"
        },
        "objectives": [
            "Validate that `cat_col` is categorical and `num_col` is numeric.",
            "Apply either Min-Max scaling or Z-score normalization to `num_col` based on `scale_method`.",
            "Calculate the mean and standard deviation of the scaled numeric column for each category in `cat_col`.",
            "Return a DataFrame with these aggregated statistics, including columns for the original mean and standard deviation of `num_col`."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
        ],
        "function_def": "def aggregate_scaled_numerics(df, cat_col, num_col, scale_method='min_max'):\n    if cat_col not in df.columns or num_col not in df.columns:\n        raise ValueError(f\"Columns '{cat_col}' and/or '{num_col}' not found in DataFrame\")\n    if not pd.api.types.is_categorical_dtype(df[cat_col]):\n        raise ValueError(f\"Column '{cat_col}' must be categorical\")\n    if not pd.api.types.is_numeric_dtype(df[num_col]):\n        raise ValueError(f\"Column '{num_col}' must be numeric\")\n    \n    # Step 1: Apply scaling\n    if scale_method == 'min_max':\n        scaler = MinMaxScaler()\n    elif scale_method == 'z_score':\n        scaler = StandardScaler()\n    else:\n        raise ValueError(\"Invalid scale method. Choose 'min_max' or 'z_score'\")\n    \n    df[f'{num_col}_scaled'] = scaler.fit_transform(df[[num_col]])\n    \n    # Step 2: Calculate mean and standard deviation of scaled column per category\n    aggregated_stats = df.groupby(cat_col).agg(\n        original_mean=(num_col, 'mean'),\n        original_std=(num_col, 'std'),\n        scaled_mean=(f'{num_col}_scaled', 'mean'),\n        scaled_std=(f'{num_col}_scaled', 'std')\n    )\n    \n    return aggregated_stats.reset_index()"
    },
    {
        "function_name": "binarize_columns",
        "file_name": "binarization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "cols_to_binarize": "list",
            "threshold_dict": "dict"
        },
        "objectives": [
            "Ensure all specified columns exist in the DataFrame and are numerical.",
            "Apply binarization based on thresholds specified in the dictionary (values < threshold -> 0, values >= threshold -> 1).",
            "Add a suffix '_bin' to the names of the binarized columns.",
            "Return the modified DataFrame."
        ],
        "import_lines": [
            "import pandas as pd",
            "import numpy as np"
        ],
        "function_def": "def binarize_columns(df, cols_to_binarize, threshold_dict):\n    # Step 1: Ensure all specified columns exist and are numerical\n    for col in cols_to_binarize:\n        if col not in df.columns or not np.issubdtype(df[col].dtype, np.number):\n            raise ValueError(f\"Column '{col}' must be present and of numerical type\")\n    \n    # Step 2: Apply binarization based on thresholds\n    for col in cols_to_binarize:\n        if col not in threshold_dict:\n            raise ValueError(f\"Threshold for column '{col}' is not provided in the threshold_dict\")\n        \n        threshold = threshold_dict[col]\n        df[col + '_bin'] = (df[col] >= threshold).astype(int)\n    \n    return df"
    },
    {
        "function_name": "extract_datetime_components",
        "file_name": "datetime_extraction.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "datetime_col": "str"
        },
        "objectives": [
            "Validate that `datetime_col` exists in the DataFrame and is of datetime type.",
            "Generate new columns for year, month, day, hour, and minute based on `datetime_col`.",
            "Ensure none of the new columns created have NaN values.",
            "Return the DataFrame with the new datetime components."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def extract_datetime_components(df, datetime_col):\n    if datetime_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n        raise ValueError(f\"Column '{datetime_col}' not found or is not a datetime column in DataFrame\")\n    \n    df = df.copy()\n    df['year'] = df[datetime_col].dt.year\n    df['month'] = df[datetime_col].dt.month\n    df['day'] = df[datetime_col].dt.day\n    df['hour'] = df[datetime_col].dt.hour\n    df['minute'] = df[datetime_col].dt.minute\n\n    for col in ['year', 'month', 'day', 'hour', 'minute']:\n        if df[col].isna().any():\n            raise RuntimeError(f\"Generated column '{col}' contains NaN values which is unexpected\")\n    \n    return df"
    },
    {
        "function_name": "encode_target_column",
        "file_name": "target_encoding.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "target_col": "str"
        },
        "objectives": [
            "Confirm that the `target_col` exists in the DataFrame and is a string type.",
            "Count the occurrences of each unique value in `target_col`.",
            "Encode the `target_col` into categorical integer codes.",
            "Return the DataFrame with an additional column for the encoded values."
        ],
        "import_lines": [
            "import pandas as pd"
        ],
        "function_def": "def encode_target_column(df, target_col):\n    if target_col not in df.columns or not pd.api.types.is_string_dtype(df[target_col]):\n        raise ValueError(f\"Column '{target_col}' not found or is not a string column in DataFrame\")\n    \n    value_counts = df[target_col].value_counts()\n    encoding_map = {value: idx for idx, value in enumerate(value_counts.index)}\n    \n    df = df.copy()\n    df[f'{target_col}_encoded'] = df[target_col].map(encoding_map)\n    \n    if df[f'{target_col}_encoded'].isna().any():\n        raise RuntimeError(f\"Failed to encode '{target_col}' properly; NaN values found in encoded column\")\n    \n    return df"
    },
    {
        "function_name": "compute_knn_means",
        "file_name": "knn_mean_calculation.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "k": "int"
        },
        "objectives": [
            "Compute the k-nearest neighbors for each row in the DataFrame (only numeric columns).",
            "Calculate the mean of the k-nearest neighbors for each numeric column.",
            "Add new columns with suffix '_knn_mean' for each numeric column to store these means.",
            "Return the DataFrame with new KNN mean columns."
        ],
        "import_lines": [
            "from sklearn.neighbors import NearestNeighbors",
            "import pandas as pd"
        ],
        "function_def": "def compute_knn_means(df, k):\n    numeric_cols = df.select_dtypes(include='number').columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"No numeric columns found in the DataFrame\")\n    \n    df = df.copy()\n    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(df[numeric_cols])\n    distances, indices = nbrs.kneighbors(df[numeric_cols])\n    \n    for col in numeric_cols:\n        knn_means = []\n        for index, neighbors in enumerate(indices):\n            if len(neighbors) <= 1:\n                knn_means.append(df.loc[index, col])\n            else:\n                knn_means.append(df.loc[neighbors[1:], col].mean())\n        \n        df[f'{col}_knn_mean'] = knn_means\n    \n    return df"
    },
    {
        "function_name": "normalize_group_statistics",
        "file_name": "category_normalization.py",
        "parameters": {
            "df": "pandas.DataFrame",
            "category_col": "str",
            "numeric_cols": "list"
        },
        "objectives": [
            "Ensure that the `category_col` exists and is of categorical type and `numeric_cols` are present and numeric.",
            "Group the DataFrame by `category_col` and calculate the mean and variance for each `numeric_col`.",
            "Normalize the mean and variance values using Z-score normalization for each `numeric_col`.",
            "Return the DataFrame containing the normalized mean and variance values alongside their respective categories."
        ],
        "import_lines": [
            "import pandas as pd",
            "from scipy.stats import zscore"
        ],
        "function_def": "def normalize_group_statistics(df, category_col, numeric_cols):\n    if category_col not in df.columns or not pd.api.types.is_categorical_dtype(df[category_col]):\n        raise ValueError(f\"Column '{category_col}' not found or is not of categorical type in DataFrame\")\n    \n    for col in numeric_cols:\n        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' not found or is not of numeric type in DataFrame\")\n    \n    grouped_df = df.groupby(category_col)[numeric_cols].agg(['mean', 'var'])\n    normalized_df = pd.DataFrame(zscore(grouped_df, axis=0), columns=grouped_df.columns, index=grouped_df.index)\n    \n    return normalized_df.reset_index()"
    }
]