[
    {
        "function_name": "process_subarrays",
        "file_name": "subarray_processor.py",
        "parameters": {
            "arr": "List of integers",
            "k": "Integer representing the size of subarrays",
            "queries": "List of queries where each query is a 2-tuple (L, R) representing the range of the subarray to be processed"
        },
        "objectives": [
            "Divide the input array 'arr' into subarrays of size 'k' and process each subarray separately.",
            "For each query (L, R), calculate the sum of all subarrays that lie within the range [L, R] and have a sum greater than the average sum of all subarrays in the range.",
            "Return the total count of such subarrays."
        ],
        "import_lines": [],
        "function_def": "def process_subarrays(arr, k, queries):\n    n = len(arr)\n    subarrays = [arr[i:i+k] for i in range(0, n, k)]\n    subarray_sums = [sum(subarray) for subarray in subarrays]\n    avg_sums = [sum(subarray_sums[i:i+k]) / k for i in range(0, n, k)]\n    count = 0\n    \n    for L, R in queries:\n        for i in range(L, R+1):\n            if i % k == 0 and i + k <= R + 1:\n                subarray_sum = sum(subarrays[i//k])\n                avg_sum = avg_sums[i//k]\n                if subarray_sum > avg_sum:\n                    count += 1\n                    \n    return count"
    },
    {
        "function_name": "tree_cutter",
        "file_name": "tree_cutter.py",
        "parameters": {
            "trees": "List of tree heights",
            "K": "Integer representing the number of trees to cut"
        },
        "objectives": [
            "Sort the trees by their heights in descending order.",
            "Cut the 'K' tallest trees and calculate the total height of the remaining trees.",
            "For each tree that is cut, calculate the number of trees that are visible from the top of the cut tree (i.e., the trees that are not blocked by other trees).",
            "Return the maximum number of visible trees from any of the cut trees."
        ],
        "import_lines": [],
        "function_def": "def tree_cutter(trees, K):\n    trees.sort(reverse=True)\n    cut_trees = trees[:K]\n    remaining_trees = trees[K:]\n    total_height = sum(remaining_trees)\n    visible_trees = []\n    \n    for i, tree in enumerate(cut_trees):\n        visible_count = 0\n        for remaining_tree in remaining_trees:\n            if remaining_tree < tree:\n                visible_count += 1\n        visible_trees.append(visible_count)\n        \n    return total_height, max(visible_trees)"
    },
    {
        "function_name": "grid_processor",
        "file_name": "grid_processor.py",
        "parameters": {
            "N": "Integer representing the number of rows in the grid",
            "M": "Integer representing the number of columns in the grid",
            "grid": "2D list representing the grid",
            "queries": "List of queries where each query is a 2-tuple (L, R) representing the range of the rows to be processed"
        },
        "objectives": [
            "For each query (L, R), calculate the maximum sum of a sub-grid within the range [L, R] that has a rectangular shape.",
            "The sub-grid can have any number of rows and columns, but its top-left corner must lie within the range [L, R].",
            "Return the maximum sum of all sub-grids processed in the queries."
        ],
        "import_lines": [],
        "function_def": "def grid_processor(N, M, grid, queries):\n    max_sum = float('-inf')\n    \n    for L, R in queries:\n        for i in range(L, R+1):\n            for j in range(M):\n                for k in range(i, R+1):\n                    subgrid_sum = sum([sum(row[j:]) for row in grid[i:k+1]])\n                    max_sum = max(max_sum, subgrid_sum)\n                    \n    return max_sum"
    },
    {
        "function_name": "longest_path_topological_sort",
        "file_name": "graph_longest_path.py",
        "parameters": {
            "n": "int",
            "m": "int",
            "edges": "list of tuples (u, v, w) where u and v are nodes and w is the edge weight"
        },
        "objectives": [
            "Create a weighted directed graph with n nodes and m edges.",
            "Perform topological sorting on the graph to order the nodes such that for every edge (u, v), node u comes before node v in the ordering.",
            "Calculate the longest path length from the source node (node 0) to all other nodes in the graph using dynamic programming."
        ],
        "import_lines": [
            "from collections import defaultdict, deque"
        ],
        "function_def": "def longest_path_topological_sort(n, m, edges):\n    graph = defaultdict(list)\n    in_degree = [0] * n\n    \n    for u, v, w in edges:\n        graph[u].append((v, w))\n        in_degree[v] += 1\n    \n    queue = deque([i for i in range(n) if in_degree[i] == 0])\n    topological_order = []\n    longest_path_length = [0] * n\n    \n    while queue:\n        node = queue.popleft()\n        topological_order.append(node)\n        for neighbor, weight in graph[node]:\n            longest_path_length[neighbor] = max(longest_path_length[neighbor], longest_path_length[node] + weight)\n            in_degree[neighbor] -= 1\n            if in_degree[neighbor] == 0:\n                queue.append(neighbor)\n    \n    return topological_order, longest_path_length"
    },
    {
        "function_name": "max_submatrix_size",
        "file_name": "binary_matrix.py",
        "parameters": {
            "n": "int",
            "m": "int",
            "matrix": "2D list of integers representing the binary matrix"
        },
        "objectives": [
            "Given a binary matrix of size n x m, calculate the maximum size of a sub-matrix with all 1's.",
            "Use dynamic programming to build a table of maximum sub-matrix sizes for each cell in the matrix.",
            "Return the maximum size of a sub-matrix with all 1's."
        ],
        "import_lines": [],
        "function_def": "def max_submatrix_size(n, m, matrix):\n    dp = [[0] * m for _ in range(n)]\n    max_size = 0\n    \n    for i in range(n):\n        for j in range(m):\n            if matrix[i][j] == 1:\n                if i == 0 or j == 0:\n                    dp[i][j] = 1\n                else:\n                    dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1\n                max_size = max(max_size, dp[i][j])\n    \n    return max_size"
    },
    {
        "function_name": "select_k_items",
        "file_name": "item_selection.py",
        "parameters": {
            "n": "int",
            "k": "int",
            "scores": "list of integers representing the scores of the n items"
        },
        "objectives": [
            "Given a list of scores of n items, select the k items with the highest scores.",
            "Use a min-heap to keep track of the k items with the highest scores.",
            "Return the k items with the highest scores in descending order."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def select_k_items(n, k, scores):\n    min_heap = []\n    \n    for score in scores:\n        if len(min_heap) < k:\n            heapq.heappush(min_heap, score)\n        else:\n            heapq.heappushpop(min_heap, score)\n    \n    return sorted(min_heap, reverse=True)"
    },
    {
        "function_name": "longest_common_prefix",
        "file_name": "string_prefix.py",
        "parameters": {
            "n": "int",
            "m": "int",
            "strings": "list of strings representing the m strings"
        },
        "objectives": [
            "Given a list of m strings, calculate the longest common prefix among all strings.",
            "Use dynamic programming to build a table of common prefixes for each pair of strings.",
            "Return the longest common prefix."
        ],
        "import_lines": [],
        "function_def": "def longest_common_prefix(n, m, strings):\n    dp = [[0] * m for _ in range(m)]\n    lcp = \"\"\n    \n    for i in range(m):\n        for j in range(i+1, m):\n            k = 0\n            while k < len(strings[i]) and k < len(strings[j]) and strings[i][k] == strings[j][k]:\n                k += 1\n            dp[i][j] = k\n            dp[j][i] = k\n    \n    for k in range(min(len(s) for s in strings)):\n        common = True\n        for i in range(m):\n            for j in range(i+1, m):\n                if dp[i][j] <= k:\n                    common = False\n                    break\n            if not common:\n                break\n        if common:\n            lcp += strings[0][k]\n        else:\n            break\n    \n    return lcp"
    },
    {
        "function_name": "grid_queries",
        "file_name": "grid_manipulations.py",
        "parameters": {
            "n": "int",
            "queries": "list of tuples (l, r, v)"
        },
        "objectives": [
            "Given a 2D grid of size n x n, initialize it with zeros.",
            "Process the queries where each query is a 3-tuple (l, r, v) representing the addition of v to the sub-grid from row l to row r (inclusive) and column l to column r (inclusive).",
            "Return the sum of all elements in the grid after processing all queries."
        ],
        "import_lines": [],
        "function_def": "def grid_queries(n, queries):\n    grid = [[0]*n for _ in range(n)]\n    for l, r, v in queries:\n        for i in range(l, r+1):\n            for j in range(l, r+1):\n                grid[i][j] += v\n    return sum(sum(row) for row in grid)"
    },
    {
        "function_name": "min_weight_path",
        "file_name": "weighted_graphs.py",
        "parameters": {
            "n": "int",
            "edges": "list of tuples (u, v, w)",
            "source": "int",
            "target": "int"
        },
        "objectives": [
            "Create a weighted graph with n nodes using the given edges and weights.",
            "Find the minimum weight path from the source node to all other nodes using Bellman-Ford algorithm.",
            "Report if there is a negative weight cycle reachable from the source node.",
            "If not, return the minimum weight path from the source node to the target node."
        ],
        "import_lines": [
            "import sys"
        ],
        "function_def": "def min_weight_path(n, edges, source, target):\n    distance = [sys.maxsize]*n\n    distance[source] = 0\n    for _ in range(n-1):\n        for u, v, w in edges:\n            if distance[u] != sys.maxsize and distance[u]+w < distance[v]:\n                distance[v] = distance[u]+w\n    for u, v, w in edges:\n        if distance[u] != sys.maxsize and distance[u]+w < distance[v]:\n            return \"Negative cycle detected\"\n    return distance[target]"
    },
    {
        "function_name": "max_prefix_sum",
        "file_name": "prefix_sum.py",
        "parameters": {
            "n": "int",
            "queries": "list of tuples (index, value)"
        },
        "objectives": [
            "Given an array of size n, initialize it with zeros.",
            "Process the queries where each query is a 2-tuple (index, value) representing the update of the array at the given index with the given value.",
            "After each query, the array is sorted in ascending order.",
            "Return the maximum prefix sum of the array after processing all queries."
        ],
        "import_lines": [],
        "function_def": "def max_prefix_sum(n, queries):\n    array = [0]*n\n    prefix_sum = [0]*(n+1)\n    max_prefix_sum = 0\n    for index, value in queries:\n        array[index] = value\n        array.sort()\n        for i in range(n):\n            prefix_sum[i+1] = prefix_sum[i] + array[i]\n        max_prefix_sum = max(max_prefix_sum, max(prefix_sum))\n    return max_prefix_sum"
    },
    {
        "function_name": "sum_of_prefix_sums",
        "file_name": "binary_indexed_tree.py",
        "parameters": {
            "n": "int",
            "inputs": "list of integers"
        },
        "objectives": [
            "Create a binary indexed tree (BIT) to store the prefix sum of the inputs.",
            "Process the inputs one by one and update the BIT accordingly.",
            "For each input, find the prefix sum of all inputs less than or equal to the current input using the BIT.",
            "Return the sum of all prefix sums."
        ],
        "import_lines": [],
        "function_def": "def sum_of_prefix_sums(n, inputs):\n    bit = [0]*(n+1)\n    sum_of_prefix_sums = 0\n    for i, x in enumerate(inputs):\n        for j in range(i+1, n, i+1):\n            bit[j] += x\n        prefix_sum = 0\n        j = i+1\n        while j > 0:\n            prefix_sum += bit[j]\n            j -= j & -j\n        sum_of_prefix_sums += prefix_sum\n    return sum_of_prefix_sums"
    },
    {
        "function_name": "min_attacks",
        "file_name": "chessboard.py",
        "parameters": {
            "n": "int",
            "queens": "list of tuples (row, col)"
        },
        "objectives": [
            "Create a chessboard of size n x n.",
            "Place the queens at the given positions on the chessboard.",
            "For each queen, find the number of other queens that it can attack.",
            "Return the queen with the minimum number of attacks and the corresponding board position."
        ],
        "import_lines": [],
        "function_def": "def min_attacks(n, queens):\n    board = [[0]*n for _ in range(n)]\n    attacks = {}\n    for row, col in queens:\n        board[row][col] = 1\n    for row, col in queens:\n        count = 0\n        # check rows\n        for i in range(n):\n            if i != row and board[i][col] == 1:\n                count += 1\n        # check cols\n        for i in range(n):\n            if i != col and board[row][i] == 1:\n                count += 1\n        # check diagonals\n        for i, j in zip(range(row-1, -1, -1), range(col-1, -1, -1)):\n            if board[i][j] == 1:\n                count += 1\n        for i, j in zip(range(row+1, n), range(col+1, n)):\n            if board[i][j] == 1:\n                count += 1\n        for i, j in zip(range(row-1, -1, -1), range(col+1, n)):\n            if board[i][j] == 1:\n                count += 1\n        for i, j in zip(range(row+1, n), range(col-1, -1, -1)):\n            if board[i][j] == 1:\n                count += 1\n        attacks[(row, col)] = count\n    return min(attacks, key=attacks.get)"
    },
    {
        "function_name": "find_strongly_connected_components",
        "file_name": "strongly_connected_components.py",
        "parameters": {
            "n": "Number of nodes in the graph.",
            "m": "Number of edges in the graph.",
            "edges": "List of edges in the graph. Each edge is a 3-tuple (u, v, w) where u and v are nodes connected by the edge and w is the weight of the edge.",
            "S": "Starting node for finding the strongly connected components."
        },
        "objectives": [
            "Identify all strongly connected components in the graph.",
            "For each strongly connected component, find the node with the highest degree (i.e., the node with the most edges connected to it).",
            "Return a list of tuples, where each tuple contains the node with the highest degree in a strongly connected component and the size of that component."
        ],
        "import_lines": [
            "import networkx as nx"
        ],
        "function_def": "def find_strongly_connected_components(n, m, edges, S):\n    G = nx.DiGraph()\n    for u, v, w in edges:\n        G.add_edge(u, v)\n    strongly_connected_components = list(nx.strongly_connected_components(G))\n    result = []\n    for component in strongly_connected_components:\n        subgraph = G.subgraph(component)\n        node_with_highest_degree = max(subgraph, key=subgraph.degree)\n        result.append((node_with_highest_degree, len(component)))\n    return result"
    },
    {
        "function_name": "process_queries_and_find_max",
        "file_name": "array_queries.py",
        "parameters": {
            "n": "Size of the array.",
            "queries": "List of queries to be processed. Each query is a 2-tuple (i, x) where i is the index of the array and x is the value to be added to the array at index i."
        },
        "objectives": [
            "Process the queries and update the array accordingly.",
            "For each query, find the maximum value in the array after updating the array.",
            "Return a list of maximum values after processing each query."
        ],
        "import_lines": [],
        "function_def": "def process_queries_and_find_max(n, queries):\n    arr = [0] * n\n    max_values = []\n    for i, x in queries:\n        arr[i] += x\n        max_values.append(max(arr))\n    return max_values"
    },
    {
        "function_name": "select_strings_with_maximum_length",
        "file_name": "string_selection.py",
        "parameters": {
            "n": "Number of strings.",
            "strings": "List of strings.",
            "k": "Number of strings to select."
        },
        "objectives": [
            "Select k strings from the list of strings such that the total length of the selected strings is maximum.",
            "Return the selected strings."
        ],
        "import_lines": [],
        "function_def": "def select_strings_with_maximum_length(n, strings, k):\n    strings.sort(key=len, reverse=True)\n    return strings[:k]"
    },
    {
        "function_name": "find_submatrices_with_zero_sum",
        "file_name": "submatrix_search.py",
        "parameters": {
            "n": "Size of the matrix.",
            "matrix": "2D list representing the matrix."
        },
        "objectives": [
            "Find all sub-matrices in the given matrix such that the sum of their elements is zero.",
            "For each sub-matrix, find the size of the sub-matrix (i.e., the number of rows and columns).",
            "Return a list of tuples, where each tuple contains the size of a sub-matrix and its top-left corner coordinates."
        ],
        "import_lines": [],
        "function_def": "def find_submatrices_with_zero_sum(n, matrix):\n    result = []\n    for i in range(n):\n        for j in range(n):\n            for k in range(i, n):\n                for last_column in range(j, n):\n                    submatrix = [row[j:last_column+1] for row in matrix[i:k+1]]\n                    if sum(sum(row) for row in submatrix) == 0:\n                        result.append(((k-i+1, last_column-j+1), (i, j)))\n    return result"
    },
    {
        "function_name": "find_overlapping_time_intervals",
        "file_name": "time_interval_search.py",
        "parameters": {
            "n": "Number of time intervals.",
            "time_intervals": "List of time intervals. Each time interval is a 2-tuple (start, end) where start and end are the start and end times of the interval."
        },
        "objectives": [
            "Find all time intervals that overlap with at least one other time interval.",
            "For each overlapping time interval, find the maximum end time of all overlapping intervals.",
            "Return a list of tuples, where each tuple contains the start and end times of an overlapping interval and the maximum end time of all overlapping intervals."
        ],
        "import_lines": [],
        "function_def": "def find_overlapping_time_intervals(n, time_intervals):\n    time_intervals.sort(key=lambda x: x[0])\n    result = []\n    for i in range(n):\n        max_end_time = 0\n        for j in range(i+1, n):\n            if time_intervals[i][1] >= time_intervals[j][0]:\n                max_end_time = max(max_end_time, time_intervals[j][1])\n        if max_end_time > 0:\n            result.append((time_intervals[i][0], time_intervals[i][1], max_end_time))\n    return result"
    },
    {
        "function_name": "transpose_and_rotate",
        "file_name": "grid_operations.py",
        "parameters": {
            "n": "Integer",
            "m": "Integer",
            "grid": "2D list of integers",
            "operations": "List of tuples, where each tuple contains an operation type (either 'row' or 'column') and an index"
        },
        "objectives": [
            "Transpose the grid along the specified row or column based on the operations provided.",
            "Perform a rotation (90 degrees clockwise) on the grid whenever a 'row' operation is encountered.",
            "Update the grid values by adding 1 to each cell whenever a 'column' operation is encountered."
        ],
        "import_lines": [],
        "function_def": "def transpose_and_rotate(n, m, grid, operations):\n    for operation in operations:\n        op_type, index = operation\n        if op_type == 'row':\n            # rotate the grid 90 degrees clockwise\n            grid = [list(reversed(x)) for x in zip(*grid)]\n            # update grid values by adding 1 to each cell in the specified row\n            for i in range(m):\n                grid[index][i] += 1\n        elif op_type == 'column':\n            # transpose the grid along the specified column\n            grid = [list(x) for x in zip(*grid)]\n            # update grid values by adding 1 to each cell\n            for i in range(n):\n                grid[i][index] += 1\n            # transpose the grid back\n            grid = [list(x) for x in zip(*grid)]\n    return grid"
    },
    {
        "function_name": "valid_permutations",
        "file_name": "permutations.py",
        "parameters": {
            "seq": "List of integers",
            "max_val": "Integer"
        },
        "objectives": [
            "Generate all possible permutations of the given sequence that do not exceed the maximum value when summed up.",
            "Filter out permutations that contain consecutive repeating elements.",
            "Return the count of valid permutations."
        ],
        "import_lines": [
            "import itertools"
        ],
        "function_def": "def valid_permutations(seq, max_val):\n    valid_permutations = 0\n    for perm in itertools.permutations(seq):\n        if sum(perm) <= max_val and all(perm[i] != perm[i+1] for i in range(len(perm)-1)):\n            valid_permutations += 1\n    return valid_permutations"
    },
    {
        "function_name": "longest_shortest_path",
        "file_name": "graph_paths.py",
        "parameters": {
            "k": "Integer",
            "paths": "List of lists of integers, representing paths in a graph"
        },
        "objectives": [
            "Find the longest path in the graph that can be traversed within k steps.",
            "If multiple paths have the same maximum length, return the path with the minimum sum of edge weights."
        ],
        "import_lines": [],
        "function_def": "def longest_shortest_path(k, paths):\n    max_length = 0\n    min_sum = float('inf')\n    best_path = None\n    for path in paths:\n        if len(path) <= k and sum(path) < min_sum:\n            max_length = len(path)\n            min_sum = sum(path)\n            best_path = path\n        elif len(path) == max_length and sum(path) < min_sum:\n            min_sum = sum(path)\n            best_path = path\n    return best_path"
    },
    {
        "function_name": "max_non_overlapping",
        "file_name": "interval_selection.py",
        "parameters": {
            "n": "Integer",
            "intervals": "List of tuples of integers, representing intervals"
        },
        "objectives": [
            "Find the maximum number of non-overlapping intervals that can be selected from the given list.",
            "Sort the selected intervals based on their start values."
        ],
        "import_lines": [],
        "function_def": "def max_non_overlapping(n, intervals):\n    intervals.sort(key=lambda x: x[1])\n    selected_intervals = [intervals[0]]\n    for interval in intervals[1:]:\n        if interval[0] >= selected_intervals[-1][1]:\n            selected_intervals.append(interval)\n    return selected_intervals"
    },
    {
        "function_name": "bitwise_operations",
        "file_name": "bitwise.py",
        "parameters": {
            "n": "Integer",
            "A": "List of integers",
            "B": "List of integers"
        },
        "objectives": [
            "Find the binary representation of the elements in A and B.",
            "Calculate the bitwise XOR and AND of the elements in A and B.",
            "Return the sum of the bitwise XOR and AND values."
        ],
        "import_lines": [],
        "function_def": "def bitwise_operations(n, A, B):\n    xor_sum = 0\n    and_sum = 0\n    for a, b in zip(A, B):\n        xor_sum += a ^ b\n        and_sum += a & b\n    return xor_sum + and_sum"
    },
    {
        "function_name": "matrix_swap_max_row",
        "file_name": "matrix_swaps.py",
        "parameters": {
            "N": "int",
            "M": "int",
            "matrix": "2D list of integers",
            "operations": "list of tuples, where each tuple contains the operation type (1 for row, 2 for column) and the index of the row/column to be swapped"
        },
        "objectives": [
            "Perform a series of swap operations on the given matrix.",
            "For each operation, swap the specified row or column with the last row or column in the matrix.",
            "After all operations are performed, return the matrix with the maximum row sum."
        ],
        "import_lines": [],
        "function_def": "def matrix_swap_max_row(N, M, matrix, operations):\n    for op in operations:\n        op_type, index = op\n        if op_type == 1:\n            matrix[index], matrix[-1] = matrix[-1], matrix[index]\n        else:\n            for row in matrix:\n                row[index], row[-1] = row[-1], row[index]\n    return max(sum(row) for row in matrix)"
    },
    {
        "function_name": "bellman_ford_shortest_path",
        "file_name": "bellman_ford.py",
        "parameters": {
            "n": "int",
            "edges": "list of tuples, where each tuple contains the source node, destination node, and weight of the edge",
            "start": "int",
            "end": "int"
        },
        "objectives": [
            "Create a graph from the given list of edges.",
            "Use Bellman-Ford algorithm to find the shortest path from the start node to all other nodes.",
            "Return the shortest path from the start node to the end node."
        ],
        "import_lines": [],
        "function_def": "def bellman_ford_shortest_path(n, edges, start, end):\n    graph = [[] for _ in range(n)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n    distance = [float('inf')] * n\n    distance[start] = 0\n    for _ in range(n - 1):\n        for u in range(n):\n            for v, w in graph[u]:\n                distance[v] = min(distance[v], distance[u] + w)\n    return distance[end]"
    },
    {
        "function_name": "kth_largest_element",
        "file_name": "priority_queue_kth_largest.py",
        "parameters": {
            "n": "int",
            "arr": "list of integers",
            "k": "int"
        },
        "objectives": [
            "Use a priority queue to find the kth largest element in the given list of integers.",
            "Return the kth largest element."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def kth_largest_element(n, arr, k):\n    pq = []\n    for num in arr:\n        heapq.heappush(pq, num)\n    for _ in range(n - k):\n        heapq.heappop(pq)\n    return heapq.heappop(pq)"
    },
    {
        "function_name": "k_means_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "matrix": "A 2D list of integers",
            "k": "An integer representing the number of clusters"
        },
        "objectives": [
            "Perform the k-means clustering algorithm on the given matrix.",
            "The matrix represents points in a 2D space, and each point is a list of two integers.",
            "The function should return the coordinates of the centroids of the clusters after the clustering process.",
            "The function should also return the label of each point in the matrix."
        ],
        "import_lines": [
            "import random",
            "import math"
        ],
        "function_def": "def k_means_clustering(matrix, k):\n    # Initialize centroids randomly\n    centroids = random.sample(matrix, k)\n    \n    while True:\n        # Assign each point to a cluster\n        labels = []\n        for point in matrix:\n            min_distance = float('inf')\n            label = -1\n            for i, centroid in enumerate(centroids):\n                distance = math.sqrt((point[0] - centroid[0])**2 + (point[1] - centroid[1])**2)\n                if distance < min_distance:\n                    min_distance = distance\n                    label = i\n            labels.append(label)\n        \n        # Calculate new centroids\n        new_centroids = []\n        for i in range(k):\n            points_in_cluster = [point for point, label in zip(matrix, labels) if label == i]\n            if points_in_cluster:\n                centroid = [sum(x)/len(points_in_cluster) for x in zip(*points_in_cluster)]\n                new_centroids.append(centroid)\n            else:\n                new_centroids.append(centroids[i])\n        \n        # Check for convergence\n        if new_centroids == centroids:\n            break\n        \n        centroids = new_centroids\n    \n    return centroids, labels"
    },
    {
        "function_name": "max_min_subarray",
        "file_name": "subarray_sums.py",
        "parameters": {
            "sequence": "A list of integers",
            "k": "An integer representing the window size"
        },
        "objectives": [
            "Calculate the maximum sum of a subarray of size k within the given sequence.",
            "Calculate the minimum sum of a subarray of size k within the given sequence.",
            "The function should return the maximum sum and the minimum sum."
        ],
        "import_lines": [
            "import collections"
        ],
        "function_def": "def max_min_subarray(sequence, k):\n    max_sum = float('-inf')\n    min_sum = float('inf')\n    \n    # Calculate sums of subarrays of size k\n    window_sum = sum(sequence[:k])\n    max_sum = max(max_sum, window_sum)\n    min_sum = min(min_sum, window_sum)\n    \n    for i in range(k, len(sequence)):\n        window_sum = window_sum - sequence[i - k] + sequence[i]\n        max_sum = max(max_sum, window_sum)\n        min_sum = min(min_sum, window_sum)\n    \n    return max_sum, min_sum"
    },
    {
        "function_name": "bfs_distance",
        "file_name": "graph_traversal.py",
        "parameters": {
            "graph": "A dictionary representing a directed graph",
            "start": "A node in the graph"
        },
        "objectives": [
            "Perform a breadth-first search (BFS) on the graph starting from the given node.",
            "The function should return the shortest distance from the start node to each node in the graph.",
            "The function should also return the predecessor of each node in the shortest path."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def bfs_distance(graph, start):\n    distance = {node: float('inf') for node in graph}\n    predecessor = {node: None for node in graph}\n    distance[start] = 0\n    \n    queue = deque([start])\n    \n    while queue:\n        node = queue.popleft()\n        for neighbor in graph[node]:\n            if distance[neighbor] > distance[node] + 1:\n                distance[neighbor] = distance[node] + 1\n                predecessor[neighbor] = node\n                queue.append(neighbor)\n    \n    return distance, predecessor"
    },
    {
        "function_name": "prefix_words",
        "file_name": "prefix_match.py",
        "parameters": {
            "words": "A list of words",
            "prefix": "A string representing a prefix"
        },
        "objectives": [
            "Find all words in the given list that start with the given prefix.",
            "The function should return the list of words that start with the prefix.",
            "The function should also return the frequency of each word in the list."
        ],
        "import_lines": [],
        "function_def": "def prefix_words(words, prefix):\n    prefix_words = [word for word in words if word.startswith(prefix)]\n    frequency = {}\n    \n    for word in words:\n        if word in frequency:\n            frequency[word] += 1\n        else:\n            frequency[word] = 1\n    \n    return prefix_words, frequency"
    },
    {
        "function_name": "grid_updater",
        "file_name": "grid_updater.py",
        "parameters": {
            "grid": "A 2D list of integers",
            "queries": "A list of tuples (l, r, v) representing the addition of v to the sub-grid from row l to row r (inclusive) and column l to column r (inclusive)",
            "constraints": "A list of tuples (i, j, limit) where (i, j) is a cell in the grid and limit is the maximum allowed value in that cell"
        },
        "objectives": [
            "Process the queries to update the grid.",
            "Ensure that after processing each query, the value of each cell does not exceed its limit specified in the constraints.",
            "If a query would cause a cell's value to exceed its limit, reduce the value added by the query to the minimum possible while still not exceeding the limit.",
            "Return the final grid."
        ],
        "import_lines": [],
        "function_def": "def grid_updater(grid, queries, constraints):\n    constraint_dict = {(i, j): limit for i, j, limit in constraints}\n    for l, r, v in queries:\n        for i in range(l, r+1):\n            for j in range(l, r+1):\n                if (i, j) in constraint_dict:\n                    limit = constraint_dict[(i, j)]\n                    if grid[i][j] + v > limit:\n                        v = limit - grid[i][j]\n                        if v < 0:\n                            v = 0\n                grid[i][j] += v\n    return grid"
    },
    {
        "function_name": "task_scheduler",
        "file_name": "task_scheduler.py",
        "parameters": {
            "n": "An integer representing the number of tasks.",
            "dependencies": "A list of tuples (u, v) representing a dependency between tasks u and v, where task u must be completed before task v."
        },
        "objectives": [
            "Create a directed acyclic graph (DAG) to represent the dependencies between tasks.",
            "Perform topological sorting on the DAG to order the tasks such that for every dependency (u, v), task u comes before task v in the ordering.",
            "Return the ordered list of tasks."
        ],
        "import_lines": [
            "from collections import defaultdict, deque"
        ],
        "function_def": "def task_scheduler(n, dependencies):\n    graph = defaultdict(list)\n    in_degree = {i: 0 for i in range(n)}\n    for u, v in dependencies:\n        graph[u].append(v)\n        in_degree[v] += 1\n    queue = deque([i for i in range(n) if in_degree[i] == 0])\n    ordering = []\n    while queue:\n        task = queue.popleft()\n        ordering.append(task)\n        for neighbor in graph[task]:\n            in_degree[neighbor] -= 1\n            if in_degree[neighbor] == 0:\n                queue.append(neighbor)\n    return ordering"
    },
    {
        "function_name": "bin_packer",
        "file_name": "bin_packer.py",
        "parameters": {
            "n": "An integer representing the number of bins.",
            "items": "A list of tuples (size, value) representing items to be packed into the bins.",
            "capacity": "An integer representing the capacity of each bin."
        },
        "objectives": [
            "Pack the items into the bins such that the total size of items in each bin does not exceed the capacity.",
            "The packing should be done using a greedy algorithm that maximizes the total value of items in the bins.",
            "Return the items packed in each bin."
        ],
        "import_lines": [],
        "function_def": "def bin_packer(n, items, capacity):\n    items.sort(key=lambda x: x[1]/x[0], reverse=True)\n    bins = [[] for _ in range(n)]\n    for size, value in items:\n        for bin in bins:\n            if sum(s for s, _ in bin) + size <= capacity:\n                bin.append((size, value))\n                break\n    return bins"
    },
    {
        "function_name": "k_shortest_paths",
        "file_name": "k_shortest_paths.py",
        "parameters": {
            "graph": "Adjacency list representation of a graph",
            "source": "Node to start the search from",
            "k": "Number of shortest paths to find"
        },
        "objectives": [
            "Find the k shortest paths from the source node to all other nodes in the graph.",
            "Use a modified Dijkstra's algorithm to consider the k shortest paths instead of the single shortest path.",
            "Return the k shortest paths as a dictionary where each key is a node and the value is a list of paths."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def k_shortest_paths(graph, source, k):\n    dist = {node: float('inf') for node in graph}\n    dist[source] = 0\n    paths = {node: [[]] for node in graph}\n    queue = [(0, source, [])]\n    while queue:\n        (dist_so_far, current, path) = heapq.heappop(queue)\n        for neighbor, neighbor_dist in graph[current].items():\n            old_dist = dist[neighbor]\n            new_dist = dist_so_far + neighbor_dist\n            if new_dist < old_dist:\n                dist[neighbor] = new_dist\n                paths[neighbor] = [path + [neighbor]]\n                heapq.heappush(queue, (new_dist, neighbor, path + [neighbor]))\n            elif new_dist == old_dist and len(paths[neighbor]) < k:\n                paths[neighbor].append(path + [neighbor])\n    return {node: paths[node][:k] for node in paths}"
    },
    {
        "function_name": "lzw_compression",
        "file_name": "lzw_compression.py",
        "parameters": {
            "text": "A string to be compressed",
            "dictionary_size": "The size of the dictionary to be used for compression"
        },
        "objectives": [
            "Use the LZW compression algorithm to compress the given text.",
            "Build a dictionary of substrings and their corresponding codes.",
            "Return the compressed text as a string of codes."
        ],
        "import_lines": [],
        "function_def": "def lzw_compression(text, dictionary_size):\n    dictionary = {chr(i): i for i in range(dictionary_size)}\n    result = []\n    w = \"\"\n    for c in text:\n        wc = w + c\n        if wc in dictionary:\n            w = wc\n        else:\n            result.append(dictionary[w])\n            dictionary[wc] = dictionary_size\n            dictionary_size += 1\n            w = c\n    if w:\n        result.append(dictionary[w])\n    return ' '.join(map(str, result))"
    },
    {
        "function_name": "majority_element",
        "file_name": "majority_element.py",
        "parameters": {
            "grid": "A 2D list of integers",
            "threshold": "An integer representing the threshold for the majority element"
        },
        "objectives": [
            "Find the majority element in the grid, i.e., the element that appears more than threshold times in the grid.",
            "Use the Boyer-Moore Majority Vote algorithm to find the majority element.",
            "Return the majority element and its count."
        ],
        "import_lines": [],
        "function_def": "def majority_element(grid, threshold):\n    m, n = len(grid), len(grid[0])\n    count = {}\n    for i in range(m):\n        for j in range(n):\n            count[grid[i][j]] = count.get(grid[i][j], 0) + 1\n            if count[grid[i][j]] > threshold:\n                return grid[i][j], count[grid[i][j]]\n    return None, 0"
    },
    {
        "function_name": "bucket_sort",
        "file_name": "bucket_sort.py",
        "parameters": {
            "numbers": "A list of integers",
            "buckets": "A list of tuples (min, max) representing the buckets"
        },
        "objectives": [
            "Distribute the numbers into the given buckets.",
            "Use a hash function to map each number to a bucket.",
            "Return the numbers in each bucket."
        ],
        "import_lines": [],
        "function_def": "def bucket_sort(numbers, buckets):\n    bucket_dict = {i: [] for i in range(len(buckets))}\n    for num in numbers:\n        for i, (min_val, max_val) in enumerate(buckets):\n            if min_val <= num <= max_val:\n                bucket_dict[i].append(num)\n                break\n    return list(bucket_dict.values())"
    },
    {
        "function_name": "top_k_cells",
        "file_name": "grid_utils.py",
        "parameters": {
            "N": "int",
            "M": "int",
            "grid": "2D list representing the grid",
            "k": "int"
        },
        "objectives": [
            "Given a 2D grid of size N x M, identify the top k cells with the highest values.",
            "For each cell, calculate its score as the maximum value in the sub-grid that includes the cell itself and all cells to its right and below it.",
            "Return the coordinates of the top k cells along with their corresponding scores."
        ],
        "import_lines": [],
        "function_def": "def top_k_cells(N, M, grid, k):\n    scores = []\n    for i in range(N):\n        for j in range(M):\n            max_val = float('-inf')\n            for x in range(i, N):\n                for y in range(j, M):\n                    max_val = max(max_val, grid[x][y])\n            scores.append(((i, j), max_val))\n    scores.sort(key=lambda x: x[1], reverse=True)\n    return scores[:k]"
    },
    {
        "function_name": "longest_substring",
        "file_name": "string_algorithms.py",
        "parameters": {
            "s": "str",
            "k": "int"
        },
        "objectives": [
            "Given a string s and an integer k, find the longest substring that contains at most k distinct characters.",
            "Use the sliding window technique to track the frequency of characters in the current window.",
            "Return the length of the longest substring."
        ],
        "import_lines": [],
        "function_def": "def longest_substring(s, k):\n    char_freq = {}\n    max_length = 0\n    window_start = 0\n    for window_end in range(len(s)):\n        right_char = s[window_end]\n        char_freq[right_char] = char_freq.get(right_char, 0) + 1\n        while len(char_freq) > k:\n            left_char = s[window_start]\n            char_freq[left_char] -= 1\n            if char_freq[left_char] == 0:\n                del char_freq[left_char]\n            window_start += 1\n        max_length = max(max_length, window_end - window_start + 1)\n    return max_length"
    },
    {
        "function_name": "min_sum_path",
        "file_name": "pathfinding.py",
        "parameters": {
            "n": "int",
            "m": "int",
            "grid": "2D list representing the grid",
            "start": "tuple (x, y)",
            "end": "tuple (x, y)"
        },
        "objectives": [
            "Given a 2D grid of size n x m and a start position and an end position, find a path from the start to the end that minimizes the sum of the values in the grid.",
            "Use Dijkstra's algorithm to find the shortest path.",
            "Return the minimum sum and the path."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def min_sum_path(n, m, grid, start, end):\n    distances = [[float('inf')] * m for _ in range(n)]\n    previous = [[None] * m for _ in range(n)]\n    distances[start[0]][start[1]] = grid[start[0]][start[1]]\n    queue = [(grid[start[0]][start[1]], start[0], start[1])]\n    \n    while queue:\n        current_distance, current_x, current_y = heapq.heappop(queue)\n        if (current_x, current_y) == end:\n            break\n        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            new_x, new_y = current_x + dx, current_y + dy\n            if 0 <= new_x < n and 0 <= new_y < m:\n                new_distance = current_distance + grid[new_x][new_y]\n                if new_distance < distances[new_x][new_y]:\n                    distances[new_x][new_y] = new_distance\n                    previous[new_x][new_y] = (current_x, current_y)\n                    heapq.heappush(queue, (new_distance, new_x, new_y))\n    \n    path = []\n    current_x, current_y = end\n    while (current_x, current_y) != start:\n        path.append((current_x, current_y))\n        current_x, current_y = previous[current_x][current_y]\n    path.append(start)\n    path.reverse()\n    return (distances[end[0]][end[1]], path)"
    },
    {
        "function_name": "max_sum_pairs",
        "file_name": "dynamic_programming.py",
        "parameters": {
            "pairs": "A list of tuples, where each tuple contains two integers",
            "k": "An integer representing the number of pairs to select"
        },
        "objectives": [
            "Find the maximum sum that can be achieved by selecting k pairs from the given list, such that the sum of the two integers in each pair is maximized.",
            "The function should use dynamic programming to solve the problem.",
            "The function should return the maximum sum and the selected pairs."
        ],
        "import_lines": [],
        "function_def": "def max_sum_pairs(pairs, k):\n    pairs.sort(key=lambda x: x[0] + x[1], reverse=True)\n    dp = [[0] * (k + 1) for _ in range(len(pairs) + 1)]\n    selected_pairs = [[[] for _ in range(k + 1)] for _ in range(len(pairs) + 1)]\n    \n    for i in range(1, len(pairs) + 1):\n        for j in range(1, min(i, k) + 1):\n            if j == 1:\n                dp[i][j] = pairs[i - 1][0] + pairs[i - 1][1]\n                selected_pairs[i][j] = [pairs[i - 1]]\n            else:\n                dp[i][j] = dp[i - 1][j]\n                selected_pairs[i][j] = selected_pairs[i - 1][j]\n                if dp[i - 1][j - 1] + pairs[i - 1][0] + pairs[i - 1][1] > dp[i][j]:\n                    dp[i][j] = dp[i - 1][j - 1] + pairs[i - 1][0] + pairs[i - 1][1]\n                    selected_pairs[i][j] = selected_pairs[i - 1][j - 1] + [pairs[i - 1]]\n    \n    return dp[-1][-1], selected_pairs[-1][-1]"
    },
    {
        "function_name": "pattern_alignment",
        "file_name": "pattern_alignment.py",
        "parameters": {
            "text": "A string to be searched",
            "pattern": "A string to be found",
            "distance": "An integer representing the maximum edit distance"
        },
        "objectives": [
            "Use the Needleman-Wunsch algorithm to find the best alignment of the pattern within the text.",
            "The alignment is defined by a 2D matrix, where each cell contains the edit distance between the corresponding substrings.",
            "Return the aligned pattern and the minimum edit distance."
        ],
        "import_lines": [],
        "function_def": "def pattern_alignment(text, pattern, distance):\n    m, n = len(text), len(pattern)\n    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n    \n    # Initialize the base cases\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    \n    # Fill in the rest of the table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if text[i - 1] == pattern[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n    \n    # Backtrack to find the alignment\n    aligned_pattern = \"\"\n    i, j = m, n\n    while i > 0 and j > 0:\n        if text[i - 1] == pattern[j - 1]:\n            aligned_pattern = pattern[j - 1] + aligned_pattern\n            i -= 1\n            j -= 1\n        elif dp[i - 1][j] <= dp[i][j - 1] and dp[i - 1][j] <= dp[i - 1][j - 1]:\n            aligned_pattern = \"-\" + aligned_pattern\n            i -= 1\n        elif dp[i][j - 1] <= dp[i - 1][j] and dp[i][j - 1] <= dp[i - 1][j - 1]:\n            aligned_pattern = pattern[j - 1] + aligned_pattern\n            j -= 1\n        else:\n            aligned_pattern = \"*\" + aligned_pattern\n            i -= 1\n            j -= 1\n    \n    return aligned_pattern, dp[m][n]"
    },
    {
        "function_name": "subset_sum",
        "file_name": "subset_sum.py",
        "parameters": {
            "matrix": "A 2D list of integers",
            "target_sum": "An integer representing the target sum"
        },
        "objectives": [
            "Use the dynamic programming approach to find the subset of rows that sums up to the target sum.",
            "The subset is defined by a binary string, where each bit represents whether the corresponding row is included or not.",
            "Return the subset as a binary string."
        ],
        "import_lines": [],
        "function_def": "def subset_sum(matrix, target_sum):\n    n, m = len(matrix), len(matrix[0])\n    dp = [[False for _ in range(target_sum + 1)] for _ in range(n + 1)]\n    dp[0][0] = True\n    \n    for i in range(1, n + 1):\n        for j in range(target_sum + 1):\n            if j < sum(matrix[i - 1]):\n                dp[i][j] = dp[i - 1][j]\n            else:\n                dp[i][j] = dp[i - 1][j] or dp[i - 1][j - sum(matrix[i - 1])]\n    \n    subset = \"\"\n    i, j = n, target_sum\n    while i > 0 and j > 0:\n        if dp[i][j] != dp[i - 1][j]:\n            subset = \"1\" + subset\n            j -= sum(matrix[i - 1])\n        else:\n            subset = \"0\" + subset\n        i -= 1\n    \n    return subset"
    },
    {
        "function_name": "longest_contiguous_subsequence",
        "file_name": "subarray_processor.py",
        "parameters": {
            "arr": "List of integers",
            "m": "Integer representing the size of the sliding window",
            "x": "Integer representing the target value"
        },
        "objectives": [
            "Divide the input array into subarrays of size 'm'.",
            "For each subarray, find the longest contiguous subsequence that sums up to 'x'.",
            "Return the length of the longest contiguous subsequence."
        ],
        "import_lines": [],
        "function_def": "def longest_contiguous_subsequence(arr, m, x):\n    n = len(arr)\n    max_length = 0\n    for i in range(0, n, m):\n        subarray = arr[i:i+m]\n        for j in range(len(subarray)):\n            current_sum = 0\n            for k in range(j, len(subarray)):\n                current_sum += subarray[k]\n                if current_sum == x:\n                    max_length = max(max_length, k - j + 1)\n    return max_length"
    },
    {
        "function_name": "matrix_multiplication",
        "file_name": "matrix_processor.py",
        "parameters": {
            "n": "Integer",
            "A": "List of integers",
            "B": "List of integers",
            "C": "List of integers"
        },
        "objectives": [
            "Calculate the matrix multiplication of A and B.",
            "Perform element-wise multiplication of the result with C.",
            "Return the sum of the elements in the resulting matrix."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def matrix_multiplication(n, A, B, C):\n    A = np.array(A).reshape(n, n)\n    B = np.array(B).reshape(n, n)\n    C = np.array(C).reshape(n, n)\n    result = np.dot(A, B) * C\n    return np.sum(result)"
    },
    {
        "function_name": "evaluate_expression",
        "file_name": "expression_evaluation.py",
        "parameters": {
            "`expression`": "A string representing a mathematical expression",
            "`variables`": "A dictionary of variable names and their corresponding values"
        },
        "objectives": [
            "Parse the mathematical expression and substitute the variable values.",
            "Evaluate the expression and return the result.",
            "Identify the operators in the expression and return their frequencies."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def evaluate_expression(expression, variables):\n    # Substitute variable values\n    for var, value in variables.items():\n        expression = expression.replace(var, str(value))\n    \n    # Evaluate the expression\n    result = eval(expression)\n    \n    # Identify operators and their frequencies\n    operators = re.findall(r'[+*/-]', expression)\n    frequencies = Counter(operators)\n    \n    return result, dict(frequencies)"
    },
    {
        "function_name": "shortest_path",
        "file_name": "graph_shortest_path.py",
        "parameters": {
            "`graph`": "A dictionary representing a weighted graph",
            "`source`": "A node in the graph",
            "`target`": "A node in the graph"
        },
        "objectives": [
            "Find the shortest path from the source node to the target node using Dijkstra's algorithm.",
            "Return the shortest distance and the path.",
            "Identify the nodes with the highest and lowest degrees in the graph."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def shortest_path(graph, source, target):\n    # Initialize distances and previous nodes\n    distances = {node: float('inf') for node in graph}\n    previous = {node: None for node in graph}\n    distances[source] = 0\n    \n    # Create a min-heap\n    min_heap = [(0, source)]\n    \n    while min_heap:\n        current_distance, current_node = heapq.heappop(min_heap)\n        if current_distance > distances[current_node]:\n            continue\n        for neighbor, weight in graph[current_node].items():\n            distance = current_distance + weight\n            if distance < distances[neighbor]:\n                distances[neighbor] = distance\n                previous[neighbor] = current_node\n                heapq.heappush(min_heap, (distance, neighbor))\n    \n    # Find the shortest path\n    path = []\n    current_node = target\n    while current_node is not None:\n        path.append(current_node)\n        current_node = previous[current_node]\n    path.reverse()\n    \n    # Identify nodes with highest and lowest degrees\n    degrees = {node: len(neighbors) for node, neighbors in graph.items()}\n    highest_degree_node = max(degrees, key=degrees.get)\n    lowest_degree_node = min(degrees, key=degrees.get)\n    \n    return distances[target], path, highest_degree_node, lowest_degree_node"
    },
    {
        "function_name": "kmp_search",
        "file_name": "string_search.py",
        "parameters": {
            "`text`": "A string",
            "`pattern`": "A string"
        },
        "objectives": [
            "Search for the pattern in the text using the Knuth-Morris-Pratt (KMP) algorithm.",
            "Return the indices of the pattern in the text.",
            "Identify the most frequent character in the text."
        ],
        "import_lines": [],
        "function_def": "def kmp_search(text, pattern):\n    # Create the lps array\n    lps = [0] * len(pattern)\n    j = 0\n    for i in range(1, len(pattern)):\n        if pattern[i] == pattern[j]:\n            j += 1\n            lps[i] = j\n        else:\n            j = lps[j-1]\n            while pattern[i] != pattern[j] and j != 0:\n                j = lps[j-1]\n            if pattern[i] == pattern[j]:\n                j += 1\n                lps[i] = j\n    \n    # Search for the pattern\n    indices = []\n    i = j = 0\n    while i < len(text):\n        if text[i] == pattern[j]:\n            i += 1\n            j += 1\n        if j == len(pattern):\n            indices.append(i-j)\n            j = lps[j-1]\n        elif i < len(text) and text[i] != pattern[j]:\n            if j != 0:\n                j = lps[j-1]\n            else:\n                i += 1\n    \n    # Identify the most frequent character\n    char_counts = {}\n    for char in text:\n        if char in char_counts:\n            char_counts[char] += 1\n        else:\n            char_counts[char] = 1\n    most_frequent_char = max(char_counts, key=char_counts.get)\n    \n    return indices, most_frequent_char"
    },
    {
        "function_name": "quickselect",
        "file_name": "array_selection.py",
        "parameters": {
            "`arr`": "A list of integers",
            "`k`": "An integer"
        },
        "objectives": [
            "Find the k-th smallest element in the array using the QuickSelect algorithm.",
            "Return the k-th smallest element.",
            "Identify the elements that are within a distance of k from the k-th smallest element."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def quickselect(arr, k):\n    # Select a pivot randomly\n    pivot = random.choice(arr)\n    \n    # Partition the array\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    \n    # Recursively search for the k-th smallest element\n    if k <= len(left):\n        return quickselect(left, k)\n    elif k <= len(left) + len(middle):\n        return middle[0]\n    else:\n        return quickselect(right, k - len(left) - len(middle))\n    \n    # Identify elements within a distance of k\n    k_th_smallest = quickselect(arr, k)\n    within_k_distance = [x for x in arr if abs(x - k_th_smallest) <= k]\n    \n    return k_th_smallest, within_k_distance"
    },
    {
        "function_name": "graph_traversal",
        "file_name": "graph_algorithms.py",
        "parameters": {
            "graph": "A dictionary representing a graph where each key is a node and each value is a list of its neighboring nodes",
            "start_node": "A string representing the starting node for the traversal",
            "target_node": "A string representing the target node to be reached"
        },
        "objectives": [
            "Implement a depth-first search (DFS) algorithm to traverse the graph and find the shortest path between the start node and the target node.",
            "Use a stack data structure to keep track of the nodes to be visited.",
            "Return the shortest path as a list of nodes."
        ],
        "import_lines": [],
        "function_def": "def graph_traversal(graph, start_node, target_node):\n    stack = [(start_node, [start_node])]\n    while stack:\n        node, path = stack.pop()\n        for neighbor in graph[node]:\n            if neighbor not in path:\n                if neighbor == target_node:\n                    return path + [neighbor]\n                stack.append((neighbor, path + [neighbor]))\n    return None"
    },
    {
        "function_name": "kmeans_clustering",
        "file_name": "clustering_algorithms.py",
        "parameters": {
            "numbers": "A list of integers",
            "k": "An integer representing the number of clusters"
        },
        "objectives": [
            "Implement the k-means clustering algorithm to partition the input numbers into k clusters.",
            "Use the mean of each cluster as the centroid.",
            "Return the cluster assignments for each number."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def kmeans_clustering(numbers, k):\n    # Initialize centroids randomly\n    centroids = random.sample(numbers, k)\n    cluster_assignments = {}\n    \n    while True:\n        # Assign each number to the closest centroid\n        new_cluster_assignments = {}\n        for num in numbers:\n            closest_centroid = min(centroids, key=lambda centroid: abs(num - centroid))\n            new_cluster_assignments[num] = closest_centroid\n        \n        # Update centroids as the mean of each cluster\n        new_centroids = []\n        for centroid in centroids:\n            cluster = [num for num, assigned_centroid in new_cluster_assignments.items() if assigned_centroid == centroid]\n            if cluster:\n                new_centroids.append(sum(cluster) / len(cluster))\n            else:\n                new_centroids.append(centroid)\n        \n        # Check for convergence\n        if new_cluster_assignments == cluster_assignments and new_centroids == centroids:\n            break\n        \n        cluster_assignments = new_cluster_assignments\n        centroids = new_centroids\n    \n    return cluster_assignments"
    },
    {
        "function_name": "eigenvalue_decomposition",
        "file_name": "linear_algebra.py",
        "parameters": {
            "matrix": "A 2D list of integers",
            "k": "An integer representing the number of eigenvalues to compute"
        },
        "objectives": [
            "Implement the power iteration method to compute the top k eigenvalues and eigenvectors of the input matrix.",
            "Use a random initial guess for the eigenvector.",
            "Return the top k eigenvalues and eigenvectors."
        ],
        "import_lines": [
            "import random",
            "import numpy as np"
        ],
        "function_def": "def eigenvalue_decomposition(matrix, k):\n    num_rows, num_cols = len(matrix), len(matrix[0])\n    eigenvalues = []\n    eigenvectors = []\n    \n    for _ in range(k):\n        # Initialize a random eigenvector\n        eigenvector = [random.random() for _ in range(num_cols)]\n        eigenvector = np.array(eigenvector) / np.linalg.norm(eigenvector)\n        \n        # Power iteration\n        while True:\n            new_eigenvector = np.dot(matrix, eigenvector)\n            new_eigenvector = new_eigenvector / np.linalg.norm(new_eigenvector)\n            if np.allclose(eigenvector, new_eigenvector):\n                break\n            eigenvector = new_eigenvector\n        \n        # Compute the eigenvalue\n        eigenvalue = np.dot(eigenvector.T, np.dot(matrix, eigenvector))\n        \n        # Add to the list of eigenvalues and eigenvectors\n        eigenvalues.append(eigenvalue)\n        eigenvectors.append(eigenvector)\n    \n    return eigenvalues, eigenvectors"
    },
    {
        "function_name": "apriori_mining",
        "file_name": "frequent_itemsets.py",
        "parameters": {
            "transactions": "A list of transactions where each transaction is a list of items"
        },
        "objectives": [
            "Implement the Apriori algorithm to mine frequent itemsets from the input transactions.",
            "Use a minimum support threshold to filter out infrequent itemsets.",
            "Return the frequent itemsets."
        ],
        "import_lines": [],
        "function_def": "def apriori_mining(transactions, min_support):\n    frequent_itemsets = {}\n    item_counts = {}\n    \n    # Count the occurrences of each item\n    for transaction in transactions:\n        for item in transaction:\n            item_counts[item] = item_counts.get(item, 0) + 1\n    \n    # Filter out items with low support\n    frequent_items = [item for item, count in item_counts.items() if count >= min_support]\n    \n    # Generate frequent itemsets\n    for item in frequent_items:\n        frequent_itemsets[tuple([item])] = item_counts[item]\n    \n    k = 2\n    while True:\n        # Generate candidate itemsets\n        candidate_itemsets = []\n        for itemset in frequent_itemsets:\n            for item in frequent_items:\n                if item not in itemset:\n                    candidate_itemset = tuple(sorted(itemset + (item,)))\n                    candidate_itemsets.append(candidate_itemset)\n        \n        # Count the occurrences of each candidate itemset\n        candidate_counts = {}\n        for transaction in transactions:\n            for candidate_itemset in candidate_itemsets:\n                if set(candidate_itemset).issubset(set(transaction)):\n                    candidate_counts[candidate_itemset] = candidate_counts.get(candidate_itemset, 0) + 1\n        \n        # Filter out itemsets with low support\n        new_frequent_itemsets = {itemset: count for itemset, count in candidate_counts.items() if count >= min_support}\n        \n        # Check for convergence\n        if not new_frequent_itemsets:\n            break\n        \n        frequent_itemsets = new_frequent_itemsets\n        k += 1\n    \n    return frequent_itemsets"
    },
    {
        "function_name": "k_means_clustering",
        "file_name": "k_means_clustering.py",
        "parameters": {
            "`points`": "A list of tuples representing points in a 2D space",
            "`k`": "An integer representing the number of clusters"
        },
        "objectives": [
            "Implement the K-Means clustering algorithm to group the points into k clusters.",
            "Return the centroid of each cluster.",
            "Calculate the sum of squared errors (SSE) for each cluster."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def k_means_clustering(points, k):\n    # Initialize centroids randomly\n    centroids = np.array(points)[np.random.choice(len(points), k, replace=False)]\n    \n    while True:\n        # Assign points to clusters\n        labels = np.argmin(np.linalg.norm(np.array(points)[:, np.newaxis] - centroids, axis=2), axis=1)\n        \n        # Calculate new centroids\n        new_centroids = np.array([np.mean(np.array(points)[labels == i], axis=0) for i in range(k)])\n        \n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    # Calculate SSE for each cluster\n    sse = []\n    for i in range(k):\n        cluster_points = np.array(points)[labels == i]\n        sse.append(np.sum((cluster_points - centroids[i]) ** 2))\n    \n    return centroids, sse"
    },
    {
        "function_name": "knapsack",
        "file_name": "dynamic_programming.py",
        "parameters": {
            "n": "Integer representing the number of items",
            "weights": "List of integers representing the weights of the items",
            "values": "List of integers representing the values of the items",
            "capacity": "Integer representing the capacity of the knapsack"
        },
        "objectives": [
            "Implement a 0/1 Knapsack problem solution using dynamic programming.",
            "Fill the knapsack with the most valuable items without exceeding the capacity.",
            "Return the maximum value that can be achieved and the corresponding items."
        ],
        "import_lines": [],
        "function_def": "def knapsack(n, weights, values, capacity):\n    dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n    for i in range(1, n + 1):\n        for w in range(1, capacity + 1):\n            if weights[i - 1] <= w:\n                dp[i][w] = max(dp[i - 1][w], values[i - 1] + dp[i - 1][w - weights[i - 1]])\n            else:\n                dp[i][w] = dp[i - 1][w]\n    \n    max_value = dp[n][capacity]\n    items = []\n    w = capacity\n    for i in range(n, 0, -1):\n        if dp[i][w] != dp[i - 1][w]:\n            items.append(i - 1)\n            w -= weights[i - 1]\n    \n    return max_value, items[::-1]"
    },
    {
        "function_name": "rotate_matrix",
        "file_name": "matrix_algorithms.py",
        "parameters": {
            "matrix": "2D list representing the matrix to be processed"
        },
        "objectives": [
            "Implement a matrix rotation algorithm to rotate the matrix by 90 degrees clockwise.",
            "Transpose the matrix and then reverse each row.",
            "Return the rotated matrix."
        ],
        "import_lines": [],
        "function_def": "def rotate_matrix(matrix):\n    n = len(matrix)\n    for i in range(n):\n        for j in range(i, n):\n            matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j]\n    \n    for row in matrix:\n        row.reverse()\n    \n    return matrix"
    },
    {
        "function_name": "merge_intervals",
        "file_name": "interval_algorithms.py",
        "parameters": {
            "intervals": "List of tuples representing the intervals to be merged"
        },
        "objectives": [
            "Implement an interval merging algorithm to merge overlapping intervals.",
            "Sort the intervals by their start value and then iterate through the sorted intervals to merge overlapping intervals.",
            "Return the merged intervals."
        ],
        "import_lines": [],
        "function_def": "def merge_intervals(intervals):\n    intervals.sort(key=lambda x: x[0])\n    merged_intervals = [intervals[0]]\n    for interval in intervals[1:]:\n        if merged_intervals[-1][1] >= interval[0]:\n            merged_intervals[-1] = (merged_intervals[-1][0], max(merged_intervals[-1][1], interval[1]))\n        else:\n            merged_intervals.append(interval)\n    \n    return merged_intervals"
    },
    {
        "function_name": "sum_of_squares_of_differences",
        "file_name": "list_processor.py",
        "parameters": {
            "n": "int",
            "A": "list of integers",
            "B": "list of integers"
        },
        "objectives": [
            "Calculate the difference between the two input lists A and B.",
            "Calculate the sum of squares of the differences.",
            "Return the result."
        ],
        "import_lines": [],
        "function_def": "def sum_of_squares_of_differences(n, A, B):\n    differences = [a - b for a, b in zip(A, B)]\n    sum_of_squares = sum([x**2 for x in differences])\n    return sum_of_squares"
    },
    {
        "function_name": "max_pairs",
        "file_name": "pair_processor.py",
        "parameters": {
            "nums": "List of integers",
            "k": "Integer representing the size of the sliding window",
            "target": "Integer representing the target sum"
        },
        "objectives": [
            "Divide the input list into sub-lists of size 'k'.",
            "For each sub-list, find the number of pairs of elements that sum up to 'target'.",
            "Return the maximum count of pairs among all sub-lists."
        ],
        "import_lines": [],
        "function_def": "def max_pairs(nums, k, target):\n    n = len(nums)\n    max_count = 0\n    for i in range(0, n, k):\n        sub_list = nums[i:i+k]\n        count = 0\n        freq = {}\n        for num in sub_list:\n            complement = target - num\n            if complement in freq:\n                count += freq[complement]\n            if num in freq:\n                freq[num] += 1\n            else:\n                freq[num] = 1\n        max_count = max(max_count, count)\n    \n    return max_count"
    },
    {
        "function_name": "min_greater_than_median",
        "file_name": "stats_processor.py",
        "parameters": {
            "nums": "List of integers",
            "k": "Integer representing the size of the sliding window",
            "target": "Integer representing the target median"
        },
        "objectives": [
            "Divide the input list into sub-lists of size 'k'.",
            "For each sub-list, find the number of elements that are greater than the target median.",
            "Return the minimum count of elements among all sub-lists."
        ],
        "import_lines": [],
        "function_def": "def min_greater_than_median(nums, k, target):\n    n = len(nums)\n    min_count = float('inf')\n    for i in range(0, n, k):\n        sub_list = nums[i:i+k]\n        sub_list.sort()\n        median = sub_list[k//2] if k % 2 != 0 else (sub_list[k//2 - 1] + sub_list[k//2]) / 2\n        count = sum(1 for num in sub_list if num > median and num > target)\n        min_count = min(min_count, count)\n    \n    return min_count"
    },
    {
        "function_name": "max_sum_subsequence",
        "file_name": "sliding_window.py",
        "parameters": {
            "`sequence`": "A list of integers representing a sequence of numbers",
            "`k`": "An integer representing the size of the sliding window"
        },
        "objectives": [
            "Find the maximum sum of a sub-sequence of size k in the given sequence",
            "Use a deque to efficiently find the maximum sum",
            "Return the maximum sum and the starting index of the sub-sequence"
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def max_sum_subsequence(sequence, k):\n    max_sum = float('-inf')\n    start_index = 0\n    window = deque()\n    \n    for i, num in enumerate(sequence):\n        while window and window[0] <= i - k:\n            window.popleft()\n        \n        while window and sequence[window[-1]] < num:\n            window.pop()\n        \n        window.append(i)\n        \n        if i >= k - 1:\n            max_sum = max(max_sum, sum(sequence[window[0]:i+1]))\n            start_index = window[0]\n    \n    return max_sum, start_index"
    },
    {
        "function_name": "find_common_substrings",
        "file_name": "string_search.py",
        "parameters": {
            "strings": "List of strings",
            "threshold": "Integer representing the minimum frequency of a substring"
        },
        "objectives": [
            "Find all substrings that appear in at least threshold number of strings.",
            "For each substring, find the total number of occurrences in all strings.",
            "Return a list of tuples, where each tuple contains a substring and its total frequency."
        ],
        "import_lines": [],
        "function_def": "def find_common_substrings(strings, threshold):\n    freq_dict = {}\n    for string in strings:\n        for i in range(len(string)):\n            for j in range(i + 1, len(string) + 1):\n                substring = string[i:j]\n                freq_dict[substring] = freq_dict.get(substring, 0) + 1\n    common_substrings = [(substring, freq) for substring, freq in freq_dict.items() if freq >= threshold]\n    return common_substrings"
    },
    {
        "function_name": "bellman_ford",
        "file_name": "graph_algorithms.py",
        "parameters": {
            "graph": "Dictionary representing the adjacency list of a graph",
            "source": "Node representing the source vertex",
            "destination": "Node representing the destination vertex"
        },
        "objectives": [
            "Find the shortest path from the source to the destination using Bellman-Ford algorithm.",
            "If a negative cycle is detected, return an error message.",
            "Return the shortest distance and the shortest path."
        ],
        "import_lines": [],
        "function_def": "def bellman_ford(graph, source, destination):\n    distance = {node: float('inf') for node in graph}\n    distance[source] = 0\n    predecessor = {node: None for node in graph}\n    \n    for _ in range(len(graph) - 1):\n        for node in graph:\n            for neighbor in graph[node]:\n                if distance[node] + graph[node][neighbor] < distance[neighbor]:\n                    distance[neighbor] = distance[node] + graph[node][neighbor]\n                    predecessor[neighbor] = node\n                    \n    for node in graph:\n        for neighbor in graph[node]:\n            if distance[node] + graph[node][neighbor] < distance[neighbor]:\n                return \"Graph contains a negative-weight cycle\"\n                \n    path = []\n    current_node = destination\n    while current_node is not None:\n        path.append(current_node)\n        current_node = predecessor[current_node]\n        \n    return distance[destination], list(reversed(path))"
    },
    {
        "function_name": "find_subsets_with_median",
        "file_name": "subset_search.py",
        "parameters": {
            "numbers": "List of numbers",
            "k": "Integer representing the size of the subsets"
        },
        "objectives": [
            "Find all subsets of size k from the given list of numbers.",
            "For each subset, calculate the median.",
            "Return a list of tuples, where each tuple contains a subset and its median."
        ],
        "import_lines": [
            "import itertools",
            "import statistics"
        ],
        "function_def": "def find_subsets_with_median(numbers, k):\n    subsets = list(itertools.combinations(numbers, k))\n    result = []\n    for subset in subsets:\n        median = statistics.median(subset)\n        result.append((subset, median))\n    return result"
    },
    {
        "function_name": "find_max_profit_period",
        "file_name": "stock_analyzer.py",
        "parameters": {
            "`k`": "int",
            "`prices`": "list of tuples (day, price)"
        },
        "objectives": [
            "Sort the list of prices by day.",
            "Use a greedy strategy to find the `k` consecutive days with the maximum total price.",
            "Calculate the maximum possible profit by buying on the day with the minimum price and selling on the day with the maximum price.",
            "Return the two-day period (start day, end day) for which the maximum profit can be achieved."
        ],
        "import_lines": [],
        "function_def": "def find_max_profit_period(k, prices):\n    prices.sort(key=lambda x: x[0])\n    max_profit = 0\n    max_profit_period = (prices[0][0], prices[0][0])\n    min_price = prices[0][1]\n    for i in range(len(prices) - k + 1):\n        max_price = max(prices[j][1] for j in range(i, i + k))\n        profit = max_price - min_price\n        if profit > max_profit:\n            max_profit = profit\n            max_profit_period = (prices[i][0], prices[i + k - 1][0])\n    return max_profit_period"
    },
    {
        "function_name": "find_most_isolated_coordinates",
        "file_name": "coordinate_analyzer.py",
        "parameters": {
            "`n`": "int",
            "`coordinates`": "list of tuples (x, y)",
            "`radius`": "int"
        },
        "objectives": [
            "Create a hash table to store the coordinates.",
            "For each coordinate, calculate its neighbors within the given `radius`.",
            "Update the hash table with the neighbors of each coordinate.",
            "Use the hash table to find the `n` most isolated coordinates (coordinates with the fewest neighbors)."
        ],
        "import_lines": [],
        "function_def": "def find_most_isolated_coordinates(n, coordinates, radius):\n    hash_table = {}\n    for x, y in coordinates:\n        hash_table[(x, y)] = 0\n        for dx in range(-radius, radius + 1):\n            for dy in range(-radius, radius + 1):\n                if (dx, dy) != (0, 0) and (x + dx, y + dy) in hash_table:\n                    hash_table[(x, y)] += 1\n    return sorted(hash_table.keys(), key=lambda x: hash_table[x])[:n]"
    },
    {
        "function_name": "find_min_inversions",
        "file_name": "reversal_analyzer.py",
        "parameters": {
            "`nums`": "list of integers",
            "`k`": "int (number of reverse operations)"
        },
        "objectives": [
            "Use a prefix sum array to calculate the total number of inversions in the list.",
            "Reverse the order of elements in the list `k` times.",
            "After each reversal, calculate the total number of inversions in the list using the prefix sum array."
        ],
        "import_lines": [],
        "function_def": "def find_min_inversions(nums, k):\n    n = len(nums)\n    prefix_sum = [0] * (n + 1)\n    min_inversions = float('inf')\n    inversions = sum(1 for i in range(n) for j in range(i + 1, n) if nums[i] > nums[j])\n    min_inversions = min(min_inversions, inversions)\n    for _ in range(k):\n        nums.reverse()\n        inversions = n * (n - 1) // 2 - inversions\n        min_inversions = min(min_inversions, inversions)\n    return min_inversions"
    },
    {
        "function_name": "matrix_multiply_mod",
        "file_name": "dynamic_programming.py",
        "parameters": {
            "`A`": "A 2D list of integers representing the matrix",
            "`B`": "A 2D list of integers representing the matrix",
            "`mod`": "An integer representing the modulo value"
        },
        "objectives": [
            "Multiply the two matrices A and B modulo the given value mod",
            "Use dynamic programming to optimize the matrix multiplication",
            "Return the resulting matrix"
        ],
        "import_lines": [],
        "function_def": "def matrix_multiply_mod(A, B, mod):\n    rows_A = len(A)\n    cols_A = len(A[0])\n    rows_B = len(B)\n    cols_B = len(B[0])\n    \n    if cols_A != rows_B:\n        raise ValueError(\"Invalid matrix dimensions for multiplication\")\n    \n    C = [[0 for _ in range(cols_B)] for _ in range(rows_A)]\n    \n    # Use dynamic programming to optimize the matrix multiplication\n    dp = [[[0 for _ in range(cols_B)] for _ in range(cols_A)] for _ in range(rows_A)]\n    \n    for i in range(rows_A):\n        for j in range(cols_B):\n            for k in range(cols_A):\n                dp[i][k][j] = (A[i][k]*B[k][j]) % mod\n                C[i][j] += dp[i][k][j]\n    \n    return [[val % mod for val in row] for row in C]"
    },
    {
        "function_name": "partition_sequence",
        "file_name": "dynamic_programming.py",
        "parameters": {
            "`sequence`": "A list of integers representing the sequence",
            "`m`": "An integer representing the number of divisions",
            "`d`": "An integer representing the capacity of each division"
        },
        "objectives": [
            "Partition the given sequence into m divisions of size d such that the total sum of each division is minimized",
            "Use dynamic programming to optimize the partitioning process",
            "Return the minimum total sum and the corresponding partitions"
        ],
        "import_lines": [],
        "function_def": "def partition_sequence(sequence, m, d):\n    n = len(sequence)\n    total_sum = sum(sequence)\n    \n    # Use dynamic programming to optimize the partitioning process\n    dp = [[float('inf')] * (d + 1) for _ in range(m + 1)]\n    \n    for i in range(m + 1):\n        for j in range(d + 1):\n            if i == 0 and j == 0:\n                dp[i][j] = 0\n            \n            elif i == 0:\n                continue\n            \n            elif j == 0:\n                continue\n            \n            min_sum = float('inf')\n            for k in range(1, j + 1):\n                if k > n:\n                    break\n                \n                min_sum = min(min_sum, sequence[k - 1] + dp[i - 1][j - k])\n            \n            dp[i][j] = min(dp[i][j - 1], min_sum)\n    \n    # Get the minimum total sum and the corresponding partitions\n    min_total_sum = total_sum\n    partitions = []\n    for i in range(m):\n        k = d\n        for j in range(d, 0, -1):\n            if sequence[j - 1] + dp[m - i - 1][k - j] < min_total_sum:\n                partitions.append(sequence[j - 1])\n                min_total_sum -= sequence[j - 1]\n                k -= j\n                break\n    \n    return min_total_sum, partitions[::-1]"
    },
    {
        "function_name": "find_motif",
        "file_name": "dna_search.py",
        "parameters": {
            "`seq`": "A string representing a DNA sequence",
            "`motif`": "A string representing a motif to search for"
        },
        "objectives": [
            "Find all occurrences of the motif in the DNA sequence using the Knuth-Morris-Pratt (KMP) algorithm.",
            "Return a list of indices where the motif starts in the sequence."
        ],
        "import_lines": [],
        "function_def": "def find_motif(seq, motif):\n    pi = [0] * len(motif)\n    j = 0\n    for i in range(1, len(motif)):\n        while j > 0 and motif[i] != motif[j]:\n            j = pi[j - 1]\n        if motif[i] == motif[j]:\n            j += 1\n        pi[i] = j\n    \n    indices = []\n    j = 0\n    for i in range(len(seq)):\n        while j > 0 and seq[i] != motif[j]:\n            j = pi[j - 1]\n        if seq[i] == motif[j]:\n            j += 1\n        if j == len(motif):\n            indices.append(i - j + 1)\n            j = pi[j - 1]\n    \n    return indices"
    },
    {
        "function_name": "interval_scheduler",
        "file_name": "interval_scheduler.py",
        "parameters": {
            "intervals": "A list of tuples representing time intervals, where each tuple contains two integers representing the start and end time of the interval.",
            "events": "A list of integers representing event times."
        },
        "objectives": [
            "Find the maximum number of non-overlapping intervals that can be scheduled given the time intervals.",
            "Calculate the number of events that occur within the scheduled intervals.",
            "Return the scheduled intervals and the number of events that occurred within these intervals."
        ],
        "import_lines": [],
        "function_def": "def interval_scheduler(intervals, events):\n    # Sort intervals by their end times\n    intervals.sort(key=lambda x: x[1])\n    \n    # Initialize variables to keep track of the scheduled intervals and the number of events\n    scheduled_intervals = []\n    num_events = 0\n    \n    # Initialize variables to keep track of the current end time and the number of scheduled intervals\n    current_end_time = -1\n    numcheduled = 0\n    \n    # Iterate over the intervals\n    for interval in intervals:\n        # If the current interval does not overlap with the previously scheduled interval, schedule it\n        if interval[0] >= current_end_time:\n            scheduled_intervals.append(interval)\n            numcheduled += 1\n            current_end_time = interval[1]\n    \n    # Calculate the number of events that occur within the scheduled intervals\n    for event in events:\n        for interval in scheduled_intervals:\n            if interval[0] <= event <= interval[1]:\n                num_events += 1\n                break\n    \n    return scheduled_intervals, num_events"
    },
    {
        "function_name": "minimum_spanning_tree",
        "file_name": "minimum_spanning_tree.py",
        "parameters": {
            "n": "An integer representing the number of nodes in a graph.",
            "edges": "A list of tuples representing the edges in the graph, where each tuple contains two integers representing the nodes connected by the edge.",
            "weights": "A list of integers representing the weights of the edges."
        },
        "objectives": [
            "Construct a minimum spanning tree of the graph using Prim's algorithm.",
            "Calculate the total weight of the minimum spanning tree.",
            "Return the edges in the minimum spanning tree and the total weight."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def minimum_spanning_tree(n, edges, weights):\n    # Create a dictionary to store the graph\n    graph = {i: {} for i in range(n)}\n    for i, edge in enumerate(edges):\n        u, v = edge\n        graph[u][v] = weights[i]\n        graph[v][u] = weights[i]\n    \n    # Initialize variables to keep track of the minimum spanning tree and its total weight\n    mst = []\n    total_weight = 0\n    \n    # Initialize a priority queue to keep track of the edges to be processed\n    pq = []\n    heapq.heappush(pq, (0, 0, 1))\n    \n    # Initialize a set to keep track of the nodes in the minimum spanning tree\n    visited = set()\n    \n    # Process the edges in the priority queue\n    while pq:\n        weight, u, v = heapq.heappop(pq)\n        if v not in visited:\n            visited.add(v)\n            mst.append((u, v))\n            total_weight += weight\n            for neighbor, neighbor_weight in graph[v].items():\n                if neighbor not in visited:\n                    heapq.heappush(pq, (neighbor_weight, v, neighbor))\n    \n    return mst, total_weight"
    },
    {
        "function_name": "longest_common_substring",
        "file_name": "longest_common_substring.py",
        "parameters": {
            "s": "A string representing the input string.",
            "t": "A string representing the target string."
        },
        "objectives": [
            "Find the longest common substring of the input string and the target string using dynamic programming.",
            "Return the longest common substring."
        ],
        "import_lines": [],
        "function_def": "def longest_common_substring(s, t):\n    # Create a 2D table to store the lengths of the common substrings\n    dp = [[0 for _ in range(len(t) + 1)] for _ in range(len(s) + 1)]\n    \n    # Initialize variables to keep track of the longest common substring\n    max_length = 0\n    end = 0\n    \n    # Fill the table using dynamic programming\n    for i in range(1, len(s) + 1):\n        for j in range(1, len(t) + 1):\n            if s[i - 1] == t[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n                if dp[i][j] > max_length:\n                    max_length = dp[i][j]\n                    end = i\n            else:\n                dp[i][j] = 0\n    \n    # Return the longest common substring\n    return s[end - max_length: end]"
    },
    {
        "function_name": "kmeans",
        "file_name": "kmeans.py",
        "parameters": {
            "k": "An integer representing the number of clusters.",
            "points": "A list of tuples representing the points to be clustered, where each tuple contains two integers representing the x and y coordinates of the point."
        },
        "objectives": [
            "Cluster the points using the k-means algorithm.",
            "Return the centroids of the clusters and the assignment of points to clusters."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def kmeans(k, points):\n    # Initialize the centroids randomly\n    centroids = random.sample(points, k)\n    \n    # Initialize the assignment of points to clusters\n    assignment = [None for _ in range(len(points))]\n    \n    # Repeat the clustering process until convergence\n    while True:\n        # Assign each point to the closest centroid\n        for i, point in enumerate(points):\n            min_distance = float('inf')\n            closest_centroid = None\n            for j, centroid in enumerate(centroids):\n                distance = (point[0] - centroid[0]) ** 2 + (point[1] - centroid[1]) ** 2\n                if distance < min_distance:\n                    min_distance = distance\n                    closest_centroid = j\n            assignment[i] = closest_centroid\n        \n        # Update the centroids\n        new_centroids = [[0, 0] for _ in range(k)]\n        counts = [0 for _ in range(k)]\n        for i, point in enumerate(points):\n            centroid_index = assignment[i]\n            new_centroids[centroid_index][0] += point[0]\n            new_centroids[centroid_index][1] += point[1]\n            counts[centroid_index] += 1\n        for i in range(k):\n            new_centroids[i][0] /= counts[i]\n            new_centroids[i][1] /= counts[i]\n        \n        # Check for convergence\n        if new_centroids == centroids:\n            break\n        \n        centroids = new_centroids\n    \n    return centroids, assignment"
    },
    {
        "function_name": "select_rows_with_target_sum",
        "file_name": "row_selection.py",
        "parameters": {
            "matrix": "2D list of integers",
            "target_sum": "Integer representing the target sum",
            "k": "Number of rows to select"
        },
        "objectives": [
            "Select k rows from the matrix such that the sum of the selected rows is closest to the target sum.",
            "Use a greedy approach to select the rows with the largest sum first.",
            "Return the indices of the selected rows and the sum of the selected rows."
        ],
        "import_lines": [],
        "function_def": "def select_rows_with_target_sum(matrix, target_sum, k):\n    row_sums = [sum(row) for row in matrix]\n    sorted_rows = sorted(enumerate(row_sums), key=lambda x: x[1], reverse=True)\n    selected_rows = sorted_rows[:k]\n    selected_row_indices = [row[0] for row in selected_rows]\n    selected_row_sum = sum(row[1] for row in selected_rows)\n    return selected_row_indices, selected_row_sum"
    },
    {
        "function_name": "find_pairs",
        "file_name": "linear_algebra.py",
        "parameters": {
            "matrix": "2D list of integers",
            "target_sum": "Integer"
        },
        "objectives": [
            "Implement a function that finds all pairs of elements in the matrix that add up to the target sum.",
            "The function should return a list of tuples, where each tuple contains the indices of the two elements that add up to the target sum.",
            "The function should also return the number of pairs found."
        ],
        "import_lines": [],
        "function_def": "def find_pairs(matrix, target_sum):\n    # Initialize a set to store the seen elements\n    seen = set()\n    \n    # Initialize a list to store the pairs of elements\n    pairs = []\n    \n    # Iterate over each row in the matrix\n    for i in range(len(matrix)):\n        # Iterate over each column in the matrix\n        for j in range(len(matrix[0])):\n            # Calculate the complement of the current element\n            complement = target_sum - matrix[i][j]\n            \n            # Check if the complement has been seen before\n            if complement in seen:\n                # Add the pair of elements to the list of pairs\n                pairs.append(((complement // len(matrix[0]), complement % len(matrix[0])), (i, j)))\n            else:\n                # Add the current element to the set of seen elements\n                seen.add(matrix[i][j] + i * len(matrix[0]))\n    \n    return pairs, len(pairs)"
    },
    {
        "function_name": "find_k_largest",
        "file_name": "data_structures.py",
        "parameters": {
            "array": "List of integers",
            "k": "Integer"
        },
        "objectives": [
            "Implement a function that finds the k largest elements in the array using a heap data structure.",
            "The function should return a list of the k largest elements in the array.",
            "The function should also return the sum of the k largest elements."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def find_k_largest(array, k):\n    # Initialize a min heap to store the k largest elements\n    heap = []\n    \n    # Iterate over the array\n    for num in array:\n        # Push the num onto the heap\n        heapq.heappush(heap, num)\n        \n        # If the size of the heap exceeds k, pop the smallest element\n        if len(heap) > k:\n            heapq.heappop(heap)\n    \n    # The heap now contains the k largest elements\n    k_largest_elements = heap\n            \n    return k_largest_elements, sum(k_largest_elements)"
    },
    {
        "function_name": "subset_sums",
        "file_name": "subset_sums.py",
        "parameters": {
            "numbers": "A list of integers",
            "k": "An integer representing the size of the subset"
        },
        "objectives": [
            "Find all subsets of the given size from the list of numbers.",
            "Calculate the sum of each subset.",
            "Return a list of tuples containing the subset and its sum."
        ],
        "import_lines": [
            "import itertools"
        ],
        "function_def": "def subset_sums(numbers, k):\n    subsets = list(itertools.combinations(numbers, k))\n    results = []\n    \n    for subset in subsets:\n        subset_sum = sum(subset)\n        results.append((subset, subset_sum))\n    \n    return results"
    },
    {
        "function_name": "three_sum",
        "file_name": "triplet_algorithms.py",
        "parameters": {
            "`seq`": "A list of integers representing a sequence of numbers",
            "`target`": "An integer representing the target sum"
        },
        "objectives": [
            "Find all unique triplets in the given sequence that sum up to the target value.",
            "Use a two-pointer technique to efficiently find the triplets in a sorted sequence.",
            "Return the list of unique triplets."
        ],
        "import_lines": [],
        "function_def": "def three_sum(seq, target):\n    seq.sort()\n    triplets = set()\n    for i in range(len(seq) - 2):\n        left, right = i + 1, len(seq) - 1\n        while left < right:\n            current_sum = seq[i] + seq[left] + seq[right]\n            if current_sum == target:\n                triplet = tuple(sorted([seq[i], seq[left], seq[right]]))\n                triplets.add(triplet)\n                left += 1\n                right -= 1\n            elif current_sum < target:\n                left += 1\n            else:\n                right -= 1\n    return list(triplets)"
    },
    {
        "function_name": "min_coins",
        "file_name": "dynamic_programming.py",
        "parameters": {
            "`coins`": "A list of integers representing the available coin denominations",
            "`amount`": "An integer representing the target amount"
        },
        "objectives": [
            "Find the minimum number of coins required to make the target amount using dynamic programming.",
            "Create a table to store the minimum number of coins for each amount from 1 to the target amount.",
            "Return the minimum number of coins."
        ],
        "import_lines": [],
        "function_def": "def min_coins(coins, amount):\n    dp = [float('inf')] * (amount + 1)\n    dp[0] = 0\n    \n    for coin in coins:\n        for i in range(coin, amount + 1):\n            dp[i] = min(dp[i], dp[i - coin] + 1)\n    \n    return dp[amount] if dp[amount] != float('inf') else -1"
    },
    {
        "function_name": "max_product_subarray",
        "file_name": "window_algorithms.py",
        "parameters": {
            "`nums`": "A list of integers representing the given sequence",
            "`k`": "An integer representing the size of the window"
        },
        "objectives": [
            "Find the maximum product of a sub-array of size k in the given sequence using a sliding window approach.",
            "Use a deque to efficiently keep track of the indices of the elements in the window.",
            "Return the maximum product."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def max_product_subarray(nums, k):\n    window = deque()\n    max_product = float('-inf')\n    \n    for i, num in enumerate(nums):\n        if num == 0:\n            window.clear()\n        else:\n            while window and window[0] <= i - k:\n                window.popleft()\n            while window and nums[window[-1]] <= num:\n                window.pop()\n            window.append(i)\n        \n        if i >= k - 1:\n            product = 1\n            for idx in window:\n                product *= nums[idx]\n            max_product = max(max_product, product)\n    \n    return max_product"
    },
    {
        "function_name": "breadth_first_search",
        "file_name": "traversal.py",
        "parameters": {
            "`graph`": "An adjacency list representation of a graph",
            "`source`": "A node to start the search from"
        },
        "objectives": [
            "Use the breadth-first search algorithm to find all nodes reachable from the source node.",
            "Return the reachable nodes as a list of nodes."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def breadth_first_search(graph, source):\n    # Initialize the queue\n    queue = deque([source])\n    reachable_nodes = set()\n    \n    while queue:\n        node = queue.popleft()\n        if node not in reachable_nodes:\n            reachable_nodes.add(node)\n            for neighbor in graph[node]:\n                if neighbor not in reachable_nodes:\n                    queue.append(neighbor)\n    \n    return list(reachable_nodes)"
    },
    {
        "function_name": "needleman_wunsch",
        "file_name": "sequence_alignment.py",
        "parameters": {
            "`sequences`": "A list of strings representing the sequences to be aligned."
        },
        "objectives": [
            "Implement the Needleman-Wunsch algorithm to perform global sequence alignment on the input sequences.",
            "Use a matrix to store the alignment scores.",
            "Return the aligned sequences."
        ],
        "import_lines": [],
        "function_def": "def needleman_wunsch(sequences):\n    # Initialize the matrix\n    matrix = [[0 for _ in range(len(sequences[1]) + 1)] for _ in range(len(sequences[0]) + 1)]\n    \n    # Initialize the gap penalty\n    gap_penalty = -1\n    \n    # Initialize the match score\n    match_score = 1\n    \n    # Initialize the mismatch score\n    mismatch_score = -1\n    \n    # Iterate over the sequences\n    for i in range(1, len(sequences[0]) + 1):\n        for j in range(1, len(sequences[1]) + 1):\n            # Calculate the score for the current cell\n            score = max(\n                matrix[i-1][j-1] + (match_score if sequences[0][i-1] == sequences[1][j-1] else mismatch_score),\n                matrix[i-1][j] + gap_penalty,\n                matrix[i][j-1] + gap_penalty\n            )\n            matrix[i][j] = score\n    \n    # Reconstruct the alignment\n    alignment = []\n    i, j = len(sequences[0]), len(sequences[1])\n    while i > 0 and j > 0:\n        if sequences[0][i-1] == sequences[1][j-1]:\n            alignment.append((sequences[0][i-1], sequences[1][j-1]))\n            i -= 1\n            j -= 1\n        elif matrix[i-1][j-1] > matrix[i-1][j] and matrix[i-1][j-1] > matrix[i][j-1]:\n            alignment.append((sequences[0][i-1], sequences[1][j-1]))\n            i -= 1\n            j -= 1\n        elif matrix[i-1][j] > matrix[i][j-1]:\n            alignment.append((sequences[0][i-1], '-'))\n            i -= 1\n        else:\n            alignment.append(('-', sequences[1][j-1]))\n            j -= 1\n    \n    # Return the aligned sequences\n    return [''.join([char[0] for char in alignment[::-1]]), ''.join([char[1] for char in alignment[::-1]])]"
    },
    {
        "function_name": "find_top_substrings",
        "file_name": "substring_frequency.py",
        "parameters": {
            "`strings`": "List of strings",
            "`k`": "Integer representing the size of the substrings"
        },
        "objectives": [
            "Divide each string into substrings of size `k` and calculate the frequency of each substring.",
            "For each string, find the top 3 substrings with the highest frequency and their corresponding frequencies.",
            "Return a dictionary where the keys are the input strings and the values are lists of tuples, each containing a substring and its frequency."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def find_top_substrings(strings, k):\n    result = {}\n    for string in strings:\n        freq_dict = defaultdict(int)\n        for i in range(len(string) - k + 1):\n            substring = string[i:i+k]\n            freq_dict[substring] += 1\n        top_substrings = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n        result[string] = top_substrings\n    return result"
    },
    {
        "function_name": "max_depth_dfs",
        "file_name": "graph_traversal.py",
        "parameters": {
            "`graphs`": "List of adjacency matrices representing different graphs",
            "`nodes`": "List of nodes to visit"
        },
        "objectives": [
            "Perform a depth-first search (DFS) on each graph starting from each node.",
            "Calculate the maximum depth reached in each graph.",
            "Return a dictionary where the keys are the input graphs and the values are lists of maximum depths reached starting from each node."
        ],
        "import_lines": [],
        "function_def": "def max_depth_dfs(graphs, nodes):\n    result = {}\n    for graph in graphs:\n        max_depths = []\n        for node in nodes:\n            max_depth = 0\n            visited = set()\n            stack = [(node, 0)]\n            while stack:\n                current_node, current_depth = stack.pop()\n                if current_node not in visited:\n                    visited.add(current_node)\n                    max_depth = max(max_depth, current_depth)\n                    for i in range(len(graph)):\n                        if graph[current_node][i] == 1 and i not in visited:\n                            stack.append((i, current_depth + 1))\n            max_depths.append(max_depth)\n        result[tuple(map(tuple, graph))] = max_depths\n    return result"
    },
    {
        "function_name": "find_pattern_frequencies",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "`sequences`": "List of strings representing different sequences",
            "`patterns`": "List of strings representing different patterns"
        },
        "objectives": [
            "For each sequence, find all occurrences of each pattern.",
            "Calculate the frequency of each pattern in each sequence.",
            "Return a dictionary where the keys are the input sequences and the values are dictionaries where the keys are the input patterns and the values are their corresponding frequencies."
        ],
        "import_lines": [],
        "function_def": "def find_pattern_frequencies(sequences, patterns):\n    result = {}\n    for sequence in sequences:\n        pattern_freqs = {}\n        for pattern in patterns:\n            freq = sequence.count(pattern)\n            pattern_freqs[pattern] = freq\n        result[sequence] = pattern_freqs\n    return result"
    },
    {
        "function_name": "matrix_operations",
        "file_name": "matrix_calculations.py",
        "parameters": {
            "`matrices`": "List of 2D matrices",
            "`epsilon`": "Small positive number for numerical stability"
        },
        "objectives": [
            "For each matrix, calculate its determinant using LU decomposition.",
            "Calculate the inverse of each matrix using Gaussian elimination.",
            "Return a dictionary where the keys are the input matrices and the values are tuples containing the determinant and the inverse of each matrix."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def matrix_operations(matrices, epsilon):\n    result = {}\n    for matrix in matrices:\n        det = np.linalg.det(matrix)\n        inv = np.linalg.inv(matrix)\n        result[tuple(map(tuple, matrix))] = (det, inv)\n    return result"
    },
    {
        "function_name": "longest_common_subsequence_sum",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "`seq1`": "List of integers",
            "`seq2`": "List of integers",
            "`k`": "Integer"
        },
        "objectives": [
            "Find the longest common subsequence (LCS) of `seq1` and `seq2`.",
            "Calculate the sum of the elements in the LCS.",
            "Return the sum and the length of the LCS."
        ],
        "import_lines": [],
        "function_def": "def longest_common_subsequence_sum(seq1, seq2, k):\n    m, n = len(seq1), len(seq2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if seq1[i - 1] == seq2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n    \n    lcs_sum = 0\n    i, j = m, n\n    while i > 0 and j > 0:\n        if seq1[i - 1] == seq2[j - 1]:\n            lcs_sum += seq1[i - 1]\n            i -= 1\n            j -= 1\n        elif dp[i - 1][j] > dp[i][j - 1]:\n            i -= 1\n        else:\n            j -= 1\n    \n    return lcs_sum, dp[m][n]"
    },
    {
        "function_name": "max_flow",
        "file_name": "flow_network.py",
        "parameters": {
            "n": "An integer representing the number of nodes",
            "edges": "A list of tuples representing the edges in the graph",
            "capacity": "An integer representing the capacity of each edge"
        },
        "objectives": [
            "Use the Ford-Fulkerson algorithm to find the maximum flow in the given graph.",
            "The graph should be represented as a flow network with source and sink nodes.",
            "Return the maximum flow."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def max_flow(n, edges, capacity):\n    graph = [[0]*n for _ in range(n)]\n    for u, v in edges:\n        graph[u][v] = capacity\n    source, sink = 0, n-1\n    max_flow = 0\n    while True:\n        parent = [-1]*n\n        queue = [source]\n        while queue:\n            u = queue.pop(0)\n            for v in range(n):\n                if graph[u][v] > 0 and parent[v] == -1:\n                    parent[v] = u\n                    queue.append(v)\n        if parent[sink] == -1:\n            break\n        path_flow = float('inf')\n        v = sink\n        while v != source:\n            u = parent[v]\n            path_flow = min(path_flow, graph[u][v])\n            v = u\n        max_flow += path_flow\n        v = sink\n        while v != source:\n            u = parent[v]\n            graph[u][v] -= path_flow\n            graph[v][u] += path_flow\n            v = u\n    return max_flow"
    },
    {
        "function_name": "top_words",
        "file_name": "tf_idf.py",
        "parameters": {
            "strings": "A list of strings representing the text files",
            "n": "An integer representing the number of top words to find"
        },
        "objectives": [
            "Use the TF-IDF algorithm to find the top n words in the given text files.",
            "Calculate the term frequency and inverse document frequency for each word in the corpus.",
            "Return the top n words."
        ],
        "import_lines": [
            "from collections import Counter",
            "import math"
        ],
        "function_def": "def top_words(strings, n):\n    corpus = ' '.join(strings)\n    words = corpus.split()\n    word_counts = Counter(words)\n    num_docs = len(strings)\n    tf_idf = {}\n    for word, count in word_counts.items():\n        tf = count / len(words)\n        idf = math.log(num_docs / sum(1 for string in strings if word in string))\n        tf_idf[word] = tf * idf\n    top_n_words = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)[:n]\n    return [word for word, _ in top_n_words]"
    },
    {
        "function_name": "matrix_pairs",
        "file_name": "matrix_pairs.py",
        "parameters": {
            "matrix": "2D list of integers representing the matrix",
            "target": "Integer representing the target sum"
        },
        "objectives": [
            "Implement a function that finds all pairs of elements in a matrix that add up to a given target sum.",
            "Use a hash set to store the elements we've seen so far and their complements.",
            "Return a list of tuples where each tuple contains a pair of elements that add up to the target sum."
        ],
        "import_lines": [],
        "function_def": "def matrix_pairs(matrix, target):\n    pairs = set()\n    complements = set()\n    \n    for row in matrix:\n        for num in row:\n            complement = target - num\n            \n            if complement in complements:\n                pairs.add((complement, num))\n            \n            complements.add(num)\n    \n    return list(pairs)"
    },
    {
        "function_name": "max_subarray_sum",
        "file_name": "array_processing.py",
        "parameters": {
            "array": "A list of integers representing the array.",
            "k": "An integer representing the size of the subarray."
        },
        "objectives": [
            "Use the Kadane's algorithm to find the maximum sum of a subarray of size k.",
            "Use a sliding window approach to keep track of the maximum sum.",
            "Return the maximum sum."
        ],
        "import_lines": [],
        "function_def": "def max_subarray_sum(array, k):\n    if len(array) < k:\n        return 0\n    \n    max_sum = current_sum = sum(array[:k])\n    \n    for i in range(k, len(array)):\n        current_sum = current_sum - array[i - k] + array[i]\n        max_sum = max(max_sum, current_sum)\n    \n    return max_sum"
    },
    {
        "function_name": "fptas_knapsack",
        "file_name": "approximation_algorithms.py",
        "parameters": {
            "`n`": "Integer representing the number of items",
            "`profits`": "List of integers representing the profits of the items",
            "`weights`": "List of integers representing the weights of the items",
            "`capacity`": "Integer representing the capacity of the knapsack"
        },
        "objectives": [
            "Use the FPTAS (Fully Polynomial-Time Approximation Scheme) algorithm to approximate the 0/1 Knapsack problem.",
            "Return the maximum profit that can be achieved and the corresponding items."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def fptas_knapsack(n, profits, weights, capacity):\n    max_profit = max(profits)\n    epsilon = 0.1\n    k = epsilon * max_profit / n\n    scaled_profits = [math.floor(p / k) for p in profits]\n    \n    dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n    for i in range(1, n + 1):\n        for w in range(1, capacity + 1):\n            if weights[i - 1] <= w:\n                dp[i][w] = max(dp[i - 1][w], scaled_profits[i - 1] + dp[i - 1][w - weights[i - 1]])\n            else:\n                dp[i][w] = dp[i - 1][w]\n                \n    max_profit = dp[n][capacity]\n    items = []\n    w = capacity\n    for i in range(n, 0, -1):\n        if dp[i][w] != dp[i - 1][w]:\n            items.append(i - 1)\n            w -= weights[i - 1]\n            \n    return max_profit * k, items[::-1]"
    },
    {
        "function_name": "a_star_search",
        "file_name": "a_star_search.py",
        "parameters": {
            "graph": "Dictionary representing a graph with weighted edges",
            "start_node": "String representing the starting node",
            "end_node": "String representing the ending node"
        },
        "objectives": [
            "Implement the A* search algorithm to find the shortest path between the start node and the end node.",
            "Use the weights of the edges as the heuristic function.",
            "Return the shortest path and its total weight."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def a_star_search(graph, start_node, end_node):\n    open_list = []\n    closed_list = set()\n    heapq.heappush(open_list, (0, start_node, []))\n    \n    while open_list:\n        current_weight, current_node, current_path = heapq.heappop(open_list)\n        \n        if current_node == end_node:\n            return current_path + [current_node], current_weight\n        \n        if current_node in closed_list:\n            continue\n        \n        closed_list.add(current_node)\n        \n        for neighbor, weight in graph[current_node].items():\n            if neighbor in closed_list:\n                continue\n            \n            new_path = current_path + [current_node]\n            new_weight = current_weight + weight\n            heapq.heappush(open_list, (new_weight, neighbor, new_path))\n    \n    return None"
    },
    {
        "function_name": "find_sub_matrix",
        "file_name": "sub_matrix.py",
        "parameters": {
            "matrix": "2D list of integers",
            "target": "Integer representing the target sum"
        },
        "objectives": [
            "Find all sub-matrices that sum up to the target value.",
            "Use a prefix sum approach to generate all possible sub-matrices.",
            "Return a list of tuples, where each tuple contains the top-left and bottom-right coordinates of a sub-matrix."
        ],
        "import_lines": [],
        "function_def": "def find_sub_matrix(matrix, target):\n    rows, cols = len(matrix), len(matrix[0])\n    prefix_sum = [[0] * (cols + 1) for _ in range(rows + 1)]\n    \n    for i in range(1, rows + 1):\n        for j in range(1, cols + 1):\n            prefix_sum[i][j] = prefix_sum[i-1][j] + prefix_sum[i][j-1] - prefix_sum[i-1][j-1] + matrix[i-1][j-1]\n    \n    sub_matrices = []\n    \n    for i in range(1, rows + 1):\n        for j in range(1, cols + 1):\n            for k in range(i, rows + 1):\n                for last_col in range(j, cols + 1):\n                    sub_matrix_sum = prefix_sum[k][last_col] - prefix_sum[k][j-1] - prefix_sum[i-1][last_col] + prefix_sum[i-1][j-1]\n                    if sub_matrix_sum == target:\n                        sub_matrices.append(((i-1, j-1), (k-1, last_col-1)))\n    \n    return sub_matrices"
    },
    {
        "function_name": "dfs_path_length",
        "file_name": "dfs_path_length.py",
        "parameters": {
            "graph": "A dictionary representing the graph where each key is a node and the value is a list of neighboring nodes",
            "start": "A node representing the starting node",
            "end": "A node representing the ending node"
        },
        "objectives": [
            "Perform a depth-first search to find a path from the start node to the end node.",
            "Calculate the length of the path.",
            "Check if the path length is within a certain tolerance of the minimum possible path length.",
            "Return the path and its length."
        ],
        "import_lines": [],
        "function_def": "def dfs_path_length(graph, start, end, min_path_length, tolerance=1):\n    visited = set()\n    stack = [(start, [start])]\n    while stack:\n        node, path = stack.pop()\n        if node == end:\n            path_length = len(path)\n            if abs(path_length - min_path_length) <= tolerance:\n                return path, path_length\n        if node not in visited:\n            visited.add(node)\n            for neighbor in graph[node]:\n                stack.append((neighbor, path + [neighbor]))\n    return None"
    },
    {
        "function_name": "hierarchical_clustering",
        "file_name": "hierarchical_clustering.py",
        "parameters": {
            "`points`": "A list of tuples representing points in 2D space",
            "`k`": "An integer representing the number of clusters"
        },
        "objectives": [
            "Implement the Hierarchical Agglomerative Clustering (HAC) algorithm to cluster the points.",
            "Use the Euclidean distance metric to calculate the distance between points.",
            "Return the cluster labels for each point."
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.spatial import distance"
        ],
        "function_def": "def hierarchical_clustering(points, k):\n    n = len(points)\n    labels = list(range(n))\n    clusters = [[point] for point in points]\n    \n    while len(clusters) > k:\n        min_distance = float('inf')\n        min_index = -1\n        for i in range(len(clusters)):\n            for j in range(i + 1, len(clusters)):\n                distance_ij = distance.euclidean(clusters[i][0], clusters[j][0])\n                if distance_ij < min_distance:\n                    min_distance = distance_ij\n                    min_index = (i, j)\n        \n        i, j = min_index\n        clusters[i].extend(clusters[j])\n        labels = [i if label == j else label for label in labels]\n        clusters.pop(j)\n    \n    return labels"
    },
    {
        "function_name": "svd_decomposition",
        "file_name": "svd.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix",
            "`k`": "An integer representing the rank of the matrix"
        },
        "objectives": [
            "Implement the Singular Value Decomposition (SVD) algorithm to decompose the matrix.",
            "Use the power iteration method to find the top k singular values and singular vectors.",
            "Return the U, \u03a3, and V matrices of the SVD decomposition."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def svd_decomposition(matrix, k):\n    U, \u03a3, V = np.linalg.svd(matrix)\n    U = U[:, :k]\n    \u03a3 = \u03a3[:k]\n    V = V[:k, :]\n    \n    return U, \u03a3, V"
    },
    {
        "function_name": "neural_network_classification",
        "file_name": "neural_networks.py",
        "parameters": {
            "network": "A dictionary representing a neural network",
            "input_data": "A list of integers representing the input data",
            "threshold": "A float representing the threshold for the neural network"
        },
        "objectives": [
            "Perform a forward pass through the neural network with the specified input data.",
            "Use the neural network to classify the input data.",
            "Return the output of the neural network and the classification result."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def neural_network_classification(network, input_data, threshold):\n    output = input_data\n    for layer in network:\n        output = np.dot(output, layer['weights']) + layer['bias']\n        output = np.where(output > threshold, 1, 0)\n    \n    classification_result = np.argmax(output)\n    return output, classification_result"
    },
    {
        "function_name": "create_ngram_index",
        "file_name": "text_indexing.py",
        "parameters": {
            "text": "A string to be indexed",
            "n": "Integer representing the size of the n-grams to use"
        },
        "objectives": [
            "Create an n-gram index of the text.",
            "Split the text into n-grams and store their positions.",
            "Return a dictionary where the keys are the n-grams and the values are lists of their positions."
        ],
        "import_lines": [],
        "function_def": "def create_ngram_index(text, n):\n    ngrams = {}\n    for i in range(len(text) - n + 1):\n        ngram = text[i:i+n]\n        if ngram in ngrams:\n            ngrams[ngram].append(i)\n        else:\n            ngrams[ngram] = [i]\n    return ngrams"
    },
    {
        "function_name": "gaussian_mixture_model",
        "file_name": "clustering_algorithms.py",
        "parameters": {
            "data": "2D list of integers representing the data points",
            "k": "Integer representing the number of means to use"
        },
        "objectives": [
            "Use the Gaussian Mixture Model (GMM) to cluster the data points.",
            "Initialize k random means and covariances.",
            "Calculate the posterior probabilities of each data point for each mean.",
            "Update the means and covariances using the posterior probabilities.",
            "Repeat until convergence."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def gaussian_mixture_model(data, k):\n    num_data_points = len(data)\n    means = np.random.rand(k, 2)\n    covariances = [np.eye(2) for _ in range(k)]\n    posterior_probabilities = np.zeros((num_data_points, k))\n    \n    for _ in range(100):  # max iterations\n        # Calculate posterior probabilities\n        for i in range(num_data_points):\n            for j in range(k):\n                probability = np.exp(-0.5 * np.dot((data[i] - means[j]), np.dot(np.linalg.inv(covariances[j]), (data[i] - means[j]).T))) / np.sqrt(np.linalg.det(covariances[j]))\n                posterior_probabilities[i][j] = probability\n        \n        # Normalize posterior probabilities\n        posterior_probabilities /= posterior_probabilities.sum(axis=1, keepdims=True)\n        \n        # Update means and covariances\n        means = np.dot(posterior_probabilities.T, data) / posterior_probabilities.sum(axis=0, keepdims=True).T\n        covariances = [np.cov(data, rowvar=False, aweights=posterior_probabilities[:, i]) for i in range(k)]\n    \n    return means, covariances"
    },
    {
        "function_name": "pca_classification",
        "file_name": "pca_classification.py",
        "parameters": {
            "`num_features`": "int",
            "`data`": "A 2D list representing the dataset",
            "`label`": "A list representing the labels for the data"
        },
        "objectives": [
            "Perform Principal Component Analysis (PCA) to reduce the dimensionality of the data",
            "Find the top k principal components that explain most of the variance in the data",
            "Project the data onto these k principal components",
            "Use a classifier to classify the data based on the projected features"
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.decomposition import PCA",
            "from sklearn.linear_model import LogisticRegression"
        ],
        "function_def": "def pca_classification(num_features, data, label):\n    pca = PCA(n_components=num_features)\n    projected_data = pca.fit_transform(data)\n    classifier = LogisticRegression()\n    classifier.fit(projected_data, label)\n    return classifier.predict(projected_data)"
    },
    {
        "function_name": "genetic_algorithm",
        "file_name": "genetic_algorithm.py",
        "parameters": {
            "`num_generations`": "An integer representing the number of generations",
            "`population_size`": "An integer representing the population size",
            "`fitness_function`": "A function representing the fitness function"
        },
        "objectives": [
            "Use the genetic algorithm to evolve a population of individuals",
            "Calculate the fitness score for each individual",
            "Select the fittest individuals and crossover to create offspring",
            "Mutate the offspring to introduce genetic variation",
            "Return the fittest individual"
        ],
        "import_lines": [
            "import random",
            "import numpy as np"
        ],
        "function_def": "def genetic_algorithm(num_generations, population_size, fitness_function):\n    population = [np.random.rand(10) for _ in range(population_size)]\n    for _ in range(num_generations):\n        fitness_scores = [fitness_function(individual) for individual in population]\n        fittest_individuals = np.array(population)[np.argsort(fitness_scores)[-2:]]\n        offspring = []\n        for _ in range(population_size - 2):\n            parent1, parent2 = random.sample(list(fittest_individuals), 2)\n            child = (parent1 + parent2) / 2\n            child += np.random.randn(10) * 0.1\n            offspring.append(child)\n        population = list(fittest_individuals) + offspring\n    return population[np.argmax([fitness_function(individual) for individual in population])]"
    },
    {
        "function_name": "depth_limited_search",
        "file_name": "graph_search.py",
        "parameters": {
            "`graph`": "A dictionary representing an adjacency list of a graph, where each key is a node and its value is a list of neighboring nodes",
            "`start_node`": "The node to start the search from",
            "`target_node`": "The node to search for",
            "`max_depth`": "The maximum depth to search"
        },
        "objectives": [
            "Perform a depth-limited search on the graph to find the target node",
            "Keep track of the nodes visited during the search",
            "Return the shortest path to the target node if found, otherwise return None"
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def depth_limited_search(graph, start_node, target_node, max_depth):\n    visited = set()\n    queue = deque([(start_node, [start_node], 0)])\n    \n    while queue:\n        node, path, depth = queue.popleft()\n        if node == target_node:\n            return path\n        if depth >= max_depth:\n            continue\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                queue.append((neighbor, path + [neighbor], depth + 1))\n                visited.add(neighbor)\n                \n    return None"
    },
    {
        "function_name": "process_transactions",
        "file_name": "accounting.py",
        "parameters": {
            "`transactions`": "A list of transactions, where each transaction is a dictionary containing the transaction amount, date, and type (deposit or withdrawal)",
            "`account_balance`": "The initial account balance",
            "`interest_rate`": "The interest rate to apply to deposits"
        },
        "objectives": [
            "Process the transactions and update the account balance accordingly",
            "Calculate the interest earned on deposits and add it to the account balance",
            "Return the final account balance and a list of transactions with their corresponding transaction IDs"
        ],
        "import_lines": [
            "import datetime"
        ],
        "function_def": "def process_transactions(transactions, account_balance, interest_rate):\n    transaction_id = 1\n    final_balance = account_balance\n    processed_transactions = []\n    \n    for transaction in transactions:\n        if transaction['type'] == 'deposit':\n            interest = transaction['amount'] * interest_rate / 100\n            final_balance += transaction['amount'] + interest\n        elif transaction['type'] == 'withdrawal':\n            final_balance -= transaction['amount']\n        processed_transactions.append({'id': transaction_id, 'date': transaction['date'], 'amount': transaction['amount'], 'type': transaction['type']})\n        transaction_id += 1\n        \n    return final_balance, processed_transactions"
    },
    {
        "function_name": "apply_filter",
        "file_name": "image_processing.py",
        "parameters": {
            "`image`": "A 2D list representing the image, where each pixel is a tuple of RGB values",
            "`filter_size`": "The size of the filter to apply",
            "`filter_type`": "The type of filter to apply (e.g. 'blur', 'edge detection')"
        },
        "objectives": [
            "Apply the specified filter to the image",
            "Calculate the new pixel values based on the filter type",
            "Return the filtered image"
        ],
        "import_lines": [],
        "function_def": "def apply_filter(image, filter_size, filter_type):\n    filtered_image = [[(0, 0, 0) for _ in range(len(image[0]))] for _ in range(len(image))]\n    \n    for i in range(len(image)):\n        for j in range(len(image[0])):\n            if filter_type == 'blur':\n                # Calculate the average pixel value in the filter window\n                total_red = 0\n                total_green = 0\n                total_blue = 0\n                count = 0\n                for x in range(max(0, i - filter_size // 2), min(len(image), i + filter_size // 2 + 1)):\n                    for y in range(max(0, j - filter_size // 2), min(len(image[0]), j + filter_size // 2 + 1)):\n                        total_red += image[x][y][0]\n                        total_green += image[x][y][1]\n                        total_blue += image[x][y][2]\n                        count += 1\n                filtered_image[i][j] = (total_red // count, total_green // count, total_blue // count)\n            elif filter_type == 'edge detection':\n                # Calculate the gradient magnitude and direction\n                gradient_x = 0\n                gradient_y = 0\n                for x in range(max(0, i - filter_size // 2), min(len(image), i + filter_size // 2 + 1)):\n                    for y in range(max(0, j - filter_size // 2), min(len(image[0]), j + filter_size // 2 + 1)):\n                        gradient_x += (image[x][y][0] - image[i][j][0]) / (x - i)\n                        gradient_y += (image[x][y][0] - image[i][j][0]) / (y - j)\n                filtered_image[i][j] = (int(gradient_x), int(gradient_y), 0)\n                \n    return filtered_image"
    },
    {
        "function_name": "partition_array",
        "file_name": "array_partitioning.py",
        "parameters": {
            "`array`": "A 1D list of integers",
            "`target_sum`": "The target sum to achieve",
            "`num_partitions`": "The number of partitions to divide the array into"
        },
        "objectives": [
            "Partition the array into the specified number of partitions such that the sum of each partition is as close to the target sum as possible",
            "Use dynamic programming to optimize the partitioning process",
            "Return the partition boundaries and the sum of each partition"
        ],
        "import_lines": [],
        "function_def": "def partition_array(array, target_sum, num_partitions):\n    total_sum = sum(array)\n    partitions = [0] * (num_partitions + 1)\n    dp = [[float('inf')] * (total_sum + 1) for _ in range(num_partitions + 1)]\n    \n    for i in range(1, num_partitions + 1):\n        for j in range(1, total_sum + 1):\n            if j < array[i - 1]:\n                dp[i][j] = dp[i - 1][j]\n            else:\n                dp[i][j] = min(dp[i - 1][j], dp[i - 1][j - array[i - 1]] + array[i - 1])\n                \n    i, j = num_partitions, total_sum\n    while i > 0:\n        if dp[i][j] != dp[i - 1][j]:\n            partitions[i] = j\n            j -= array[i - 1]\n        i -= 1\n        \n    return partitions, [sum(array[partitions[i]:partitions[i + 1]]) for i in range(num_partitions)]"
    },
    {
        "function_name": "extract_kmers",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "`sequences`": "A list of strings representing biological sequences",
            "`k`": "The length of the k-mer to extract",
            "`threshold`": "The minimum frequency threshold for k-mers"
        },
        "objectives": [
            "Extract k-mers from the sequences and count their frequencies",
            "Filter out k-mers with frequencies below the threshold",
            "Return the top k-mers and their frequencies"
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def extract_kmers(sequences, k, threshold):\n    kmers = defaultdict(int)\n    \n    for sequence in sequences:\n        for i in range(len(sequence) - k + 1):\n            kmer = sequence[i:i + k]\n            kmers[kmer] += 1\n            \n    top_kmers = sorted(kmers.items(), key=lambda x: x[1], reverse=True)\n    top_kmers = [(kmer, freq) for kmer, freq in top_kmers if freq >= threshold]\n    \n    return top_kmers"
    },
    {
        "function_name": "first_fit_decreasing",
        "file_name": "bin_packing.py",
        "parameters": {
            "items": "A list of integers representing the weights of items",
            "capacity": "An integer representing the capacity of the bin"
        },
        "objectives": [
            "Implement the First-Fit Decreasing (FFD) algorithm to pack items into bins.",
            "Sort the items in non-increasing order before packing.",
            "Return the minimum number of bins required to pack all items."
        ],
        "import_lines": [],
        "function_def": "def first_fit_decreasing(items, capacity):\n    items.sort(reverse=True)\n    bins = []\n    \n    for item in items:\n        for bin in bins:\n            if sum(bin) + item <= capacity:\n                bin.append(item)\n                break\n        else:\n            bins.append([item])\n    \n    return len(bins)"
    },
    {
        "function_name": "sliding_window_maximum",
        "file_name": "array_processing.py",
        "parameters": {
            "arr": "A list of integers",
            "window_size": "An integer representing the size of the window"
        },
        "objectives": [
            "Implement the Sliding Window Maximum algorithm to find the maximum element in each window.",
            "Use a deque to efficiently maintain the maximum element in the current window.",
            "Return the maximum element in each window."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def sliding_window_maximum(arr, window_size):\n    if not arr or window_size > len(arr):\n        return []\n    \n    result = []\n    dq = deque()\n    \n    for i, num in enumerate(arr):\n        while dq and dq[0] < i - window_size + 1:\n            dq.popleft()\n        \n        while dq and arr[dq[-1]] < num:\n            dq.pop()\n        \n        dq.append(i)\n        \n        if i >= window_size - 1:\n            result.append(arr[dq[0]])\n    \n    return result"
    },
    {
        "function_name": "calculate_median_mean",
        "file_name": "statistical_analysis.py",
        "parameters": {
            "`num_list`": "A list of integers",
            "`window_size`": "An integer representing the size of the sliding window"
        },
        "objectives": [
            "Divide the input list into sublists of size 'window_size' using a sliding window approach.",
            "For each sublist, calculate the median and the mean.",
            "Return the medians and the means."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def calculate_median_mean(num_list, window_size):\n    medians = []\n    means = []\n    for i in range(len(num_list) - window_size + 1):\n        window = num_list[i:i + window_size]\n        median = np.median(window)\n        mean = np.mean(window)\n        medians.append(median)\n        means.append(mean)\n    return medians, means"
    },
    {
        "function_name": "binary_search_matrix",
        "file_name": "matrix_operations.py",
        "parameters": {
            "`matrix`": "A 2D list representing the input matrix",
            "`target`": "An integer representing the target value"
        },
        "objectives": [
            "Find the row and column of the target value in the input matrix using a binary search approach.",
            "If the target value is not found, return (-1, -1).",
            "Return the row and column of the target value."
        ],
        "import_lines": [],
        "function_def": "def binary_search_matrix(matrix, target):\n    rows, cols = len(matrix), len(matrix[0])\n    low, high = 0, rows * cols - 1\n    while low <= high:\n        mid = (low + high) // 2\n        mid_value = matrix[mid // cols][mid % cols]\n        if mid_value == target:\n            return mid // cols, mid % cols\n        elif mid_value < target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1, -1"
    },
    {
        "function_name": "delaunay_triangulation",
        "file_name": "triangulation.py",
        "parameters": {
            "n": "Integer",
            "points": "A list of tuples representing the points to be triangulated, where each tuple contains two integers representing the x and y coordinates of the point.",
            "edges": "A list of tuples representing the edges of the triangulation, where each tuple contains two integers representing the indices of the points that the edge connects."
        },
        "objectives": [
            "Perform a Delaunay triangulation on the given points.",
            "Check if any edge in the resulting triangulation is longer than the minimum distance between any two points.",
            "Return a list of tuples, where each tuple contains the indices of the points that form a triangle in the triangulation and a boolean indicating whether any edge in the triangle is longer than the minimum distance between any two points."
        ],
        "import_lines": [
            "import math",
            "from scipy.spatial import Delaunay"
        ],
        "function_def": "def delaunay_triangulation(n, points, edges):\n    tri = Delaunay(points)\n    min_distance = float('inf')\n    for i in range(n):\n        for j in range(i + 1, n):\n            distance = math.sqrt((points[i][0] - points[j][0]) ** 2 + (points[i][1] - points[j][1]) ** 2)\n            min_distance = min(min_distance, distance)\n    result = []\n    for triangle in tri.simplices:\n        long_edge = False\n        for i in range(3):\n            p1 = triangle[i]\n            p2 = triangle[(i + 1) % 3]\n            distance = math.sqrt((points[p1][0] - points[p2][0]) ** 2 + (points[p1][1] - points[p2][1]) ** 2)\n            if distance > min_distance:\n                long_edge = True\n                break\n        result.append((tuple(triangle), long_edge))\n    return result"
    },
    {
        "function_name": "ford_fulkerson",
        "file_name": "maximum_flow.py",
        "parameters": {
            "graph": "A dictionary representing an adjacency list of a graph",
            "source": "An integer representing the source node",
            "sink": "An integer representing the sink node",
            "k": "An integer representing the maximum number of augmenting paths"
        },
        "objectives": [
            "Implement the Ford-Fulkerson algorithm to find the maximum flow in the given graph from the source to the sink.",
            "Use Breadth-First Search (BFS) to find augmenting paths in the residual graph.",
            "Return the maximum flow value and the number of augmenting paths used."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def ford_fulkerson(graph, source, sink, k):\n    parent = {}\n    max_flow = 0\n    for _ in range(k):\n        residual_graph = {u: v.copy() for u, v in graph.items()}\n        queue = deque([source])\n        parent.clear()\n        while queue:\n            u = queue.popleft()\n            for v, capacity in residual_graph[u].items():\n                if capacity > 0 and v not in parent:\n                    parent[v] = u\n                    queue.append(v)\n            if sink in parent:\n                break\n        else:\n            break\n        path_flow = float('inf')\n        s = sink\n        while s != source:\n            path_flow = min(path_flow, residual_graph[parent[s]][s])\n            s = parent[s]\n        max_flow += path_flow\n        v = sink\n        while v != source:\n            u = parent[v]\n            residual_graph[u][v] -= path_flow\n            residual_graph[v][u] = residual_graph.get(v, {}).get(u, 0) + path_flow\n            v = parent[v]\n    return max_flow, _ + 1"
    },
    {
        "function_name": "matrix_factorization",
        "file_name": "matrix_factorization.py",
        "parameters": {
            "sparse_matrix": "A dictionary representing a sparse matrix",
            "k": "An integer representing the number of latent factors"
        },
        "objectives": [
            "Implement the matrix factorization technique to factorize the sparse matrix into two lower-dimensional matrices.",
            "Use the Alternating Least Squares (ALS) algorithm to optimize the factorization.",
            "Return the two factorized matrices."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def matrix_factorization(sparse_matrix, k):\n    num_users = max(key[0] for key in sparse_matrix.keys()) + 1\n    num_items = max(key[1] for key in sparse_matrix.keys()) + 1\n    user_factors = np.random.rand(num_users, k)\n    item_factors = np.random.rand(num_items, k)\n    \n    for _ in range(10):\n        for user in range(num_users):\n            item_indices = [key[1] for key in sparse_matrix if key[0] == user]\n            item_factors_item_indices = item_factors[item_indices]\n            user_factors[user] = np.linalg.lstsq(item_factors_item_indices, [sparse_matrix[(user, item)] for item in item_indices], rcond=None)[0]\n        for item in range(num_items):\n            user_indices = [key[0] for key in sparse_matrix if key[1] == item]\n            user_factors_user_indices = user_factors[user_indices]\n            item_factors[item] = np.linalg.lstsq(user_factors_user_indices, [sparse_matrix[(user, item)] for user in user_indices], rcond=None)[0]\n    \n    return user_factors, item_factors"
    },
    {
        "function_name": "earliest_pattern_match",
        "file_name": "string_processor.py",
        "parameters": {
            "`strings`": "A list of strings",
            "`pattern`": "A string representing the pattern to search for"
        },
        "objectives": [
            "Find the longest common prefix among all the strings in the list.",
            "Search for the pattern in each string and find the string with the earliest occurrence of the pattern.",
            "If the pattern is not found in any string, return the longest common prefix.",
            "Otherwise, return the string with the earliest occurrence of the pattern."
        ],
        "import_lines": [],
        "function_def": "def earliest_pattern_match(strings, pattern):\n    common_prefix = ''\n    for chars in zip(*strings):\n        if len(set(chars)) == 1:\n            common_prefix += chars[0]\n        else:\n            break\n    earliest_match = None\n    earliest_index = float('inf')\n    for string in strings:\n        index = string.find(pattern)\n        if index != -1 and index < earliest_index:\n            earliest_index = index\n            earliest_match = string\n    return earliest_match if earliest_match else common_prefix"
    },
    {
        "function_name": "max_sum_submatrix",
        "file_name": "matrix_operations.py",
        "parameters": {
            "`matrix`": "A 2D list representing the input matrix",
            "`rows`": "An integer representing the number of rows to select",
            "`cols`": "An integer representing the number of columns to select"
        },
        "objectives": [
            "Select the rows and columns of the matrix that have the maximum sum.",
            "If there are multiple rows or columns with the same maximum sum, select the ones with the minimum index.",
            "Return a 2D list representing the sub-matrix with the selected rows and columns."
        ],
        "import_lines": [],
        "function_def": "def max_sum_submatrix(matrix, rows, cols):\n    row_sums = [sum(row) for row in matrix]\n    col_sums = [sum(col) for col in zip(*matrix)]\n    selected_rows = sorted(range(len(row_sums)), key=lambda i: (-row_sums[i], i))[:rows]\n    selected_cols = sorted(range(len(col_sums)), key=lambda i: (-col_sums[i], i))[:cols]\n    return [[matrix[i][j] for j in selected_cols] for i in selected_rows]"
    },
    {
        "function_name": "k_nearest_neighbors",
        "file_name": "nearest_neighbors.py",
        "parameters": {
            "`points`": "A list of tuples representing the points in 2D space",
            "`k`": "An integer representing the number of nearest neighbors"
        },
        "objectives": [
            "Implement the k-Nearest Neighbors (k-NN) algorithm to find the k nearest neighbors to each point.",
            "Use the Euclidean distance metric to calculate the distance between points.",
            "Return the k nearest neighbors for each point."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def k_nearest_neighbors(points, k):\n    neighbors = []\n    \n    for i, point in enumerate(points):\n        distances = []\n        for j, other_point in enumerate(points):\n            if i != j:\n                distance = math.sqrt((point[0] - other_point[0])**2 + (point[1] - other_point[1])**2)\n                distances.append((distance, j))\n        distances.sort()\n        neighbors.append([points[j] for _, j in distances[:k]])\n    \n    return neighbors"
    },
    {
        "function_name": "rle_compression",
        "file_name": "compression_algorithms.py",
        "parameters": {
            "`text`": "A string representing the text to compress"
        },
        "objectives": [
            "Implement the Run-Length Encoding (RLE) algorithm to compress the text.",
            "Use a dictionary to keep track of the frequency of each character.",
            "Return the compressed text."
        ],
        "import_lines": [],
        "function_def": "def rle_compression(text):\n    compressed = ''\n    count = 1\n    \n    for i in range(1, len(text)):\n        if text[i] == text[i - 1]:\n            count += 1\n        else:\n            compressed += text[i - 1] + str(count)\n            count = 1\n    \n    compressed += text[-1] + str(count)\n    \n    return compressed"
    },
    {
        "function_name": "kadane_algorithm",
        "file_name": "dynamic_programming_algorithms.py",
        "parameters": {
            "`n`": "An integer representing the number of elements in the array.",
            "`A`": "A list of integers representing the array to be calculated."
        },
        "objectives": [
            "Use the kadane's algorithm to find the maximum sum of a subarray within the given array.",
            "Return the maximum sum and the starting and ending indices of the subarray.",
            "Implement a dynamic programming approach to improve efficiency."
        ],
        "import_lines": [],
        "function_def": "def kadane_algorithm(n, A):\n    max_sum = A[0]\n    current_sum = A[0]\n    start = 0\n    end = 0\n    temp_start = 0\n    for i in range(1, n):\n        if current_sum < 0:\n            current_sum = A[i]\n            temp_start = i\n        else:\n            current_sum += A[i]\n        if current_sum > max_sum:\n            max_sum = current_sum\n            start = temp_start\n            end = i\n    return max_sum, start, end"
    },
    {
        "function_name": "calculate_number_of_ways",
        "file_name": "subset_sums.py",
        "parameters": {
            "n": "Integer representing the number of elements in the array",
            "arr": "List of integers representing the array",
            "target": "Integer representing the target sum",
            "k": "Integer representing the number of elements to select"
        },
        "objectives": [
            "Calculate the number of ways to select k elements from the array that sum up to the target.",
            "Use dynamic programming to calculate the number of ways.",
            "Return the number of ways to select k elements that sum up to the target."
        ],
        "import_lines": [],
        "function_def": "def calculate_number_of_ways(n, arr, target, k):\n    # Create a 2D array to store the number of ways for each subproblem\n    dp = [[0] * (target + 1) for _ in range(k + 1)]\n    dp[0][0] = 1\n    \n    for i in range(1, k + 1):\n        for j in range(1, target + 1):\n            for num in arr:\n                if j >= num:\n                    dp[i][j] += dp[i - 1][j - num]\n    \n    return dp[k][target]"
    },
    {
        "function_name": "calculate_word_importance",
        "file_name": "text_analysis.py",
        "parameters": {
            "text": "String representing the input text",
            "k": "Integer representing the maximum number of keywords to consider"
        },
        "objectives": [
            "Calculate the importance score for each word in the text.",
            "Use the Term Frequency-Inverse Document Frequency (TF-IDF) algorithm to calculate the importance score.",
            "Return a dictionary where each key is a word and the value is its corresponding importance score."
        ],
        "import_lines": [
            "from collections import Counter",
            "from math import log"
        ],
        "function_def": "def calculate_word_importance(text, k):\n    # Tokenize the text into words\n    words = text.split()\n    \n    # Calculate the term frequency for each word\n    tf = Counter(words)\n    \n    # Calculate the document frequency for each word\n    df = {}\n    for word in tf:\n        df[word] = len([t for t in text.split() if t == word])\n    \n    # Calculate the importance score for each word using TF-IDF\n    importance_scores = {}\n    for word in tf:\n        idf = log(len(text.split()) / df[word])\n        importance_scores[word] = tf[word] * idf\n    \n    # Return the top k words with the highest importance scores\n    return dict(sorted(importance_scores.items(), key=lambda item: item[1], reverse=True)[:k])"
    },
    {
        "function_name": "longest_matches",
        "file_name": "regular_expressions.py",
        "parameters": {
            "`strings`": "A list of strings.",
            "`pattern`": "A regular expression pattern."
        },
        "objectives": [
            "Use the regular expression pattern to search for matches in each string.",
            "For each string, find the longest match and its starting and ending indices.",
            "Return a list of tuples, where each tuple contains the string, the longest match, and its starting and ending indices."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def longest_matches(strings, pattern):\n    results = []\n    regex = re.compile(pattern)\n    \n    for string in strings:\n        matches = list(regex.finditer(string))\n        longest_match = max(matches, key=lambda match: match.end() - match.start()) if matches else None\n        \n        if longest_match:\n            results.append((string, longest_match.group(), longest_match.start(), longest_match.end()))\n    \n    return results"
    },
    {
        "function_name": "largest_less_than",
        "file_name": "binary_search.py",
        "parameters": {
            "`matrix`": "A 2D list of integers.",
            "`threshold`": "An integer representing the threshold value."
        },
        "objectives": [
            "Use a binary search approach to find the largest number in the matrix that is less than or equal to the threshold value.",
            "Return the largest number if it exists; otherwise, return -1."
        ],
        "import_lines": [],
        "function_def": "def largest_less_than(matrix, threshold):\n    rows, cols = len(matrix), len(matrix[0])\n    low, high = matrix[0][0], matrix[rows-1][cols-1]\n    \n    while low <= high:\n        mid = (low + high) // 2\n        \n        if any(any(num <= mid for num in row) for row in matrix):\n            low = mid + 1\n        else:\n            high = mid - 1\n    \n    return low - 1 if low != matrix[0][0] else -1"
    },
    {
        "function_name": "find_close_points",
        "file_name": "point_proximity.py",
        "parameters": {
            "`points`": "A list of 2D points (x, y)",
            "`radius`": "An integer representing the radius of the circle"
        },
        "objectives": [
            "Find all pairs of points that are within the given radius of each other.",
            "Use a sweep line approach to sort the points and improve efficiency.",
            "Return a list of tuples, where each tuple contains a pair of points that are within the given radius of each other."
        ],
        "import_lines": [],
        "function_def": "def find_close_points(points, radius):\n    points.sort(key=lambda x: x[0])\n    close_points = []\n    for i in range(len(points)):\n        for j in range(i+1, len(points)):\n            if points[j][0] - points[i][0] > radius:\n                break\n            distance = ((points[j][0] - points[i][0])**2 + (points[j][1] - points[i][1])**2)**0.5\n            if distance <= radius:\n                close_points.append((points[i], points[j]))\n    return close_points"
    },
    {
        "function_name": "boolean_search",
        "file_name": "search_algorithms.py",
        "parameters": {
            "`text`": "A string representing the input text",
            "`query_terms`": "A list of strings representing the query terms",
            "`exclusion_terms`": "A list of strings representing the exclusion terms"
        },
        "objectives": [
            "Perform a boolean search on the input text to find all occurrences of the query terms.",
            "Exclude any occurrences that also contain the exclusion terms.",
            "Return a list of tuples, where each tuple contains the start and end indices of an occurrence that matches the query terms and does not contain exclusion terms."
        ],
        "import_lines": [],
        "function_def": "def boolean_search(text, query_terms, exclusion_terms):\n    occurrences = []\n    for i in range(len(text)):\n        match = True\n        for term in query_terms:\n            if term not in text[i:i+10]:\n                match = False\n                break\n        if not match:\n            continue\n        for term in exclusion_terms:\n            if term in text[i:i+10]:\n                match = False\n                break\n        if match:\n            occurrences.append((i, i+10))\n    return occurrences"
    },
    {
        "function_name": "anomaly_detection",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "`time_series`": "A list of numbers representing a time series",
            "`window_size`": "An integer representing the size of the window",
            "`threshold`": "A number representing the threshold for anomalies"
        },
        "objectives": [
            "Use a moving average filter to smooth the time series",
            "Calculate the mean and standard deviation of the residuals",
            "Identify anomalies as points that are more than threshold standard deviations away from the mean",
            "Return the smoothed time series, the mean and standard deviation of the residuals, and the anomalies"
        ],
        "import_lines": [],
        "function_def": "def anomaly_detection(time_series, window_size, threshold):\n    smoothed_time_series = []\n    residuals = []\n    for i in range(len(time_series)):\n        start = max(0, i - window_size + 1)\n        smoothed_time_series.append(sum(time_series[start:i+1]) / (i - start + 1))\n        residuals.append(time_series[i] - smoothed_time_series[-1])\n    mean_residual = sum(residuals) / len(residuals)\n    std_residual = (sum((r - mean_residual) ** 2 for r in residuals) / len(residuals)) ** 0.5\n    anomalies = [i for i, residual in enumerate(residuals) if abs(residual - mean_residual) > threshold * std_residual]\n    return smoothed_time_series, mean_residual, std_residual, anomalies"
    },
    {
        "function_name": "principal_component_analysis",
        "file_name": "dimensionality_reduction.py",
        "parameters": {
            "`matrix`": "A 2D list representing the input matrix",
            "`k`": "An integer representing the number of principal components"
        },
        "objectives": [
            "Use the Principal Component Analysis (PCA) algorithm to reduce the dimensionality of the input matrix.",
            "Calculate the covariance matrix of the input matrix.",
            "Compute the eigenvalues and eigenvectors of the covariance matrix.",
            "Select the top k eigenvectors corresponding to the largest eigenvalues.",
            "Project the input matrix onto the selected eigenvectors."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def principal_component_analysis(matrix, k):\n    # Calculate the covariance matrix of the input matrix\n    covariance_matrix = np.cov(matrix.T)\n    \n    # Compute the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n    \n    # Select the top k eigenvectors corresponding to the largest eigenvalues\n    idx = np.argsort(eigenvalues)[::-1]\n    selected_eigenvectors = eigenvectors[:, idx[:k]]\n    \n    # Project the input matrix onto the selected eigenvectors\n    projected_matrix = np.dot(matrix, selected_eigenvectors)\n    \n    return projected_matrix"
    },
    {
        "function_name": "ngram_model",
        "file_name": "language_model.py",
        "parameters": {
            "`text`": "A string representing the input text",
            "`n`": "An integer representing the number of n-grams"
        },
        "objectives": [
            "Use the n-gram model to generate a language model for the input text.",
            "Calculate the frequency of each n-gram in the text.",
            "Calculate the probability of each n-gram using the frequency and the total number of n-grams.",
            "Return a dictionary where each key is an n-gram and the value is its corresponding probability."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def ngram_model(text, n):\n    # Calculate the frequency of each n-gram in the text\n    frequency = defaultdict(int)\n    ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n    for ngram in ngrams:\n        frequency[ngram] += 1\n    \n    # Calculate the probability of each n-gram\n    total_ngrams = len(ngrams)\n    probability = {}\n    for ngram, freq in frequency.items():\n        probability[ngram] = freq / total_ngrams\n    \n    return probability"
    },
    {
        "function_name": "remedial_program_assignment",
        "file_name": "grade_analysis.py",
        "parameters": {
            "`grades`": "list of lists containing student grades in different subjects",
            "`threshold`": "float representing the minimum required grade"
        },
        "objectives": [
            "Normalize the grades for each subject by subtracting the mean and dividing by the standard deviation.",
            "Calculate the overall grade for each student by taking the weighted average of the normalized grades.",
            "Identify students who have an overall grade below the threshold and assign them to a remedial program."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def remedial_program_assignment(grades, threshold):\n    num_students = len(grades)\n    num_subjects = len(grades[0])\n    normalized_grades = np.zeros((num_students, num_subjects))\n    for subject in range(num_subjects):\n        subject_grades = [grades[student][subject] for student in range(num_students)]\n        mean = np.mean(subject_grades)\n        std_dev = np.std(subject_grades)\n        normalized_grades[:, subject] = [(grade - mean) / std_dev for grade in subject_grades]\n    overall_grades = np.average(normalized_grades, axis=1, weights=[1/num_subjects]*num_subjects)\n    remedial_students = [student for student in range(num_students) if overall_grades[student] < threshold]\n    return remedial_students"
    },
    {
        "function_name": "infection_simulation",
        "file_name": "network_simulation.py",
        "parameters": {
            "`network`": "adjacency matrix representing the network",
            "`seed_nodes`": "list of nodes to start the infection from",
            "`infection_rate`": "float representing the rate at which the infection spreads"
        },
        "objectives": [
            "Simulate the spread of the infection through the network using a probabilistic model.",
            "Identify the nodes that become infected and calculate the time it takes for the infection to reach each node.",
            "Calculate the total number of infected nodes and the average time for the infection to spread through the network."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def infection_simulation(network, seed_nodes, infection_rate):\n    num_nodes = len(network)\n    infected_nodes = set(seed_nodes)\n    infection_times = {node: 0 for node in seed_nodes}\n    time = 0\n    while True:\n        new_infected_nodes = set()\n        for infected_node in infected_nodes:\n            for neighbor in range(num_nodes):\n                if network[infected_node, neighbor] == 1 and np.random.rand() < infection_rate:\n                    new_infected_nodes.add(neighbor)\n        if not new_infected_nodes:\n            break\n        infected_nodes.update(new_infected_nodes)\n        time += 1\n        for node in new_infected_nodes:\n            infection_times[node] = time\n    total_infected = len(infected_nodes)\n    average_time = sum(infection_times.values()) / total_infected\n    return total_infected, average_time"
    },
    {
        "function_name": "k_means_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "`points`": "list of tuples representing the points in 2D space",
            "`k`": "integer representing the number of clusters"
        },
        "objectives": [
            "Initialize the centroids of the clusters randomly.",
            "Assign each point to the closest cluster based on the Euclidean distance.",
            "Update the centroids of the clusters based on the assigned points."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def k_means_clustering(points, k):\n    num_points = len(points)\n    centroids = np.random.rand(k, 2)\n    cluster_assignments = np.zeros(num_points, dtype=int)\n    while True:\n        for i, point in enumerate(points):\n            distances = np.linalg.norm(centroids - point, axis=1)\n            cluster_assignments[i] = np.argmin(distances)\n        new_centroids = np.zeros((k, 2))\n        for i in range(k):\n            points_in_cluster = [point for j, point in enumerate(points) if cluster_assignments[j] == i]\n            if points_in_cluster:\n                new_centroids[i] = np.mean(points_in_cluster, axis=0)\n        if np.array_equal(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return centroids, cluster_assignments"
    },
    {
        "function_name": "expected_reachable_nodes",
        "file_name": "graph_probability.py",
        "parameters": {
            "`graphs`": "A list of adjacency lists representing different graphs.",
            "`p`": "A float representing the probability of a node being selected."
        },
        "objectives": [
            "Use the concept of probabilistic graph theory to find the expected number of nodes that can be reached from a randomly selected node in each graph.",
            "Calculate the probability of each node being selected.",
            "Return a dictionary where the keys are the input graphs and the values are the expected number of nodes that can be reached."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def expected_reachable_nodes(graphs, p):\n    result = {}\n    for graph in graphs:\n        num_nodes = len(graph)\n        reachable_nodes = 0\n        for _ in range(num_nodes):\n            selected_node = random.randint(0, num_nodes - 1)\n            visited = set()\n            stack = [selected_node]\n            while stack:\n                node = stack.pop()\n                if node not in visited:\n                    visited.add(node)\n                    for neighbor in graph[node]:\n                        if neighbor not in visited and random.random() < p:\n                            stack.append(neighbor)\n            reachable_nodes += len(visited)\n        result[tuple(tuple(row) for row in graph)] = reachable_nodes / num_nodes\n    return result"
    },
    {
        "function_name": "matrix_multiplication",
        "file_name": "matrix_multiplication.py",
        "parameters": {
            "`matrices`": "A list of 2D lists representing different matrices.",
            "`threshold`": "An integer representing the threshold value."
        },
        "objectives": [
            "Use the concept of matrix multiplication to find the number of matrices that can be multiplied together without exceeding the threshold value.",
            "Calculate the total number of matrix multiplications.",
            "Return the total number of matrices that can be multiplied together without exceeding the threshold value."
        ],
        "import_lines": [],
        "function_def": "def matrix_multiplication(matrices, threshold):\n    num_matrices = 0\n    total_multiplications = 0\n    for matrix in matrices:\n        rows, cols = len(matrix), len(matrix[0])\n        if total_multiplications + rows * cols > threshold:\n            break\n        num_matrices += 1\n        total_multiplications += rows * cols\n    return num_matrices"
    },
    {
        "function_name": "same_subarray",
        "file_name": "array_hashing.py",
        "parameters": {
            "`arrays`": "A list of 1D lists representing different arrays.",
            "`k`": "An integer representing the size of the subarray."
        },
        "objectives": [
            "Use the concept of hash sets to find the number of arrays that have a subarray of size k with all elements being the same.",
            "Calculate the total number of such subarrays.",
            "Return the total number of arrays that have a subarray of size k with all elements being the same."
        ],
        "import_lines": [],
        "function_def": "def same_subarray(arrays, k):\n    result = 0\n    for array in arrays:\n        if len(array) < k:\n            continue\n        for i in range(len(array) - k + 1):\n            subarray = array[i:i + k]\n            if len(set(subarray)) == 1:\n                result += 1\n    return result"
    },
    {
        "function_name": "pattern_matches",
        "file_name": "string_search.py",
        "parameters": {
            "`strings`": "A list of strings",
            "`pattern`": "A regular expression pattern",
            "`max_matches`": "An integer"
        },
        "objectives": [
            "Use the `re` module to find all occurrences of `pattern` in each string in `strings`.",
            "For each string, return the number of occurrences of `pattern`.",
            "Return the top `max_matches` strings with the most occurrences of `pattern`."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def pattern_matches(strings, pattern, max_matches):\n    matches = []\n    for s in strings:\n        count = len(re.findall(pattern, s))\n        matches.append((s, count))\n    matches.sort(key=lambda x: x[1], reverse=True)\n    return matches[:max_matches]"
    },
    {
        "function_name": "svd_analysis",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`matrix`": "A 2D list of integers",
            "`row_index`": "An integer",
            "`col_index`": "An integer"
        },
        "objectives": [
            "Use the `numpy` library to calculate the singular value decomposition (SVD) of the submatrix of `matrix` starting at `row_index` and `col_index`.",
            "Return the top 3 singular values and the corresponding singular vectors."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def svd_analysis(matrix, row_index, col_index):\n    submatrix = np.array(matrix[row_index:, col_index:])\n    u, s, vh = np.linalg.svd(submatrix)\n    return s[:3], u[:, :3], vh[:3, :]"
    },
    {
        "function_name": "maximum_parsimony",
        "file_name": "phylogenetics.py",
        "parameters": {
            "`tree`": "A dictionary representing a phylogenetic tree",
            "`taxon`": "A string representing a taxon to find the closest relatives of"
        },
        "objectives": [
            "Implement the maximum parsimony algorithm to find the closest relatives of a taxon in a phylogenetic tree",
            "Initialize the trait assignments by randomly assigning traits to each taxon",
            "Update the trait assignments by iteratively reassigning the trait of each taxon based on the traits of its neighbors"
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def maximum_parsimony(tree, taxon):\n    # Initialize the trait assignments\n    traits = {taxon: random.choice(['trait1', 'trait2', 'trait3']) for taxon in tree}\n    while True:\n        for taxon in tree:\n            # Get the neighbors of the current taxon\n            neighbors = tree[taxon]\n            \n            # Count the traits of the neighbors\n            trait_counts = {'trait1': 0, 'trait2': 0, 'trait3': 0}\n            for neighbor in neighbors:\n                if neighbor in traits:\n                    trait_counts[traits[neighbor]] += 1\n            \n            # Reassign the trait of the current taxon\n            max_trait = max(trait_counts, key=trait_counts.get)\n            traits[taxon] = max_trait\n        \n        # Check convergence\n        if len(set(traits.values())) == 1:\n            break\n    \n    # Find the closest relatives of the query taxon\n    closest_relatives = []\n    for taxon in tree:\n        if taxon != taxon:\n            if traits[taxon] == traits[taxon]:\n                closest_relatives.append(taxon)\n    \n    return closest_relatives"
    },
    {
        "function_name": "pattern_match",
        "file_name": "string_matching.py",
        "parameters": {
            "strings": "List of strings",
            "pattern": "String"
        },
        "objectives": [
            "Find all substrings in the given list of strings that match the given pattern.",
            "For each matching substring, calculate its length and store it in a dictionary where the keys are the strings and the values are lists of lengths.",
            "Sort the dictionary by the sum of the lengths of the matching substrings for each string in descending order.",
            "Return the sorted dictionary."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def pattern_match(strings, pattern):\n    result = {}\n    for string in strings:\n        matches = re.findall(pattern, string)\n        lengths = [len(match) for match in matches]\n        result[string] = lengths\n    sorted_result = dict(sorted(result.items(), key=lambda item: sum(item[1]), reverse=True))\n    return sorted_result"
    },
    {
        "function_name": "matrix_rotation",
        "file_name": "matrix_operations.py",
        "parameters": {
            "matrix": "2D list of integers",
            "k": "Integer"
        },
        "objectives": [
            "Rotate the given matrix clockwise by 90 degrees.",
            "Calculate the sum of all elements in the rotated matrix that are greater than or equal to k.",
            "Find the maximum sum of a subarray of size k within each row of the rotated matrix.",
            "Return the maximum sum of a subarray and the sum of all elements greater than or equal to k."
        ],
        "import_lines": [],
        "function_def": "def matrix_rotation(matrix, k):\n    n = len(matrix)\n    rotated_matrix = [[matrix[n-j-1][i] for j in range(n)] for i in range(n)]\n    sum_greater_than_k = sum(num for row in rotated_matrix for num in row if num >= k)\n    max_subarray_sum = float('-inf')\n    for row in rotated_matrix:\n        window_sum = sum(row[:k])\n        max_subarray_sum = max(max_subarray_sum, window_sum)\n        for i in range(k, len(row)):\n            window_sum = window_sum - row[i - k] + row[i]\n            max_subarray_sum = max(max_subarray_sum, window_sum)\n    return max_subarray_sum, sum_greater_than_k"
    },
    {
        "function_name": "pair_product",
        "file_name": "pair_finder.py",
        "parameters": {
            "array": "List of integers",
            "target": "Integer"
        },
        "objectives": [
            "Find the first pair of elements in the array that sum up to the target.",
            "Find the last pair of elements in the array that sum up to the target.",
            "Calculate the product of all elements between the first and last pair (inclusive).",
            "Return the product and the indices of the first and last pair."
        ],
        "import_lines": [],
        "function_def": "def pair_product(array, target):\n    n = len(array)\n    first_pair = None\n    last_pair = None\n    for i in range(n):\n        for j in range(i + 1, n):\n            if array[i] + array[j] == target:\n                if first_pair is None or i < first_pair[0]:\n                    first_pair = (i, j)\n                if last_pair is None or j > last_pair[1]:\n                    last_pair = (i, j)\n    if first_pair is None or last_pair is None:\n        return None\n    product = 1\n    for i in range(first_pair[0], last_pair[1] + 1):\n        product *= array[i]\n    return product, first_pair, last_pair"
    },
    {
        "function_name": "best_subsequence",
        "file_name": "sequence_processor.py",
        "parameters": {
            "`sequences`": "A list of lists of integers, representing sequences of numbers",
            "`target_sum`": "An integer representing the target sum",
            "`threshold`": "An integer representing the minimum length of the subsequence"
        },
        "objectives": [
            "Divide each sequence into all possible subsequences of length at least `threshold`.",
            "For each subsequence, calculate the sum of its elements.",
            "Find the subsequence with the sum closest to `target_sum` without exceeding it.",
            "Return the sum of the elements in the best subsequence and the subsequence itself."
        ],
        "import_lines": [
            "import itertools"
        ],
        "function_def": "def best_subsequence(sequences, target_sum, threshold):\n    best_sum = float('-inf')\n    best_subsequence = None\n    for sequence in sequences:\n        for r in range(threshold, len(sequence) + 1):\n            for subsequence in itertools.combinations(sequence, r):\n                subsequence_sum = sum(subsequence)\n                if subsequence_sum <= target_sum and subsequence_sum > best_sum:\n                    best_sum = subsequence_sum\n                    best_subsequence = subsequence\n    return best_sum, best_subsequence"
    },
    {
        "function_name": "max_determinant",
        "file_name": "matrix_calculator.py",
        "parameters": {
            "`matrices`": "A list of 2D lists of integers, representing a list of matrices",
            "`threshold`": "An integer representing the minimum determinant value"
        },
        "objectives": [
            "Calculate the determinant of each matrix.",
            "Find the matrix with the determinant greater than or equal to `threshold` and the maximum determinant.",
            "Return the matrix with the maximum determinant and its determinant."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def max_determinant(matrices, threshold):\n    best_matrix = None\n    best_determinant = float('-inf')\n    for matrix in matrices:\n        determinant = np.linalg.det(matrix)\n        if determinant >= threshold and determinant > best_determinant:\n            best_determinant = determinant\n            best_matrix = matrix\n    return best_matrix, best_determinant"
    },
    {
        "function_name": "schedule_tasks",
        "file_name": "task_scheduling.py",
        "parameters": {
            "`num_tasks`": "Integer",
            "`tasks`": "List of tasks, where each task is a tuple (start_time, end_time, priority)",
            "`num_resources`": "Integer",
            "`resources`": "List of resources, where each resource is a tuple (availability, capacity)"
        },
        "objectives": [
            "Schedule the tasks on the resources using a priority scheduling algorithm.",
            "Return a list of tuples, where each tuple contains the task ID and the resource ID it is assigned to.",
            "Identify the resources with the highest and lowest availability."
        ],
        "import_lines": [],
        "function_def": "def schedule_tasks(num_tasks, tasks, num_resources, resources):\n    tasks.sort(key=lambda x: x[2], reverse=True)\n    resources.sort(key=lambda x: x[0], reverse=True)\n    assignments = []\n    resource_availability = {i: resources[i][0] for i in range(num_resources)}\n    \n    for i, task in enumerate(tasks):\n        for j, resource in enumerate(resources):\n            if resource_availability[j] >= task[1] - task[0]:\n                assignments.append((i, j))\n                resource_availability[j] -= task[1] - task[0]\n                break\n    \n    highest_availability_resource = max(resource_availability, key=resource_availability.get)\n    lowest_availability_resource = min(resource_availability, key=resource_availability.get)\n    \n    return assignments, highest_availability_resource, lowest_availability_resource"
    },
    {
        "function_name": "_astar_search",
        "file_name": "pathfinding.py",
        "parameters": {
            "`grid`": "A 2D list of integers representing a matrix",
            "`start`": "A tuple (i, j) representing the starting cell",
            "`end`": "A tuple (i, j) representing the ending cell"
        },
        "objectives": [
            "Perform an A\\* search on the grid to find a path from the start cell to the end cell.",
            "Calculate the Manhattan distance between each cell and the end cell.",
            "Use the Manhattan distance as the heuristic function to guide the search.",
            "Return the path and its length."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def _astar_search(grid, start, end):\n    rows, cols = len(grid), len(grid[0])\n    open_list = []\n    heapq.heappush(open_list, (0, start))\n    came_from = {}\n    g_score = {cell: float('inf') for cell in [(i, j) for i in range(rows) for j in range(cols)]}\n    g_score[start] = 0\n    f_score = {cell: float('inf') for cell in [(i, j) for i in range(rows) for j in range(cols)]}\n    f_score[start] = abs(end[0] - start[0]) + abs(end[1] - start[1])\n    \n    while open_list:\n        _, current = heapq.heappop(open_list)\n        if current == end:\n            path = []\n            while current in came_from:\n                path.append(current)\n                current = came_from[current]\n            path.append(start)\n            path = path[::-1]\n            return path, len(path)\n        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n            neighbor = (current[0] + dx, current[1] + dy)\n            if 0 <= neighbor[0] < rows and 0 <= neighbor[1] < cols:\n                tentative_g_score = g_score[current] + 1\n                if tentative_g_score < g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f_score[neighbor] = tentative_g_score + abs(end[0] - neighbor[0]) + abs(end[1] - neighbor[1])\n                    heapq.heappush(open_list, (f_score[neighbor], neighbor))\n    return None"
    },
    {
        "function_name": "pca_analysis",
        "file_name": "matrix_operations.py",
        "parameters": {
            "matrix": "A 2D list representing the input matrix",
            "k": "An integer representing the number of principal components to find"
        },
        "objectives": [
            "Perform a principal component analysis (PCA) on the input matrix.",
            "Use a singular value decomposition (SVD) approach to find the principal components.",
            "Return the k principal components and the explained variance ratio."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def pca_analysis(matrix, k):\n    matrix = np.array(matrix)\n    U, S, Vt = np.linalg.svd(matrix)\n    principal_components = Vt[:k]\n    explained_variance_ratio = (S[:k] ** 2) / (S ** 2).sum()\n    \n    return principal_components, explained_variance_ratio"
    },
    {
        "function_name": "voronoi_diagram",
        "file_name": "geometric_algorithms.py",
        "parameters": {
            "`n`": "Integer",
            "`points`": "A list of tuples representing the points to be triangulated, where each tuple contains two integers representing the x and y coordinates of the point."
        },
        "objectives": [
            "Perform a Voronoi diagram construction on the given points.",
            "The function should return a list of tuples, where each tuple contains the coordinates of the points that form a Voronoi cell.",
            "The function should also return a list of tuples, where each tuple contains the coordinates of the points that form a Voronoi edge."
        ],
        "import_lines": [
            "from scipy.spatial import Voronoi"
        ],
        "function_def": "def voronoi_diagram(n, points):\n    vor = Voronoi(points)\n    cells = []\n    edges = []\n    for region in vor.regions:\n        cell = []\n        for index in region:\n            if index >= 0:\n                cell.append(tuple(vor.vertices[index]))\n        if len(cell) > 0:\n            cells.append(tuple(cell))\n    for ridge in vor.ridge_vertices:\n        edge = []\n        for index in ridge:\n            if index >= 0:\n                edge.append(tuple(vor.vertices[index]))\n        if len(edge) > 0:\n            edges.append(tuple(edge))\n    return cells, edges"
    },
    {
        "function_name": "reservoir_sampling",
        "file_name": "random_sampling.py",
        "parameters": {
            "`nums`": "A list of integers",
            "`k`": "An integer representing the number of elements to select",
            "`seed`": "An integer representing the seed for the random number generator"
        },
        "objectives": [
            "Use the Reservoir Sampling algorithm to select `k` elements randomly from `nums`.",
            "Ensure that each element in `nums` has an equal probability of being selected.",
            "Return the selected elements in the order they were selected."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def reservoir_sampling(nums, k, seed):\n    random.seed(seed)\n    selected = []\n    for i, num in enumerate(nums):\n        if i < k:\n            selected.append(num)\n        else:\n            j = random.randint(0, i)\n            if j < k:\n                selected[j] = num\n    return selected"
    },
    {
        "function_name": "max_consecutive_ones",
        "file_name": "bitset_operations.py",
        "parameters": {
            "`bitset`": "A binary string",
            "`k`": "An integer representing the number of bits to flip"
        },
        "objectives": [
            "Flip `k` bits in the binary string to obtain a new binary string with the maximum number of consecutive 1s.",
            "Ensure that the bits are flipped in the order of their positions.",
            "Return the new binary string and the maximum number of consecutive 1s."
        ],
        "import_lines": [],
        "function_def": "def max_consecutive_ones(bitset, k):\n    n = len(bitset)\n    max_ones = 0\n    new_bitset = ''\n    flips = 0\n    \n    for i in range(n):\n        if flips < k and bitset[i] == '0':\n            new_bitset += '1'\n            flips += 1\n        else:\n            new_bitset += bitset[i]\n    \n    current_ones = 0\n    \n    for i in range(n):\n        if new_bitset[i] == '1':\n            current_ones += 1\n            max_ones = max(max_ones, current_ones)\n        else:\n            current_ones = 0\n    \n    return new_bitset, max_ones"
    },
    {
        "function_name": "event_scheduling",
        "file_name": "scheduling_algorithms.py",
        "parameters": {
            "`n`": "Integer representing the number of events",
            "`events`": "List of tuples representing the events",
            "`cpu`": "Integer representing the number of CPUs"
        },
        "objectives": [
            "Use the Greedy Algorithm to schedule the events on multiple CPUs.",
            "Calculate the total execution time of the events.",
            "Return the schedule of events and the total execution time."
        ],
        "import_lines": [],
        "function_def": "def event_scheduling(n, events, cpu):\n    events.sort(key=lambda x: x[1])\n    schedule = [[] for _ in range(cpu)]\n    total_time = [0] * cpu\n    \n    for event in events:\n        cpu_idx = total_time.index(min(total_time))\n        schedule[cpu_idx].append(event)\n        total_time[cpu_idx] += event[1]\n    \n    return schedule, max(total_time)"
    },
    {
        "function_name": "bidirectional_dijkstra",
        "file_name": "pathfinding_algorithms.py",
        "parameters": {
            "`_graph`": "weighted adjacency matrix representing the graph",
            "`source`": "node to start the search from",
            "`k`": "number of shortest paths to find"
        },
        "objectives": [
            "Find the k shortest paths from the source node to all other nodes in the graph.",
            "Use a bidirectional search algorithm to improve efficiency.",
            "Handle negative weight edges using a modified version of the Bellman-Ford algorithm.",
            "Return a dictionary where the keys are the destination nodes and the values are lists of the k shortest paths."
        ],
        "import_lines": [
            "import numpy as np",
            "import heapq"
        ],
        "function_def": "def bidirectional_dijkstra(graph, source, k):\n    num_nodes = len(graph)\n    forward_distances = np.full(num_nodes, np.inf)\n    forward_distances[source] = 0\n    backward_distances = np.full(num_nodes, np.inf)\n    shortest_paths = {}\n    \n    # Forward Dijkstra\n    queue = [(0, source, [])]\n    while queue:\n        (dist, current, path) = heapq.heappop(queue)\n        for neighbor, neighbor_dist in enumerate(graph[current]):\n            old_dist = forward_distances[neighbor]\n            new_dist = dist + neighbor_dist\n            if new_dist < old_dist:\n                forward_distances[neighbor] = new_dist\n                heapq.heappush(queue, (new_dist, neighbor, path + [neighbor]))\n    \n    # Backward Dijkstra\n    queue = [(0, source, [])]\n    while queue:\n        (dist, current, path) = heapq.heappop(queue)\n        for neighbor, neighbor_dist in enumerate(graph[current]):\n            old_dist = backward_distances[neighbor]\n            new_dist = dist + neighbor_dist\n            if new_dist < old_dist:\n                backward_distances[neighbor] = new_dist\n                heapq.heappush(queue, (new_dist, neighbor, path + [neighbor]))\n    \n    # Find k shortest paths\n    for i in range(num_nodes):\n        distances = []\n        for j in range(k):\n            distances.append(forward_distances[i] + backward_distances[i])\n        shortest_paths[i] = sorted(distances)[:k]\n    \n    return shortest_paths"
    },
    {
        "function_name": "hungarian_algorithm",
        "file_name": "assignment_algorithms.py",
        "parameters": {
            "`cost_matrix`": "square matrix representing the cost of assigning each worker to each task"
        },
        "objectives": [
            "Solve the assignment problem using the Hungarian algorithm.",
            "Find the optimal assignment that minimizes the total cost.",
            "Return a dictionary where the keys are the worker indices and the values are the assigned task indices."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def hungarian_algorithm(cost_matrix):\n    num_workers, num_tasks = cost_matrix.shape\n    if num_workers != num_tasks:\n        raise ValueError(\"Cost matrix must be square\")\n    \n    # Initialize labels\n    labels = np.zeros(num_workers)\n    \n    # Find initial feasible solution\n    for i in range(num_workers):\n        min_cost = np.inf\n        for j in range(num_tasks):\n            if cost_matrix[i, j] < min_cost:\n                min_cost = cost_matrix[i, j]\n                labels[i] = j\n    \n    # Improve solution\n    while True:\n        # Find augmenting path\n        path = []\n        for i in range(num_workers):\n            if labels[i] == -1:\n                path.append(i)\n                break\n        \n        if not path:\n            break\n        \n        # Find shortest augmenting path\n        shortest_path = []\n        shortest_distance = np.inf\n        for i in range(num_workers):\n            if labels[i] == -1:\n                distance = np.inf\n                for j in range(num_tasks):\n                    if cost_matrix[i, j] < distance:\n                        distance = cost_matrix[i, j]\n                        shortest_path.append(j)\n        \n        if distance < shortest_distance:\n            shortest_distance = distance\n            shortest_path = [i]\n        \n        # Update labels\n        for i in range(num_workers):\n            labels[i] += shortest_distance\n        \n        # Update assignment\n        for i in range(num_workers):\n            if labels[i] == -1:\n                labels[i] = shortest_path[i]\n    \n    assignment = {}\n    for i in range(num_workers):\n        assignment[i] = labels[i]\n    \n    return assignment"
    },
    {
        "function_name": "k_longest_strings",
        "file_name": "string_algorithms.py",
        "parameters": {
            "`strings`": "A list of strings to be sorted.",
            "`K`": "An integer representing the number of strings to select."
        },
        "objectives": [
            "Sort the strings lexicographically and select the K longest strings.",
            "Calculate the total length of the selected strings.",
            "Return the selected strings and their total length."
        ],
        "import_lines": [],
        "function_def": "def k_longest_strings(strings, K):\n    sorted_strings = sorted(strings, key=len, reverse=True)\n    selected_strings = sorted_strings[:K]\n    total_length = sum(len(s) for s in selected_strings)\n    \n    return selected_strings, total_length"
    },
    {
        "function_name": "rabin_karp_search",
        "file_name": "pattern_search.py",
        "parameters": {
            "`binary_strings`": "A list of binary strings.",
            "`target`": "A binary string representing the target pattern."
        },
        "objectives": [
            "Use the Rabin-Karp algorithm to search for the target pattern in each binary string.",
            "Return a list of indices where the target pattern starts in each binary string."
        ],
        "import_lines": [],
        "function_def": "def rabin_karp_search(binary_strings, target):\n    d = 256\n    q = 101\n    n = len(target)\n    patterns = []\n    \n    for binary_string in binary_strings:\n        m = len(binary_string)\n        pattern = []\n        target_hash = 0\n        string_hash = 0\n        h = 1\n        \n        for _ in range(n - 1):\n            h = (h * d) % q\n        \n        for i in range(n):\n            target_hash = (d * target_hash + ord(target[i])) % q\n            string_hash = (d * string_hash + ord(binary_string[i])) % q\n        \n        for i in range(m - n + 1):\n            if target_hash == string_hash:\n                match = True\n                for j in range(n):\n                    if binary_string[i + j] != target[j]:\n                        match = False\n                        break\n                if match:\n                    pattern.append(i)\n            if i < m - n:\n                string_hash = (d * (string_hash - ord(binary_string[i]) * h) + ord(binary_string[i + n])) % q\n                if string_hash < 0:\n                    string_hash += q\n        \n        patterns.append(pattern)\n    \n    return patterns"
    },
    {
        "function_name": "magic_square",
        "file_name": "matrix_operations.py",
        "parameters": {
            "`n`": "The number of rows and columns in the matrix",
            "`values`": "A list of values to fill the matrix with"
        },
        "objectives": [
            "Create a magic square matrix of size `n x n` where the sum of each row, column, and diagonal is the same",
            "Fill the matrix with the given values",
            "Return the magic square matrix"
        ],
        "import_lines": [],
        "function_def": "def magic_square(n, values):\n    matrix = [[0]*n for _ in range(n)]\n    i, j = 0, n//2\n    \n    for value in values:\n        matrix[i][j] = value\n        next_i, next_j = (i-1) % n, (j+1) % n\n        if matrix[next_i][next_j]:\n            i += 1\n        else:\n            i, j = next_i, next_j\n    \n    return matrix"
    },
    {
        "function_name": "textrank",
        "file_name": "keyword_extraction.py",
        "parameters": {
            "text": "A string representing the input text.",
            "keywords": "A list of strings representing the keywords to extract."
        },
        "objectives": [
            "Implement the TextRank algorithm to extract keywords from the given text.",
            "Use the PageRank algorithm to rank the keywords based on their importance.",
            "Return a list of keywords and their corresponding scores."
        ],
        "import_lines": [
            "from collections import defaultdict",
            "from operator import itemgetter"
        ],
        "function_def": "def textrank(text, keywords):\n    graph = defaultdict(list)\n    sentences = text.split('. ')\n    for sentence in sentences:\n        words = sentence.split()\n        for word in words:\n            if word in keywords:\n                for neighbor in words:\n                    if neighbor in keywords and neighbor != word:\n                        graph[word].append(neighbor)\n    scores = {word: 1.0 for word in keywords}\n    for _ in range(10):\n        for word in keywords:\n            score = 0.0\n            for neighbor in graph[word]:\n                score += scores[neighbor] / len(graph[neighbor])\n            scores[word] = 0.85 * score + 0.15\n    return sorted(scores.items(), key=itemgetter(1), reverse=True)"
    },
    {
        "function_name": "min_edges_cluster",
        "file_name": "cluster_analyzer.py",
        "parameters": {
            "`num_of_nodes`": "int",
            "`num_of_clusters`": "int",
            "`node_connections`": "list of tuples, where each tuple contains two nodes that are connected"
        },
        "objectives": [
            "Create an adjacency matrix representing the graph with the given node connections.",
            "Use the K-Means clustering algorithm to cluster the nodes into the specified number of clusters.",
            "Calculate the total number of edges within each cluster.",
            "Return the cluster with the minimum number of edges and its corresponding node IDs."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def min_edges_cluster(num_of_nodes, num_of_clusters, node_connections):\n    adjacency_matrix = np.zeros((num_of_nodes, num_of_nodes))\n    for node1, node2 in node_connections:\n        adjacency_matrix[node1][node2] = 1\n        adjacency_matrix[node2][node1] = 1\n    \n    clusters = []\n    for _ in range(num_of_clusters):\n        cluster = []\n        for node in range(num_of_nodes):\n            if np.random.rand() < 0.5:\n                cluster.append(node)\n        clusters.append(cluster)\n    \n    for _ in range(100):\n        new_clusters = [[] for _ in range(num_of_clusters)]\n        centroid = np.zeros((num_of_clusters, num_of_nodes))\n        for cluster in clusters:\n            for node in cluster:\n                centroid[clusters.index(cluster)][node] += 1\n            centroid[clusters.index(cluster)] /= len(cluster)\n        \n        for node in range(num_of_nodes):\n            closest_cluster = np.argmin(np.linalg.norm(centroid - adjacency_matrix[node], axis=1))\n            new_clusters[closest_cluster].append(node)\n        \n        clusters = new_clusters\n    \n    min_edges = float('inf')\n    min_edges_cluster = None\n    for cluster in clusters:\n        edges = 0\n        for node1 in cluster:\n            for node2 in cluster:\n                edges += adjacency_matrix[node1][node2]\n        edges /= 2\n        if edges < min_edges:\n            min_edges = edges\n            min_edges_cluster = cluster\n    \n    return min_edges_cluster, len(min_edges_cluster)"
    },
    {
        "function_name": "svm_classifier",
        "file_name": "svm.py",
        "parameters": {
            "`num_of_features`": "int",
            "`num_of_samples`": "int",
            "`data_points`": "list of lists representing the data points",
            "`labels`": "list of integers representing the labels of the data points"
        },
        "objectives": [
            "Use the Support Vector Machine algorithm to classify the data points into their respective labels.",
            "Calculate the accuracy of the classifier by comparing the predicted labels with the actual labels.",
            "Use the forward feature selection algorithm to select the most relevant features.",
            "Return the accuracy of the classifier and the selected features."
        ],
        "import_lines": [
            "from sklearn import svm",
            "from sklearn.metrics import accuracy_score"
        ],
        "function_def": "def svm_classifier(num_of_features, num_of_samples, data_points, labels):\n    classifier = svm.SVC()\n    classifier.fit(data_points, labels)\n    \n    predicted_labels = classifier.predict(data_points)\n    accuracy = accuracy_score(labels, predicted_labels)\n    \n    selected_features = []\n    current_features = list(range(num_of_features))\n    for _ in range(num_of_features):\n        max_accuracy = 0\n        best_feature = None\n        for feature in current_features:\n            new_features = selected_features + [feature]\n            new_data_points = [[data_point[i] for i in new_features] for data_point in data_points]\n            classifier.fit(new_data_points, labels)\n            predicted_labels = classifier.predict(new_data_points)\n            new_accuracy = accuracy_score(labels, predicted_labels)\n            if new_accuracy > max_accuracy:\n                max_accuracy = new_accuracy\n                best_feature = feature\n        selected_features.append(best_feature)\n        current_features.remove(best_feature)\n    \n    return accuracy, selected_features"
    },
    {
        "function_name": "superset_finder",
        "file_name": "set_analyzer.py",
        "parameters": {
            "`sets`": "A list of sets representing a collection of sets",
            "`target_set`": "A set representing the target set"
        },
        "objectives": [
            "Find all sets in the collection that are supersets of the target set.",
            "Calculate the Jaccard similarity between each set and the target set.",
            "Return the sets that are supersets of the target set and their Jaccard similarities."
        ],
        "import_lines": [],
        "function_def": "def superset_finder(sets, target_set):\n    result = []\n    \n    for s in sets:\n        if target_set.issubset(s):\n            jaccard_similarity = len(target_set & s) / len(s)\n            result.append((s, jaccard_similarity))\n    \n    return result"
    },
    {
        "function_name": "find_pairs",
        "file_name": "binary_tree_algorithms.py",
        "parameters": {
            "`trees`": "A list of root nodes of binary trees",
            "`target`": "An integer representing the target sum"
        },
        "objectives": [
            "Find all pairs of nodes from different trees that sum up to the target value.",
            "Use a set to efficiently store and retrieve node values from the trees.",
            "Return the list of pairs of nodes."
        ],
        "import_lines": [],
        "function_def": "def find_pairs(trees, target):\n    node_values = set()\n    pairs = set()\n    for tree in trees:\n        stack = [tree]\n        while stack:\n            node = stack.pop()\n            if node:\n                complement = target - node.val\n                if complement in node_values:\n                    pairs.add(tuple(sorted([node.val, complement])))\n                node_values.add(node.val)\n                stack.append(node.left)\n                stack.append(node.right)\n    return list(pairs)"
    },
    {
        "function_name": "coordinates_within_radius",
        "file_name": "geometry_algorithms.py",
        "parameters": {
            "`coordinates`": "A list of (x, y) coordinates",
            "`radius`": "An integer representing the radius of the circle"
        },
        "objectives": [
            "Find all coordinates that are within the given radius of the origin (0, 0).",
            "Use a mathematical approach to efficiently calculate the distance between coordinates and the origin.",
            "Return the list of coordinates that are within the radius."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def coordinates_within_radius(coordinates, radius):\n    result = []\n    for x, y in coordinates:\n        distance = math.sqrt(x**2 + y**2)\n        if distance <= radius:\n            result.append((x, y))\n    return result"
    },
    {
        "function_name": "schedule_intervals",
        "file_name": "scheduling_algorithms.py",
        "parameters": {
            "`schedule`": "A list of (start, end) time intervals",
            "`resource`": "An integer representing the number of resources available"
        },
        "objectives": [
            "Find the maximum number of non-overlapping time intervals that can be scheduled with the given resources.",
            "Use a greedy algorithm to efficiently select the non-overlapping intervals.",
            "Return the maximum number of scheduled intervals."
        ],
        "import_lines": [],
        "function_def": "def schedule_intervals(schedule, resources):\n    schedule.sort(key=lambda x: x[1])\n    end_times = [-1] * resources\n    scheduled = 0\n    for start, end in schedule:\n        for i in range(resources):\n            if end_times[i] <= start:\n                end_times[i] = end\n                scheduled += 1\n                break\n    return scheduled"
    },
    {
        "function_name": "robot_escape",
        "file_name": "grid_robotics.py",
        "parameters": {
            "grid": "A 2D list of integers representing a grid",
            "n": "An integer representing the size of the grid",
            "m": "An integer representing the number of robots",
            "robot_positions": "A list of tuples representing the initial positions of the robots"
        },
        "objectives": [
            "Find the shortest path for each robot to reach a designated exit in the grid.",
            "The path should avoid obstacles (represented by 0 in the grid) and other robots.",
            "The function should return a list of tuples, where each tuple contains the path for a robot."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def robot_escape(grid, n, m, robot_positions):\n    exits = [(i, j) for i in range(n) for j in range(n) if grid[i][j] == 2]\n    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n    result = []\n    \n    for _ in range(m):\n        queue = deque([robot_positions[_]])\n        parent = {}\n        visited = set()\n        while queue:\n            x, y = queue.popleft()\n            visited.add((x, y))\n            if (x, y) in exits:\n                path = []\n                while (x, y) != robot_positions[_]:\n                    path.append((x, y))\n                    x, y = parent[(x, y)]\n                path.append(robot_positions[_])\n                path.reverse()\n                result.append(path)\n                break\n            for dx, dy in directions:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < n and 0 <= ny < n and grid[nx][ny] != 0 and (nx, ny) not in visited and (nx, ny) not in robot_positions:\n                    queue.append((nx, ny))\n                    parent[(nx, ny)] = (x, y)\n                    visited.add((nx, ny))\n                    \n    return result"
    },
    {
        "function_name": "forest_cutting",
        "file_name": "forest_management.py",
        "parameters": {
            "trees": "A list of lists of integers representing the heights of trees in a forest",
            "n": "An integer representing the number of trees",
            "m": "An integer representing the number of tools",
            "tool_efficiencies": "A list of integers representing the efficiency of each tool"
        },
        "objectives": [
            "Find the minimum number of tools required to cut down all trees.",
            "The function should return a list of integers, where each integer represents the number of tools required to cut down a tree."
        ],
        "import_lines": [],
        "function_def": "def forest_cutting(trees, n, m, tool_efficiencies):\n    result = []\n    tools = sorted(tool_efficiencies, reverse=True)\n    \n    for i in range(n):\n        num_tools = 0\n        remaining_height = sum(trees[i])\n        j = 0\n        while remaining_height > 0 and j < m:\n            remaining_height -= tools[j]\n            num_tools += 1\n            j += 1\n        result.append(num_tools)\n        \n    return result"
    },
    {
        "function_name": "min_edges",
        "file_name": "shortest_paths.py",
        "parameters": {
            "n": "An integer representing the number of nodes in the graph",
            "edges": "A list of tuples representing the edges in the graph",
            "start": "An integer representing the starting node",
            "end": "An integer representing the ending node"
        },
        "objectives": [
            "Find the minimum number of edges required to travel from the start node to the end node.",
            "The function should return a list of integers, where each integer represents the minimum number of edges required to travel from the start node to each node."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def min_edges(n, edges, start, end):\n    graph = [[] for _ in range(n + 1)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        graph[v].append((u, w))\n    distances = [float('inf')] * (n + 1)\n    distances[start] = 0\n    pq = [(0, start)]\n    \n    while pq:\n        dist, node = heapq.heappop(pq)\n        if dist > distances[node]:\n            continue\n        for neighbor, weight in graph[node]:\n            new_dist = dist + weight\n            if new_dist < distances[neighbor]:\n                distances[neighbor] = new_dist\n                heapq.heappush(pq, (new_dist, neighbor))\n                \n    return distances[1:]"
    },
    {
        "function_name": "levenshtein_distance",
        "file_name": "string_distance.py",
        "parameters": {
            "`string`": "The input string",
            "`k`": "The maximum number of errors allowed"
        },
        "objectives": [
            "Implement the Levenshtein distance algorithm to calculate the minimum number of operations (insertions, deletions, and substitutions) needed to transform the input string into another string.",
            "Use dynamic programming to build a matrix that stores the Levenshtein distances between substrings.",
            "Return the minimum number of operations needed to transform the input string into another string with at most k errors."
        ],
        "import_lines": [],
        "function_def": "def levenshtein_distance(string1, string2, k):\n    dp = [[0] * (len(string2) + 1) for _ in range(len(string1) + 1)]\n    \n    for i in range(len(string1) + 1):\n        dp[i][0] = i\n    for j in range(len(string2) + 1):\n        dp[0][j] = j\n        \n    for i in range(1, len(string1) + 1):\n        for j in range(1, len(string2) + 1):\n            cost = 0 if string1[i - 1] == string2[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            \n    return dp[-1][-1] if dp[-1][-1] <= k else -1"
    },
    {
        "function_name": "edf_scheduling",
        "file_name": "scheduling.py",
        "parameters": {
            "`scheduling_tasks`": "A list of tasks with their respective execution times and deadlines"
        },
        "objectives": [
            "Implement the Earliest Deadline First (EDF) scheduling algorithm to schedule tasks based on their deadlines.",
            "Sort tasks by their deadlines and execute them in that order.",
            "Use a priority queue to store tasks and their deadlines.",
            "Return the scheduled tasks."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def edf_scheduling(scheduling_tasks):\n    tasks = []\n    for task in scheduling_tasks:\n        task_name, execution_time, deadline = task\n        heapq.heappush(tasks, (deadline, task_name, execution_time))\n        \n    scheduled_tasks = []\n    while tasks:\n        deadline, task_name, execution_time = heapq.heappop(tasks)\n        scheduled_tasks.append((task_name, execution_time, deadline))\n        \n    return scheduled_tasks"
    },
    {
        "function_name": "scheduler",
        "file_name": "scheduler.py",
        "parameters": {
            "`schedule`": "A dictionary representing a scheduling problem, where each key is a task and each value is the duration of the task.",
            "`resources`": "A list of tuples (resource, capacity), where 'resource' is the name of the resource and 'capacity' is its availability.",
            "`dependencies`": "A list of tuples (task1, task2), where task1 depends on the completion of task2."
        },
        "objectives": [
            "Implement a topological sorting algorithm to sort the tasks based on their dependencies.",
            "Use a greedy algorithm to schedule the tasks on the resources such that the tasks are completed as early as possible.",
            "Return the schedule, including the resource assigned to each task and its completion time."
        ],
        "import_lines": [
            "from collections import defaultdict, deque"
        ],
        "function_def": "def scheduler(schedule, resources, dependencies):\n    graph = defaultdict(list)\n    in_degree = {task: 0 for task in schedule}\n    for task1, task2 in dependencies:\n        graph[task2].append(task1)\n        in_degree[task1] += 1\n    \n    queue = deque([task for task in schedule if in_degree[task] == 0])\n    scheduled_tasks = {}\n    resource_utilization = {resource: 0 for resource, _ in resources}\n    \n    while queue:\n        task = queue.popleft()\n        possible_resources = [resource for resource, capacity in resources if capacity >= schedule[task] - resource_utilization[resource]]\n        if possible_resources:\n            assigned_resource = min(possible_resources, key=lambda x: resource_utilization[x])\n            scheduled_tasks[task] = (assigned_resource, schedule[task] - resource_utilization[assigned_resource])\n            resource_utilization[assigned_resource] += schedule[task]\n            for neighbor in graph[task]:\n                in_degree[neighbor] -= 1\n                if in_degree[neighbor] == 0:\n                    queue.append(neighbor)\n    \n    return scheduled_tasks"
    },
    {
        "function_name": "great_circle_distance_calculator",
        "file_name": "spatial_analysis.py",
        "parameters": {
            "`coordinates`": "A list of tuples representing points in 2D space",
            "`x`": "An integer representing the x-coordinate of the pivot point",
            "`y`": "An integer representing the y-coordinate of the pivot point",
            "`radius`": "An integer representing the radius of the circle"
        },
        "objectives": [
            "Calculate the great-circle distance between each point in `coordinates` and the pivot point `(x, y)`.",
            "Determine which points fall within the circle of radius `radius` centered at the pivot point.",
            "Return the list of points within the circle and the great-circle distances."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def great_circle_distance_calculator(coordinates, x, y, radius):\n    points_in_circle = []\n    distances = []\n    for coord in coordinates:\n        lat1, lon1 = math.radians(y), math.radians(x)\n        lat2, lon2 = math.radians(coord[1]), math.radians(coord[0])\n        dlon = lon2 - lon1\n        dlat = lat2 - lat1\n        a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n        distance = 6371 * c  # Radius of the Earth in kilometers\n        if distance <= radius:\n            points_in_circle.append(coord)\n            distances.append(distance)\n    \n    return points_in_circle, distances"
    },
    {
        "function_name": "max_threshold_rows",
        "file_name": "greedy_selection.py",
        "parameters": {
            "`matrix`": "A 2D list of integers",
            "`threshold`": "An integer representing the threshold value"
        },
        "objectives": [
            "Use a greedy algorithm to select a subset of rows in `matrix` that contain at least one element greater than or equal to `threshold`.",
            "Ensure that the selected rows contain the maximum sum of elements greater than or equal to `threshold`.",
            "Return the selected rows."
        ],
        "import_lines": [],
        "function_def": "def max_threshold_rows(matrix, threshold):\n    row_sums = []\n    for row in matrix:\n        row_sum = 0\n        for num in row:\n            if num >= threshold:\n                row_sum += num\n        row_sums.append(row_sum)\n    selected_rows = []\n    while row_sums:\n        max_sum_index = row_sums.index(max(row_sums))\n        selected_rows.append(matrix[max_sum_index])\n        row_sums.pop(max_sum_index)\n    return selected_rows"
    },
    {
        "function_name": "sphere_kmeans",
        "file_name": "sphere_kmeans.py",
        "parameters": {
            "`points`": "A list of tuples representing points in 3D space.",
            "`center`": "A tuple representing the center of the sphere.",
            "`radius`": "A float representing the radius of the sphere."
        },
        "objectives": [
            "Implement the k-Means clustering algorithm to group points into clusters based on their distance from the center of the sphere.",
            "Use the Euclidean distance metric to calculate the distance between points and the center of the sphere.",
            "Return the cluster centers and the points in each cluster."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def sphere_kmeans(points, center, radius):\n    cluster_centers = [center]\n    clusters = [[] for _ in range(len(cluster_centers))]\n    \n    for point in points:\n        distance = math.sqrt((point[0] - center[0])**2 + (point[1] - center[1])**2 + (point[2] - center[2])**2)\n        if distance <= radius:\n            cluster_index = 0\n            for i, cluster_center in enumerate(cluster_centers):\n                cluster_distance = math.sqrt((point[0] - cluster_center[0])**2 + (point[1] - cluster_center[1])**2 + (point[2] - cluster_center[2])**2)\n                if cluster_distance < distance:\n                    distance = cluster_distance\n                    cluster_index = i\n            clusters[cluster_index].append(point)\n    \n    return cluster_centers, clusters"
    },
    {
        "function_name": "floyd_warshall",
        "file_name": "floyd_warshall.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix.",
            "`threshold`": "An integer representing a threshold value."
        },
        "objectives": [
            "Implement the Floyd-Warshall algorithm to find the shortest path between all pairs of nodes in the matrix.",
            "Use a dynamic programming approach to efficiently calculate the shortest paths.",
            "Return the matrix of shortest path lengths."
        ],
        "import_lines": [],
        "function_def": "def floyd_warshall(matrix, threshold):\n    rows, cols = len(matrix), len(matrix[0])\n    \n    for k in range(rows):\n        for i in range(rows):\n            for j in range(cols):\n                if matrix[i][k] + matrix[k][j] < matrix[i][j] and matrix[i][k] + matrix[k][j] >= threshold:\n                    matrix[i][j] = matrix[i][k] + matrix[k][j]\n    \n    return matrix"
    },
    {
        "function_name": "n_gram_frequency",
        "file_name": "text_processing.py",
        "parameters": {
            "`text`": "A string representing the text to be processed.",
            "`n_grams`": "An integer representing the size of the n-grams."
        },
        "objectives": [
            "Implement a function to calculate the frequency of each n-gram in the given text.",
            "Handle cases where the text contains punctuation and capital letters.",
            "Calculate the frequency of each n-gram and return a dictionary with the n-grams as keys and their frequencies as values."
        ],
        "import_lines": [
            "import re",
            "from collections import defaultdict"
        ],
        "function_def": "def n_gram_frequency(text, n_grams):\n    # Convert the text to lowercase and remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    \n    # Initialize a dictionary to store the frequency of each n-gram\n    frequency = defaultdict(int)\n    \n    # For each n-gram in the text\n    for i in range(len(text) - n_grams + 1):\n        n_gram = text[i:i+n_grams]\n        \n        # Increment the frequency of the n-gram\n        frequency[n_gram] += 1\n    \n    return dict(frequency)"
    },
    {
        "function_name": "combination_product",
        "file_name": "array_combinations.py",
        "parameters": {
            "`arrays`": "A list of lists of integers representing different arrays",
            "`target`": "An integer representing the target product"
        },
        "objectives": [
            "Find all combinations of one element from each array that multiply to the target product.",
            "Return a list of tuples, where each tuple contains a combination of elements that multiply to the target."
        ],
        "import_lines": [
            "import itertools"
        ],
        "function_def": "def combination_product(arrays, target):\n    combinations = list(itertools.product(*arrays))\n    result = [comb for comb in combinations if eval('*'.join(map(str, comb))) == target]\n    return result"
    },
    {
        "function_name": "towers_of_hanoi",
        "file_name": "recursive_algorithms.py",
        "parameters": {
            "`n`": "Integer representing the number of disks",
            "`source`": "Integer representing the source peg",
            "`auxiliary`": "Integer representing the auxiliary peg",
            "`target`": "Integer representing the target peg"
        },
        "objectives": [
            "Implement the Towers of Hanoi problem to move disks from the source peg to the target peg.",
            "Use a recursive approach to solve the problem.",
            "Represent the disks as a list of integers.",
            "Return the sequence of moves to solve the problem."
        ],
        "import_lines": [],
        "function_def": "def towers_of_hanoi(n, source, auxiliary, target):\n    if n == 1:\n        return [(source, target)]\n    \n    moves = towers_of_hanoi(n-1, source, target, auxiliary)\n    moves.append((source, target))\n    moves.extend(towers_of_hanoi(n-1, auxiliary, source, target))\n    \n    return moves"
    },
    {
        "function_name": "cluster_analysis",
        "file_name": "graph_clustering.py",
        "parameters": {
            "`graph`": "A 2D list representing the adjacency matrix of an undirected graph",
            "`k`": "An integer representing the number of clustering iterations",
            "`method`": "A string representing the clustering method to use, either 'kmeans' or 'hierarchical'"
        },
        "objectives": [
            "Perform a clustering analysis on the input graph using the specified method.",
            "For each cluster, calculate the average degree of the nodes in the cluster.",
            "Return the clusters, the average degree of each cluster, and the number of clusters."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.cluster import KMeans, AgglomerativeClustering"
        ],
        "function_def": "def cluster_analysis(graph, k, method):\n    if method == 'kmeans':\n        clustering = KMeans(n_clusters=k)\n    elif method == 'hierarchical':\n        clustering = AgglomerativeClustering(n_clusters=k)\n    else:\n        raise ValueError(\"Invalid clustering method\")\n    \n    clusters = clustering.fit_predict(graph)\n    average_degrees = []\n    for cluster in set(clusters):\n        cluster_nodes = [i for i, x in enumerate(clusters) if x == cluster]\n        average_degree = np.mean([sum(row) for row in [graph[i] for i in cluster_nodes]])\n        average_degrees.append(average_degree)\n    \n    return clusters, average_degrees, len(set(clusters))"
    },
    {
        "function_name": "regression_analysis",
        "file_name": "regression.py",
        "parameters": {
            "`data`": "A 2D list representing a dataset of numerical features",
            "`target`": "A list representing the target variable",
            "`method`": "A string representing the regression method to use, either 'linear' or 'ridge'"
        },
        "objectives": [
            "Use the specified regression method to model the relationship between the features and the target variable.",
            "Calculate the mean squared error (MSE) of the model on the training data.",
            "Return the model's coefficients and the MSE."
        ],
        "import_lines": [
            "from sklearn.linear_model import LinearRegression, Ridge"
        ],
        "function_def": "def regression_analysis(data, target, method):\n    if method == 'linear':\n        model = LinearRegression()\n    elif method == 'ridge':\n        model = Ridge()\n    else:\n        raise ValueError(\"Invalid regression method\")\n    \n    model.fit(data, target)\n    coefficients = model.coef_\n    predictions = model.predict(data)\n    mse = np.mean((predictions - target) ** 2)\n    \n    return coefficients, mse"
    },
    {
        "function_name": "power_iteration",
        "file_name": "eigenvalues.py",
        "parameters": {
            "`$matrix`": "A 2D list representing a matrix of integers.",
            "`$k`": "An integer representing the number of eigenvalues to find."
        },
        "objectives": [
            "Use the power iteration method to find the k largest eigenvalues of the given matrix.",
            "Return the eigenvalues and the corresponding eigenvectors."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def power_iteration(matrix, k):\n    eigenvalues = []\n    eigenvectors = []\n    for _ in range(k):\n        v = np.random.rand(len(matrix))\n        v = v / np.linalg.norm(v)\n        for _ in range(len(matrix)):\n            v = np.dot(matrix, v)\n            v = v / np.linalg.norm(v)\n        eigenvalue = np.dot(v.T, np.dot(matrix, v))\n        eigenvalues.append(eigenvalue)\n        eigenvectors.append(v)\n    return eigenvalues, eigenvectors"
    },
    {
        "function_name": "motif_finder",
        "file_name": "motif_finding.py",
        "parameters": {
            "`sequences`": "A list of lists of integers representing DNA sequences",
            "`k`": "An integer representing the length of the motif"
        },
        "objectives": [
            "Generate all possible motifs of length k from the first sequence.",
            "For each motif, count the number of occurrences in each sequence.",
            "Return the motif with the maximum total count across all sequences."
        ],
        "import_lines": [
            "import itertools"
        ],
        "function_def": "def motif_finder(sequences, k):\n    motifs = [sequences[0][i:i+k] for i in range(len(sequences[0]) - k + 1)]\n    max_count = 0\n    best_motif = None\n    for motif in motifs:\n        count = 0\n        for sequence in sequences:\n            count += sum(all(sequence[j] == motif[j-i] for j in range(i, i+k)) for i in range(len(sequence) - k + 1))\n        if count > max_count:\n            max_count = count\n            best_motif = motif\n    return best_motif"
    },
    {
        "function_name": "submatrix_finder",
        "file_name": "submatrix_search.py",
        "parameters": {
            "`array`": "A 2D array of integers representing a matrix",
            "`target`": "An integer representing the target sum"
        },
        "objectives": [
            "Initialize a 2D table to store the sums of submatrices.",
            "Fill the table in a bottom-up manner by calculating the sum of each submatrix.",
            "Find the largest submatrix with a sum equal to the target sum.",
            "Return the dimensions of the largest submatrix."
        ],
        "import_lines": [],
        "function_def": "def submatrix_finder(array, target):\n    rows, cols = len(array), len(array[0])\n    table = [[0] * (cols + 1) for _ in range(rows + 1)]\n    for i in range(1, rows + 1):\n        for j in range(1, cols + 1):\n            table[i][j] = table[i-1][j] + table[i][j-1] - table[i-1][j-1] + array[i-1][j-1]\n    max_size = 0\n    best_submatrix = None\n    for i in range(1, rows + 1):\n        for j in range(1, cols + 1):\n            for k in range(i, rows + 1):\n                for end_col in range(j, cols + 1):\n                    submatrix_sum = table[k][end_col] - table[k][j-1] - table[i-1][end_col] + table[i-1][j-1]\n                    if submatrix_sum == target:\n                        size = (k - i + 1) * (end_col - j + 1)\n                        if size > max_size:\n                            max_size = size\n                            best_submatrix = (i, j, k, end_col)\n    return best_submatrix"
    },
    {
        "function_name": "topological_sorting",
        "file_name": "graph_algorithms.py",
        "parameters": {
            "`n`": "An integer representing the number of nodes",
            "`edges`": "A list of tuples representing the edges between nodes"
        },
        "objectives": [
            "Use the topological sorting algorithm to order the nodes such that for every edge (u, v), node u comes before node v in the ordering.",
            "Apply the Kahn's algorithm to detect cycles in the graph.",
            "Return the topological ordering of the nodes and a boolean indicating whether the graph contains a cycle."
        ],
        "import_lines": [
            "from collections import defaultdict, deque"
        ],
        "function_def": "def topological_sorting(n, edges):\n    in_degree = {i: 0 for i in range(n)}\n    graph = defaultdict(list)\n    for u, v in edges:\n        graph[u].append(v)\n        in_degree[v] += 1\n    \n    queue = deque([node for node in in_degree if in_degree[node] == 0])\n    ordering = []\n    \n    while queue:\n        node = queue.popleft()\n        ordering.append(node)\n        \n        for neighbor in graph[node]:\n            in_degree[neighbor] -= 1\n            if in_degree[neighbor] == 0:\n                queue.append(neighbor)\n    \n    contains_cycle = len(ordering) != n\n    \n    return ordering, contains_cycle"
    },
    {
        "function_name": "most_frequent_words",
        "file_name": "trie_operations.py",
        "parameters": {
            "`text`": "A string representing the input text",
            "`k`": "An integer representing the number of most frequent words to return",
            "`stop_words`": "A list of strings representing the stop words to ignore"
        },
        "objectives": [
            "Preprocess the input text by removing punctuation and converting to lower case.",
            "Use a trie data structure to store the words and their frequencies.",
            "Find the k most frequent words by using a heap data structure.",
            "Return the k most frequent words along with their frequencies."
        ],
        "import_lines": [
            "from collections import defaultdict",
            "from heapq import nlargest",
            "import string"
        ],
        "function_def": "def most_frequent_words(text, k, stop_words):\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    words = text.split()\n    trie = defaultdict(int)\n    \n    for word in words:\n        if word not in stop_words:\n            trie[word] += 1\n    \n    most_frequent = nlargest(k, trie, key=trie.get)\n    return [(word, trie[word]) for word in most_frequent]"
    },
    {
        "function_name": "contiguous_sub_sequence",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "sequence": "List of integers",
            "length": "Integer"
        },
        "objectives": [
            "Implement a function to find all contiguous sub-sequences within the given sequence that sum to the target value.",
            "Handle cases where the sequence contains duplicate numbers.",
            "For each sub-sequence found, calculate its mean and return a list of tuples, where each tuple contains the sub-sequence and its mean."
        ],
        "import_lines": [
            "from itertools import combinations"
        ],
        "function_def": "def contiguous_sub_sequence(sequence, length, target):\n    result = []\n    for i in range(len(sequence) - length + 1):\n        sub_sequence = sequence[i:i + length]\n        if sum(sub_sequence) == target:\n            sub_sequence_mean = sum(sub_sequence) / length\n            result.append((sub_sequence, sub_sequence_mean))\n    return result"
    },
    {
        "function_name": "social_network_traversal",
        "file_name": "social_network_analysis.py",
        "parameters": {
            "social_network": "Dictionary representing a social network where each key is a person and each value is a list of their friends.",
            "person": "String representing the person whose network needs to be traversed."
        },
        "objectives": [
            "Implement a function to traverse the social network using a Breadth-First Search (BFS) algorithm.",
            "Find all connections of the given person within a maximum distance of 3.",
            "Return a dictionary with the connections as keys and their distances from the given person as values."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def social_network_traversal(social_network, person):\n    visited = set()\n    connections = {}\n    queue = deque([(person, 0)])\n    visited.add(person)\n    while queue:\n        current_person, distance = queue.popleft()\n        if distance > 3:\n            break\n        connections[current_person] = distance\n        for friend in social_network[current_person]:\n            if friend not in visited:\n                queue.append((friend, distance + 1))\n                visited.add(friend)\n    return connections"
    },
    {
        "function_name": "ranking_update",
        "file_name": "ranking_update.py",
        "parameters": {
            "ranking": "List of integers representing a ranking of items.",
            "updates": "List of tuples representing updates to the ranking."
        },
        "objectives": [
            "Implement a function to update the ranking based on the given updates.",
            "Use a priority queue to efficiently update the ranking.",
            "Return the updated ranking."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def ranking_update(ranking, updates):\n    heap = []\n    for i, item in enumerate(ranking):\n        heapq.heappush(heap, (-item, i))\n    for update in updates:\n        item, new_rank = update\n        index = ranking.index(item)\n        ranking[index] = new_rank\n        heapq.heappush(heap, (-new_rank, index))\n    heapq.heapify(heap)\n    return [heapq.heappop(heap)[0] for _ in range(len(heap))]"
    },
    {
        "function_name": "lu_decomposition",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`matrix`": "2D list representing the matrix"
        },
        "objectives": [
            "Use the LU decomposition algorithm to decompose the matrix into lower and upper triangular matrices.",
            "Handle the case when the matrix is singular.",
            "Calculate the determinant of the matrix using the LU decomposition."
        ],
        "import_lines": [],
        "function_def": "def lu_decomposition(matrix):\n    # Initialize the lower and upper triangular matrices\n    lower_triangle = [[1 if i == j else 0 for j in range(len(matrix))] for i in range(len(matrix))]\n    upper_triangle = [[0 for j in range(len(matrix))] for i in range(len(matrix))]\n    \n    for i in range(len(matrix)):\n        for k in range(i, len(matrix)):\n            upper_triangle[i][k] = matrix[i][k] - sum(lower_triangle[i][j] * upper_triangle[j][k] for j in range(i))\n        for k in range(i + 1, len(matrix)):\n            lower_triangle[k][i] = (matrix[k][i] - sum(lower_triangle[k][j] * upper_triangle[j][i] for j in range(i))) / upper_triangle[i][i]\n    \n    # Calculate the determinant\n    determinant = 1\n    for i in range(len(matrix)):\n        determinant *= upper_triangle[i][i]\n    \n    return lower_triangle, upper_triangle, determinant"
    },
    {
        "function_name": "lda_topic_identification",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`text`": "a string representing the input text",
            "`num_topics`": "an integer representing the number of topics"
        },
        "objectives": [
            "Use the Latent Dirichlet Allocation (LDA) algorithm to identify the topics in the text.",
            "Initialize the topic assignments by randomly assigning topics to each word.",
            "Update the topic assignments by iteratively resampling the topics based on the word frequencies."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def lda_topic_identification(text, num_topics):\n    # Tokenize the text into words\n    words = text.split()\n    \n    # Initialize the topic assignments\n    topic_assignments = [random.randint(0, num_topics - 1) for _ in words]\n    \n    # Initialize the word frequencies\n    word_frequencies = [[0 for _ in range(num_topics)] for _ in range(len(words))]\n    \n    for i in range(len(words)):\n        word_frequencies[i][topic_assignments[i]] += 1\n    \n    while True:\n        # Resample the topic assignments\n        for i in range(len(words)):\n            topic_probability = [0 for _ in range(num_topics)]\n            for topic in range(num_topics):\n                topic_probability[topic] = (word_frequencies[i][topic] + 1) / (sum(word_frequencies[i]) + num_topics)\n            topic_assignments[i] = random.choices(range(num_topics), weights=topic_probability, k=1)[0]\n            word_frequencies[i] = [0 for _ in range(num_topics)]\n            word_frequencies[i][topic_assignments[i]] += 1\n        \n        # Check convergence\n        if all(any(word_frequency > 0 for word_frequency in word_frequencies[i]) for i in range(len(words))):\n            break\n    \n    # Identify the topics\n    topics = {}\n    for i in range(len(words)):\n        topic = topic_assignments[i]\n        if topic not in topics:\n            topics[topic] = []\n        topics[topic].append(words[i])\n    \n    return topics"
    },
    {
        "function_name": "detect_anomalies",
        "file_name": "anomaly_detection.py",
        "parameters": {
            "`data`": "A list of dictionaries, where each dictionary represents a time series data point with 'time' and 'value' keys.",
            "`window_size`": "An integer representing the size of the sliding window.",
            "`threshold`": "A float representing the anomaly detection threshold."
        },
        "objectives": [
            "Use a sliding window approach to divide the time series data into sub-sequences.",
            "For each sub-sequence, calculate the mean and standard deviation.",
            "Identify anomalies by checking if the mean of the sub-sequence is outside the threshold range of the overall mean and standard deviation.",
            "Return a list of indices of the anomalies."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def detect_anomalies(data, window_size, threshold):\n    overall_mean = np.mean([point['value'] for point in data])\n    overall_std = np.std([point['value'] for point in data])\n    anomalies = []\n    for i in range(len(data) - window_size + 1):\n        sub_sequence = data[i:i + window_size]\n        sub_mean = np.mean([point['value'] for point in sub_sequence])\n        sub_std = np.std([point['value'] for point in sub_sequence])\n        if sub_mean < overall_mean - threshold * overall_std or sub_mean > overall_mean + threshold * overall_std:\n            anomalies.extend(list(range(i, i + window_size)))\n    return list(set(anomalies))"
    },
    {
        "function_name": "fuel_constrained_bfs",
        "file_name": "graph_search.py",
        "parameters": {
            "`graph`": "A dictionary representing an adjacency list of a graph, where each key is a node and its value is a list of neighboring nodes with their corresponding edge weights.",
            "`start_node`": "The node to start the search from.",
            "`end_node`": "The node to search for.",
            "`fuel`": "The maximum amount of fuel available for the search."
        },
        "objectives": [
            "Perform a breadth-first search on the graph to find the shortest path from the start node to the end node without exceeding the fuel limit.",
            "Keep track of the minimum fuel required to reach each node.",
            "Return the shortest path and the minimum fuel required to reach the end node if found, otherwise return None."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def fuel_constrained_bfs(graph, start_node, end_node, fuel):\n    visited = set()\n    queue = deque([(start_node, [start_node], 0)])\n    min_fuel_required = {node: float('inf') for node in graph}\n    min_fuel_required[start_node] = 0\n    \n    while queue:\n        node, path, current_fuel = queue.popleft()\n        if node == end_node:\n            return path, min_fuel_required[node]\n        if current_fuel > fuel:\n            continue\n        for neighbor, weight in graph[node]:\n            if neighbor not in visited:\n                new_path = path + [neighbor]\n                new_fuel = current_fuel + weight\n                if new_fuel < min_fuel_required[neighbor]:\n                    min_fuel_required[neighbor] = new_fuel\n                    queue.append((neighbor, new_path, new_fuel))\n                visited.add(neighbor)\n                \n    return None"
    },
    {
        "function_name": "triangle_properties",
        "file_name": "geometry.py",
        "parameters": {
            "`triangles`": "A list of tuples representing the sides of different triangles",
            "`precision`": "An integer representing the precision for floating point calculations"
        },
        "objectives": [
            "Use Heron's formula to calculate the area of each triangle.",
            "Calculate the perimeter of each triangle.",
            "Return the areas and perimeters."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def triangle_properties(triangles, precision):\n    areas = []\n    perimeters = []\n    \n    for triangle in triangles:\n        a, b, c = triangle\n        \n        semi_perimeter = (a + b + c) / 2\n        area = math.sqrt(semi_perimeter * (semi_perimeter - a) * (semi_perimeter - b) * (semi_perimeter - c))\n        \n        areas.append(round(area, precision))\n        perimeters.append(round(a + b + c, precision))\n    \n    return areas, perimeters"
    },
    {
        "function_name": "sliding_window",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "`sequence`": "A list of integers representing the sequence",
            "`threshold`": "An integer representing the threshold value",
            "`window_size`": "An integer representing the size of the window"
        },
        "objectives": [
            "Use the sliding window technique to find the number of sub-sequences with a sum greater than the threshold.",
            "Calculate the sum of the sub-sequences.",
            "Return the number of sub-sequences and the maximum sum."
        ],
        "import_lines": [],
        "function_def": "def sliding_window(sequence, threshold, window_size):\n    result = 0\n    max_sum = 0\n    \n    for i in range(len(sequence) - window_size + 1):\n        sub_sequence = sequence[i:i + window_size]\n        \n        if sum(sub_sequence) > threshold:\n            result += 1\n        \n        max_sum = max(max_sum, sum(sub_sequence))\n    \n    return result, max_sum"
    },
    {
        "function_name": "job_scheduling",
        "file_name": "scheduling.py",
        "parameters": {
            "intervals": "List of tuples, where each tuple represents a time interval",
            "k": "Integer representing the number of jobs to schedule",
            "machines": "List of integers representing the number of machines available at each time point"
        },
        "objectives": [
            "Sort the time intervals based on their end times.",
            "Use a greedy approach to assign the jobs to the available machines, ensuring that no machine is assigned more than one job at the same time.",
            "Return the schedule as a list of tuples, where each tuple contains the job index, machine index, and assigned time interval."
        ],
        "import_lines": [],
        "function_def": "def job_scheduling(intervals, k, machines):\n    intervals.sort(key=lambda x: x[1])\n    schedule = []\n    machine_available = {i: 0 for i in range(len(machines))}\n    for i, (start, end) in enumerate(intervals):\n        available_machine = min(machine_available, key=machine_available.get)\n        if machine_available[available_machine] <= start:\n            schedule.append((i, available_machine, (start, end)))\n            machine_available[available_machine] = end\n        if len(schedule) == k:\n            break\n    return schedule"
    },
    {
        "function_name": "largest_elements",
        "file_name": "matrix_processor.py",
        "parameters": {
            "matrix": "2D list of integers representing a matrix",
            "k": "Integer representing the number of largest elements to find"
        },
        "objectives": [
            "Use a heap data structure to find the k largest elements in the matrix.",
            "Return the k largest elements in a list, sorted in descending order."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def largest_elements(matrix, k):\n    min_heap = []\n    for row in matrix:\n        for num in row:\n            if len(min_heap) < k:\n                heapq.heappush(min_heap, num)\n            elif num > min_heap[0]:\n                heapq.heappop(min_heap)\n                heapq.heappush(min_heap, num)\n    return sorted(min_heap, reverse=True)"
    },
    {
        "function_name": "lz77_compress",
        "file_name": "compression.py",
        "parameters": {
            "`bits`": "List of integers representing the bits to compress",
            "`window_size`": "Integer representing the size of the sliding window"
        },
        "objectives": [
            "Use the LZ77 algorithm to compress the given bits using a sliding window of size window_size.",
            "Return the compressed bits and the corresponding decompression table."
        ],
        "import_lines": [],
        "function_def": "def lz77_compress(bits, window_size):\n    compressed_bits = []\n    decompression_table = []\n    i = 0\n    while i < len(bits):\n        window = bits[max(0, i - window_size):i]\n        max_match_len = 0\n        max_match_offset = 0\n        for j in range(max(0, i - window_size), i):\n            match_len = 0\n            while i + match_len < len(bits) and j + match_len < len(window) and bits[i + match_len] == window[j + match_len]:\n                match_len += 1\n            if match_len > max_match_len:\n                max_match_len = match_len\n                max_match_offset = i - j - window_size\n        \n        if max_match_len > 0:\n            compressed_bits.extend([max_match_offset, max_match_len])\n            decompression_table.append((max_match_offset, max_match_len))\n            i += max_match_len\n        else:\n            compressed_bits.append(bits[i])\n            decompression_table.append(bits[i])\n            i += 1\n    \n    return compressed_bits, decompression_table"
    },
    {
        "function_name": "closest_sequence",
        "file_name": "sequence_alignment.py",
        "parameters": {
            "`sequences`": "A list of lists of integers representing the sequences",
            "`target_sequence`": "A list of integers representing the target sequence",
            "`max_distance`": "An integer representing the maximum distance"
        },
        "objectives": [
            "Find the sequence in `sequences` that has the minimum edit distance to `target_sequence`.",
            "Use the Levenshtein distance algorithm to calculate the edit distance.",
            "Return the sequence with the minimum edit distance and its distance."
        ],
        "import_lines": [],
        "function_def": "def closest_sequence(sequences, target_sequence, max_distance):\n    min_distance = float('inf')\n    closest_sequence = None\n    \n    for sequence in sequences:\n        distance = 0\n        i, j = 0, 0\n        while i < len(sequence) and j < len(target_sequence):\n            if sequence[i] != target_sequence[j]:\n                distance += 1\n            i += 1\n            j += 1\n        \n        distance += abs(len(sequence) - len(target_sequence))\n        \n        if distance < min_distance and distance <= max_distance:\n            min_distance = distance\n            closest_sequence = sequence\n    \n    return closest_sequence, min_distance"
    },
    {
        "function_name": "employee_placement",
        "file_name": "placement.py",
        "parameters": {
            "`num_employees`": "An integer representing the number of employees",
            "`num_departments`": "An integer representing the number of departments",
            "`employees_departments`": "A list of tuples representing the employees departments",
            "`department_min_size`": "An integer representing the minimum department size",
            "`employee_load`": "A list of integers representing the load of each employee"
        },
        "objectives": [
            "Implement the Hungarian algorithm to assign employees to departments such that the load is minimized",
            "Use the bin packing algorithm to pack employees into departments of minimum size",
            "Return the assignment of employees to departments and the minimum load"
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.optimize import linear_sum_assignment"
        ],
        "function_def": "def employee_placement(num_employees, num_departments, employees_departments, department_min_size, employee_load):\n    cost_matrix = np.zeros((num_employees, num_departments))\n    \n    for i, (employee, departments) in enumerate(employees_departments):\n        for department in departments:\n            cost_matrix[i, department-1] = -employee_load[i]\n    \n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    \n    assignment = {}\n    \n    for i, j in zip(row_ind, col_ind):\n        if len(assignment.setdefault(j+1, [])) < department_min_size:\n            assignment[j+1].append(i+1)\n    \n    min_load = 0\n    \n    for employees in assignment.values():\n        load = sum(employee_load[i-1] for i in employees)\n        min_load = max(min_load, load)\n        \n    return assignment, min_load"
    },
    {
        "function_name": "markov_chain",
        "file_name": "markov_chain.py",
        "parameters": {
            "`populations`": "A list of populations",
            "`transitions`": "A list of transition matrices",
            "`income`": "A list of income values"
        },
        "objectives": [
            "Implement the Markov chain algorithm to calculate the transition probabilities",
            "Use the power method to calculate the stationary distribution",
            "Calculate the expected income based on the stationary distribution"
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def markov_chain(populations, transitions, income):\n    stationary_distributions = []\n    \n    for transition in transitions:\n        stationary_distribution = np.linalg.matrix_power(transition, 1000)[0]\n        stationary_distributions.append(stationary_distribution)\n        \n    expected_income = np.dot(stationary_distributions, income)\n    \n    return stationary_distributions, expected_income"
    },
    {
        "function_name": "k_means",
        "file_name": "clustering.py",
        "parameters": {
            "`ITHETA`": "A list of tuples representing the elements in ITHETA",
            "`IR`": "A list of lists representing the elements in IR",
            "`allData`": "A list of lists representing the elements in allData",
            "`.Take**": "A list of tuples representing the subset of indices to be selected from allData**"
        },
        "objectives": [
            "Implement k-means clustering to group the elements of allData based on the elements of ITHETA",
            "Use the distance metric from the Euclidean distance to calculate the distance between elements",
            "Return the cluster labels of the elements in .Take***"
        ],
        "import_lines": [
            "from sklearn.cluster import KMeans",
            "import numpy as np"
        ],
        "function_def": "def k_means(ITHETA, IR, allData, Take):\n    # Convert lists to NumPy arrays\n    ITHETA_array = np.array(ITHETA)\n    IR_array = np.array(IR)\n    allData_array = np.array(allData)\n    Take_array = np.array(Take)\n    \n    # Flatten Take_array\n    Take_flat = Take_array[:,1]\n    \n    # Select the relevant data from allData based on Take\n    selected_data = allData_array[Take_flat]\n    \n    # Perform k-means clustering\n    kmeans = KMeans(n_clusters=10)\n    kmeans.fit(selected_data)\n    labels = kmeans.labels_\n    \n    return labels"
    },
    {
        "function_name": "meme_motif_discovery",
        "file_name": "motif_discovery.py",
        "parameters": {
            "`sequences`": "A list of DNA sequences as strings.",
            "`motif_length`": "An integer representing the length of the motif to find."
        },
        "objectives": [
            "Implement the MEME algorithm to find a motif in the input DNA sequences.",
            "Use a probabilistic approach to model the motif.",
            "Initialize the motif randomly from the input sequences.",
            "Iterate until convergence or a maximum number of iterations.",
            "Return the most likely motif."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def meme_motif_discovery(sequences, motif_length):\n    num_sequences, sequence_length = len(sequences), len(sequences[0])\n    motif = [random.choice('ACGT') for _ in range(motif_length)]\n    \n    max_iterations = 1000\n    for _ in range(max_iterations):\n        new_motif = [None] * motif_length\n        for i in range(motif_length):\n            counts = {'A': 0, 'C': 0, 'G': 0, 'T': 0}\n            for sequence in sequences:\n                counts[sequence[i]] += 1\n            new_motif[i] = max(counts, key=counts.get)\n        \n        if new_motif == motif:\n            break\n        \n        motif = new_motif\n    \n    return ''.join(motif)"
    },
    {
        "function_name": "matrix_statistics",
        "file_name": "matrix_statistics.py",
        "parameters": {
            "`matrix`": "A 2D list of integers",
            "`row_index`": "An integer",
            "`col_index`": "An integer",
            "`window_size`": "An integer"
        },
        "objectives": [
            "Use the `numpy` library to calculate the mean and standard deviation of the submatrix of `matrix` starting at `row_index` and `col_index` with a window size of `window_size`.",
            "Return the mean and standard deviation of the submatrix."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def matrix_statistics(matrix, row_index, col_index, window_size):\n    submatrix = np.array(matrix[row_index:row_index + window_size, col_index:col_index + window_size])\n    mean = np.mean(submatrix)\n    std_dev = np.std(submatrix)\n    \n    return mean, std_dev"
    },
    {
        "function_name": "spectral_clustering",
        "file_name": "graph_clustering.py",
        "parameters": {
            "`adjacency_matrix`": "A 2D list of integers representing the adjacency matrix of a graph",
            "`num_of_clusters`": "An integer representing the number of clusters to form",
            "`threshold`": "A float representing the minimum similarity threshold for two nodes to be in the same cluster"
        },
        "objectives": [
            "Implement the Spectral Clustering algorithm to cluster the nodes in the graph into `num_of_clusters` clusters.",
            "Use the adjacency matrix to calculate the similarity matrix.",
            "Apply the k-means algorithm to the eigenvectors of the Laplacian matrix to form the clusters.",
            "Return the cluster assignments for each node."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def spectral_clustering(adjacency_matrix, num_of_clusters, threshold):\n    # Calculate the similarity matrix\n    similarity_matrix = np.array(adjacency_matrix)\n    \n    # Calculate the Laplacian matrix\n    laplacian_matrix = np.diag(np.sum(similarity_matrix, axis=1)) - similarity_matrix\n    \n    # Calculate the eigenvectors of the Laplacian matrix\n    eigenvectors = np.linalg.eig(laplacian_matrix)[1]\n    \n    # Apply k-means to the eigenvectors\n    kmeans = KMeans(n_clusters=num_of_clusters)\n    cluster_assignments = kmeans.fit_predict(eigenvectors)\n    \n    return cluster_assignments"
    },
    {
        "function_name": "bayesian_coin_flip",
        "file_name": "probability_algorithms.py",
        "parameters": {
            "`_failure_rate`": "A float representing the failure rate of the coin.",
            "`n`": "An integer representing the number of trials.",
            "`observed_successes`": "An integer representing the number of observed successes."
        },
        "objectives": [
            "Calculate the probability of success using the Bayesian inference with a beta prior.",
            "Calculate the expected number of successes.",
            "Return the probability of success and the expected number of successes."
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.special import beta"
        ],
        "function_def": "def bayesian_coin_flip(failure_rate, n, observed_successes):\n    # Define the prior distribution\n    a = 1\n    b = 1\n    \n    # Update the prior with the observed data\n    a_post = a + observed_successes\n    b_post = b + n - observed_successes\n    \n    # Calculate the probability of success\n    prob_success = a_post / (a_post + b_post)\n    \n    # Calculate the expected number of successes\n    expected_successes = (a_post / (a_post + b_post)) * n\n    \n    return prob_success, expected_successes"
    },
    {
        "function_name": "max_benefit_subset",
        "file_name": "optimization_algorithms.py",
        "parameters": {
            "`ages`": "A list of integers representing the ages of the individuals.",
            "`budget`": "An integer representing the budget.",
            "`benefits`": "A list of integers representing the benefits of each year of life."
        },
        "objectives": [
            "Implement a variant of the 0/1 Knapsack Problem algorithm to select the subset of individuals with the maximum total benefit within the budget.",
            "Return the selected subset of individuals and the total benefit."
        ],
        "import_lines": [],
        "function_def": "def max_benefit_subset(ages, budget, benefits):\n    n = len(ages)\n    dp = [[0 for _ in range(budget + 1)] for _ in range(n + 1)]\n    \n    for i in range(1, n + 1):\n        for j in range(1, budget + 1):\n            if ages[i - 1] <= j:\n                dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - ages[i - 1]] + benefits[i - 1])\n            else:\n                dp[i][j] = dp[i - 1][j]\n    \n    selected_individuals = []\n    i, j = n, budget\n    while i > 0 and j > 0:\n        if dp[i][j] != dp[i - 1][j]:\n            selected_individuals.append(i - 1)\n            j -= ages[i - 1]\n        i -= 1\n    \n    max_benefit = dp[n][budget]\n    \n    return selected_individuals, max_benefit"
    },
    {
        "function_name": "convolve2d",
        "file_name": "computer_vision.py",
        "parameters": {
            "`matrix`": "A 2D matrix representing a 2D convolutional neural network filter.",
            "`image`": "A 2D matrix representing the input image.",
            "`stride`": "An integer representing the stride of the convolution operation.",
            "`padding`": "An integer representing the padding of the convolution operation."
        },
        "objectives": [
            "Perform a 2D convolution operation on the input image using the given filter.",
            "Apply the `stride` and `padding` values to the convolution operation.",
            "Return the output feature map."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def convolve2d(matrix, image, stride, padding):\n    filter_height, filter_width = matrix.shape\n    image_height, image_width = image.shape\n    output_height = (image_height + 2 * padding - filter_height) // stride + 1\n    output_width = (image_width + 2 * padding - filter_width) // stride + 1\n    \n    # Initialize the output feature map\n    output = np.zeros((output_height, output_width))\n    \n    # Perform the convolution operation\n    for i in range(output_height):\n        for j in range(output_width):\n            output[i, j] = np.sum(matrix * image[i * stride:i * stride + filter_height, j * stride:j * stride + filter_width])\n    \n    return output"
    },
    {
        "function_name": "smith_waterman",
        "file_name": "sequence_alignment.py",
        "parameters": {
            "`sequences`": "A list of sequences (e.g. strings, lists, etc.)",
            "`threshold`": "The minimum similarity threshold"
        },
        "objectives": [
            "Implement the Smith-Waterman algorithm to find the maximum similarity between all pairs of sequences",
            "Use dynamic programming to efficiently compute the similarity scores",
            "Return all pairs of sequences with a similarity score above the threshold"
        ],
        "import_lines": [],
        "function_def": "def smith_waterman(sequences, threshold):\n    similarities = []\n    for i in range(len(sequences)):\n        for j in range(i + 1, len(sequences)):\n            seq1, seq2 = sequences[i], sequences[j]\n            dp = [[0] * (len(seq2) + 1) for _ in range(len(seq1) + 1)]\n            max_similarity = 0\n            for k in range(1, len(seq1) + 1):\n                for l in range(1, len(seq2) + 1):\n                    if seq1[k - 1] == seq2[l - 1]:\n                        score = dp[k - 1][l - 1] + 1\n                    else:\n                        score = max(dp[k - 1][l], dp[k][l - 1], 0)\n                    dp[k][l] = score\n                    max_similarity = max(max_similarity, score)\n            if max_similarity >= threshold:\n                similarities.append((seq1, seq2, max_similarity))\n    return similarities"
    },
    {
        "function_name": "dna_similarity",
        "file_name": "bioinformatics.py",
        "parameters": {
            "`sequences`": "A list of DNA sequences, where each sequence is a string of 'A', 'C', 'G', and 'T'",
            "`threshold`": "A float between 0 and 1, representing the minimum similarity score required",
            "`network`": "A dictionary representing a network, where each key is a node and its value is a list of neighboring nodes",
            "`source`": "A node in the network, representing the source of the information",
            "`target`": "A node in the network, representing the target of the information",
            "`max_hops`": "An integer, representing the maximum number of hops allowed to reach the target",
            "` stocks`": "A list of stock prices, where each price is a dictionary with keys 'symbol', 'name', and 'price'",
            "`budget`": "A float, representing the maximum amount of money available to invest",
            "`target_return`": "A float, representing the minimum target return on investment",
            "` texts`": "A list of text documents, where each document is a string",
            "`vocabulary`": "A set of words to include in the analysis",
            "`max_features`": "An integer, representing the maximum number of features to extract",
            "`emails`": "A list of emails, where each email is a dictionary with keys 'subject', 'body', and 'sender'",
            "`names`": "A list of names to extract from the emails",
            "`max_emails`": "An integer, representing the maximum number of emails to search"
        },
        "objectives": [
            "`threshold`: A float between 0 and 1, representing the minimum similarity score required",
            "Calculate the similarity score between each pair of sequences using the Jaccard similarity measure.",
            "Return the pairs of sequences with similarity scores above the threshold."
        ],
        "import_lines": [
            "from itertools import combinations",
            "from collections import Counter"
        ],
        "function_def": "def dna_similarity(sequences, threshold):\n    result = []\n    for seq1, seq2 in combinations(sequences, 2):\n        set1, set2 = set(seq1), set(seq2)\n        intersection = set1 & set2\n        union = set1 | set2\n        similarity = len(intersection) / len(union)\n        if similarity >= threshold:\n            result.append((seq1, seq2, similarity))\n    return result"
    },
    {
        "function_name": "network_bfs",
        "file_name": "network_analysis.py",
        "parameters": {
            "`network`": "A dictionary representing a network, where each key is a node and its value is a list of neighboring nodes",
            "`source`": "A node in the network, representing the source of the information",
            "`target`": "A node in the network, representing the target of the information",
            "`max_hops`": "An integer, representing the maximum number of hops allowed to reach the target",
            "` stocks`": "A list of stock prices, where each price is a dictionary with keys 'symbol', 'name', and 'price'",
            "`budget`": "A float, representing the maximum amount of money available to invest",
            "`target_return`": "A float, representing the minimum target return on investment",
            "` texts`": "A list of text documents, where each document is a string",
            "`vocabulary`": "A set of words to include in the analysis",
            "`max_features`": "An integer, representing the maximum number of features to extract",
            "`emails`": "A list of emails, where each email is a dictionary with keys 'subject', 'body', and 'sender'",
            "`names`": "A list of names to extract from the emails",
            "`max_emails`": "An integer, representing the maximum number of emails to search"
        },
        "objectives": [
            "`source`: A node in the network, representing the source of the information",
            "`target`: A node in the network, representing the target of the information",
            "`max_hops`: An integer, representing the maximum number of hops allowed to reach the target",
            "Use the Breadth-First Search (BFS) algorithm to find the shortest path from the source node to the target node.",
            "Keep track of the nodes visited during the search.",
            "If the target node is not reachable within the maximum number of hops, return None."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def network_bfs(network, source, target, max_hops):\n    visited = set()\n    queue = deque([(source, [source], 0)])\n    while queue:\n        node, path, hops = queue.popleft()\n        if node == target:\n            return path\n        if hops >= max_hops:\n            continue\n        for neighbor in network[node]:\n            if neighbor not in visited:\n                queue.append((neighbor, path + [neighbor], hops + 1))\n                visited.add(neighbor)\n    return None"
    },
    {
        "function_name": "stock_clustering",
        "file_name": "finance.py",
        "parameters": {
            "` stocks`": "A list of stock prices, where each price is a dictionary with keys 'symbol', 'name', and 'price'",
            "`budget`": "A float, representing the maximum amount of money available to invest",
            "`target_return`": "A float, representing the minimum target return on investment",
            "` texts`": "A list of text documents, where each document is a string",
            "`vocabulary`": "A set of words to include in the analysis",
            "`max_features`": "An integer, representing the maximum number of features to extract",
            "`emails`": "A list of emails, where each email is a dictionary with keys 'subject', 'body', and 'sender'",
            "`names`": "A list of names to extract from the emails",
            "`max_emails`": "An integer, representing the maximum number of emails to search"
        },
        "objectives": [
            "`budget`: A float, representing the maximum amount of money available to invest",
            "`target_return`: A float, representing the minimum target return on investment",
            "Use the K-Means clustering algorithm to group the stocks into clusters based on their prices and names.",
            "For each cluster, calculate the average price and the average name length.",
            "Select the cluster with the highest average price and the shortest average name length that satisfies the target return on investment.",
            "Return the stocks in the selected cluster."
        ],
        "import_lines": [
            "from sklearn.cluster import KMeans",
            "import numpy as np"
        ],
        "function_def": "def stock_clustering(stocks, budget, target_return):\n    prices = np.array([stock['price'] for stock in stocks])\n    names = np.array([len(stock['name']) for stock in stocks])\n    clusters = KMeans(n_clusters=5).fit(np.column_stack((prices, names)))\n    cluster_centers = clusters.cluster_centers_\n    labels = clusters.labels_\n    best_cluster = None\n    best_price = 0\n    best_name_length = float('inf')\n    for i, center in enumerate(cluster_centers):\n        cluster_stocks = [stock for j, stock in enumerate(stocks) if labels[j] == i]\n        cluster_prices = np.array([stock['price'] for stock in cluster_stocks])\n        cluster_name_lengths = np.array([len(stock['name']) for stock in cluster_stocks])\n        avg_price = np.mean(cluster_prices)\n        avg_name_length = np.mean(cluster_name_lengths)\n        if avg_price > best_price and avg_name_length < best_name_length and avg_price * len(cluster_stocks) <= budget:\n            best_cluster = cluster_stocks\n            best_price = avg_price\n            best_name_length = avg_name_length\n            if best_price / budget >= target_return:\n                break\n    return best_cluster if best_cluster is not None else []"
    },
    {
        "function_name": "email_extraction",
        "file_name": "email_analysis.py",
        "parameters": {
            "`emails`": "A list of emails, where each email is a dictionary with keys 'subject', 'body', and 'sender'",
            "`names`": "A list of names to extract from the emails",
            "`max_emails`": "An integer, representing the maximum number of emails to search"
        },
        "objectives": [
            "`names`: A list of names to extract from the emails",
            "`max_emails`: An integer, representing the maximum number of emails to search",
            "Use regular expressions to extract the names from the email subjects and bodies.",
            "Keep track of the number of emails that match each name.",
            "Sort the names by their frequency and return the top max_emails names."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def email_extraction(emails, names, max_emails):\n    name_counts = {name: 0 for name in names}\n    for email in emails:\n        subject = email['subject']\n        body = email['body']\n        for name in names:\n            if re.search(name, subject + body):\n                name_counts[name] += 1\n    sorted_names = sorted(name_counts.items(), key=lambda x: x[1], reverse=True)\n    return [name for name, count in sorted_names[:max_emails]]"
    },
    {
        "function_name": "window_median",
        "file_name": "window_statistics.py",
        "parameters": {
            "`n`": "An integer representing the number of elements in the array",
            "`arr`": "A list of integers representing the array",
            "`window_size`": "An integer representing the size of the window",
            "`threshold`": "An integer representing the minimum value of the window"
        },
        "objectives": [
            "Divide the array into windows of size window_size.",
            "For each window, calculate the median of the elements in the window.",
            "Return a list of integers representing the indices of the windows with median not less than the threshold."
        ],
        "import_lines": [
            "import statistics"
        ],
        "function_def": "def window_median(n, arr, window_size, threshold):\n    medians = []\n    for i in range(n - window_size + 1):\n        window = arr[i:i + window_size]\n        median = statistics.median(window)\n        if median >= threshold:\n            medians.append(i)\n    return medians"
    },
    {
        "function_name": "most_frequent_subsequence",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "`sequences`": "A list of lists of integers representing the input sequences",
            "`threshold`": "An integer representing the minimum frequency of the subsequence"
        },
        "objectives": [
            "Use a sliding window approach to find the most frequent contiguous subsequence across all sequences",
            "Ensure that the frequency of the subsequence found is at least the minimum frequency",
            "Return the most frequent subsequence and its frequency"
        ],
        "import_lines": [],
        "function_def": "def most_frequent_subsequence(sequences, threshold):\n    max_length = min(len(sequence) for sequence in sequences)\n    max_frequency = 0\n    best_subsequence = None\n    \n    for window_size in range(1, max_length + 1):\n        for sequence in sequences:\n            for start in range(len(sequence) - window_size + 1):\n                subsequence = sequence[start:start + window_size]\n                frequency = sum(all(subsequence == sequence[i:i + window_size] for i in range(len(sequence) - window_size + 1)) for sequence in sequences)\n                if frequency > max_frequency and frequency >= threshold:\n                    max_frequency = frequency\n                    best_subsequence = subsequence\n    \n    return best_subsequence, max_frequency"
    },
    {
        "function_name": "memory_allocation",
        "file_name": "memory_management.py",
        "parameters": {
            "`memory`": "A list of integers representing the available memory blocks",
            "`requests`": "A list of integers representing the memory requests"
        },
        "objectives": [
            "Use the First-Fit algorithm to allocate the memory requests to the available memory blocks",
            "Ensure that the total memory allocated does not exceed the total available memory",
            "Return the allocation of memory requests to memory blocks"
        ],
        "import_lines": [],
        "function_def": "def memory_allocation(memory, requests):\n    allocation = {request: None for request in requests}\n    Memoryessoa = {block: None for block in memory}\n    for request in requests:\n        for block in memory:\n            if request <= block and Memoryessoa[block] is None:\n                allocation[request] = block\n                Memoryessoa[block] = request\n                break\n    return allocation"
    },
    {
        "function_name": "gibbs_motif_finder",
        "file_name": "bioinformatics.py",
        "parameters": {
            "`sequences`": "A list of strings representing DNA sequences",
            "`k`": "An integer representing the length of the motifs to find"
        },
        "objectives": [
            "Implement a Gibbs sampling algorithm to find the most probable motifs in the DNA sequences.",
            "Initialize the motifs randomly and update them iteratively based on the probabilities of each position.",
            "Use a position-specific scoring matrix (PSSM) to score the motifs.",
            "Return the most probable motifs."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def gibbs_motif_finder(sequences, k):\n    motifs = [random.choice(seq) for seq in sequences]\n    while True:\n        pssm = [[0 for _ in range(4)] for _ in range(k)]\n        for motif in motifs:\n            for i, base in enumerate(motif):\n                if base == 'A':\n                    pssm[i][0] += 1\n                elif base == 'C':\n                    pssm[i][1] += 1\n                elif base == 'G':\n                    pssm[i][2] += 1\n                elif base == 'T':\n                    pssm[i][3] += 1\n        new_motifs = []\n        for seq in sequences:\n            scores = []\n            for i in range(len(seq) - k + 1):\n                motif = seq[i:i + k]\n                score = 0\n                for j, base in enumerate(motif):\n                    if base == 'A':\n                        score += pssm[j][0]\n                    elif base == 'C':\n                        score += pssm[j][1]\n                    elif base == 'G':\n                        score += pssm[j][2]\n                    elif base == 'T':\n                        score += pssm[j][3]\n                scores.append((motif, score))\n            new_motifs.append(max(scores, key=lambda x: x[1])[0])\n        if new_motifs == motifs:\n            break\n        motifs = new_motifs\n    return motifs"
    },
    {
        "function_name": "svd_dimensionality_reduction",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`arrays`": "A list of 2D lists representing matrices",
            "`k`": "An integer representing the number of principal components to retain"
        },
        "objectives": [
            "Implement a Singular Value Decomposition (SVD) algorithm to reduce the dimensionality of the matrices.",
            "Use the SVD to transform the matrices into a lower-dimensional space.",
            "Retain the top k principal components.",
            "Return the transformed matrices."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def svd_dimensionality_reduction(arrays, k):\n    matrices = [np.array(array) for array in arrays]\n    svd_matrices = []\n    for matrix in matrices:\n        u, s, vh = np.linalg.svd(matrix)\n        u_reduced = u[:, :k]\n        s_reduced = s[:k]\n        vh_reduced = vh[:k, :]\n        svd_matrix = u_reduced @ np.diag(s_reduced) @ vh_reduced\n        svd_matrices.append(svd_matrix)\n    return svd_matrices"
    },
    {
        "function_name": "lsa_topic_identifier",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`text`": "A string representing the input text",
            "`n`": "An integer representing the number of topics to identify"
        },
        "objectives": [
            "Implement a Latent Semantic Analysis (LSA) algorithm to identify topics in the text.",
            "Use a term-document matrix to represent the text.",
            "Apply Singular Value Decomposition (SVD) to the term-document matrix to reduce its dimensionality.",
            "Identify the topics by clustering the reduced matrix.",
            "Return the identified topics."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def lsa_topic_identifier(text, n):\n    # Tokenize the text into words\n    words = text.split()\n    \n    # Create the term-document matrix\n    term_document_matrix = np.zeros((len(words), len(words)))\n    for i, word in enumerate(words):\n        for j, other_word in enumerate(words):\n            if word == other_word:\n                term_document_matrix[i, j] = 1\n    \n    # Apply SVD to the term-document matrix\n    u, s, vh = np.linalg.svd(term_document_matrix)\n    \n    # Reduce the dimensionality of the matrix\n    reduced_matrix = u[:, :n] @ np.diag(s[:n]) @ vh[:n, :]\n    \n    # Cluster the reduced matrix to identify topics\n    topics = []\n    for i in range(n):\n        topic = []\n        for j, value in enumerate(reduced_matrix[:, i]):\n            if value > 0:\n                topic.append(words[j])\n        topics.append(topic)\n    \n    return topics"
    },
    {
        "function_name": "pattern_matcher",
        "file_name": "pattern_matcher.py",
        "parameters": {
            "`strings`": "A list of strings",
            "`pattern`": "A string representing a regular expression pattern",
            "`threshold`": "An integer representing the minimum number of matches"
        },
        "objectives": [
            "Use the `re` library to find all strings that match the given pattern.",
            "Calculate the total number of matches for each string.",
            "Return the strings with at least `threshold` matches and the total number of matches for each string."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def pattern_matcher(strings, pattern, threshold):\n    results = []\n    for string in strings:\n        matches = len(re.findall(pattern, string))\n        if matches >= threshold:\n            results.append((string, matches))\n    return results"
    },
    {
        "function_name": "matrix_rotator",
        "file_name": "matrix_rotator.py",
        "parameters": {
            "`matrix`": "A 2D list of integers",
            "`threshold`": "An integer representing the threshold value"
        },
        "objectives": [
            "Use the concept of matrix rotation to rotate the matrix 90 degrees clockwise.",
            "Calculate the sum of the elements in the rotated matrix.",
            "If the sum exceeds the threshold, rotate the matrix another 90 degrees clockwise.",
            "Return the final rotated matrix and the sum of its elements."
        ],
        "import_lines": [],
        "function_def": "def matrix_rotator(matrix, threshold):\n    sum_matrix = sum(sum(row) for row in matrix)\n    if sum_matrix > threshold:\n        matrix = [list(reversed(x)) for x in zip(*matrix)]\n        sum_matrix = sum(sum(row) for row in matrix)\n    return matrix, sum_matrix"
    },
    {
        "function_name": "bit_counter",
        "file_name": "bit_counter.py",
        "parameters": {
            "`bit_strings`": "A list of strings representing binary numbers",
            "`threshold`": "An integer representing the threshold value"
        },
        "objectives": [
            "Use bitwise operations to count the number of bits set in each binary number.",
            "Return the binary numbers with at least `threshold` bits set."
        ],
        "import_lines": [],
        "function_def": "def bit_counter(bit_strings, threshold):\n    results = []\n    for bit_string in bit_strings:\n        count = bin(int(bit_string, 2)).count('1')\n        if count >= threshold:\n            results.append(bit_string)\n    return results"
    },
    {
        "function_name": "longest_sub_array_sum",
        "file_name": "array_processing.py",
        "parameters": {
            "`arr`": "list of integers representing the array",
            "`k`": "integer representing the size of the sub-array"
        },
        "objectives": [
            "Find the longest sub-array with a sum equal to k.",
            "Use a prefix sum array and a hash table to efficiently calculate the sum of any sub-array and look up prefix sums.",
            "Keep track of the longest sub-array found and return the result."
        ],
        "import_lines": [],
        "function_def": "def longest_sub_array_sum(arr, k):\n    prefix_sum = {0: -1}\n    current_sum = 0\n    max_length = 0\n    start = 0\n    for end, num in enumerate(arr):\n        current_sum += num\n        if current_sum - k in prefix_sum:\n            start = prefix_sum[current_sum - k] + 1\n            max_length = max(max_length, end - start + 1)\n        prefix_sum[current_sum] = end\n    return max_length"
    },
    {
        "function_name": "knuth_morris_pratt",
        "file_name": "pattern_recognition.py",
        "parameters": {
            "`sequence`": "A list of integers representing the sequence to search for patterns",
            "`pattern`": "A list of integers representing the pattern to search for"
        },
        "objectives": [
            "Use the Knuth-Morris-Pratt algorithm to search for the pattern in the sequence.",
            "Preprocess the pattern to improve the efficiency of the algorithm.",
            "Return the starting index of the pattern in the sequence if found."
        ],
        "import_lines": [],
        "function_def": "def knuth_morris_pratt(sequence, pattern):\n    n = len(sequence)\n    m = len(pattern)\n    lps = [0] * m\n    j = 0\n    \n    for i in range(1, m):\n        if pattern[i] == pattern[j]:\n            j += 1\n            lps[i] = j\n        else:\n            if j != 0:\n                j = lps[j - 1]\n            else:\n                lps[i] = 0\n    \n    i = j = 0\n    \n    while i < n:\n        if sequence[i] == pattern[j]:\n            i += 1\n            j += 1\n        \n        if j == m:\n            return i - m\n        \n        elif i < n and sequence[i] != pattern[j]:\n            if j != 0:\n                j = lps[j - 1]\n            else:\n                i += 1\n    \n    return -1"
    },
    {
        "function_name": "quickselect",
        "file_name": "value_partitioning.py",
        "parameters": {
            "`values`": "A list of integers representing the values to partition",
            "`threshold`": "The threshold value to partition the values based on"
        },
        "objectives": [
            "Use the QuickSelect algorithm to partition the values around the threshold value.",
            "Use recursion to improve the efficiency of the algorithm.",
            "Return the partitioned values."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def quickselect(values, threshold):\n    if len(values) <= 1:\n        return values\n    \n    pivot_index = random.randint(0, len(values) - 1)\n    pivot = values[pivot_index]\n    equal_values = [x for x in values if x == pivot]\n    greater_values = [x for x in values if x > pivot and x > threshold]\n    lesser_values = [x for x in values if x < pivot or (x > pivot and x <= threshold)]\n    \n    return quickselect(greater_values, threshold) + equal_values + quickselect(lesser_values, threshold)"
    },
    {
        "function_name": "influence_maximization",
        "file_name": "influence_maximization.py",
        "parameters": {
            "`network`": "adjacency matrix representing a network",
            "`source`": "node representing the source of the diffusion process",
            "`diffusion_rate`": "float representing the rate at which the diffusion process spreads",
            "`num_steps`": "integer representing the number of steps to simulate"
        },
        "objectives": [
            "Simulate the diffusion process in the network using the Independent Cascade model.",
            "At each step, the diffusion process spreads from the current set of active nodes to their neighbors with a probability equal to the diffusion rate.",
            "Calculate the influence of the source node based on the number of active nodes at the end of the simulation.",
            "Return the influence of the source node."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def influence_maximization(network, source, diffusion_rate, num_steps):\n    num_nodes = len(network)\n    active_nodes = {source}\n    for _ in range(num_steps):\n        new_active_nodes = set()\n        for active_node in active_nodes:\n            for neighbor in range(num_nodes):\n                if network[active_node, neighbor] == 1 and np.random.rand() < diffusion_rate:\n                    new_active_nodes.add(neighbor)\n        active_nodes.update(new_active_nodes)\n    return len(active_nodes)"
    },
    {
        "function_name": "extract_sentences",
        "file_name": "text_analysis.py",
        "parameters": {
            "`text`": "A string representing the input text.",
            "`n`": "An integer representing the number of sentences to extract.",
            "`keyword`": "A string representing the keyword to search for."
        },
        "objectives": [
            "Use regular expressions to split the text into sentences.",
            "Calculate the sentiment of each sentence using a sentiment analysis algorithm.",
            "Extract the top `n` sentences with the highest sentiment score that contain the `keyword`."
        ],
        "import_lines": [
            "import re",
            "from nltk.sentiment import SentimentIntensityAnalyzer"
        ],
        "function_def": "def extract_sentences(text, n, keyword):\n    # Initialize sentiment intensity analyzer\n    sia = SentimentIntensityAnalyzer()\n    \n    # Split text into sentences\n    sentences = re.split(r'[.!?]', text)\n    \n    # Calculate sentiment of each sentence and store in a list of tuples\n    sentiment_sentences = [(sentence, sia.polarity_scores(sentence)['compound']) for sentence in sentences if keyword in sentence]\n    \n    # Sort the list of tuples based on sentiment score\n    sentiment_sentences.sort(key=lambda x: x[1], reverse=True)\n    \n    # Extract the top n sentences\n    top_sentences = [sentence for sentence, _ in sentiment_sentences[:n]]\n    \n    return top_sentences"
    },
    {
        "function_name": "convolutional_blur",
        "file_name": "image_processing.py",
        "parameters": {
            "`image`": "A 2D array representing an image",
            "`filter_size`": "An integer representing the size of the filter"
        },
        "objectives": [
            "Implement a convolutional filter to blur the image.",
            "Use a Gaussian distribution as the filter kernel.",
            "Return the blurred image."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def convolutional_blur(image, filter_size):\n    # Create a Gaussian filter kernel\n    sigma = filter_size / 2\n    kernel = np.zeros((filter_size, filter_size))\n    for i in range(filter_size):\n        for j in range(filter_size):\n            kernel[i, j] = np.exp(-((i - filter_size // 2) ** 2 + (j - filter_size // 2) ** 2) / (2 * sigma ** 2))\n    kernel /= kernel.sum()\n    \n    # Pad the image to handle edges\n    padded_image = np.pad(image, filter_size // 2, mode='reflect')\n    \n    # Apply the filter to the image\n    blurred_image = np.zeros(image.shape)\n    for i in range(image.shape[0]):\n        for j in range(image.shape[1]):\n            blurred_image[i, j] = np.sum(padded_image[i:i + filter_size, j:j + filter_size] * kernel)\n    \n    return blurred_image"
    },
    {
        "function_name": "linear_threshold_model",
        "file_name": "social_network.py",
        "parameters": {
            "`network`": "A dictionary representing a social network with weighted edges",
            "`influence_threshold`": "A float representing the influence threshold"
        },
        "objectives": [
            "Implement the Linear Threshold Model to simulate the spread of influence in the network.",
            "Use a random order of node activations.",
            "Return the final set of influenced nodes."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def linear_threshold_model(network, influence_threshold):\n    # Initialize the set of influenced nodes\n    influenced_nodes = set()\n    \n    # Initialize the set of activated nodes\n    activated_nodes = set()\n    \n    # Randomly order the nodes\n    nodes = list(network.keys())\n    random.shuffle(nodes)\n    \n    # Activate nodes iteratively\n    for node in nodes:\n        # Check if the node is influenced\n        if sum(network[node][neighbor] for neighbor in influenced_nodes) >= influence_threshold:\n            influenced_nodes.add(node)\n            activated_nodes.add(node)\n    \n    # Return the final set of influenced nodes\n    return influenced_nodes"
    },
    {
        "function_name": "histogram",
        "file_name": "data_analysis.py",
        "parameters": {
            "`num_bins`": "An integer representing the number of bins in the histogram.",
            "`data`": "A list of integers representing the data to be histogrammed.",
            "`bin_size`": "An integer representing the size of each bin."
        },
        "objectives": [
            "Calculate the minimum and maximum values in the data to determine the range of the histogram.",
            "Create a histogram with the specified number of bins and bin size.",
            "Calculate the frequency of each bin."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def histogram(num_bins, data, bin_size):\n    min_value = min(data)\n    max_value = max(data)\n    bins = np.arange(min_value, max_value + bin_size, bin_size)\n    frequencies = np.histogram(data, bins)[0]\n    return frequencies"
    },
    {
        "function_name": "equation_solver",
        "file_name": "algebraic_solvers.py",
        "parameters": {
            "`equation`": "A string representing the equation to be solved.",
            "`x`": "A float representing the value of x."
        },
        "objectives": [
            "Parse the equation to extract the coefficients and constants.",
            "Evaluate the equation using the given value of x.",
            "Return the result of the equation."
        ],
        "import_lines": [
            "import sympy as sp"
        ],
        "function_def": "def equation_solver(equation, x):\n    equation = equation.replace('^', '**')\n    x_value = sp.sympify(equation).subs('x', sp.symbols('x')).subs(sp.symbols('x'), x)\n    return x_value"
    },
    {
        "function_name": "text_processor",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`text`": "A string representing the text to be processed.",
            "`stopwords`": "A list of strings representing the stopwords to be removed."
        },
        "objectives": [
            "Tokenize the text into individual words.",
            "Remove the stopwords from the tokenized text.",
            "Calculate the frequency of each word in the text."
        ],
        "import_lines": [
            "from collections import Counter",
            "import re"
        ],
        "function_def": "def text_processor(text, stopwords):\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    words = [word for word in words if word not in stopwords]\n    frequencies = Counter(words)\n    return frequencies"
    },
    {
        "function_name": "execute_actions",
        "file_name": "text_analysis.py",
        "parameters": {
            "`num`": "Integer",
            "`have_repr`": "String",
            "`want_pos_int`": "Integer",
            "`allow_negospel`": "Boolean",
            "`have_actions`": "List of strings"
        },
        "objectives": [
            "Given a text, count how many times the string `have_repr` appears in it and is immediately followed by a positive integer.",
            "If `allow_negospel` is True, also consider the case where the integer is negative.",
            "Use the calculated count to determine how many times to execute each action in `have_actions`.",
            "Return the list of executed actions."
        ],
        "import_lines": [],
        "function_def": "def execute_actions(num, have_repr, want_pos_int, allow_negospel, have_actions):\n    import re\n    \n    count = 0\n    if allow_negospel:\n        pattern = re.compile(r'{}([-+]?\\d+)'.format(have_repr))\n    else:\n        pattern = re.compile(r'{}(\\+?\\d+)'.format(have_repr))\n        \n    for match in pattern.finditer(have_repr):\n        count += 1\n        \n    executed_actions = []\n    for action in have_actions:\n        executed_actions.extend([action] * count)\n        \n    return executed_actions"
    },
    {
        "function_name": "generate_random_forest",
        "file_name": "random_forest.py",
        "parameters": {
            "`num_trees`": "Integer",
            "`grow.Disabled`": "List of strings"
        },
        "objectives": [
            "Generate a random forest with `num_trees` decision trees.",
            "Each decision tree should have a random number of nodes between 5 and 10.",
            "The `grow.Disabled` list contains the names of features that should not be used in the decision trees.",
            "Use the features from the `grow Disabled` list to create a set of rules for each decision tree.",
            "Return the rules for each decision tree."
        ],
        "import_lines": [
            "import numpy as np",
            "import random"
        ],
        "function_def": "def generate_random_forest(num_trees, grow_disabled):\n    random_forest = []\n    \n    for _ in range(num_trees):\n        num_nodes = random.randint(5, 10)\n        tree = {}\n        \n        # Create a random tree structure\n        for i in range(num_nodes):\n            tree[i] = {'feature': random.choice(['feature1', 'feature2', 'feature3'] + grow_disabled),\n                       'threshold': random.random(),\n                       'left': random.randint(0, num_nodes),\n                       'right': random.randint(0, num_nodes)}\n        \n        # Create rules from the tree structure\n        rules = []\n        for node in tree.values():\n            if node['feature'] not in grow_disabled:\n                rule = 'IF {} > {} THEN GO TO {}'.format(node['feature'], node['threshold'], node['left'])\n                rules.append(rule)\n        \n        random_forest.append(rules)\n    \n    return random_forest"
    },
    {
        "function_name": "generate_quiz",
        "file_name": "quiz_generator.py",
        "parameters": {
            "`quiz_length`": "Integer",
            "`questions`": "List of strings",
            "`options`": "List of lists of strings"
        },
        "objectives": [
            "Generate a quiz with `quiz_length` questions.",
            "Each question should be randomly selected from the `questions` list.",
            "For each question, create a set of answer options by randomly selecting `num_options` options from the corresponding list in the `options` list.",
            "Use a probability distribution to randomly select the correct answer for each question.",
            "Return the quiz with the questions, answer options, and correct answers."
        ],
        "import_lines": [
            "import numpy as np",
            "import random"
        ],
        "function_def": "def generate_quiz(quiz_length, questions, options):\n    quiz = []\n    \n    for _ in range(quiz_length):\n        question = random.choice(questions)\n        num_options = random.randint(2, 5)\n        answer_options = random.sample(options[questions.index(question)], num_options)\n        \n        # Use a probability distribution to select the correct answer\n        probabilities = np.random.dirichlet(np.ones(num_options), size=1)[0]\n        correct_answer_index = np.argmax(probabilities)\n        \n        quiz.append({'question': question,\n                     'options': answer_options,\n                     'correct_answer': answer_options[correct_answer_index]})\n    \n    return quiz"
    },
    {
        "function_name": "simulate_flooding",
        "file_name": "grid_simulation.py",
        "parameters": {
            "`elevations`": "2D list of integers",
            "`water_level`": "Integer"
        },
        "objectives": [
            "Given a 2D grid of elevations, simulate the flooding of the grid by raising the water level to the given height.",
            "Use a breadth-first search algorithm to find all the areas that are reachable by the water.",
            "Return the flooded grid with the water level marked as -1."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def simulate_flooding(elevations, water_level):\n    flooded_grid = [row[:] for row in elevations]\n    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n    \n    # Find all the cells that are initially flooded\n    flooded_cells = deque()\n    for i in range(len(elevations)):\n        for j in range(len(elevations[i])):\n            if elevations[i][j] <= water_level:\n                flooded_cells.append((i, j))\n    \n    # Perform a breadth-first search to find all the reachable cells\n    while flooded_cells:\n        x, y = flooded_cells.popleft()\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if (0 <= nx < len(elevations) and 0 <= ny < len(elevations[nx]) and\n                    elevations[nx][ny] <= water_level and flooded_grid[nx][ny] != -1):\n                flooded_grid[nx][ny] = -1\n                flooded_cells.append((nx, ny))\n    \n    return flooded_grid"
    },
    {
        "function_name": "optimal_trading",
        "file_name": "trading_strategy.py",
        "parameters": {
            "`prices`": "List of integers",
            "`target`": "Integer",
            "`state`": "List of integers",
            "`history`": "List of integers"
        },
        "objectives": [
            "Given a list of prices and a target price, use dynamic programming to find the optimal sequence of buy and sell actions to reach the target price.",
            "Consider the current state of the market, represented by the `state` list.",
            "Use the `history` list to keep track of the previously made decisions.",
            "Return the optimal sequence of actions and the final profit."
        ],
        "import_lines": [],
        "function_def": "def optimal_trading(prices, target, state, history):\n    dp = [[float('-inf')] * (target + 1) for _ in range(len(prices) + 1)]\n    dp[0][0] = 0\n    actions = []\n    \n    for i in range(1, len(prices) + 1):\n        for j in range(target + 1):\n            if dp[i - 1][j] != float('-inf'):\n                dp[i][j] = max(dp[i][j], dp[i - 1][j])\n                if prices[i - 1] <= j:\n                    dp[i][j] = max(dp[i][j], dp[i - 1][j - prices[i - 1]] + prices[i - 1])\n                    actions.append('Buy' if history == [] or history[-1] == 'Sell' else 'Hold')\n                else:\n                    actions.append('Sell' if history == [] or history[-1] == 'Buy' else 'Hold')\n    \n    profit = dp[-1][-1]\n    \n    return actions, profit"
    },
    {
        "function_name": "kmeans_clustering",
        "file_name": "kmeans_clustering.py",
        "parameters": {
            "`num_clusters`": "int",
            "`data`": "A 2D list representing the dataset",
            "`max_iterations`": "int",
            "`tolerance`": "float"
        },
        "objectives": [
            "Implement a function to cluster the data using the K-Means clustering algorithm.",
            "Initialize the centroids randomly and update them iteratively based on the mean of the assigned data points.",
            "Assign each data point to the cluster with the closest centroid.",
            "Repeat the process until the centroids converge or the maximum number of iterations is reached."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def kmeans_clustering(num_clusters, data, max_iterations, tolerance):\n    np.random.seed(0)\n    centroids = data[np.random.choice(range(len(data)), num_clusters, replace=False)]\n    for _ in range(max_iterations):\n        assignments = np.argmin(np.linalg.norm(data[:, np.newaxis] - centroids, axis=2), axis=1)\n        new_centroids = np.array([data[assignments == i].mean(axis=0) for i in range(num_clusters)])\n        if np.linalg.norm(centroids - new_centroids) < tolerance:\n            break\n        centroids = new_centroids\n    return centroids, assignments"
    },
    {
        "function_name": "svd_decomposition",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`matrix`": "A 2D list representing the matrix",
            "`threshold`": "float"
        },
        "objectives": [
            "Implement a function to perform singular value decomposition (SVD) on the given matrix.",
            "Compute the left and right singular vectors and the singular values.",
            "Threshold the singular values such that only values greater than the threshold are retained."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def svd_decomposition(matrix, threshold):\n    u, s, vh = np.linalg.svd(matrix)\n    s = np.where(s > threshold, s, 0)\n    return u, s, vh"
    },
    {
        "function_name": "schedule_task",
        "file_name": "task_scheduler.py",
        "parameters": {
            "`schedule`": "A list of tuples representing a schedule of tasks with their start and end times",
            "`new_task`": "A tuple representing a new task with its start and end time"
        },
        "objectives": [
            "Implement a scheduling algorithm to determine if the new task can be added to the existing schedule without conflicts.",
            "Use a greedy algorithm to check for conflicts with existing tasks.",
            "Return True if the new task can be added, False otherwise."
        ],
        "import_lines": [],
        "function_def": "def schedule_task(schedule, new_task):\n    schedule.sort(key=lambda x: x[0])\n    new_start, new_end = new_task\n    \n    for task in schedule:\n        start, end = task\n        if new_start < end and new_end > start:\n            return False\n    \n    return True"
    },
    {
        "function_name": "grid_analyzer",
        "file_name": "pathfinder.py",
        "parameters": {
            "`grids`": "A list of 2D lists of integers representing the grids to be analyzed",
            "`starting_positions`": "A list of tuples representing the starting positions in the grids"
        },
        "objectives": [
            "Use Dijkstra's algorithm to find the shortest paths in all grids from the starting positions to all other cells",
            "Calculate the total cost of following the shortest paths for each grid",
            "If the total cost exceeds a threshold of 1000, exclude the grid from the results",
            "Return the grids with their corresponding shortest paths and total costs"
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def grid_analyzer(grids, starting_positions):\n    results = []\n    for grid, start in zip(grids, starting_positions):\n        distances = [[float('inf')] * len(row) for row in grid]\n        distances[start[0]][start[1]] = 0\n        paths = [[None] * len(row) for row in grid]\n        queue = [(0, start[0], start[1])]\n        while queue:\n            dist, x, y = heapq.heappop(queue)\n            if dist > distances[x][y]:\n                continue\n            for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < len(grid) and 0 <= ny < len(grid[0]) and grid[nx][ny] != 1:\n                    new_dist = dist + 1\n                    if new_dist < distances[nx][ny]:\n                        distances[nx][ny] = new_dist\n                        paths[nx][ny] = (x, y)\n                        heapq.heappush(queue, (new_dist, nx, ny))\n        total_cost = sum(distances[i][j] for i in range(len(grid)) for j in range(len(grid[0])) if grid[i][j] != 1)\n        if total_cost <= 1000:\n            results.append((grid, distances, paths, total_cost))\n    return results"
    },
    {
        "function_name": "paper_query",
        "file_name": "bayes_net.py",
        "parameters": {
            "`database`": "A list of dictionaries representing the database of authors and papers",
            "`query`": "A dictionary representing the query with 'author', 'year', and 'title' as keys"
        },
        "objectives": [
            "Implement a Bayesian network to infer the probability of a paper being written by a specific author given the query",
            "Use a naive Bayes classifier to classify the papers in the database based on the query",
            "Return a list of tuples containing the title of the paper and the probability of it being written by the specific author"
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def paper_query(database, query):\n    # randomly assign weights to the edges of the Bayes net for demonstration\n    weights = {\n        'author': 0.8,\n        'year': 0.6,\n        'title': 0.4\n    }\n    \n    results = []\n    for paper in database:\n        prob = 1\n        if 'author' in query and query['author'] in paper['authors']:\n            prob *= weights['author']\n        else:\n            prob *= (1 - weights['author'])\n        if 'year' in query and paper['year'] == query['year']:\n            prob *= weights['year']\n        else:\n            prob *= (1 - weights['year'])\n        if 'title' in query and query['title'] in paper['title']:\n            prob *= weights['title']\n        else:\n            prob *= (1 - weights['title'])\n        results.append((paper['title'], prob))\n    return results"
    },
    {
        "function_name": "decrypter",
        "file_name": "crypto.py",
        "parameters": {
            "`encrypted_text`": "A string representing the encrypted text",
            "`password`": "A string representing the password for decryption"
        },
        "objectives": [
            "Implement a basic decryption algorithm using frequency analysis",
            "Calculate the frequency of each character in the password",
            "Use the frequency of each character to shift the characters in the encrypted text",
            "Return the decrypted text"
        ],
        "import_lines": [],
        "function_def": "def decrypter(encrypted_text, password):\n    # calculate frequency of each character in the password\n    char_frequency = {}\n    for char in password:\n        if char in char_frequency:\n            char_frequency[char] += 1\n        else:\n            char_frequency[char] = 1\n    \n    # decrypt the text\n    decrypted_text = ''\n    password_index = 0\n    for char in encrypted_text:\n        if char.isalpha():\n            shift = char_frequency[password[password_index % len(password)]]\n            if char.isupper():\n                decrypted_text += chr((ord(char) - ord('A') - shift) % 26 + ord('A'))\n            else:\n                decrypted_text += chr((ord(char) - ord('a') - shift) % 26 + ord('a'))\n            password_index += 1\n        else:\n            decrypted_text += char\n    \n    return decrypted_text"
    },
    {
        "function_name": "most_frequent_words",
        "file_name": "text_analysis.py",
        "parameters": {
            "`text`": "A string representing the text to be processed.",
            "`n`": "An integer representing the number of most frequent words to return."
        },
        "objectives": [
            "Split the text into individual words.",
            "Remove non-alphanumeric characters from the words.",
            "Convert all words to lowercase.",
            "Calculate the frequency of each word in the text.",
            "Return the n most frequent words and their frequencies."
        ],
        "import_lines": [
            "from collections import Counter",
            "import re"
        ],
        "function_def": "def most_frequent_words(text, n):\n    words = re.findall(r'\\w+', text.lower())\n    frequency = Counter(words)\n    return frequency.most_common(n)"
    },
    {
        "function_name": "max_average_submatrix",
        "file_name": "matrix_analysis.py",
        "parameters": {
            "`matrix`": "A 2D list of integers",
            "`threshold`": "An integer representing the threshold for the average value"
        },
        "objectives": [
            "Calculate the average value of the entire matrix.",
            "Identify the sub-matrix with the maximum average value greater than the threshold.",
            "Return the sub-matrix with the maximum average value."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def max_average_submatrix(matrix, threshold):\n    max_average = 0\n    max_submatrix = None\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            for k in range(i, len(matrix)):\n                for end_col in range(j, len(matrix[0])):\n                    submatrix = [row[j:end_col+1] for row in matrix[i:k+1]]\n                    average = np.mean(submatrix)\n                    if average > max_average and average > threshold:\n                        max_average = average\n                        max_submatrix = submatrix\n    return max_submatrix"
    },
    {
        "function_name": "vigenere_cipher",
        "file_name": "cryptography.py",
        "parameters": {
            "`text`": "A string of text.",
            "`key`": "A list of characters to use for encryption."
        },
        "objectives": [
            "Implement a Vigen\u00e8re cipher to encrypt the given text using the provided key.",
            "Use a matrix of alphabets to find the encrypted character based on the corresponding key character.",
            "Return the encrypted text as a string."
        ],
        "import_lines": [],
        "function_def": "def vigenere_cipher(text, key):\n    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n    encrypted_text = ''\n    key_index = 0\n    \n    for char in text:\n        if char.isalpha():\n            shift = alphabet.index(key[key_index % len(key)].lower())\n            if char.isupper():\n                encrypted_text += alphabet[(alphabet.index(char.lower()) + shift) % 26].upper()\n            else:\n                encrypted_text += alphabet[(alphabet.index(char) + shift) % 26]\n            key_index += 1\n        else:\n            encrypted_text += char\n    \n    return encrypted_text"
    },
    {
        "function_name": "edge_detection",
        "file_name": "image_processing.py",
        "parameters": {
            "`image`": "A 2D list representing the image",
            "`threshold`": "A numerical value representing the threshold for edge detection"
        },
        "objectives": [
            "Use the Sobel operator to detect edges in the image",
            "Apply non-maximum suppression to thin the edges",
            "Return the edge-detected image"
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def edge_detection(image, threshold):\n    # Initialize Sobel kernels\n    kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n    kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n    \n    # Apply Sobel operator\n    grad_x = np.zeros(image.shape)\n    grad_y = np.zeros(image.shape)\n    for i in range(1, image.shape[0] - 1):\n        for j in range(1, image.shape[1] - 1):\n            grad_x[i, j] = np.sum(image[i-1:i+2, j-1:j+2] * kernel_x)\n            grad_y[i, j] = np.sum(image[i-1:i+2, j-1:j+2] * kernel_y)\n    \n    # Calculate gradient magnitude\n    grad_mag = np.sqrt(grad_x ** 2 + grad_y ** 2)\n    \n    # Apply non-maximum suppression\n    edges = np.zeros(image.shape)\n    for i in range(1, image.shape[0] - 1):\n        for j in range(1, image.shape[1] - 1):\n            if grad_mag[i, j] > threshold:\n                edges[i, j] = grad_mag[i, j]\n    \n    return edges"
    },
    {
        "function_name": "dbscan_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "`data`": "A list of tuples representing the data where each tuple contains a string and a numerical value",
            "`k`": "An integer representing the number of clusters"
        },
        "objectives": [
            "Use the DBSCAN algorithm to cluster the data into k clusters based on the numerical values",
            "Handle the case when the data contains noise",
            "Return the clustered data"
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def dbscan_clustering(data, k):\n    # Initialize epsilon and min_points parameters\n    epsilon = 0.5\n    min_points = 10\n    \n    # Calculate distances between data points\n    distances = np.zeros((len(data), len(data)))\n    for i in range(len(data)):\n        for j in range(i + 1, len(data)):\n            distances[i, j] = np.abs(data[i][1] - data[j][1])\n            distances[j, i] = distances[i, j]\n    \n    # Initialize cluster labels\n    labels = [0] * len(data)\n    \n    # Perform DBSCAN\n    for i in range(len(data)):\n        if labels[i] != 0:\n            continue\n        \n        # Find neighboring points\n        neighbors = [j for j in range(len(data)) if distances[i, j] <= epsilon]\n        \n        # Handle noise points\n        if len(neighbors) < min_points:\n            labels[i] = -1\n            continue\n        \n        # Create a new cluster\n        cluster_label = k\n        labels[i] = cluster_label\n        \n        # Expand the cluster\n        for neighbor in neighbors:\n            if labels[neighbor] == 0:\n                labels[neighbor] = cluster_label\n                new_neighbors = [j for j in range(len(data)) if distances[neighbor, j] <= epsilon]\n                if len(new_neighbors) >= min_points:\n                    neighbors.extend(new_neighbors)\n    \n    # Return the clustered data\n    clustered_data = []\n    for i in range(len(data)):\n        if labels[i] != -1:\n            clustered_data.append((data[i], labels[i]))\n    \n    return clustered_data"
    },
    {
        "function_name": "moving_average",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "`time_series`": "A list of integers representing a time series data",
            "`window_size`": "An integer representing the size of the sliding window"
        },
        "objectives": [
            "Implement a function to calculate the moving average of the time series data using a sliding window approach.",
            "Use a deque to efficiently keep track of the elements within the sliding window.",
            "Return the moving averages calculated for each window."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def moving_average(time_series, window_size):\n    window = deque(maxlen=window_size)\n    moving_averages = []\n    for i, value in enumerate(time_series):\n        window.append(value)\n        if len(window) == window_size:\n            moving_averages.append(sum(window) / window_size)\n    return moving_averages"
    },
    {
        "function_name": "regular_expression_match_lengths",
        "file_name": "string_algorithms.py",
        "parameters": {
            "`str`": "A string",
            "`pattern`": "A string"
        },
        "objectives": [
            "Perform a regular expression search on the string to find all occurrences of the pattern.",
            "Calculate the length of each match.",
            "Return a list of lengths."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def regular_expression_match_lengths(string, pattern):\n    matches = re.findall(pattern, string)\n    return [len(match) for match in matches]"
    },
    {
        "function_name": "max_subarray_sum",
        "file_name": "max_subarray_sum.py",
        "parameters": {
            "`arr`": "A list of integers",
            "`size`": "An integer representing the size of sub-arrays",
            "`queries`": "A list of queries where each query is a 2-tuple (L, R) representing the range of the sub-array to be processed"
        },
        "objectives": [
            "Divide the input array into sub-arrays of size 'size'.",
            "For each query (L, R), calculate the maximum sum of all sub-arrays that lie within the range [L, R].",
            "Return the total maximum sum."
        ],
        "import_lines": [],
        "function_def": "def max_subarray_sum(arr, size, queries):\n    n = len(arr)\n    subarrays = [arr[i:i+size] for i in range(0, n, size)]\n    subarray_sums = [sum(subarray) for subarray in subarrays]\n    max_sum = 0\n    \n    for L, R in queries:\n        for i in range(L, R+1):\n            if i % size == 0 and i + size <= R + 1:\n                max_sum = max(max_sum, subarray_sums[i//size])\n                \n    return max_sum"
    },
    {
        "function_name": "median_calculator",
        "file_name": "median_calculator.py",
        "parameters": {
            "`nums`": "A list of integers",
            "`k`": "An integer representing the size of sub-arrays"
        },
        "objectives": [
            "Divide the input list into sub-arrays of size 'k'.",
            "For each sub-array, calculate the median.",
            "Return the list of medians."
        ],
        "import_lines": [],
        "function_def": "def median_calculator(nums, k):\n    subarrays = [nums[i:i+k] for i in range(0, len(nums), k)]\n    medians = []\n    \n    for subarray in subarrays:\n        sorted_subarray = sorted(subarray)\n        n = len(sorted_subarray)\n        median = sorted_subarray[n//2] if n % 2 != 0 else (sorted_subarray[n//2-1] + sorted_subarray[n//2]) / 2\n        medians.append(median)\n        \n    return medians"
    },
    {
        "function_name": "binomial_transform",
        "file_name": "matrix_transform.py",
        "parameters": {
            "matrix": "A 2D list representing the input matrix",
            "threshold": "An integer representing the threshold value"
        },
        "objectives": [
            "Implement the Binomial Transform algorithm to calculate the binomial coefficients of the matrix.",
            "Use dynamic programming to fill up the binomial coefficient matrix.",
            "Return the binomial coefficient matrix."
        ],
        "import_lines": [],
        "function_def": "def binomial_transform(matrix, threshold):\n    rows, cols = len(matrix), len(matrix[0])\n    binomial_matrix = [[0] * cols for _ in range(rows)]\n    \n    for i in range(rows):\n        for j in range(cols):\n            if i == 0 or j == 0:\n                binomial_matrix[i][j] = matrix[i][j]\n            else:\n                binomial_matrix[i][j] = binomial_matrix[i - 1][j] + binomial_matrix[i][j - 1]\n    \n    for i in range(rows):\n        for j in range(cols):\n            if binomial_matrix[i][j] > threshold:\n                binomial_matrix[i][j] = 0\n    \n    return binomial_matrix"
    },
    {
        "function_name": "word_break",
        "file_name": "string_algorithms.py",
        "parameters": {
            "`s`": "string",
            "`words`": "list of strings representing a dictionary of words"
        },
        "objectives": [
            "Implement the word break algorithm to find all possible ways to segment the input string into words from the dictionary.",
            "Use dynamic programming to build a 1D table that stores the possible segmentations for each prefix of the input string.",
            "Return the list of all possible segmentations."
        ],
        "import_lines": [],
        "function_def": "def word_break(s, words):\n    dp = [[] for _ in range(len(s) + 1)]\n    dp[0] = [[]]\n    \n    for i in range(1, len(s) + 1):\n        for j in range(i):\n            if s[j:i] in words:\n                dp[i].extend([path + [s[j:i]] for path in dp[j]])\n                \n    return dp[-1]"
    },
    {
        "function_name": "longest_increasing_subsequence",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "`sequence`": "A list of integers representing the sequence",
            "`k`": "An integer representing the length of the subsequence"
        },
        "objectives": [
            "Find the longest increasing subsequence of length k in the given sequence.",
            "Use dynamic programming to build a 2D table of lengths of increasing subsequences.",
            "Use a binary search to find the longest increasing subsequence of length k."
        ],
        "import_lines": [],
        "function_def": "def longest_increasing_subsequence(sequence, k):\n    n = len(sequence)\n    table = [[1] * (n + 1) for _ in range(n + 1)]\n    \n    # Use dynamic programming to build a 2D table of lengths of increasing subsequences\n    for i in range(1, n + 1):\n        for j in range(1, n + 1):\n            if sequence[i - 1] > sequence[j - 1]:\n                table[i][j] = max(table[i - 1][j], table[i][j - 1] + 1)\n            else:\n                table[i][j] = table[i - 1][j]\n    \n    # Use a binary search to find the longest increasing subsequence of length k\n    low = 0\n    high = n - 1\n    while low < high:\n        mid = (low + high) // 2\n        if table[-1][mid] >= k:\n            high = mid\n        else:\n            low = mid + 1\n    \n    # Reconstruct the longest increasing subsequence\n    subsequence = []\n    i, j = n, low\n    while i > 0 and j > 0:\n        if table[i][j] == k:\n            subsequence.append(sequence[i - 1])\n            i -= 1\n            j -= 1\n        elif table[i - 1][j] == table[i][j]:\n            i -= 1\n        else:\n            j -= 1\n    \n    return subsequence[::-1]"
    },
    {
        "function_name": "nmf_factorization",
        "file_name": "matrix_factorization.py",
        "parameters": {
            "`matrix`": "A 2D list representing the matrix",
            "`threshold`": "float"
        },
        "objectives": [
            "Perform a non-negative matrix factorization (NMF) on the given matrix.",
            "Use an iterative algorithm to find the optimal factorization.",
            "Threshold the resulting factors such that only values greater than the threshold are retained."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def nmf_factorization(matrix, threshold):\n    n, m = len(matrix), len(matrix[0])\n    W = np.random.rand(n, n)  # basis matrix\n    H = np.random.rand(n, m)  # coefficient matrix\n    \n    # Use an iterative algorithm to find the optimal factorization\n    for _ in range(1000):\n        WH = np.dot(W, H)\n        W_new = W * np.dot(matrix, H.T) / np.dot(WH, H.T)\n        H_new = H * np.dot(W.T, matrix) / np.dot(W.T, WH)\n        W, H = W_new, H_new\n    \n    # Threshold the resulting factors\n    W = np.where(W > threshold, W, 0)\n    H = np.where(H > threshold, H, 0)\n    \n    return W, H"
    },
    {
        "function_name": "assign_teachers",
        "file_name": "student_teacher_assgn.py",
        "parameters": {
            "`students`": "List of integers representing the students' arrival times",
            "`teachers`": "List of integers representing the teachers' arrival times",
            "`k`": "Integer representing the number of teachers to be assigned"
        },
        "objectives": [
            "Use a greedy algorithm to assign teachers to students based on their arrival times.",
            "Return the maximum number of students that can be assigned to teachers.",
            "For each teacher, calculate the number of students assigned and return the assignments."
        ],
        "import_lines": [],
        "function_def": "def assign_teachers(students, teachers, k):\n    students.sort()\n    teachers.sort()\n    \n    assigned_students = 0\n    teacher_index = 0\n    \n    for student in students:\n        while teacher_index < len(teachers) and teachers[teacher_index] < student:\n            teacher_index += 1\n        \n        if teacher_index < len(teachers):\n            assigned_students += 1\n            teacher_index += 1\n    \n    return assigned_students"
    },
    {
        "function_name": "constrained_knapsack",
        "file_name": "constrained_knapsack.py",
        "parameters": {
            "`values`": "List of integers",
            "`weights`": "List of integers",
            "`capacity`": "Integer",
            "`min_weight`": "Integer",
            "`max_weight`": "Integer"
        },
        "objectives": [
            "Use dynamic programming to solve the 0/1 Knapsack problem with a twist.",
            "The function must maximize the total value while ensuring that the total weight is within a specified range (`min_weight` to `max_weight`).",
            "Return the maximum total value and the corresponding items."
        ],
        "import_lines": [],
        "function_def": "def constrained_knapsack(values, weights, capacity, min_weight, max_weight):\n    dp = [[-1 for _ in range(capacity + 1)] for _ in range(len(values) + 1)]\n    dp[0][0] = 0\n    for i in range(1, len(values) + 1):\n        for w in range(capacity + 1):\n            if weights[i - 1] <= w:\n                dp[i][w] = max(dp[i - 1][w], dp[i - 1][w - weights[i - 1]] + values[i - 1])\n            else:\n                dp[i][w] = dp[i - 1][w]\n    \n    max_value = -1\n    best_items = []\n    for w in range(min_weight, max_weight + 1):\n        if dp[-1][w] > max_value:\n            max_value = dp[-1][w]\n            best_items = [i - 1 for i in range(1, len(values) + 1) if dp[i][w] != dp[i - 1][w]]\n    \n    return max_value, best_items"
    },
    {
        "function_name": "node2vec",
        "file_name": "node2vec.py",
        "parameters": {
            "`graph`": "Dictionary where each key is a node and each value is a list of its neighbors",
            "`source`": "Node",
            "`num_walks`": "Integer",
            "`walk_length`": "Integer"
        },
        "objectives": [
            "Use random walks to estimate the similarity between nodes in the graph.",
            "The function must perform `num_walks` random walks of length `walk_length` starting from each node, including the `source` node.",
            "Return the similarity matrix."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def node2vec(graph, source, num_walks, walk_length):\n    # Initialize the similarity matrix\n    similarity = {node: {node: 0 for node in graph} for node in graph}\n    \n    # Perform random walks\n    for _ in range(num_walks):\n        current_node = source\n        walk = [current_node]\n        for _ in range(walk_length - 1):\n            neighbors = graph[current_node]\n            current_node = random.choice(neighbors)\n            walk.append(current_node)\n        # Update the similarity matrix\n        for node in walk:\n            for other_node in walk:\n                if node != other_node:\n                    similarity[node][other_node] += 1\n    \n    # Normalize the similarity matrix\n    for node in similarity:\n        total = sum(similarity[node].values())\n        for other_node in similarity[node]:\n            similarity[node][other_node] /= total\n    \n    return similarity"
    },
    {
        "function_name": "top_k",
        "file_name": "top_k.py",
        "parameters": {
            "`values`": "List of integers",
            "`k`": "Integer"
        },
        "objectives": [
            "Use the Top-K algorithm to find the `k` most frequent values in the list.",
            "The function must use a heap to efficiently select the top `k` values.",
            "Return the top `k` values and their frequencies."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def top_k(values, k):\n    # Use a dictionary to count the frequency of each value\n    frequency = {}\n    for value in values:\n        frequency[value] = frequency.get(value, 0) + 1\n    \n    # Use a heap to select the top k values\n    heap = []\n    for value, count in frequency.items():\n        heapq.heappush(heap, (count, value))\n        if len(heap) > k:\n            heapq.heappop(heap)\n    \n    # Return the top k values and their frequencies\n    top_k = []\n    while heap:\n        count, value = heapq.heappop(heap)\n        top_k.append((value, count))\n    \n    return top_k"
    },
    {
        "function_name": "polynomial_regression",
        "file_name": "regression_algorithms.py",
        "parameters": {
            "`regression_data`": "A list of tuples representing the regression data points.",
            "`polynomial_degree`": "An integer, representing the degree of the polynomial."
        },
        "objectives": [
            "Use the Polynomial Regression algorithm to fit the regression data points to a polynomial curve.",
            "Calculate the coefficients of the polynomial based on the regression data points.",
            "Return the fitted polynomial coefficients."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def polynomial_regression(regression_data, polynomial_degree):\n    x_values, y_values = zip(*regression_data)\n    x_values = np.array(x_values)\n    y_values = np.array(y_values)\n    coefficients = np.polyfit(x_values, y_values, polynomial_degree)\n    return coefficients"
    },
    {
        "function_name": "n_queens",
        "file_name": "backtracking.py",
        "parameters": {
            "`n`": "The number of queens",
            "`board`": "A 2D list representing the chessboard"
        },
        "objectives": [
            "Implement the N-Queens problem using Backtracking to place n queens on the board such that no two queens attack each other.",
            "Use a helper function to check if a queen can be placed at a given position on the board.",
            "Return the solution as a 2D list representing the final board configuration."
        ],
        "import_lines": [],
        "function_def": "def is_safe(board, row, col):\n    for i in range(row):\n        if board[i] == col or \\\n           board[i] - i == col - row or \\\n           board[i] + i == col + row:\n            return False\n    return True"
    },
    {
        "function_name": "word_frequency",
        "file_name": "text_analysis.py",
        "parameters": {
            "`text`": "A string of text",
            "`n`": "The number of most frequent words to return"
        },
        "objectives": [
            "Implement a Word Frequency Counter using a Hash Table to count the frequency of each word in the given text.",
            "Remove stop words and punctuation from the text before counting the word frequencies.",
            "Return the n most frequent words as a list of tuples containing the word and its frequency."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def word_frequency(text, n):\n    stop_words = set([\"a\", \"an\", \"the\", \"and\", \"is\", \"in\", \"it\", \"of\", \"to\"])\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    words = [word for word in words if word not in stop_words]\n    frequency = Counter(words)\n    return frequency.most_common(n)"
    },
    {
        "function_name": "node_degree_analyzer",
        "file_name": "graph_analyzer.py",
        "parameters": {
            "`graph`": "An adjacency list representing a graph",
            "`source_node`": "An integer representing the source node",
            "`min-degree`": "An integer representing the minimum degree of the nodes to consider"
        },
        "objectives": [
            "Implement a breadth-first search algorithm to traverse the graph",
            "Use a priority queue to prioritize the nodes based on their degree",
            "Return the nodes with a degree greater than or equal to the minimum degree, in the order they were visited"
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def node_degree_analyzer(graph, source_node, min_degree):\n    queue = deque([(source_node, 0)])\n    visited = set()\n    result = []\n    \n    while queue:\n        node, degree = queue.popleft()\n        if node not in visited:\n            visited.add(node)\n            if degree >= min_degree:\n                result.append(node)\n            for neighbor in graph[node]:\n                queue.append((neighbor, len(graph[neighbor])))\n    \n    return result"
    },
    {
        "function_name": "misra_gries_algorithm",
        "file_name": "misra_gries_algorithm.py",
        "parameters": {
            "`numbers`": "A list of numbers",
            "`threshold`": "A float representing the threshold for considering a number as a heavy hitter"
        },
        "objectives": [
            "Calculate the Misra-Gries algorithm to find the heavy hitters in the list of numbers.",
            "A number is considered a heavy hitter if its count exceeds the threshold.",
            "Return a dictionary where the keys are the heavy hitters and the values are their corresponding counts."
        ],
        "import_lines": [],
        "function_def": "def misra_gries_algorithm(numbers, threshold):\n    k = int(1 / threshold)\n    counts = {}\n    \n    for num in numbers:\n        if num not in counts:\n            counts[num] = 0\n        \n        counts[num] += 1\n        \n        if len(counts) > k:\n            decrement = min(counts.values())\n            \n            for num in list(counts.keys()):\n                counts[num] -= decrement\n                \n                if counts[num] == 0:\n                    del counts[num]\n    \n    heavy_hitters = {num: count for num, count in counts.items() if count > threshold * len(numbers)}\n    \n    return heavy_hitters"
    },
    {
        "function_name": "bfs_traversal",
        "file_name": "bfs_traversal.py",
        "parameters": {
            "`tree`": "A binary tree represented as a dictionary where each key is a node and its corresponding value is a list of its children",
            "`k`": "An integer representing the target depth"
        },
        "objectives": [
            "Traverse the binary tree using a breadth-first search (BFS) algorithm to find all nodes at depth k.",
            "Return a list of nodes at depth k.",
            "Calculate the sum of the values of all nodes at depth k."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def bfs_traversal(tree, k):\n    if not tree:\n        return []\n    \n    root = list(tree.keys())[0]\n    queue = deque([(root, 0)])\n    nodes_at_depth_k = []\n    \n    while queue:\n        node, depth = queue.popleft()\n        \n        if depth == k:\n            nodes_at_depth_k.append(node)\n        elif depth < k:\n            for child in tree[node]:\n                queue.append((child, depth + 1))\n    \n    return nodes_at_depth_k"
    },
    {
        "function_name": "rabin_karp",
        "file_name": "string_matching_algorithms.py",
        "parameters": {
            "`text`": "string representing the input text",
            "`pattern`": "string representing the pattern to find"
        },
        "objectives": [
            "Implement the Rabin-Karp algorithm to find all occurrences of the pattern in the text.",
            "Use a rolling hash function to efficiently compute the hash values of the substrings of the text.",
            "Return the list of all occurrences of the pattern in the text."
        ],
        "import_lines": [],
        "function_def": "def rabin_karp(text, pattern):\n    n = len(text)\n    m = len(pattern)\n    d = 256  # number of possible characters in the character set\n    q = 101  # a prime number\n    pattern_hash = 0\n    text_hash = 0\n    h = 1\n    \n    # Calculate h value, h = d^(m-1) % q\n    for _ in range(m - 1):\n        h = (h * d) % q\n    \n    # Calculate the initial hash values for the pattern and the first substring of the text\n    for i in range(m):\n        pattern_hash = (d * pattern_hash + ord(pattern[i])) % q\n        text_hash = (d * text_hash + ord(text[i])) % q\n    \n    occurrences = []\n    \n    # Compare the hash values of the pattern and the first substring of the text\n    for i in range(n - m + 1):\n        if pattern_hash == text_hash:\n            # If the hash values match, compare the characters of the pattern and the substring\n            match = True\n            for j in range(m):\n                if text[i + j] != pattern[j]:\n                    match = False\n                    break\n            if match:\n                occurrences.append(i)\n        \n        # Calculate the hash value for the next substring of the text\n        if i < n - m:\n            text_hash = (d * (text_hash - ord(text[i]) * h) + ord(text[i + m])) % q\n            if text_hash < 0:\n                text_hash += q\n    \n    return occurrences"
    },
    {
        "function_name": "jacobi_eigen",
        "file_name": "eigen_algorithms.py",
        "parameters": {
            "`matrix`": "2D list representing a square matrix",
            "`k`": "number of eigenvectors to find"
        },
        "objectives": [
            "Implement the Jacobi eigenvalue algorithm to find the eigenvalues and eigenvectors of the matrix.",
            "Use the QR algorithm to iteratively update the matrix and find the eigenvectors.",
            "Return the eigenvalues and eigenvectors of the matrix."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def jacobi_eigen(matrix, k):\n    n = len(matrix)\n    eigenvalues = np.zeros(k)\n    eigenvectors = np.eye(n)\n    \n    for _ in range(100):  # max iterations\n        max_off_diagonal = 0\n        p = 0\n        q = 0\n        \n        # Find the maximum off-diagonal element\n        for i in range(n):\n            for j in range(i + 1, n):\n                if abs(matrix[i][j]) > max_off_diagonal:\n                    max_off_diagonal = abs(matrix[i][j])\n                    p = i\n                    q = j\n        \n        # If the maximum off-diagonal element is small enough, stop\n        if max_off_diagonal < 1e-10:\n            break\n        \n        # Compute the Jacobi rotation matrix\n        tau = (matrix[q][q] - matrix[p][p]) / (2 * matrix[p][q])\n        t = 1 / (abs(tau) + np.sqrt(1 + tau ** 2))\n        c = 1 / np.sqrt(1 + t ** 2)\n        s = t * c\n        \n        # Update the matrix and eigenvectors\n        G = np.eye(n)\n        G[p][p] = c\n        G[p][q] = s\n        G[q][p] = -s\n        G[q][q] = c\n        \n        matrix = np.dot(np.dot(G.T, matrix), G)\n        eigenvectors = np.dot(eigenvectors, G)\n    \n    # Compute the eigenvalues\n    for i in range(k):\n        eigenvalues[i] = matrix[i][i]\n    \n    return eigenvalues, eigenvectors"
    },
    {
        "function_name": "forest_fire_simulation",
        "file_name": "forest_fire.py",
        "parameters": {
            "`forest`": "A 2D list of integers representing the forest, where 0 is an empty cell and 1 is a tree.",
            "`fire_source`": "A tuple of two integers representing the coordinates of the fire source.",
            "`wind_direction`": "A string representing the direction of the wind, which can be 'N', 'S', 'E', or 'W'."
        },
        "objectives": [
            "Simulate the spread of fire in the forest using a breadth-first search algorithm.",
            "Use the wind direction to determine the probability of the fire spreading to adjacent cells.",
            "Return the final state of the forest after the fire has spread."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def forest_fire_simulation(forest, fire_source, wind_direction):\n    rows, cols = len(forest), len(forest[0])\n    directions = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n    wind_prob = 0.5\n    if wind_direction == 'N':\n        wind_prob += 0.1\n    elif wind_direction == 'S':\n        wind_prob -= 0.1\n    elif wind_direction == 'E':\n        wind_prob += 0.05\n    elif wind_direction == 'W':\n        wind_prob -= 0.05\n    \n    queue = deque([fire_source])\n    visited = set([fire_source])\n    \n    while queue:\n        x, y = queue.popleft()\n        for dx, dy in directions.values():\n            nx, ny = x + dx, y + dy\n            if (0 <= nx < rows) and (0 <= ny < cols) and (nx, ny) not in visited:\n                if forest[nx][ny] == 1 and (dx, dy) == directions[wind_direction]:\n                    forest[nx][ny] = 0\n                    queue.append((nx, ny))\n                    visited.add((nx, ny))\n                elif forest[nx][ny] == 1 and np.random.rand() < wind_prob:\n                    forest[nx][ny] = 0\n                    queue.append((nx, ny))\n                    visited.add((nx, ny))\n    \n    return forest"
    },
    {
        "function_name": "n_gram_model",
        "file_name": "n_gram.py",
        "parameters": {
            "`tagged_text`": "A list of tuples containing the tagged text, where each tuple contains the word and its corresponding tag.",
            "`window_size`": "An integer representing the size of the window to use for the n-gram model."
        },
        "objectives": [
            "Use the tagged text to train an n-gram model.",
            "Calculate the probabilities of each n-gram in the model.",
            "Return the trained n-gram model."
        ],
        "import_lines": [
            "from collections import defaultdict",
            "from math import log"
        ],
        "function_def": "def n_gram_model(tagged_text, window_size):\n    n_grams = defaultdict(lambda: defaultdict(int))\n    for i in range(len(tagged_text) - window_size + 1):\n        words, tags = zip(*tagged_text[i:i + window_size])\n        for j in range(window_size - 1):\n            n_grams[(tags[j], words[j])][words[j + 1]] += 1\n    \n    # calculate probabilities\n    probabilities = {}\n    for (tag, word), freq in n_grams.items():\n        probabilities[(tag, word)] = {next_word: freq[next_word] / sum(freq.values()) for next_word in freq}\n    \n    return probabilities"
    },
    {
        "function_name": "cross_correlation",
        "file_name": "cross_correlation.py",
        "parameters": {
            "`signals`": "A list of integers representing the input signals.",
            "`reference_signal`": "An integer representing the reference signal.",
            "`window_size`": "An integer representing the size of the window to use for the cross-correlation."
        },
        "objectives": [
            "Use the cross-correlation technique to measure the similarity between the input signals and the reference signal.",
            "Calculate the cross-correlation coefficient.",
            "Return the cross-correlation coefficient."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def cross_correlation(signals, reference_signal, window_size):\n    windowed_signals = [signals[i:i + window_size] for i in range(len(signals) - window_size + 1)]\n    cross_correlations = []\n    \n    for signal in windowed_signals:\n        cross_correlation = np.corrcoef(signal, reference_signal)[0, 1]\n        cross_correlations.append(cross_correlation)\n    \n    return np.mean(cross_correlations)"
    },
    {
        "function_name": "graham_scan",
        "file_name": "convex_hull.py",
        "parameters": {
            "`vertices`": "A list of tuples representing the vertices of the polygons, where each tuple contains two integers representing the x and y coordinates",
            "`num_polygon`": "An integer representing the number of polygons",
            "`num_holes`": "An integer representing the number of holes"
        },
        "objectives": [
            "Implement the Graham scan algorithm to find the convex hull of the given polygons.",
            "Use the concept of orientation and convexity to determine the convex hull.",
            "Return the convex hull and its area."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def orientation(p, q, r):\n    return (q[1] - p[1]) * (r[0] - q[0]) - (q[0] - p[0]) * (r[1] - q[1])"
    },
    {
        "function_name": "max_subgrid",
        "file_name": "grid_operations.py",
        "parameters": {
            "`grid`": "A 2D list of integers representing the grid",
            "`k`": "An integer representing the size of the sub-grid"
        },
        "objectives": [
            "Divide the grid into all possible sub-grids of size `k x k`.",
            "For each sub-grid, calculate the sum of its elements.",
            "Find the sub-grid with the maximum sum.",
            "Return the maximum sum and the coordinates of the top-left corner of the sub-grid with the maximum sum."
        ],
        "import_lines": [],
        "function_def": "def max_subgrid(grid, k):\n    max_sum = float('-inf')\n    max_coords = None\n    \n    # For each sub-grid of size k x k\n    for i in range(len(grid) - k + 1):\n        for j in range(len(grid[0]) - k + 1):\n            subgrid_sum = 0\n            \n            # Calculate the sum of the sub-grid\n            for x in range(k):\n                for y in range(k):\n                    subgrid_sum += grid[i+x][j+y]\n            \n            # Check if the sub-grid sum is greater than the current maximum sum\n            if subgrid_sum > max_sum:\n                max_sum = subgrid_sum\n                max_coords = (i, j)\n    \n    return max_sum, max_coords"
    },
    {
        "function_name": "lz78_compression",
        "file_name": "text_compression.py",
        "parameters": {
            "`text`": "A string representing the text to be compressed",
            "`dictionary_size`": "An integer representing the size of the dictionary"
        },
        "objectives": [
            "Use the LZ78 algorithm to compress the text.",
            "Build a dictionary of substrings and their corresponding codes.",
            "Represent the text as a sequence of codes.",
            "Return the compressed text and the dictionary."
        ],
        "import_lines": [],
        "function_def": "def lz78_compression(text, dictionary_size):\n    dictionary = [chr(i) for i in range(256)]\n    compressed_text = []\n    i = 0\n    while i < len(text):\n        substring = \"\"\n        for j in range(i, len(text)):\n            substring += text[j]\n            if substring in dictionary:\n                i = j + 1\n            else:\n                break\n        code = dictionary.index(substring[:-1]) + 1\n        compressed_text.append(code)\n        dictionary.append(substring)\n        if len(dictionary) > dictionary_size:\n            dictionary = dictionary[:-1]\n    return compressed_text, dictionary"
    },
    {
        "function_name": "seasonal_decomposition",
        "file_name": "time_series_decomposition.py",
        "parameters": {
            "`time_series`": "A list of numbers representing the time series",
            "`seasonal_period`": "An integer representing the seasonal period"
        },
        "objectives": [
            "Use the Seasonal Decomposition of Time Series (SDTS) algorithm to decompose the time series into trend, seasonal, and residual components.",
            "Calculate the trend and seasonal components using the moving averages method.",
            "Calculate the residual component by subtracting the trend and seasonal components from the original time series.",
            "Return the decomposed time series."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def seasonal_decomposition(time_series, seasonal_period):\n    trend = np.zeros(len(time_series))\n    seasonal = np.zeros(len(time_series))\n    for i in range(len(time_series)):\n        start = max(0, i - seasonal_period + 1)\n        trend[i] = sum(time_series[start:i+1]) / (i - start + 1)\n    for i in range(len(time_series)):\n        seasonal[i] = time_series[i] - trend[i]\n    seasonal = np.array([seasonal[j % seasonal_period] for j in range(len(time_series))])\n    residual = time_series - trend - seasonal\n    return trend, seasonal, residual"
    },
    {
        "function_name": "sentence_similarity_calculator",
        "file_name": "nlp_operations.py",
        "parameters": {
            "`sentences`": "A list of sentences",
            "`keywords`": "A list of keywords",
            "`threshold`": "A float representing the minimum similarity score"
        },
        "objectives": [
            "Tokenize the sentences and calculate the TF-IDF scores for each word.",
            "Calculate the similarity score between the sentences using the TF-IDF scores.",
            "Filter the sentences that have a similarity score greater than or equal to `threshold`.",
            "Return the top `n` sentences with the highest similarity score."
        ],
        "import_lines": [
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity"
        ],
        "function_def": "def sentence_similarity_calculator(sentences, keywords, threshold, n=10):\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform(sentences)\n    similarity_scores = cosine_similarity(tfidf, tfidf)\n    \n    keyword_tfidf = vectorizer.transform(keywords)\n    keyword_similarity_scores = cosine_similarity(tfidf, keyword_tfidf)\n    \n    scores = []\n    for i in range(len(sentences)):\n        score = 0\n        for j in range(len(keywords)):\n            score += keyword_similarity_scores[i][j]\n        scores.append((sentences[i], score))\n        \n    scores.sort(key=lambda x: x[1], reverse=True)\n    \n    filtered_sentences = []\n    for sentence, score in scores:\n        if score >= threshold:\n            filtered_sentences.append((sentence, score))\n            \n    return filtered_sentences[:n]"
    },
    {
        "function_name": "image_compressor",
        "file_name": "image_operations.py",
        "parameters": {
            "`images`": "A list of image files",
            "`quality`": "An integer representing the desired image quality"
        },
        "objectives": [
            "Compress each image using a compression algorithm (e.g. JPEG).",
            "Calculate the compression ratio for each image.",
            "Filter the images that have a compression ratio greater than or equal to `threshold`.",
            "Return the compressed images."
        ],
        "import_lines": [
            "from PIL import Image",
            "import io"
        ],
        "function_def": "def image_compressor(images, quality, threshold):\n    compressed_images = []\n    for image in images:\n        img = Image.open(image)\n        buffer = io.BytesIO()\n        img.save(buffer, format='JPEG', quality=quality)\n        compressed_img = buffer.getvalue()\n        compression_ratio = len(compressed_img) / img.size[0] / img.size[1]\n        if compression_ratio >= threshold:\n            compressed_images.append((image, compressed_img))\n            \n    return compressed_images"
    },
    {
        "function_name": "moving_statistics",
        "file_name": "sequence_algorithms.py",
        "parameters": {
            "`sequence`": "A list of numbers",
            "`window_size`": "The size of the sliding window"
        },
        "objectives": [
            "Calculate the moving average of the sequence using a sliding window of the specified size.",
            "Calculate the moving standard deviation of the sequence using the same sliding window.",
            "Return a list of tuples containing the moving average and standard deviation for each position in the sequence."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def moving_statistics(sequence, window_size):\n    result = []\n    for i in range(len(sequence) - window_size + 1):\n        window = sequence[i:i + window_size]\n        avg = np.mean(window)\n        std = np.std(window)\n        result.append((avg, std))\n    return result"
    },
    {
        "function_name": "matrix_thresholding",
        "file_name": "matrix_algorithms.py",
        "parameters": {
            "`matrices`": "A list of 2D matrices",
            "`threshold`": "A threshold value"
        },
        "objectives": [
            "Calculate the Frobenius norm of each matrix in the list.",
            "Apply a threshold to each matrix to zero out elements below the threshold.",
            "Return a list of the thresholded matrices."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def matrix_thresholding(matrices, threshold):\n    result = []\n    for matrix in matrices:\n        norm = np.linalg.norm(matrix, 'fro')\n        matrix_thresholded = np.where(np.abs(matrix) < threshold, 0, matrix)\n        result.append(matrix_thresholded)\n    return result"
    },
    {
        "function_name": "cluster_strings_by_similarity",
        "file_name": "string_algorithms.py",
        "parameters": {
            "`strings`": "A list of strings.",
            "`threshold`": "A float representing the minimum similarity required between two strings."
        },
        "objectives": [
            "Calculate the similarity between each pair of strings using the Jaccard similarity metric.",
            "Group the strings into clusters based on their similarity, such that the similarity between any two strings in a cluster is greater than or equal to the threshold.",
            "Return the clusters."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def cluster_strings_by_similarity(strings, threshold):\n    # Preprocess the strings by converting them to sets of words\n    word_sets = [set(re.findall(r'\\b\\w+\\b', s)) for s in strings]\n    \n    # Initialize the clusters\n    clusters = []\n    \n    for string, word_set in zip(strings, word_sets):\n        found_cluster = False\n        \n        for cluster in clusters:\n            if any(len(word_set & ws) / len(word_set | ws) >= threshold for ws in cluster):\n                cluster.append(word_set)\n                found_cluster = True\n                break\n        \n        if not found_cluster:\n            clusters.append([word_set])\n    \n    # Convert the clusters back to lists of strings\n    clusters = [[s for s, ws in zip(strings, word_sets) if ws in cluster] for cluster in clusters]\n    \n    return clusters"
    },
    {
        "function_name": "max_submatrix_sum",
        "file_name": "matrix_analysis.py",
        "parameters": {
            "`matrix`": "A 2D list of integers",
            "`threshold`": "An integer"
        },
        "objectives": [
            "Implement a function that finds the maximum sum of a submatrix of the given matrix that is below the given threshold.",
            "The function should use Kadane's algorithm to find the maximum sum of a subarray.",
            "The function should return the maximum sum and the submatrix itself."
        ],
        "import_lines": [],
        "function_def": "def max_submatrix_sum(matrix, threshold):\n    max_sum = float('-inf')\n    max_submatrix = None\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            for k in range(i, len(matrix)):\n                for col in range(j, len(matrix[0])):\n                    submatrix_sum = 0\n                    for row in range(i, k + 1):\n                        for c in range(j, col + 1):\n                            submatrix_sum += matrix[row][c]\n                    if submatrix_sum < threshold and submatrix_sum > max_sum:\n                        max_sum = submatrix_sum\n                        max_submatrix = [row[j:col + 1] for row in matrix[i:k + 1]]\n    return max_sum, max_submatrix"
    },
    {
        "function_name": "counting_sort_row",
        "file_name": "sorting_algorithms.py",
        "parameters": {
            "`matrix`": "A 2D list representing a matrix",
            "`row`": "An integer representing the row to sort",
            "`col`": "An integer representing the column to sort"
        },
        "objectives": [
            "Sort the given row in the matrix using the counting sort algorithm.",
            "Use the column as a stable sort key.",
            "Return the sorted matrix."
        ],
        "import_lines": [],
        "function_def": "def counting_sort_row(matrix, row, col):\n    min_val = min(matrix[row])\n    max_val = max(matrix[row])\n    count = [0] * (max_val - min_val + 1)\n    for i, val in enumerate(matrix[row]):\n        count[val - min_val] += 1\n    \n    sorted_row = []\n    for i, cnt in enumerate(count):\n        sorted_row.extend([i + min_val] * cnt)\n    \n    # stable sort\n    sorted_matrix = []\n    for i, r in enumerate(matrix):\n        if i == row:\n            sorted_matrix.append(sorted_row[:])\n        else:\n            sorted_matrix.append(r[:])\n    \n    for i, val in enumerate(sorted_row):\n        sorted_matrix[col][i] = val\n    \n    return sorted_matrix"
    },
    {
        "function_name": "course_scheduler",
        "file_name": "scheduler.py",
        "parameters": {
            "`schedule`": "A dictionary representing a course schedule, where each key is a course ID and each value is a list of tuples containing the course duration and the number of students enrolled.",
            "`num_rooms`": "An integer representing the number of available rooms.",
            "`room_capacity`": "An integer representing the maximum capacity of each room."
        },
        "objectives": [
            "Schedule courses in a way that minimizes the total number of rooms used.",
            "Ensure that no room is overloaded, i.e., the total number of students in a room does not exceed the room capacity.",
            "Return the allocated room for each course."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def course_scheduler(schedule, num_rooms, room_capacity):\n    # Sort the courses by the number of students in descending order\n    courses = sorted(schedule.items(), key=lambda x: x[1][1], reverse=True)\n    \n    # Initialize the room allocation\n    room_allocation = {}\n    rooms = [room_capacity] * num_rooms\n    \n    # Iterate over the courses\n    for course, (duration, students) in courses:\n        # Find a room that can accommodate the course\n        for i, capacity in enumerate(rooms):\n            if capacity >= students:\n                room_allocation[course] = i\n                rooms[i] -= students\n                break\n    \n    return room_allocation"
    },
    {
        "function_name": "genome_mutator",
        "file_name": "genomics.py",
        "parameters": {
            "`genome`": "A string representing a genome sequence.",
            "`mutations`": "A list of tuples containing the position and the type of mutation (insertion, deletion, or substitution)."
        },
        "objectives": [
            "Simulate the mutations on the genome sequence.",
            "Return the mutated genome sequence."
        ],
        "import_lines": [],
        "function_def": "def genome_mutator(genome, mutations):\n    # Convert the genome to a list for easier manipulation\n    genome_list = list(genome)\n    \n    # Iterate over the mutations\n    for position, mutation_type in mutations:\n        if mutation_type == 'insertion':\n            genome_list.insert(position, 'A')  # Insert a random base\n        elif mutation_type == 'deletion':\n            del genome_list[position]\n        elif mutation_type == 'substitution':\n            genome_list[position] = 'C' if genome_list[position] == 'G' else 'G'  # Substitute with a different base\n    \n    # Convert the list back to a string\n    mutated_genome = ''.join(genome_list)\n    \n    return mutated_genome"
    },
    {
        "function_name": "fraud_detector",
        "file_name": "fraud_detection.py",
        "parameters": {
            "`transactions`": "A list of tuples containing the sender, receiver, and amount of a transaction.",
            "`threshold`": "An integer representing the minimum amount for a transaction to be considered suspicious."
        },
        "objectives": [
            "Identify suspicious transactions based on the threshold.",
            "Return the suspicious transactions along with the total amount of suspicious transactions."
        ],
        "import_lines": [],
        "function_def": "def fraud_detector(transactions, threshold):\n    # Initialize the suspicious transactions\n    suspicious_transactions = []\n    total_amount = 0\n    \n    # Iterate over the transactions\n    for sender, receiver, amount in transactions:\n        if amount > threshold:\n            suspicious_transactions.append((sender, receiver, amount))\n            total_amount += amount\n    \n    return suspicious_transactions, total_amount"
    },
    {
        "function_name": "product_adoption",
        "file_name": "social_network.py",
        "parameters": {
            "`network`": "A dictionary representing a social network, where each key is a user ID and each value is a list of friend IDs.",
            "`seeds`": "A list of user IDs representing the initial adopters of a product.",
            "`threshold`": "An integer representing the minimum number of friends required for a user to adopt a product."
        },
        "objectives": [
            "Simulate the adoption of a product in the social network using a threshold model.",
            "Return the final adoption rate."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def product_adoption(network, seeds, threshold):\n    # Initialize the adoption rate\n    adoption_rate = len(seeds)\n    queue = deque(seeds)\n    \n    # Iterate over the adopters\n    while queue:\n        user = queue.popleft()\n        friends = network[user]\n        for friend in friends:\n            if friend not in seeds and len([f for f in friends if f in seeds]) >= threshold:\n                seeds.append(friend)\n                adoption_rate += 1\n                queue.append(friend)\n    \n    return adoption_rate / len(network)"
    },
    {
        "function_name": "route_allocator",
        "file_name": "route_allocation.py",
        "parameters": {
            "`routes`": "A dictionary representing a transportation network, where each key is a route ID and each value is a list of tuples containing the source, destination, and distance of a route.",
            "`capacity`": "An integer representing the maximum capacity of each route.",
            "`demand`": "A dictionary representing the demand for each route, where each key is a route ID and each value is an integer representing the number of passengers."
        },
        "objectives": [
            "Allocate passengers to routes in a way that maximizes the utilization of the transportation network.",
            "Ensure that no route is overloaded, i.e., the total number of passengers on a route does not exceed the route capacity.",
            "Return the allocated passengers for each route."
        ],
        "import_lines": [
            "from scipy.optimize import linear_sum_assignment"
        ],
        "function_def": "def route_allocator(routes, capacity, demand):\n    # Create a cost matrix\n    cost_matrix = []\n    for route, passengers in demand.items():\n        row = []\n        for _, _, distance in routes[route]:\n            row.append(passengers * distance)\n        cost_matrix.append(row)\n    \n    # Use the Hungarian algorithm to find the optimal assignment\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    optimal_allocation = {route: passengers for route, passengers in zip(demand.keys(), col_ind)}\n    \n    return optimal_allocation"
    },
    {
        "function_name": "bayesian_network_score",
        "file_name": "bayesian_network.py",
        "parameters": {
            "`dag`": "A dictionary representing a directed acyclic graph (DAG), where each key is a node and each value is a list of its parents",
            "`node`": "A string representing the node for which to calculate the Bayesian network score",
            "`data`": "A 2D list representing a dataset of observations, where each row is a sample and each column is a feature"
        },
        "objectives": [
            "Calculate the Bayesian network score for the given node in the DAG, given the observed data.",
            "Use the Bayesian Information Criterion (BIC) to penalize complex models.",
            "Return the Bayesian network score."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def bayesian_network_score(dag, node, data):\n    # Calculate the likelihood of the data given the node and its parents\n    likelihood = 1\n    for i, x in enumerate(data):\n        parents = dag[node]\n        parent_values = [x[parents.index(parent)] for parent in parents]\n        likelihood *= np.prod([parent_values.count(x) / len(parent_values) for x in set(parent_values)])\n    \n    # Calculate the BIC penalty term\n    num_parents = len(dag[node])\n    bic_penalty = 0.5 * num_parents * np.log(len(data))\n    \n    # Calculate the Bayesian network score\n    score = np.log(likelihood) - bic_penalty\n    \n    return score"
    },
    {
        "function_name": "peak_detection",
        "file_name": "signal_processing.py",
        "parameters": {
            "`signal`": "A list of integers representing a signal",
            "`window_size`": "An integer representing the size of the window for the moving average calculation"
        },
        "objectives": [
            "Calculate the moving average of the signal using the given window size.",
            "Detect the peaks in the signal that exceed the moving average by more than 2 standard deviations.",
            "Return the indices of the detected peaks."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def peak_detection(signal, window_size):\n    # Calculate the moving average\n    moving_average = np.convolve(signal, np.ones(window_size) / window_size, mode='same')\n    \n    # Calculate the standard deviation of the signal\n    std_dev = np.std(signal)\n    \n    # Detect the peaks\n    peaks = []\n    for i, x in enumerate(signal):\n        if x > moving_average[i] + 2 * std_dev:\n            peaks.append(i)\n    \n    return peaks"
    },
    {
        "function_name": "max_clique",
        "file_name": "clique_algorithms.py",
        "parameters": {
            "`G`": "A graph represented as an adjacency list.",
            "`k`": "An integer representing the size of the clique."
        },
        "objectives": [
            "Find the maximum clique in the graph.",
            "The clique should be of size at least `k`.",
            "Return the vertices of the maximum clique."
        ],
        "import_lines": [
            "import networkx as nx"
        ],
        "function_def": "def max_clique(G, k):\n    graph = nx.Graph()\n    for node, neighbors in G.items():\n        for neighbor in neighbors:\n            graph.add_edge(node, neighbor)\n    \n    max_clique = None\n    max_clique_size = 0\n    \n    for node in graph.nodes:\n        clique = set()\n        clique.add(node)\n        stack = list(graph.neighbors(node))\n        while stack:\n            node = stack.pop()\n            if node not in clique:\n                clique.add(node)\n                stack.extend(neighbor for neighbor in graph.neighbors(node) if neighbor not in clique)\n        if len(clique) > max_clique_size and len(clique) >= k:\n            max_clique = clique\n            max_clique_size = len(clique)\n    \n    return max_clique"
    },
    {
        "function_name": "calculate_determinants",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`matrix`": "A 2D list representing the input matrix",
            "`k`": "An integer representing the size of the sub-matrices"
        },
        "objectives": [
            "Divide the matrix into sub-matrices of size k x k.",
            "Calculate the determinant of each sub-matrix.",
            "Return the sub-matrices and their corresponding determinants."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def calculate_determinants(matrix, k):\n    sub_matrices = []\n    determinants = []\n    \n    for i in range(0, len(matrix), k):\n        for j in range(0, len(matrix[0]), k):\n            sub_matrix = [row[j:j+k] for row in matrix[i:i+k]]\n            sub_matrices.append(sub_matrix)\n            determinants.append(np.linalg.det(sub_matrix))\n    \n    return sub_matrices, determinants"
    },
    {
        "function_name": "data_imputation",
        "file_name": "data_preprocessing.py",
        "parameters": {
            "`data`": "A list of lists representing a dataset with missing values",
            "`imputation_method`": "A string representing the imputation method to use (e.g. mean, median, k-nearest neighbors)"
        },
        "objectives": [
            "Implement a data imputation algorithm to fill in missing values",
            "Use the specified imputation method to compute the imputed values",
            "Return the imputed dataset"
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.impute import KNNImputer"
        ],
        "function_def": "def data_imputation(data, imputation_method):\n    # Initialize the imputer\n    if imputation_method == 'mean':\n        imputer = np.mean\n    elif imputation_method == 'median':\n        imputer = np.median\n    elif imputation_method == 'k-nearest neighbors':\n        imputer = KNNImputer(n_neighbors=5)\n    \n    # Impute the missing values\n    imputed_data = []\n    for row in data:\n        imputed_row = []\n        for value in row:\n            if np.isnan(value):\n                if imputation_method == 'mean' or imputation_method == 'median':\n                    imputed_value = imputer(row)\n                else:\n                    imputed_value = imputer.fit_transform(np.array([row]))[0]\n                imputed_row.append(imputed_value)\n            else:\n                imputed_row.append(value)\n        imputed_data.append(imputed_row)\n    \n    return imputed_data"
    },
    {
        "function_name": "dijkstra",
        "file_name": "graph.py",
        "parameters": {
            "`graph`": "A dictionary representing an adjacency list of a graph",
            "`source`": "An integer representing the source node",
            "`k`": "An integer representing the number of shortest paths to find"
        },
        "objectives": [
            "Implement Dijkstra's algorithm to find the shortest path from the source node to all other nodes in the graph.",
            "Use a priority queue to efficiently select the next node to visit.",
            "Return the k shortest paths from the source node to all other nodes."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def dijkstra(graph, source, k):\n    distances = {node: float('inf') for node in graph}\n    distances[source] = 0\n    previous = {node: None for node in graph}\n    priority_queue = [(0, source)]\n    \n    while priority_queue:\n        current_distance, current_node = heapq.heappop(priority_queue)\n        \n        if current_distance > distances[current_node]:\n            continue\n        \n        for neighbor, weight in graph[current_node].items():\n            distance = current_distance + weight\n            \n            if distance < distances[neighbor]:\n                distances[neighbor] = distance\n                previous[neighbor] = current_node\n                heapq.heappush(priority_queue, (distance, neighbor))\n    \n    shortest_paths = []\n    for node, distance in distances.items():\n        if node != source and distance != float('inf'):\n            path = []\n            current_node = node\n            while current_node is not None:\n                path.append(current_node)\n                current_node = previous[current_node]\n            shortest_paths.append((distance, path[::-1]))\n    \n    return sorted(shortest_paths)[:k]"
    },
    {
        "function_name": "kth_smallest",
        "file_name": "kth_smallest.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing the matrix",
            "`k`": "An integer representing the kth smallest element"
        },
        "objectives": [
            "Implement a function that finds the kth smallest element in a matrix.",
            "Use a min-heap to store the elements we've seen so far and their positions.",
            "Return the kth smallest element."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def kth_smallest(matrix, k):\n    min_heap = []\n    for i in range(len(matrix)):\n        heapq.heappush(min_heap, (matrix[i][0], i, 0))\n    \n    count = 0\n    while min_heap:\n        num, row, col = heapq.heappop(min_heap)\n        count += 1\n        if count == k:\n            return num\n        if col < len(matrix[0]) - 1:\n            heapq.heappush(min_heap, (matrix[row][col + 1], row, col + 1))\n    \n    return None"
    },
    {
        "function_name": "markov_chain_steady_state",
        "file_name": "markov_chain.py",
        "parameters": {
            "`transitions`": "A 3D list of floats representing the transition probabilities in a Markov chain",
            "`initial_state`": "A list of floats representing the initial state probabilities",
            "`steps`": "An integer representing the number of steps in the Markov chain"
        },
        "objectives": [
            "Calculate the state probabilities at each step using the Markov chain transition probabilities and initial state probabilities.",
            "Calculate the steady-state probabilities of the Markov chain by finding the equilibrium state where the probabilities do not change over time.",
            "Normalize the steady-state probabilities to ensure they sum up to 1.",
            "Return the steady-state probabilities."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def markov_chain_steady_state(transitions, initial_state, steps):\n    # Initialize state probabilities\n    state_probabilities = np.array(initial_state)\n    \n    # Calculate state probabilities at each step\n    for _ in range(steps):\n        state_probabilities = np.dot(state_probabilities, transitions)\n    \n    # Calculate steady-state probabilities\n    steady_state_probabilities = np.linalg.lstsq(np.array(transitions) - np.eye(len(transitions)), np.ones(len(transitions)), rcond=None)[0]\n    steady_state_probabilities = np.abs(steady_state_probabilities) / np.sum(np.abs(steady_state_probabilities))\n    \n    return steady_state_probabilities"
    },
    {
        "function_name": "window_sums",
        "file_name": "window_processor.py",
        "parameters": {
            "nums": "A list of integers",
            "k": "An integer representing the size of the sliding window"
        },
        "objectives": [
            "Divide the input list into sub-lists of size 'k'.",
            "For each sub-list, find the sum of its elements and the sum of the squares of its elements.",
            "Return the list of sums and the list of sums of squares."
        ],
        "import_lines": [],
        "function_def": "def window_sums(nums, k):\n    n = len(nums)\n    sums = []\n    sums_of_squares = []\n    for i in range(0, n, k):\n        sub_list = nums[i:i+k]\n        window_sum = sum(sub_list)\n        window_sum_of_squares = sum([x**2 for x in sub_list])\n        sums.append(window_sum)\n        sums_of_squares.append(window_sum_of_squares)\n    return sums, sums_of_squares"
    },
    {
        "function_name": "lexicographically_smallest_match",
        "file_name": "string_matcher.py",
        "parameters": {
            "strings": "A list of strings",
            "pattern": "A string representing the pattern to search for"
        },
        "objectives": [
            "Find the lexicographically smallest string in the list that contains the given pattern.",
            "If no string contains the pattern, return an empty string.",
            "Return the lexicographically smallest string that contains the pattern."
        ],
        "import_lines": [],
        "function_def": "def lexicographically_smallest_match(strings, pattern):\n    matches = [s for s in strings if pattern in s]\n    if not matches:\n        return \"\"\n    return min(matches)"
    },
    {
        "function_name": "exponential_smoothing",
        "file_name": "time_series_forecasting.py",
        "parameters": {
            "`series`": "A list of integers representing the input time series data.",
            "`window_size`": "An integer representing the size of the sliding window."
        },
        "objectives": [
            "Implement the Exponential Smoothing (ES) method to forecast future values in the input time series data.",
            "Use the Holt-Winters method for seasonal decomposition.",
            "Return the forecasted values for the next 'window_size' time steps."
        ],
        "import_lines": [],
        "function_def": "def exponential_smoothing(series, window_size):\n    # Holt-Winters method for seasonal decomposition\n    trend = [0] * len(series)\n    seasonality = [0] * len(series)\n    smoothed = [0] * len(series)\n    \n    alpha = 0.2\n    beta = 0.1\n    gamma = 0.05\n    \n    for i in range(len(series)):\n        if i == 0:\n            trend[i] = series[i]\n            seasonality[i] = series[i]\n            smoothed[i] = series[i]\n        else:\n            trend[i] = alpha * (series[i] - seasonality[i - 1]) + (1 - alpha) * (trend[i - 1] + smoothed[i - 1])\n            seasonality[i] = beta * (series[i] - trend[i]) + (1 - beta) * seasonality[i - 1]\n            smoothed[i] = gamma * series[i] + (1 - gamma) * (trend[i] + seasonality[i])\n    \n    # Forecast future values\n    forecast = [0] * window_size\n    for i in range(window_size):\n        forecast[i] = smoothed[-1] + trend[-1] * (i + 1) + seasonality[-1]\n    \n    return forecast"
    },
    {
        "function_name": "max_matrix_sum",
        "file_name": "matrix_algorithms.py",
        "parameters": {
            "`matrix`": "A 2D list representing a matrix.",
            "`target`": "An integer representing the target sum.",
            "`k`": "An integer representing the maximum number of elements to select."
        },
        "objectives": [
            "Find the maximum sum of k elements in the matrix, such that no two elements are from the same row or column.",
            "Use dynamic programming to solve the problem.",
            "Return the maximum sum and the selected elements."
        ],
        "import_lines": [],
        "function_def": "def max_matrix_sum(matrix, target, k):\n    rows, cols = len(matrix), len(matrix[0])\n    dp = [[0 for _ in range(k + 1)] for _ in range(rows * cols + 1)]\n    selected_elements = [[[] for _ in range(k + 1)] for _ in range(rows * cols + 1)]\n    \n    for i in range(1, rows * cols + 1):\n        for j in range(1, min(i, k) + 1):\n            row, col = divmod(i - 1, cols)\n            if matrix[row][col] <= target:\n                if j == 1:\n                    dp[i][j] = matrix[row][col]\n                    selected_elements[i][j] = [(row, col)]\n                else:\n                    dp[i][j] = dp[i - 1][j]\n                    selected_elements[i][j] = selected_elements[i - 1][j]\n                    for prev_i in range(i - 1, max(0, i - cols - 1), -1):\n                        prev_row, prev_col = divmod(prev_i - 1, cols)\n                        if prev_row != row and prev_col != col and dp[prev_i][j - 1] + matrix[row][col] > dp[i][j]:\n                            dp[i][j] = dp[prev_i][j - 1] + matrix[row][col]\n                            selected_elements[i][j] = selected_elements[prev_i][j - 1] + [(row, col)]\n    \n    return dp[-1][-1], selected_elements[-1][-1]"
    },
    {
        "function_name": "matrix_filtering",
        "file_name": "matrix_operations.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix",
            "`threshold`": "An integer representing the threshold value for matrix filtering",
            "`k`": "An integer representing the number of clusters to form"
        },
        "objectives": [
            "Use the K-Means clustering algorithm to cluster the rows of the matrix into k clusters.",
            "Filter the matrix by selecting only the rows with a mean value above the threshold.",
            "Calculate the inertia (sum of squared distances) of the clusters.",
            "Return the filtered matrix and the inertia of the clusters."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def matrix_filtering(matrix, threshold, k):\n    filtered_matrix = [row for row in matrix if np.mean(row) > threshold]\n    kmeans = KMeans(n_clusters=k)\n    inertia = kmeans.fit(filtered_matrix).inertia_\n    return filtered_matrix, inertia"
    },
    {
        "function_name": "feature_selection",
        "file_name": "machine_learning.py",
        "parameters": {
            "`data`": "A pandas DataFrame representing a dataset",
            "`target_variable`": "A string representing the target variable to predict"
        },
        "objectives": [
            "Use a feature selection algorithm (e.g., recursive feature elimination) to select the most important features for predicting the target variable.",
            "Train a machine learning model (e.g., random forest) on the selected features to predict the target variable.",
            "Evaluate the performance of the model using metrics such as accuracy, precision, and recall.",
            "Return the selected features and the performance metrics."
        ],
        "import_lines": [
            "import pandas as pd",
            "from sklearn.feature_selection import RFE",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.metrics import accuracy_score, precision_score, recall_score"
        ],
        "function_def": "def feature_selection(data, target_variable):\n    rfe = RFE(RandomForestClassifier(n_estimators=100), n_features_to_select=10)\n    rfe.fit(data.drop(target_variable, axis=1), data[target_variable])\n    selected_features = data.drop(target_variable, axis=1).columns[rfe.support_]\n    model = RandomForestClassifier(n_estimators=100)\n    model.fit(data[selected_features], data[target_variable])\n    y_pred = model.predict(data[selected_features])\n    accuracy = accuracy_score(data[target_variable], y_pred)\n    precision = precision_score(data[target_variable], y_pred, average='weighted')\n    recall = recall_score(data[target_variable], y_pred, average='weighted')\n    return selected_features, accuracy, precision, recall"
    },
    {
        "function_name": "top_k_prefix_sums",
        "file_name": "array_manipulations.py",
        "parameters": {
            "`arr1`": "A list of integers representing the first array",
            "`arr2`": "A list of integers representing the second array",
            "`k`": "An integer representing the number of top elements to return"
        },
        "objectives": [
            "Merge the two arrays into a single sorted array.",
            "Calculate the prefix sum of the merged array.",
            "Return the indices of the top k maximum prefix sums."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def top_k_prefix_sums(arr1, arr2, k):\n    # Merge and sort the arrays\n    merged = sorted(arr1 + arr2)\n    prefix_sums = [0] * (len(merged) + 1)\n    for i in range(len(merged)):\n        prefix_sums[i + 1] = prefix_sums[i] + merged[i]\n    \n    # Find the indices of the top k maximum prefix sums\n    max_heap = [(-prefix_sums[i], i) for i in range(1, len(prefix_sums))]\n    heapq.heapify(max_heap)\n    top_k_indices = []\n    for _ in range(k):\n        _, index = heapq.heappop(max_heap)\n        top_k_indices.append(index)\n    \n    return top_k_indices"
    },
    {
        "function_name": "shortest_path_avoiding_obstacles",
        "file_name": "grid_traversal.py",
        "parameters": {
            "`n`": "An integer representing the size of the grid",
            "`obstacles`": "A list of tuples representing the coordinates of the obstacles",
            "`start`": "A tuple representing the coordinates of the start cell",
            "`end`": "A tuple representing the coordinates of the end cell"
        },
        "objectives": [
            "Create an n x n grid with obstacles represented as 1 and empty cells as 0.",
            "Use breadth-first search to find the shortest path from the start cell to the end cell while avoiding obstacles.",
            "Return the length of the shortest path."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def shortest_path_avoiding_obstacles(n, obstacles, start, end):\n    # Create the grid with obstacles\n    grid = [[0] * n for _ in range(n)]\n    for x, y in obstacles:\n        grid[x][y] = 1\n    \n    # Use BFS to find the shortest path\n    queue = deque([(start, 0)])\n    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n    visited = set([start])\n    while queue:\n        (x, y), distance = queue.popleft()\n        if (x, y) == end:\n            return distance\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < n and 0 <= ny < n and grid[nx][ny] == 0 and (nx, ny) not in visited:\n                queue.append(((nx, ny), distance + 1))\n                visited.add((nx, ny))\n    \n    # If no path is found, return -1\n    return -1"
    },
    {
        "function_name": "min_operations_to_transform",
        "file_name": "string_manipulations.py",
        "parameters": {
            "`s`": "A string representing the source string",
            "`t`": "A string representing the target string",
            "`k`": "An integer representing the maximum number of operations allowed"
        },
        "objectives": [
            "Use dynamic programming to find the longest common subsequence between the source and target strings.",
            "Calculate the edit distance between the source and target strings.",
            "Return the minimum number of operations required to transform the source string into the target string."
        ],
        "import_lines": [],
        "function_def": "def min_operations_to_transform(s, t, k):\n    # Find the longest common subsequence\n    m, n = len(s), len(t)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if s[i - 1] == t[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n    \n    # Calculate the edit distance\n    edit_distance = m + n - 2 * dp[m][n]\n    \n    # Return the minimum number of operations required\n    return edit_distance if edit_distance <= k else -1"
    },
    {
        "function_name": "evaluate_polynomial_and_derivative",
        "file_name": "polynomial_manipulations.py",
        "parameters": {
            "`a`": "A list of integers representing the coefficients of the polynomial",
            "`x`": "An integer representing the value of x",
            "`n`": "An integer representing the degree of the polynomial"
        },
        "objectives": [
            "Use Horner's method to evaluate the polynomial at the given value of x.",
            "Calculate the derivative of the polynomial.",
            "Return the value of the polynomial and its derivative at the given value of x."
        ],
        "import_lines": [],
        "function_def": "def evaluate_polynomial_and_derivative(a, x, n):\n    # Evaluate the polynomial using Horner's method\n    polynomial_value = 0\n    for coefficient in reversed(a):\n        polynomial_value = coefficient + x * polynomial_value\n    \n    # Calculate the derivative of the polynomial\n    derivative_value = 0\n    for i in range(n - 1, 0, -1):\n        derivative_value = i * a[i] + x * derivative_value\n    \n    # Return the values of the polynomial and its derivative\n    return polynomial_value, derivative_value"
    },
    {
        "function_name": "generate_association_rules",
        "file_name": "association_analysis.py",
        "parameters": {
            "`transactions`": "A list of lists representing the transactions",
            "`min_support`": "A float representing the minimum support required for a rule",
            "`min_confidence`": "A float representing the minimum confidence required for a rule"
        },
        "objectives": [
            "Use the Apriori algorithm to generate association rules from the transactions.",
            "Calculate the support and confidence of each rule.",
            "Return the association rules with their support and confidence."
        ],
        "import_lines": [
            "from itertools import combinations"
        ],
        "function_def": "def generate_association_rules(transactions, min_support, min_confidence):\n    # Generate frequent itemsets\n    frequent_itemsets = []\n    for k in range(1, len(max(transactions, key=len)) + 1):\n        itemsets = set()\n        for transaction in transactions:\n            itemsets.update(combinations(transaction, k))\n        frequent_itemsets.extend([itemset for itemset in itemsets if sum(1 for transaction in transactions if set(itemset).issubset(transaction)) / len(transactions) >= min_support])\n    \n    # Generate association rules\n    association_rules = []\n    for itemset in frequent_itemsets:\n        for subset in frequent_itemsets:\n            if set(subset).issubset(itemset) and len(itemset) > len(subset):\n                support = sum(1 for transaction in transactions if set(itemset).issubset(transaction)) / len(transactions)\n                confidence = support / sum(1 for transaction in transactions if set(subset).issubset(transaction)) / len(transactions)\n                if confidence >= min_confidence:\n                    association_rules.append((subset, tuple(set(itemset) - set(subset)), support, confidence))\n    \n    return association_rules"
    },
    {
        "function_name": "degree_limited_search",
        "file_name": "social_network_analysis.py",
        "parameters": {
            "`network`": "A dictionary representing the social network where each key is a person and the value is a list of their friends.",
            "`person`": "The person to start the search from.",
            "`degrees`": "An integer representing the number of degrees to search."
        },
        "objectives": [
            "Perform a degree-limited search to find the people within the specified degrees of the person.",
            "Use a breadth-first search approach to traverse the network.",
            "Return the people within the specified degrees as a set of person names."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def degree_limited_search(network, person, degrees):\n    queue = deque([(person, 0)])\n    people = set()\n    while queue:\n        current_person, degree = queue.popleft()\n        if degree <= degrees:\n            people.add(current_person)\n            for friend in network[current_person]:\n                if friend not in people:\n                    queue.append((friend, degree + 1))\n    return people"
    },
    {
        "function_name": "sentence_extractor",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`text`": "A string of text",
            "`n`": "An integer, representing the number of sentences to extract",
            "`keyword`": "A string, representing the keyword to search for in the text"
        },
        "objectives": [
            "Split the text into sentences",
            "Extract the n sentences with the highest frequency of the keyword",
            "Return the extracted sentences"
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def sentence_extractor(text, n, keyword):\n    # Split the text into sentences\n    sentences = re.split(r'[.!?]', text)\n    \n    # Remove empty strings from the list of sentences\n    sentences = list(filter(None, sentences))\n    \n    # Calculate the frequency of the keyword in each sentence\n    keyword_frequency = {sentence: sentence.count(keyword) for sentence in sentences}\n    \n    # Extract the n sentences with the highest frequency of the keyword\n    extracted_sentences = sorted(keyword_frequency, key=keyword_frequency.get, reverse=True)[:n]\n    \n    return extracted_sentences"
    },
    {
        "function_name": "z_algorithm_search",
        "file_name": "string_search.py",
        "parameters": {
            "`sequence`": "A list of integers representing the sequence to search for patterns",
            "`pattern`": "A list of integers representing the pattern to search for"
        },
        "objectives": [
            "Use the Z-algorithm to preprocess the pattern and calculate the Z-values",
            "Use the Z-algorithm to search for the pattern in the sequence",
            "Return the starting index of the pattern in the sequence if found"
        ],
        "import_lines": [],
        "function_def": "def z_algorithm_search(sequence, pattern):\n    n = len(sequence)\n    m = len(pattern)\n    z_values = [0] * (n + m - 1)\n    \n    # Preprocess pattern and calculate Z-values\n    for i in range(m):\n        z_values[i] = 0\n        for j in range(i, m):\n            if pattern[j] == pattern[j - i]:\n                z_values[i] += 1\n            else:\n                break\n    \n    # Search for pattern in sequence\n    for i in range(n):\n        match_length = 0\n        for j in range(m):\n            if sequence[i + j] == pattern[j]:\n                match_length += 1\n            else:\n                break\n        \n        if match_length == m:\n            return i\n    \n    return -1"
    },
    {
        "function_name": "motif_search",
        "file_name": "dna_analysis.py",
        "parameters": {
            "`sequences`": "A list of lists of integers representing the DNA sequences to be analyzed",
            "`motif_length`": "An integer representing the length of the motif to be searched",
            "`hamming_distance`": "An integer representing the maximum Hamming distance allowed between the motif and the occurrences"
        },
        "objectives": [
            "Use the expectation-maximization (EM) algorithm to search for a motif in the DNA sequences",
            "Initialize the motif as a random sequence of the given length and iteratively update its frequency and occurrences in the sequences",
            "Filter out the occurrences with a Hamming distance greater than the specified threshold and update the motif's frequency accordingly"
        ],
        "import_lines": [
            "import random",
            "import numpy as np"
        ],
        "function_def": "def motif_search(sequences, motif_length, hamming_distance):\n    # Initialize the motif as a random sequence\n    motif = [random.randint(0, 3) for _ in range(motif_length)]\n    \n    # Initialize the frequency and occurrences of the motif\n    freq = np.zeros((motif_length, 4))\n    occurrences = []\n    \n    for _ in range(100):  # max iterations\n        # Initialize the occurrences for this iteration\n        occurrences = []\n        \n        # Search for the motif in the sequences with the given Hamming distance\n        for seq in sequences:\n            for i in range(len(seq) - motif_length + 1):\n                dist = sum(el1 != el2 for el1, el2 in zip(seq[i:i+motif_length], motif))\n                if dist <= hamming_distance:\n                    occurrences.append((seq, i))\n        \n        # Update the frequency of the motif\n        freq.fill(0)\n        for seq, i in occurrences:\n            for j, el in enumerate(seq[i:i+motif_length]):\n                freq[j, el] += 1 / len(occurrences)\n        \n        # Update the motif using the expectation-maximization (EM) algorithm\n        motif = np.argmax(freq, axis=1).tolist()\n    \n    return motif"
    },
    {
        "function_name": "graph_embedder",
        "file_name": "graph_embedding.py",
        "parameters": {
            "`graph`": "A dictionary representing the adjacency list of a graph",
            "`vector`": "A list of integers representing the vector to be represented as a graph",
            "`dimension`": "An integer representing the dimension of the vector"
        },
        "objectives": [
            "Use a graph embedding algorithm (e.g., GraphSAGE) to embed the given vector as a graph",
            "Initialize the node features randomly and iteratively update them using the embedding algorithm",
            "Filter out nodes with a degree greater than the dimension and normalize the node features"
        ],
        "import_lines": [
            "import random",
            "import numpy as np"
        ],
        "function_def": "def graph_embedder(graph, vector, dimension):\n    # Initialize the node features randomly\n    node_features = {node: [random.random() for _ in range(dimension)] for node in graph}\n    \n    # Initialize the edge features randomly\n    edge_features = {(node1, node2): [random.random() for _ in range(dimension)] for node1 in graph for node2 in graph[node1]}\n    \n    for _ in range(100):  # max iterations\n        # Update the node features using GraphSAGE\n        for node in graph:\n            neighbors = graph[node]\n            node_features[node] = np.mean([node_features[neighbor] for neighbor in neighbors], axis=0)\n        \n        # Update the edge features using GraphSAGE\n        for node1, node2 in edge_features:\n            node_features[node1] += node_features[node2]\n            node_features[node2] += node_features[node1]\n        \n        # Filter out nodes with a degree greater than the dimension and normalize the node features\n        filtered_node_features = {node: features for node, features in node_features.items() if len(graph[node]) <= dimension}\n        filtered_node_features = {node: features / np.linalg.norm(features) for node, features in filtered_node_features.items()}\n    \n    # Return the embedded vector\n    return filtered_node_features"
    },
    {
        "function_name": "disease_spreader",
        "file_name": "epidemiology.py",
        "parameters": {
            "`network`": "A dictionary representing the social network",
            "`initial_infected`": "A list of nodes representing the initially infected individuals",
            "`infection_rate`": "A float representing the infection rate of the disease",
            "`recovery_rate`": "A float representing the recovery rate of the disease",
            "`time_steps`": "An integer representing the number of time steps to simulate"
        },
        "objectives": [
            "Simulate the spread of a disease through a social network using the SIR model",
            "Initialize the infected, recovered, and susceptible individuals and iteratively update their statuses",
            "Return the final number of infected and recovered individuals"
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def disease_spreader(network, initial_infected, infection_rate, recovery_rate, time_steps):\n    # Initialize the infected, recovered, and susceptible individuals\n    infected = set(initial_infected)\n    recovered = set()\n    susceptible = set(network) - infected - recovered\n    \n    for _ in range(time_steps):\n        # Infect susceptible individuals\n        for node in susceptible.copy():\n            for neighbor in network[node]:\n                if neighbor in infected:\n                    if random.random() < infection_rate:\n                        infected.add(node)\n                        susceptible.remove(node)\n        \n        # Recover infected individuals\n        for node in infected.copy():\n            if random.random() < recovery_rate:\n                recovered.add(node)\n                infected.remove(node)\n    \n    return len(infected), len(recovered)"
    },
    {
        "function_name": "merge_intervals",
        "file_name": "interval_operations.py",
        "parameters": {
            "intervals": "A list of tuples representing intervals, where each tuple contains two integers",
            "new_interval": "A tuple representing a new interval"
        },
        "objectives": [
            "Merge the given intervals with the new interval.",
            "Handle cases where the new interval overlaps with multiple existing intervals.",
            "Return the merged list of intervals."
        ],
        "import_lines": [],
        "function_def": "def merge_intervals(intervals, new_interval):\n    intervals.append(new_interval)\n    intervals.sort(key=lambda x: x[0])\n    merged = [intervals[0]]\n    for current in intervals[1:]:\n        if merged[-1][1] >= current[0]:\n            merged[-1] = (merged[-1][0], max(merged[-1][1], current[1]))\n        else:\n            merged.append(current)\n    return merged"
    },
    {
        "function_name": "rotate_list",
        "file_name": "list_operations.py",
        "parameters": {
            "nums": "A list of integers",
            "k": "An integer representing the number of rotations"
        },
        "objectives": [
            "Rotate the given list of numbers to the right by k steps.",
            "Handle cases where k is greater than the length of the list.",
            "Return the rotated list."
        ],
        "import_lines": [],
        "function_def": "def rotate_list(nums, k):\n    k = k % len(nums)\n    return nums[-k:] + nums[:-k]"
    },
    {
        "function_name": "collaborative_filtering",
        "file_name": "social_network.py",
        "parameters": {
            "`network`": "A dictionary representing a social network with users and their connections.",
            "`seed_user`": "A string representing the seed user.",
            "`num_recommendations`": "An integer representing the number of recommendations to generate."
        },
        "objectives": [
            "Use collaborative filtering to generate recommendations for the seed user.",
            "Calculate the similarity between the seed user and other users in the network.",
            "Return a list of recommended users."
        ],
        "import_lines": [
            "from scipy.spatial.distance import cosine"
        ],
        "function_def": "def collaborative_filtering(network, seed_user, num_recommendations):\n    seed_user_connections = network[seed_user]\n    user_similarities = {}\n    \n    for user, connections in network.items():\n        if user != seed_user:\n            similarity = 1 - cosine(seed_user_connections, connections)\n            user_similarities[user] = similarity\n    \n    recommended_users = sorted(user_similarities, key=user_similarities.get, reverse=True)[:num_recommendations]\n    \n    return recommended_users"
    },
    {
        "function_name": "tfidf",
        "file_name": "tfidf.py",
        "parameters": {
            "`text`": "A string representing the input text",
            "`k`": "An integer representing the number of top words to return",
            "`stop_words`": "A list of strings representing the stop words to ignore"
        },
        "objectives": [
            "Use the TF-IDF (Term Frequency-Inverse Document Frequency) algorithm to calculate the importance of each word in the given text.",
            "Calculate the term frequency by counting the number of occurrences of each word in the text.",
            "Calculate the inverse document frequency by considering the rarity of each word in a large corpus of texts.",
            "Use the TF-IDF scores to find the k most important words in the text.",
            "Return the k most important words along with their TF-IDF scores."
        ],
        "import_lines": [
            "from collections import defaultdict",
            "from math import log",
            "from nltk.corpus import stopwords",
            "import nltk"
        ],
        "function_def": "def tfidf(text, k, stop_words):\n    # Calculate the term frequency\n    tf = defaultdict(int)\n    words = text.split()\n    for word in words:\n        if word not in stop_words:\n            tf[word] += 1\n    \n    # Calculate the inverse document frequency\n    idf = defaultdict(int)\n    total_documents = 10000\n    for word in tf:\n        idf[word] = log(total_documents / (tf[word] + 1))\n    \n    # Use the TF-IDF scores to find the k most important words\n    tfidf_scores = defaultdict(int)\n    for word in tf:\n        tfidf_scores[word] = tf[word] * idf[word]\n    \n    top_words = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n    \n    return top_words"
    },
    {
        "function_name": "row_clustering",
        "file_name": "matrix_analysis.py",
        "parameters": {
            "`matrix`": "A 2D list representing the input matrix",
            "`k`": "An integer representing the number of clusters"
        },
        "objectives": [
            "Use the k-means clustering algorithm to partition the rows of the matrix into clusters.",
            "Calculate the silhouette score of the clustering.",
            "Return the clusters and the silhouette score."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def row_clustering(matrix, k):\n    num_rows = len(matrix)\n    row_vectors = np.array(matrix)\n    cluster_centers = np.random.rand(k, len(matrix[0]))\n    for _ in range(10):\n        for i in range(num_rows):\n            distances = np.linalg.norm(row_vectors[i] - cluster_centers, axis=1)\n            cluster_assignment = np.argmin(distances)\n            cluster_centers[cluster_assignment] += row_vectors[i]\n        for i in range(k):\n            cluster_centers[i] = cluster_centers[i] / len([node for node in range(num_rows) if np.argmin(np.linalg.norm(row_vectors[node] - cluster_centers, axis=1)) == i])\n    clusters = [list() for _ in range(k)]\n    for i in range(num_rows):\n        distances = np.linalg.norm(row_vectors[i] - cluster_centers, axis=1)\n        clusters[np.argmin(distances)].append(i)\n    silhouette = []\n    for i in range(num_rows):\n        cluster_assignment = np.argmin(np.linalg.norm(row_vectors[i] - cluster_centers, axis=1))\n        cluster_distances = np.linalg.norm(row_vectors[i] - row_vectors[clusters[cluster_assignment]], axis=1)\n        min_distance = np.min(cluster_distances)\n        max_distance = np.max(cluster_distances)\n        silhouette.append((max_distance - min_distance) / max(cluster_distances))\n    return clusters, np.mean(silhouette)"
    },
    {
        "function_name": "dynamic_histogram_binning",
        "file_name": "image_processing.py",
        "parameters": {
            "`freq`": "A list of frequencies of different colors in an image.",
            "`intensities`": "A list of intensities corresponding to the frequencies.",
            "`num_bins`": "The number of bins for the histogram."
        },
        "objectives": [
            "Implement the Dynamic Histogram Binning algorithm to reduce the number of bins for better visualization.",
            "Use dynamic programming to find the optimal binning strategy.",
            "Return the resulting histogram."
        ],
        "import_lines": [],
        "function_def": "def dynamic_histogram_binning(freq, intensities, num_bins):\n    n = len(freq)\n    dp = [[0] * (num_bins + 1) for _ in range(n + 1)]\n    bins = [[[] for _ in range(num_bins + 1)] for _ in range(n + 1)]\n    \n    for i in range(1, n + 1):\n        for j in range(1, min(i, num_bins) + 1):\n            if j == 1:\n                dp[i][j] = sum(freq[:i])\n                bins[i][j] = [list(range(i))]\n            else:\n                max_val = float('-inf')\n                best_split = -1\n                for k in range(1, i):\n                    val = dp[k][j - 1] + sum(freq[k:i])\n                    if val > max_val:\n                        max_val = val\n                        best_split = k\n                dp[i][j] = max_val\n                bins[i][j] = bins[best_split][j - 1] + [list(range(best_split, i))]\n                \n    histogram = []\n    for bin in bins[n][num_bins][::-1]:\n        count = sum(freq[i] for i in bin)\n        intensity = sum(intensities[i] for i in bin) / count\n        histogram.append((count, intensity))\n        \n    return histogram"
    },
    {
        "function_name": "exponential_moving_average",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "`nums`": "A list of integers.",
            "`window_size`": "An integer representing the size of the window."
        },
        "objectives": [
            "Implement the Exponential Moving Average (EMA) algorithm to smooth the data.",
            "Use a recursive formula to calculate the EMA at each step.",
            "Return the smoothed data."
        ],
        "import_lines": [],
        "function_def": "def exponential_moving_average(nums, window_size):\n    alpha = 2 / (window_size + 1)\n    ema = [nums[0]]\n    \n    for i in range(1, len(nums)):\n        ema.append(alpha * nums[i] + (1 - alpha) * ema[-1])\n        \n    return ema"
    },
    {
        "function_name": "find_motifs",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "`sequences`": "A list of DNA sequences as strings",
            "`motif_length`": "An integer representing the length of the motif to search for",
            "`num_occurrences`": "An integer representing the minimum number of occurrences of the motif"
        },
        "objectives": [
            "Implement the Apriori algorithm to find all possible motifs of the given length.",
            "Filter the motifs based on their occurrences in the sequences.",
            "Return the top 10 most frequent motifs."
        ],
        "import_lines": [
            "import collections",
            "import re"
        ],
        "function_def": "def find_motifs(sequences, motif_length, num_occurrences):\n    motifs = set()\n    for sequence in sequences:\n        for i in range(len(sequence) - motif_length + 1):\n            motif = sequence[i:i + motif_length]\n            motifs.add(motif)\n    \n    motif_counts = collections.defaultdict(int)\n    for motif in motifs:\n        for sequence in sequences:\n            motif_counts[motif] += len(re.findall(motif, sequence))\n    \n    top_motifs = sorted(motif_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n    return top_motifs"
    },
    {
        "function_name": "caesar_decipher",
        "file_name": "cryptography.py",
        "parameters": {
            "`text`": "A string of text.",
            "`key`": "A list of characters to use for decryption."
        },
        "objectives": [
            "Implement a Caesar cipher to decrypt the given text using the provided key.",
            "Shift the characters in the text back by the corresponding key character.",
            "Return the decrypted text."
        ],
        "import_lines": [],
        "function_def": "def caesar_decipher(text, key):\n    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n    decrypted_text = ''\n    key_index = 0\n    \n    for char in text:\n        if char.isalpha():\n            shift = alphabet.index(key[key_index % len(key)].lower())\n            if char.isupper():\n                decrypted_text += alphabet[(alphabet.index(char.lower()) - shift) % 26].upper()\n            else:\n                decrypted_text += alphabet[(alphabet.index(char) - shift) % 26]\n            key_index += 1\n        else:\n            decrypted_text += char\n    \n    return decrypted_text"
    },
    {
        "function_name": "substring_frequencies",
        "file_name": "string_frequencies.py",
        "parameters": {
            "`strings`": "A list of strings",
            "`k`": "An integer representing the size of the substrings"
        },
        "objectives": [
            "For each string, generate all possible substrings of size 'k'.",
            "Count the frequency of each substring across all strings.",
            "Return a dictionary mapping each substring to its frequency."
        ],
        "import_lines": [],
        "function_def": "def substring_frequencies(strings, k):\n    frequency_dict = {}\n    for string in strings:\n        for i in range(len(string) - k + 1):\n            substring = string[i:i+k]\n            if substring in frequency_dict:\n                frequency_dict[substring] += 1\n            else:\n                frequency_dict[substring] = 1\n    \n    return frequency_dict"
    },
    {
        "function_name": "top_words_extractor",
        "file_name": "nlp_operations.py",
        "parameters": {
            "`text`": "A string representing the text to be processed.",
            "`n`": "An integer representing the number of top words to extract.",
            "`stop_words`": "A list of words to be ignored during the extraction process."
        },
        "objectives": [
            "Tokenize the text into individual words.",
            "Remove the stop words from the tokenized text.",
            "Calculate the frequency of each word.",
            "Extract the top n words with the highest frequency.",
            "Return the top n words along with their frequencies."
        ],
        "import_lines": [
            "from collections import Counter",
            "import re"
        ],
        "function_def": "def top_words_extractor(text, n, stop_words):\n    # Tokenize the text\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Remove stop words\n    words = [word for word in words if word not in stop_words]\n    \n    # Calculate word frequency\n    word_freq = Counter(words)\n    \n    # Extract top n words\n    top_n_words = word_freq.most_common(n)\n    \n    return top_n_words"
    },
    {
        "function_name": "non_maximum_suppression",
        "file_name": "image_processing.py",
        "parameters": {
            "`matrix`": "A 2D list representing the matrix.",
            "`threshold`": "A float representing the threshold value."
        },
        "objectives": [
            "Implement a function to perform non-maximum suppression on the given matrix.",
            "Apply the non-maximum suppression algorithm in both horizontal and vertical directions.",
            "Return the suppressed matrix."
        ],
        "import_lines": [],
        "function_def": "def non_maximum_suppression(matrix, threshold):\n    # Create a copy of the matrix\n    suppressed_matrix = [row[:] for row in matrix]\n    \n    # Apply non-maximum suppression in the horizontal direction\n    for i in range(len(matrix)):\n        for j in range(len(matrix[i])):\n            if matrix[i][j] > threshold:\n                for k in range(len(matrix[i])):\n                    if k != j and matrix[i][k] > matrix[i][j]:\n                        suppressed_matrix[i][j] = 0\n    \n    # Apply non-maximum suppression in the vertical direction\n    for j in range(len(matrix[0])):\n        for i in range(len(matrix)):\n            if matrix[i][j] > threshold:\n                for k in range(len(matrix)):\n                    if k != i and matrix[k][j] > matrix[i][j]:\n                        suppressed_matrix[i][j] = 0\n    \n    return suppressed_matrix"
    },
    {
        "function_name": "convolution",
        "file_name": "image_processing_algorithms.py",
        "parameters": {
            "`image`": "A 2D list representing the pixels of the image",
            "`kernel`": "A 2D list representing the kernel for convolution"
        },
        "objectives": [
            "Implement the convolution operation with padding and stride.",
            "Apply the kernel to the image by flipping the kernel horizontally and vertically.",
            "Return the output image after convolution."
        ],
        "import_lines": [],
        "function_def": "def convolution(image, kernel):\n    # Calculate the shape of the output image after convolution\n    output_rows = len(image) - len(kernel) + 1\n    output_cols = len(image[0]) - len(kernel[0]) + 1\n    \n    # Initialize the output image with zeros\n    output_image = [[0 for _ in range(output_cols)] for _ in range(output_rows)]\n    \n    # Apply the kernel to the image by flipping the kernel horizontally and vertically\n    for row in range(output_rows):\n        for col in range(output_cols):\n            for kernel_row in range(len(kernel)):\n                for kernel_col in range(len(kernel[0])):\n                    output_image[row][col] += image[row + kernel_row][col + kernel_col] * kernel[len(kernel) - kernel_row - 1][len(kernel[0]) - kernel_col - 1]\n    \n    return output_image"
    },
    {
        "function_name": "product_recommendations",
        "file_name": "data_mining.py",
        "parameters": {
            "`transactions`": "A list of lists, where each sublist contains a customer's transaction history in the form [date, item, quantity, price]",
            "`num_recommendations`": "An integer representing the number of product recommendations to return for each customer"
        },
        "objectives": [
            "Use association rule mining to find the most popular product combinations for each customer.",
            "Implement a measure of confidence and support to filter out weak associations.",
            "Return the top N product recommendations for each customer."
        ],
        "import_lines": [
            "import pandas as pd",
            "from collections import defaultdict"
        ],
        "function_def": "def product_recommendations(transactions, num_recommendations):\n    # Create a dictionary to store the transaction history for each customer\n    customer_transactions = defaultdict(list)\n    for transaction in transactions:\n        customer_transactions[transaction[0]].append((transaction[1], transaction[3]))\n    \n    # Implement the Apriori algorithm to find frequent itemsets\n    frequent_itemsets = []\n    for customer in customer_transactions:\n        itemsets = []\n        for item in customer_transactions[customer]:\n            itemsets.append(item[0])\n        itemsets = list(set(itemsets))\n        for i in range(len(itemsets)):\n            for j in range(i + 1, len(itemsets)):\n                frequent_itemsets.append((itemsets[i], itemsets[j]))\n    \n    # Calculate the confidence and support for each itemset\n    itemset_confidence = {}\n    itemset_support = {}\n    for itemset in frequent_itemsets:\n        confidence = 0\n        support = 0\n        for customer in customer_transactions:\n            if itemset[0] in [item[0] for item in customer_transactions[customer]] and itemset[1] in [item[0] for item in customer_transactions[customer]]:\n                confidence += 1\n                support += 1\n        confidence /= len(customer_transactions)\n        support /= len(customer_transactions)\n        itemset_confidence[itemset] = confidence\n        itemset_support[itemset] = support\n    \n    # Filter out weak associations\n    filtered_itemsets = []\n    for itemset in itemset_confidence:\n        if itemset_confidence[itemset] > 0.5 and itemset_support[itemset] > 0.1:\n            filtered_itemsets.append(itemset)\n    \n    # Return the top N product recommendations for each customer\n    recommendations = {}\n    for customer in customer_transactions:\n        recommended_products = []\n        for itemset in filtered_itemsets:\n            if itemset[0] in [item[0] for item in customer_transactions[customer]] or itemset[1] in [item[0] for item in customer_transactions[customer]]:\n                recommended_products.append(itemset[0])\n                recommended_products.append(itemset[1])\n        recommended_products = list(set(recommended_products))\n        recommended_products.sort(key=lambda x: recommended_products.count(x), reverse=True)\n        recommendations[customer] = recommended_products[:num_recommendations]\n    \n    return recommendations"
    },
    {
        "function_name": "feature_extraction",
        "file_name": "tree_algorithms.py",
        "parameters": {
            "`trees`": "A list of trees, where each tree is represented as a list of nodes and their edges",
            "`num_features`": "An integer representing the number of features to extract from each tree"
        },
        "objectives": [
            "Use a feature extraction algorithm to extract relevant features from each tree.",
            "Select the top N features based on their importance score.",
            "Return the feature vector for each tree."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def feature_extraction(trees, num_features):\n    # Extract features from each tree\n    feature_vectors = []\n    for tree in trees:\n        features = []\n        for node in tree:\n            features.append(len(node))\n            features.append(len([edge for edge in node[1] if edge in tree]))\n        feature_vectors.append(features)\n    \n    # Calculate the importance score for each feature\n    importance_scores = []\n    for feature in zip(*feature_vectors):\n        score = np.std(feature)\n        importance_scores.append(score)\n    \n    # Select the top N features\n    top_features = np.argsort(importance_scores)[-num_features:]\n    \n    # Return the feature vector for each tree\n    feature_vectors = [np.array([feature_vectors[i][j] for j in top_features]) for i in range(len(feature_vectors))]\n    return feature_vectors"
    },
    {
        "function_name": "information_spread",
        "file_name": "network_algorithms.py",
        "parameters": {
            "`network`": "A dictionary representing a social network, where each key is a node and each value is a list of neighboring nodes",
            "`num_steps`": "An integer representing the number of steps to perform in the random walk"
        },
        "objectives": [
            "Use a random walk approach to simulate the spread of information through the network.",
            "Calculate the probability of reaching each node from a given start node.",
            "Return the probability distribution over the nodes."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def information_spread(network, num_steps):\n    # Initialize the probability distribution\n    probabilities = {node: 0 for node in network}\n    start_node = random.choice(list(network.keys()))\n    probabilities[start_node] = 1\n    \n    # Perform the random walk\n    for _ in range(num_steps):\n        new_probabilities = {node: 0 for node in network}\n        for node in probabilities:\n            neighbors = network[node]\n            probability = probabilities[node] / len(neighbors)\n            for neighbor in neighbors:\n                new_probabilities[neighbor] += probability\n        probabilities = new_probabilities\n    \n    return probabilities"
    },
    {
        "function_name": "puzzle_solver",
        "file_name": "puzzle_algorithms.py",
        "parameters": {
            "`tiles`": "A list of lists representing a puzzle, where each inner list is a row of tiles",
            "`target_tile`": "A tile that needs to be moved to the target position",
            "`target_position`": "A tuple representing the target position"
        },
        "objectives": [
            "Implement a breadth-first search (BFS) algorithm to find the shortest path to move the target tile to the target position.",
            "If no path is found, return an empty list.",
            "If a path is found, return the sequence of moves as a list of tuples."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def puzzle_solver(tiles, target_tile, target_position):\n    queue = deque([(tiles, [])])\n    visited = set()\n    \n    while queue:\n        puzzle, moves = queue.popleft()\n        puzzle_tuple = tuple(tuple(row) for row in puzzle)\n        \n        if puzzle_tuple in visited:\n            continue\n        visited.add(puzzle_tuple)\n        \n        for i in range(len(puzzle)):\n            for j in range(len(puzzle[i])):\n                if puzzle[i][j] == target_tile:\n                    target_i, target_j = i, j\n                    break\n        \n        if (target_i, target_j) == target_position:\n            return moves\n        \n        for di, dj in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            new_i, new_j = target_i + di, target_j + dj\n            \n            if 0 <= new_i < len(puzzle) and 0 <= new_j < len(puzzle[i]):\n                new_puzzle = [row[:] for row in puzzle]\n                new_puzzle[target_i][target_j], new_puzzle[new_i][new_j] = new_puzzle[new_i][new_j], new_puzzle[target_i][target_j]\n                queue.append((new_puzzle, moves + [(di, dj)]))\n    \n    return []"
    },
    {
        "function_name": "apply_functions_to_forest",
        "file_name": "tree_processing.py",
        "parameters": {
            "`trees`": "A list of lists representing the nodes of each tree in a forest, where each node is represented as a tuple (value, left child index, right child index)",
            "`functions`": "A list of functions that can be applied to each node in the trees",
            "`aggregation_function`": "A function that takes a list of values and returns a single value"
        },
        "objectives": [
            "For each tree in the forest, apply each function in `functions` to each node in the tree, and store the results in a list.",
            "For each tree, use the `aggregation_function` to aggregate the results from each function, and store the final result in a list.",
            "Return the list of final results for each tree."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def apply_functions_to_forest(trees, functions, aggregation_function):\n    results = []\n    for tree in trees:\n        node_values = [0] * len(tree)\n        for func in functions:\n            function_results = [0] * len(tree)\n            for i, node in enumerate(tree):\n                value, left, right = node\n                function_results[i] = func(value, tree[left] if left != -1 else None, tree[right] if right != -1 else None)\n            node_values = np.add(node_values, function_results)\n        results.append(aggregation_function(node_values))\n    return results"
    },
    {
        "function_name": "max_flow_with_capacity_constraints",
        "file_name": "graph_analysis.py",
        "parameters": {
            "`graph`": "A dictionary representing the adjacency list of a graph, where each key is a node and its value is a list of its neighbors",
            "`source`": "A node in the graph representing the source node",
            "`env_capacity`": "An integer representing the capacity of the environment"
        },
        "objectives": [
            "Use Dijkstra's algorithm to find the shortest path from the source node to all other nodes in the graph, taking into account the capacity constraints.",
            "For each node, calculate the maximum flow that can be pushed from the source node to that node.",
            "Return a dictionary where the keys are the nodes and the values are the maximum flows."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def max_flow_with_capacity_constraints(graph, source, env_capacity):\n    max_flows = {node: 0 for node in graph}\n    max_flows[source] = env_capacity\n    queue = [(0, source)]\n    while queue:\n        flow, node = heapq.heappop(queue)\n        for neighbor in graph[node]:\n            residual_capacity = min(max_flows[node] - flow, env_capacity)\n            if residual_capacity > 0:\n                new_flow = flow + residual_capacity\n                if new_flow > max_flows[neighbor]:\n                    max_flows[neighbor] = new_flow\n                    heapq.heappush(queue, (new_flow, neighbor))\n    return max_flows"
    },
    {
        "function_name": "newton_method_for_roots",
        "file_name": "polynomial_equations.py",
        "parameters": {
            "`roots`": "A list of integers representing the roots of a polynomial equation",
            "`coefficients`": "A list of integers representing the coefficients of the polynomial equation",
            "`precision`": "An integer representing the desired precision of the result"
        },
        "objectives": [
            "Use Newton's method to find the roots of the polynomial equation.",
            "For each root, calculate the multiplicity of the root.",
            "Return a list of tuples, where each tuple contains the root and its multiplicity."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def newton_method_for_roots(roots, coefficients, precision):\n    results = []\n    for root in roots:\n        x = root\n        for _ in range(precision):\n            x = x - np.polyval(coefficients, x) / np.polyval(np.polyder(coefficients), x)\n        multiplicity = 1\n        while np.isclose(np.polyval(coefficients, x), 0):\n            multiplicity += 1\n            x += 1e-6\n        results.append((root, multiplicity))\n    return results"
    },
    {
        "function_name": "edmonds_karp_algorithm",
        "file_name": "flow_algorithms.py",
        "parameters": {
            "graph": "A dictionary representing an adjacency list of the graph.",
            "source_node": "A string representing the source node for the shortest path.",
            "target_node": "A string representing the target node for the shortest path.",
            "capacity": "A dictionary representing the capacity of each edge in the graph.",
            "flow": "A dictionary representing the current flow of each edge in the graph."
        },
        "objectives": [
            "Find the shortest path from the source node to the target node in the residual graph.",
            "Update the residual capacities of the edges along the path.",
            "Return a dictionary containing the updated residual capacities and the maximum flow that can be pushed along the path."
        ],
        "import_lines": [
            "from collections import defaultdict, deque"
        ],
        "function_def": "def edmonds_karp_algorithm(graph, source_node, target_node, capacity, flow):\n    residual_capacity = defaultdict(int)\n    for u in graph:\n        for v in graph[u]:\n            residual_capacity[(u, v)] = capacity[(u, v)] - flow.get((u, v), 0)\n            residual_capacity[(v, u)] = flow.get((u, v), 0)\n    \n    parent = {}\n    queue = deque()\n    queue.append(source_node)\n    while queue and target_node not in parent:\n        u = queue.popleft()\n        for v in graph[u]:\n            if v not in parent and residual_capacity[(u, v)] > 0:\n                parent[v] = u\n                queue.append(v)\n                \n    if target_node not in parent:\n        return residual_capacity, 0\n    \n    path_flow = float(\"inf\")\n    s = target_node\n    while s != source_node:\n        path_flow = min(path_flow, residual_capacity[(parent[s], s)])\n        s = parent[s]\n        \n    v = target_node\n    while v != source_node:\n        u = parent[v]\n        residual_capacity[(u, v)] -= path_flow\n        residual_capacity[(v, u)] += path_flow\n        flow[(u, v)] = flow.get((u, v), 0) + path_flow\n        v = parent[v]\n        \n    return residual_capacity, path_flow"
    },
    {
        "function_name": "ngram_segmenter",
        "file_name": "nlp_segmenter.py",
        "parameters": {
            "`text`": "A string representing the text to be segmented.",
            "`ngrams`": "A list of strings representing the n-grams."
        },
        "objectives": [
            "Segment the input text into n-grams.",
            "Use a greedy approach to find the longest matching n-gram at each position.",
            "Handle out-of-vocabulary words by splitting them into sub-word units.",
            "Return the segmented text as a list of n-grams."
        ],
        "import_lines": [],
        "function_def": "def ngram_segmenter(text, ngrams):\n    result = []\n    i = 0\n    while i < len(text):\n        for n in range(len(ngrams), 0, -1):\n            if text[i:i + n] == ngrams[n - 1]:\n                result.append(ngrams[n - 1])\n                i += n\n                break\n        else:\n            # Handle out-of-vocabulary word\n            if text[i] == \" \":\n                result.append(\" \")\n                i += 1\n            else:\n                # Split into sub-word units\n                for j in range(len(text[i:]), 0, -1):\n                    if text[i:i + j] in ngrams:\n                        result.append(text[i:i + j])\n                        i += j\n                        break\n                else:\n                    # If the sub-word unit is not found in ngrams, split it into individual characters\n                    result.append(text[i])\n                    i += 1\n                    \n    return result"
    },
    {
        "function_name": "sequence_finder",
        "file_name": "sequence_finder.py",
        "parameters": {
            "sequences": "A list of lists of integers representing the sequences.",
            "k": "An integer representing the size of the sequences."
        },
        "objectives": [
            "Find all sequences of size k that appear at least once in the input sequences.",
            "Use a sliding window approach to find all such sequences.",
            "Return a list of such sequences."
        ],
        "import_lines": [],
        "function_def": "def sequence_finder(sequences, k):\n    seen = set()\n    result = []\n    \n    for sequence in sequences:\n        for i in range(len(sequence) - k + 1):\n            sub_sequence = tuple(sequence[i:i + k])\n            if sub_sequence not in seen:\n                seen.add(sub_sequence)\n                result.append(sub_sequence)\n                \n    return result"
    },
    {
        "function_name": "population_simulation",
        "file_name": "population_model.py",
        "parameters": {
            "`population`": "A list of integers representing the population of different cities",
            "`migration_rates`": "A list of lists of floats representing the migration rates between cities",
            "`growth_rates`": "A list of floats representing the growth rates of different cities"
        },
        "objectives": [
            "Simulate the population growth and migration between cities using the Leslie matrix model.",
            "At each time step, calculate the new population of each city based on the current population, migration rates, and growth rates.",
            "Find the city with the highest population at the end of the simulation.",
            "Return the final population distribution and the city with the highest population."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def population_simulation(population, migration_rates, growth_rates, num_steps):\n    num_cities = len(population)\n    leslie_matrix = np.array([[growth_rates[i] + migration_rates[i][i] for i in range(num_cities)] for _ in range(num_cities)])\n    for i in range(num_cities):\n        for j in range(num_cities):\n            if i != j:\n                leslie_matrix[i][j] = migration_rates[i][j]\n    population_distribution = np.array([population])\n    for _ in range(num_steps):\n        new_population = np.dot(leslie_matrix, population_distribution[-1])\n        population_distribution = np.vstack((population_distribution, new_population))\n    max_population_city = np.argmax(population_distribution[-1])\n    return population_distribution, max_population_city"
    },
    {
        "function_name": "sample_generator",
        "file_name": "sample_generator.py",
        "parameters": {
            "`probability_distribution`": "A list of floats representing a probability distribution",
            "`num_samples`": "An integer representing the number of samples to generate"
        },
        "objectives": [
            "Generate samples from the probability distribution using the inverse transform sampling method.",
            "Handle the case where the probability distribution has zero probabilities.",
            "Return the generated samples."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def sample_generator(probability_distribution, num_samples):\n    # Handle the case where the probability distribution has zero probabilities\n    non_zero_probabilities = [p for p in probability_distribution if p > 0]\n    cumulative_distribution = np.cumsum(non_zero_probabilities)\n    \n    # Generate samples from the probability distribution using the inverse transform sampling method\n    samples = []\n    for _ in range(num_samples):\n        uniform_sample = np.random.rand()\n        sample = np.searchsorted(cumulative_distribution, uniform_sample)\n        samples.append(sample)\n    \n    return samples"
    },
    {
        "function_name": "matrix_window_stats",
        "file_name": "matrix_stats.py",
        "parameters": {
            "`matrix`": "A 2D list of integers",
            "`k`": "An integer representing the size of the window"
        },
        "objectives": [
            "Divide the matrix into sub-matrices of size 'k x k'.",
            "For each sub-matrix, calculate the median and standard deviation of its elements.",
            "Return the list of medians and the list of standard deviations."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def matrix_window_stats(matrix, k):\n    medians = []\n    std_devs = []\n    for i in range(0, len(matrix), k):\n        for j in range(0, len(matrix[0]), k):\n            sub_matrix = [row[j:j+k] for row in matrix[i:i+k]]\n            median = np.median(sub_matrix)\n            std_dev = np.std(sub_matrix)\n            medians.append(median)\n            std_devs.append(std_dev)\n    return medians, std_devs"
    },
    {
        "function_name": "sort_tuples",
        "file_name": "data_structures.py",
        "parameters": {
            "`tuples`": "A list of tuples representing the items to be sorted, where each tuple contains two integers representing the item's value and priority",
            "`n`": "An integer representing the number of items"
        },
        "objectives": [
            "Implement a priority queue using a binary heap to sort the items based on their priorities",
            "Use the heap to efficiently extract the item with the highest priority",
            "Return the sorted list of items"
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def sort_tuples(tuples, n):\n    heap = []\n    for tuple in tuples:\n        heapq.heappush(heap, (-tuple[1], tuple[0]))\n    sorted_tuples = []\n    for _ in range(n):\n        priority, value = heapq.heappop(heap)\n        sorted_tuples.append((value, -priority))\n    return sorted_tuples"
    },
    {
        "function_name": "lee_algorithm",
        "file_name": "game_theory.py",
        "parameters": {
            "`grid`": "A 2D list representing the game grid, where each cell is an integer representing the cell's value",
            "`n`": "An integer representing the number of rows in the grid",
            "`m`": "An integer representing the number of columns in the grid",
            "`k`": "An integer representing the number of steps"
        },
        "objectives": [
            "Implement the Lee algorithm to find the shortest path from the top-left cell to the bottom-right cell in the grid",
            "Use dynamic programming to efficiently explore the grid and find the shortest path",
            "Return the shortest path as a list of coordinates"
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def lee_algorithm(grid, n, m, k):\n    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n    queue = deque([(0, 0, 0)])\n    distance = {(0, 0): 0}\n    while queue:\n        x, y, step = queue.popleft()\n        if (x, y) == (n-1, m-1):\n            break\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if (0 <= nx < n) and (0 <= ny < m) and grid[nx][ny] != -1:\n                new_distance = distance[(x, y)] + 1\n                if (nx, ny) not in distance or new_distance < distance[(nx, ny)]:\n                    distance[(nx, ny)] = new_distance\n                    queue.append((nx, ny, new_distance))\n    path = []\n    current = (n-1, m-1)\n    while current != (0, 0):\n        path.append(current)\n        for dx, dy in directions:\n            nx, ny = current[0] + dx, current[1] + dy\n            if (0 <= nx < n) and (0 <= ny < m) and distance[current] - 1 == distance.get((nx, ny), float('inf')):\n                current = (nx, ny)\n                break\n    path.append((0, 0))\n    return list(reversed(path))"
    },
    {
        "function_name": "forest_planning",
        "file_name": "optimization_algorithms.py",
        "parameters": {
            "`trees`": "A list of tuples representing the trees, where each tuple contains two integers representing the height and width of the tree",
            "`forest_width`": "An integer representing the width of the forest"
        },
        "objectives": [
            "Implement a greedy algorithm to plant the trees in the forest such that the total width of the trees is maximized.",
            "Calculate the total width of the trees in the forest.",
            "Return the planting order of the trees and the total width."
        ],
        "import_lines": [],
        "function_def": "def forest_planning(trees, forest_width):\n    trees.sort(key=lambda x: x[1], reverse=True)\n    planting_order = []\n    total_width = 0\n    \n    for tree in trees:\n        if total_width + tree[1] <= forest_width:\n            planting_order.append(tree)\n            total_width += tree[1]\n    \n    return planting_order, total_width"
    },
    {
        "function_name": "median_filter",
        "file_name": "image_processing.py",
        "parameters": {
            "`matrix`": "A 2D list representing the matrix",
            "`radius`": "An integer representing the radius of the filter",
            "`threshold`": "A float representing the threshold value"
        },
        "objectives": [
            "Implement the median filter algorithm to reduce noise in the given matrix.",
            "Use a sliding window approach to calculate the median value of neighboring elements.",
            "Threshold the resulting matrix such that only values greater than the threshold are retained."
        ],
        "import_lines": [],
        "function_def": "def median_filter(matrix, radius, threshold):\n    result = [[0 for _ in range(len(matrix[0]))] for _ in range(len(matrix))]\n    \n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            window = [matrix[x][y] for x in range(max(0, i - radius), min(len(matrix), i + radius + 1)) \n                      for y in range(max(0, j - radius), min(len(matrix[0]), j + radius + 1))]\n            window.sort()\n            median = window[len(window) // 2]\n            result[i][j] = median if median > threshold else 0\n    \n    return result"
    },
    {
        "function_name": "keyword_frequency",
        "file_name": "text_analysis.py",
        "parameters": {
            "`text`": "A string representing the input text.",
            "`keywords`": "A list of strings representing the keywords to search for."
        },
        "objectives": [
            "Use the KMP algorithm to find all occurrences of the keywords in the input text.",
            "Calculate the frequency of each keyword.",
            "Return the keyword frequencies."
        ],
        "import_lines": [],
        "function_def": "def keyword_frequency(text, keywords):\n    frequencies = {}\n    for keyword in keywords:\n        frequencies[keyword] = 0\n        prefix = [0] * len(keyword)\n        j = 0\n        for i in range(1, len(keyword)):\n            while j > 0 and keyword[i] != keyword[j]:\n                j = prefix[j - 1]\n            if keyword[i] == keyword[j]:\n                j += 1\n            prefix[i] = j\n        j = 0\n        for i in range(len(text)):\n            while j > 0 and text[i] != keyword[j]:\n                j = prefix[j - 1]\n            if text[i] == keyword[j]:\n                j += 1\n            if j == len(keyword):\n                frequencies[keyword] += 1\n                j = prefix[j - 1]\n    \n    return frequencies"
    },
    {
        "function_name": "cut_down_trees",
        "file_name": "forestry.py",
        "parameters": {
            "`trees`": "A list of strings representing a list of trees in a forest",
            "`budget`": "An integer representing the available budget"
        },
        "objectives": [
            "Find the optimal subset of trees to cut down in order to maximize the profit.",
            "Use dynamic programming to build a table of maximum profits.",
            "Return the maximum profit and the optimal subset of trees."
        ],
        "import_lines": [],
        "function_def": "def cut_down_trees(trees, budget):\n    profits = {}\n    for tree in trees:\n        profits[tree] = int(tree.split('_')[-1])\n    \n    dp = [[0] * (budget + 1) for _ in range(len(trees) + 1)]\n    \n    for i in range(1, len(trees) + 1):\n        for j in range(1, budget + 1):\n            if j >= profits[trees[i - 1]]:\n                dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - profits[trees[i - 1]]] + profits[trees[i - 1]])\n            else:\n                dp[i][j] = dp[i - 1][j]\n    \n    i, j = len(trees), budget\n    optimal_subset = []\n    while i > 0 and j > 0:\n        if dp[i][j] != dp[i - 1][j]:\n            optimal_subset.append(trees[i - 1])\n            j -= profits[trees[i - 1]]\n        i -= 1\n    \n    return dp[-1][-1], optimal_subset[::-1]"
    },
    {
        "function_name": "graph_coloring",
        "file_name": "graph_theory.py",
        "parameters": {
            "`vertices`": "A list of strings representing the vertices of a graph",
            "`edges`": "A list of tuples representing the edges of a graph",
            "`colors`": "A list of strings representing the available colors"
        },
        "objectives": [
            "Find a graph coloring that minimizes the number of colors used.",
            "Use a greedy approach to assign colors to vertices.",
            "Return the minimum number of colors used and the graph coloring."
        ],
        "import_lines": [],
        "function_def": "def graph_coloring(vertices, edges, colors):\n    graph = {}\n    for vertex in vertices:\n        graph[vertex] = []\n    \n    for edge in edges:\n        graph[edge[0]].append(edge[1])\n        graph[edge[1]].append(edge[0])\n    \n    coloring = {}\n    \n    for vertex in vertices:\n        used_colors = set([coloring.get(neighbor) for neighbor in graph[vertex] if coloring.get(neighbor)])\n        available_colors = [color for color in colors if color not in used_colors]\n        coloring[vertex] = available_colors[0]\n    \n    return len(set(coloring.values())), coloring"
    },
    {
        "function_name": "unique_paths",
        "file_name": "matrix_paths.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix.",
            "`target`": "An integer representing the target sum."
        },
        "objectives": [
            "Find all unique paths in the matrix from the top-left corner to the bottom-right corner that sum up to the target.",
            "Each path can only move right or down.",
            "Return the number of such paths."
        ],
        "import_lines": [],
        "function_def": "def unique_paths(matrix, target):\n    rows, cols = len(matrix), len(matrix[0])\n    dp = [[0] * cols for _ in range(rows)]\n    dp[0][0] = 1 if matrix[0][0] == target else 0\n    \n    for i in range(rows):\n        for j in range(cols):\n            if i == 0 and j == 0:\n                continue\n            if i > 0:\n                dp[i][j] += dp[i-1][j]\n            if j > 0:\n                dp[i][j] += dp[i][j-1]\n            if matrix[i][j] == target - matrix[i-1][j-1] if i > 0 and j > 0 else target - matrix[0][0]:\n                dp[i][j] += 1\n    \n    return dp[-1][-1]"
    },
    {
        "function_name": "bigram_model",
        "file_name": "bigram.py",
        "parameters": {
            "`text`": "A string representing the input text",
            "`n`": "An integer representing the size of the n-grams"
        },
        "objectives": [
            "Implement a bi-gram language model using the input text.",
            "Calculate the probability of each bi-gram in the text.",
            "Return a dictionary where the keys are the bi-grams and the values are their probabilities."
        ],
        "import_lines": [],
        "function_def": "def bigram_model(text, n):\n    words = text.split()\n    bigrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n    bigram_counts = {}\n    for bigram in bigrams:\n        if bigram in bigram_counts:\n            bigram_counts[bigram] += 1\n        else:\n            bigram_counts[bigram] = 1\n    \n    bigram_probabilities = {}\n    for bigram, count in bigram_counts.items():\n        bigram_probabilities[bigram] = count / len(bigrams)\n    \n    return bigram_probabilities"
    },
    {
        "function_name": "basis_vector_finder",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`vectors`": "A list of vectors",
            "`num_basis_vectors`": "An integer representing the number of basis vectors to find"
        },
        "objectives": [
            "Use the Singular Value Decomposition (SVD) algorithm to find the basis vectors for the given vectors.",
            "Select the top `num_basis_vectors` basis vectors based on their singular values.",
            "Return the selected basis vectors."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def basis_vector_finder(vectors, num_basis_vectors):\n    # Stack the vectors into a matrix\n    matrix = np.vstack(vectors)\n    \n    # Perform SVD\n    u, s, vh = np.linalg.svd(matrix)\n    \n    # Select the top num_basis_vectors basis vectors\n    basis_vectors = u[:, :num_basis_vectors]\n    \n    return basis_vectors"
    },
    {
        "function_name": "influence_propagator",
        "file_name": "influence_propagator.py",
        "parameters": {
            "`network`": "A dictionary representing a network, where each key is a node and its value is a list of neighboring nodes",
            "`influence`": "A dictionary representing the influence of each node, where each key is a node and its value is a float"
        },
        "objectives": [
            "Implement the linear threshold model to simulate the influence propagation in the network.",
            "Use a random process to select the nodes to activate at each step.",
            "Handle cases where the network contains cycles or self-loops.",
            "Return the final activated nodes."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def influence_propagator(network, influence):\n    # Initialize the activated nodes\n    activated_nodes = set()\n    \n    # Perform 100 iterations of the influence propagation\n    for _ in range(100):\n        # Select a node to activate\n        node = random.choice(list(network.keys()))\n        \n        # Calculate the influence of the node\n        node_influence = influence[node]\n        \n        # Activate the node if its influence is above the threshold\n        if node_influence > random.random():\n            activated_nodes.add(node)\n        \n        # Activate the neighbors of the node if their influence is above the threshold\n        for neighbor in network[node]:\n            neighbor_influence = influence[neighbor]\n            if neighbor_influence > random.random() and neighbor not in activated_nodes:\n                activated_nodes.add(neighbor)\n    \n    return activated_nodes"
    },
    {
        "function_name": "find_peaks",
        "file_name": "matrix_algorithms.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix.",
            "`threshold`": "An integer representing the threshold to determine if a cell is a peak."
        },
        "objectives": [
            "Implement a peak finding algorithm to find all peak elements in the matrix.",
            "A peak element is an element that is greater than or equal to its neighbors.",
            "Return a list of coordinates of the peak elements."
        ],
        "import_lines": [],
        "function_def": "def find_peaks(matrix, threshold):\n    peaks = []\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            if matrix[i][j] >= threshold:\n                is_peak = True\n                for x, y in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n                    if 0 <= i + x < len(matrix) and 0 <= j + y < len(matrix[0]) and matrix[i + x][j + y] >= matrix[i][j]:\n                        is_peak = False\n                        break\n                if is_peak:\n                    peaks.append((i, j))\n    return peaks"
    },
    {
        "function_name": "budget_exceeded_categories",
        "file_name": "budget_analysis.py",
        "parameters": {
            "`transactions`": "list of lists containing transaction data (date, amount, category)",
            "`budget`": "float representing the maximum allowed budget"
        },
        "objectives": [
            "Group transactions by category and calculate the total spent in each category.",
            "Identify categories where the total spent exceeds the budget.",
            "Return a dictionary with the categories that exceeded the budget and the amount by which they exceeded it."
        ],
        "import_lines": [
            "from collections import defaultdict",
            "from datetime import datetime"
        ],
        "function_def": "def budget_exceeded_categories(transactions, budget):\n    # Group transactions by category\n    category_transactions = defaultdict(list)\n    for transaction in transactions:\n        category = transaction[2]\n        category_transactions[category].append(transaction)\n    \n    # Calculate total spent in each category\n    category_spent = {}\n    for category, transactions in category_transactions.items():\n        total_spent = sum(transaction[1] for transaction in transactions)\n        category_spent[category] = total_spent\n    \n    # Identify categories that exceeded the budget\n    exceeded_categories = {}\n    for category, spent in category_spent.items():\n        if spent > budget:\n            exceeded_categories[category] = spent - budget\n    \n    return exceeded_categories"
    },
    {
        "function_name": "consensus_motif",
        "file_name": "motif_analysis.py",
        "parameters": {
            "`sequences`": "list of lists containing DNA sequences",
            "`motif_length`": "integer representing the length of the motif"
        },
        "objectives": [
            "Find the most frequent motif in each sequence.",
            "Align the motifs across all sequences to identify a consensus motif.",
            "Return the consensus motif."
        ],
        "import_lines": [
            "from collections import defaultdict",
            "from scipy.stats import norm"
        ],
        "function_def": "def consensus_motif(sequences, motif_length):\n    # Find the most frequent motif in each sequence\n    sequence_motifs = []\n    for sequence in sequences:\n        motif_counts = defaultdict(int)\n        for i in range(len(sequence) - motif_length + 1):\n            motif = sequence[i:i+motif_length]\n            motif_counts[motif] += 1\n        most_frequent_motif = max(motif_counts, key=motif_counts.get)\n        sequence_motifs.append(most_frequent_motif)\n    \n    # Align the motifs across all sequences\n    aligned_motifs = []\n    for i in range(motif_length):\n        position_counts = defaultdict(int)\n        for motif in sequence_motifs:\n            base = motif[i]\n            position_counts[base] += 1\n        most_frequent_base = max(position_counts, key=position_counts.get)\n        aligned_motifs.append(most_frequent_base)\n    \n    # Create the consensus motif\n    consensus_motif = ''.join(aligned_motifs)\n    \n    return consensus_motif"
    },
    {
        "function_name": "graph_decomposition",
        "file_name": "graph_algorithms.py",
        "parameters": {
            "`graphs`": "A list of adjacency matrices representing undirected graphs.",
            "`threshold`": "An integer representing the minimum number of connected components."
        },
        "objectives": [
            "Implement a graph decomposition algorithm to decompose each graph into its connected components.",
            "For each graph, calculate the number of connected components and their respective sizes.",
            "Return a list of tuples, where each tuple contains the graph index, number of connected components, and a dictionary mapping component indices to their sizes."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def graph_decomposition(graphs, threshold):\n    results = []\n    for i, graph in enumerate(graphs):\n        num_nodes = graph.shape[0]\n        visited = [False] * num_nodes\n        components = []\n        \n        for node in range(num_nodes):\n            if not visited[node]:\n                component = []\n                stack = [node]\n                while stack:\n                    current_node = stack.pop()\n                    if not visited[current_node]:\n                        visited[current_node] = True\n                        component.append(current_node)\n                        for neighbor in range(num_nodes):\n                            if graph[current_node][neighbor] == 1 and not visited[neighbor]:\n                                stack.append(neighbor)\n                components.append(component)\n        \n        num_components = len(components)\n        component_sizes = {i: len(component) for i, component in enumerate(components)}\n        \n        if num_components >= threshold:\n            results.append((i, num_components, component_sizes))\n    \n    return results"
    },
    {
        "function_name": "levenshtein_search",
        "file_name": "sequence_search.py",
        "parameters": {
            "`sequences`": "A list of strings representing the sequences to be searched.",
            "`pattern`": "A regular expression pattern."
        },
        "objectives": [
            "Use the regular expression pattern to search for matches in each sequence.",
            "For each match, calculate the Levenshtein distance to the pattern.",
            "Return a list of tuples, where each tuple contains the sequence index, match, and Levenshtein distance."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def levenshtein_search(sequences, pattern):\n    results = []\n    regex = re.compile(pattern)\n    \n    for i, sequence in enumerate(sequences):\n        matches = list(regex.finditer(sequence))\n        for match in matches:\n            match_str = match.group()\n            levenshtein_distance = 0\n            for j in range(min(len(match_str), len(pattern))):\n                if match_str[j] != pattern[j]:\n                    levenshtein_distance += 1\n            for j in range(min(len(match_str), len(pattern)), max(len(match_str), len(pattern))):\n                levenshtein_distance += 1\n            results.append((i, match_str, levenshtein_distance))\n    \n    return results"
    },
    {
        "function_name": "merge_sorted_arrays",
        "file_name": "sorting_algorithms.py",
        "parameters": {
            "`arrays`": "A list of sorted arrays"
        },
        "objectives": [
            "Implement a modified version of the merge sort algorithm to merge the sorted arrays into a single sorted array.",
            "Use a priority queue to efficiently select the next element to add to the result.",
            "Return the merged sorted array."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def merge_sorted_arrays(arrays):\n    queue = [(array[0], i, 0) for i, array in enumerate(arrays)]\n    heapq.heapify(queue)\n    result = []\n    while queue:\n        value, array_index, element_index = heapq.heappop(queue)\n        result.append(value)\n        if element_index + 1 < len(arrays[array_index]):\n            heapq.heappush(queue, (arrays[array_index][element_index + 1], array_index, element_index + 1))\n    return result"
    },
    {
        "function_name": "cube_shortest_path",
        "file_name": "cube_algorithms.py",
        "parameters": {
            "`cube`": "A 3D list of integers representing a 3D cube",
            "`target`": "An integer representing the target value",
            "`max_moves`": "An integer representing the maximum number of moves allowed"
        },
        "objectives": [
            "Implement a breadth-first search algorithm to find the shortest path between two opposite corners of the cube that add up to the target value.",
            "The path can only be constructed by moving from one corner to an adjacent corner.",
            "Return the length of the shortest path."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def cube_shortest_path(cube, target, max_moves):\n    directions = [(1, 0, 0), (-1, 0, 0), (0, 1, 0), (0, -1, 0), (0, 0, 1), (0, 0, -1)]\n    queue = deque([(0, 0, 0, 0)])\n    visited = set((0, 0, 0))\n    \n    while queue:\n        x, y, z, moves = queue.popleft()\n        if moves > max_moves:\n            break\n        value = cube[x][y][z]\n        if value + cube[-x-1][-y-1][-z-1] == target:\n            return moves\n        \n        for dx, dy, dz in directions:\n            nx, ny, nz = x + dx, y + dy, z + dz\n            if 0 <= nx < len(cube) and 0 <= ny < len(cube[nx]) and 0 <= nz < len(cube[nx][ny]) and (nx, ny, nz) not in visited:\n                queue.append((nx, ny, nz, moves + 1))\n                visited.add((nx, ny, nz))\n                \n    return -1"
    },
    {
        "function_name": "point_in_polygon",
        "file_name": "geometry_algorithms.py",
        "parameters": {
            "`polygons`": "A list of lists of integers representing a list of polygons",
            "`points`": "A list of tuples representing a list of points"
        },
        "objectives": [
            "Implement a point-in-polygon test to determine whether each point lies within one of the polygons.",
            "Use the ray-casting algorithm to perform the test.",
            "Return a list of booleans where each boolean corresponds to whether the point lies within a polygon."
        ],
        "import_lines": [],
        "function_def": "def point_in_polygon(polygons, points):\n    results = []\n    for point in points:\n        for polygon in polygons:\n            inside = False\n            x, y = point\n            p1x, p1y = polygon[0]\n            for i in range(len(polygon) + 1):\n                p2x, p2y = polygon[i % len(polygon)]\n                if y > min(p1y, p2y):\n                    if y <= max(p1y, p2y):\n                        if x <= max(p1x, p2x):\n                            if p1y != p2y:\n                                xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n                            if p1x == p2x or x <= xinters:\n                                inside = not inside\n                p1x, p1y = p2x, p2y\n            results.append(inside)\n    return results"
    },
    {
        "function_name": "node_selection",
        "file_name": "graph_algorithms.py",
        "parameters": {
            "`graph`": "An adjacency list representing a graph",
            "`source`": "An integer representing the source node",
            "`budget`": "An integer representing the maximum budget"
        },
        "objectives": [
            "Implement a modified version of the Knapsack algorithm to select a subset of nodes from the graph such that the total weight of the nodes does not exceed the budget.",
            "Each node has a weight and a value, which are given by the graph.",
            "Return the maximum total value that can be achieved without exceeding the budget."
        ],
        "import_lines": [],
        "function_def": "def node_selection(graph, source, budget):\n    weights = [node[0] for node in graph]\n    values = [node[1] for node in graph]\n    n = len(graph)\n    dp = [[0] * (budget + 1) for _ in range(n + 1)]\n    \n    for i in range(n + 1):\n        for w in range(budget + 1):\n            if i == 0 or w == 0:\n                dp[i][w] = 0\n            elif weights[i - 1] <= w:\n                dp[i][w] = max(values[i - 1] + dp[i - 1][w - weights[i - 1]], dp[i - 1][w])\n            else:\n                dp[i][w] = dp[i - 1][w]\n                \n    return dp[n][budget]"
    },
    {
        "function_name": "template_matching",
        "file_name": "image_processing_algorithms.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix",
            "`pattern`": "A 2D list of integers representing a pattern",
            "`threshold`": "An integer representing the threshold value"
        },
        "objectives": [
            "Implement a template matching algorithm to find all occurrences of the pattern in the matrix.",
            "Use a sliding window approach to compare the pattern with the matrix.",
            "If the sum of the absolute differences between the pattern and the corresponding sub-matrix is less than or equal to the `threshold`, mark the top-left corner of the sub-matrix as a match.",
            "Return a list of tuples representing the coordinates of the matches."
        ],
        "import_lines": [],
        "function_def": "def template_matching(matrix, pattern, threshold):\n    matches = []\n    for i in range(len(matrix) - len(pattern) + 1):\n        for j in range(len(matrix[i]) - len(pattern[0]) + 1):\n            diff = 0\n            for x in range(len(pattern)):\n                for y in range(len(pattern[x])):\n                    diff += abs(matrix[i + x][j + y] - pattern[x][y])\n            if diff <= threshold:\n                matches.append((i, j))\n    return matches"
    },
    {
        "function_name": "linear_programming",
        "file_name": "optimization_algorithms.py",
        "parameters": {
            "`n`": "An integer representing the number of variables in the linear programming problem",
            "`constraints`": "A 2D list representing the coefficients of the constraints",
            "`bounds`": "A list of tuples representing the bounds of the variables",
            "`c`": "A list representing the coefficients of the objective function"
        },
        "objectives": [
            "Implement the Simplex algorithm to find the optimal solution for the given linear programming problem.",
            "Calculate the number of pivot operations performed during the Simplex algorithm.",
            "Return the optimal solution, the optimal value, and the number of pivot operations."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def linear_programming(n, constraints, bounds, c):\n    num_constraints = len(constraints)\n    tableau = np.zeros((num_constraints + 1, n + 1))\n    for i in range(num_constraints):\n        for j in range(n):\n            tableau[i, j] = -constraints[i][j]\n        tableau[i, n] = bounds[i][0]\n    for j in range(n):\n        tableau[num_constraints, j] = c[j]\n    \n    pivot_operations = 0\n    while True:\n        pivot_row = -1\n        for i in range(num_constraints):\n            if tableau[i, n] < 0:\n                pivot_row = i\n                break\n        if pivot_row == -1:\n            break\n        \n        pivot_col = -1\n        min_ratio = float('inf')\n        for j in range(n):\n            if tableau[pivot_row, j] > 0 and tableau[num_constraints, j] / tableau[pivot_row, j] < min_ratio:\n                pivot_col = j\n                min_ratio = tableau[num_constraints, j] / tableau[pivot_row, j]\n        \n        tableau[pivot_row] /= tableau[pivot_row, pivot_col]\n        for i in range(num_constraints + 1):\n            if i != pivot_row:\n                tableau[i] -= tableau[i, pivot_col] * tableau[pivot_row]\n        pivot_operations += 1\n    \n    optimal_solution = np.zeros(n)\n    for j in range(n):\n        if tableau[num_constraints, j] < 0:\n            optimal_solution[j] = tableau[j, n]\n    \n    optimal_value = tableau[num_constraints, n]\n    return optimal_solution, optimal_value, pivot_operations"
    },
    {
        "function_name": "streaming_frequency",
        "file_name": "streaming.py",
        "parameters": {
            "`streams`": "A list of integers representing streaming data.",
            "`window_size`": "An integer representing the size of the sliding window.",
            "`threshold`": "An integer representing the minimum frequency of elements in the window."
        },
        "objectives": [
            "Process the streaming data using a sliding window approach.",
            "Count the frequency of each element within the current window.",
            "Return the elements with a frequency greater than or equal to the threshold and their corresponding frequencies."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def streaming_frequency(streams, window_size, threshold):\n    freq_dict = defaultdict(int)\n    result = []\n    \n    for i in range(len(streams)):\n        freq_dict[streams[i]] += 1\n        \n        if i >= window_size:\n            freq_dict[streams[i - window_size]] -= 1\n            if freq_dict[streams[i - window_size]] == 0:\n                del freq_dict[streams[i - window_size]]\n        \n        if i >= window_size - 1:\n            for elem, freq in freq_dict.items():\n                if freq >= threshold:\n                    result.append((elem, freq))\n    \n    return result"
    },
    {
        "function_name": "knn",
        "file_name": "knn.py",
        "parameters": {
            "`coordinates`": "A list of tuples representing the coordinates of points in a 2D plane.",
            "`k`": "An integer representing the number of nearest neighbors to find."
        },
        "objectives": [
            "Use the k-nearest neighbors (KNN) algorithm to find the nearest neighbors for each point.",
            "Calculate the Euclidean distance between points.",
            "Return the nearest neighbors and their corresponding distances."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def knn(coordinates, k):\n    result = []\n    \n    for i in range(len(coordinates)):\n        distances = []\n        \n        for j in range(len(coordinates)):\n            if i != j:\n                distance = math.sqrt((coordinates[i][0] - coordinates[j][0])**2 + (coordinates[i][1] - coordinates[j][1])**2)\n                distances.append((j, distance))\n        \n        distances.sort(key=lambda x: x[1])\n        nearest_neighbors = distances[:k]\n        \n        result.append(nearest_neighbors)\n    \n    return result"
    },
    {
        "function_name": "evaluate_expression",
        "file_name": "expression_evaluator.py",
        "parameters": {
            "`expression`": "String representing a mathematical expression",
            "`variable`": "String representing the variable in the expression",
            "`lower_bound`": "Integer representing the lower bound of the variable",
            "`upper_bound`": "Integer representing the upper bound of the variable"
        },
        "objectives": [
            "Parse the mathematical expression and identify the terms involving the variable.",
            "Use the identified terms to create a new expression that represents the sum of the terms.",
            "Use the `sympy` library to evaluate the new expression at different values of the variable within the given bounds.",
            "Return a list of tuples, where each tuple contains the value of the variable and the corresponding value of the expression."
        ],
        "import_lines": [
            "import sympy as sp",
            "import re"
        ],
        "function_def": "def evaluate_expression(expression, variable, lower_bound, upper_bound):\n    # Parse the expression and identify the terms involving the variable\n    terms = re.findall(r'([+-]?\\d*\\*?)' + variable, expression)\n    \n    # Create a new expression that represents the sum of the terms\n    new_expression = ''\n    for term in terms:\n        new_expression += term + variable\n    new_expression = sp.sympify(new_expression)\n    \n    # Evaluate the new expression at different values of the variable within the given bounds\n    values = []\n    for i in range(lower_bound, upper_bound + 1):\n        value = new_expression.subs(variable, i)\n        values.append((i, value))\n    \n    return values"
    },
    {
        "function_name": "jaccard_similarity",
        "file_name": "set_operations.py",
        "parameters": {
            "`sets`": "A list of sets",
            "`threshold`": "A float representing the minimum Jaccard similarity"
        },
        "objectives": [
            "Implement an algorithm to find all pairs of sets with a Jaccard similarity above the threshold.",
            "Use the intersection and union of the sets to calculate the Jaccard similarity.",
            "Return a list of tuples, where each tuple contains two sets and their Jaccard similarity."
        ],
        "import_lines": [],
        "function_def": "def jaccard_similarity(sets, threshold):\n    results = []\n    for i in range(len(sets)):\n        for j in range(i + 1, len(sets)):\n            intersection = sets[i] & sets[j]\n            union = sets[i] | sets[j]\n            similarity = len(intersection) / len(union)\n            if similarity >= threshold:\n                results.append((sets[i], sets[j], similarity))\n    return results"
    },
    {
        "function_name": "stacking_ensemble",
        "file_name": "ensemble_methods.py",
        "parameters": {
            "`regressors`": "A list of regression models",
            "`X`": "A 2D list representing the feature matrix",
            "`y`": "A 1D list representing the target variable"
        },
        "objectives": [
            "Implement the stacking ensemble method to combine the predictions of multiple regression models.",
            "Train each regressor on the feature matrix.",
            "Make predictions using each regressor.",
            "Combine the predictions using a meta-regressor.",
            "Return the final prediction."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.linear_model import LinearRegression"
        ],
        "function_def": "def stacking_ensemble(regressors, X, y):\n    # Train each regressor\n    trained_regressors = []\n    for regressor in regressors:\n        regressor.fit(X, y)\n        trained_regressors.append(regressor)\n    \n    # Make predictions using each regressor\n    predictions = []\n    for regressor in trained_regressors:\n        prediction = regressor.predict(X)\n        predictions.append(prediction)\n    \n    # Combine predictions using a meta-regressor\n    meta_regressor = LinearRegression()\n    meta_regressor.fit(np.array(predictions).T, y)\n    final_prediction = meta_regressor.predict(np.array(predictions).T)\n    \n    return final_prediction"
    },
    {
        "function_name": "bitwise_rotation",
        "file_name": "bitwise_operations.py",
        "parameters": {
            "`bits`": "An integer representing a binary number.",
            "`n`": "An integer representing the number of bits to rotate."
        },
        "objectives": [
            "Implement a function that performs a bitwise rotation on the given binary number by the specified number of bits.",
            "Use bitwise operations to perform the rotation.",
            "Return the rotated binary number as an integer."
        ],
        "import_lines": [],
        "function_def": "def bitwise_rotation(bits, n):\n    n %= 32\n    return (bits >> n) | (bits << (32 - n))"
    },
    {
        "function_name": "k_highest_scores",
        "file_name": "heap_operations.py",
        "parameters": {
            "`scores`": "A list of integers representing scores.",
            "`k`": "An integer representing the number of highest scores to keep track of."
        },
        "objectives": [
            "Implement a function that keeps track of the k highest scores in the list of scores.",
            "Use a heap data structure to efficiently keep track of the highest scores.",
            "Return the k highest scores as a list of integers."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def k_highest_scores(scores, k):\n    return heapq.nlargest(k, scores)"
    },
    {
        "function_name": "cells_above_threshold",
        "file_name": "dynamic_programming.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix.",
            "`threshold`": "An integer representing the threshold value."
        },
        "objectives": [
            "Implement a function that finds all cells in the matrix that have a value greater than the threshold.",
            "Use a dynamic programming approach to efficiently find the cells.",
            "Return a list of coordinates of the cells that have a value greater than the threshold."
        ],
        "import_lines": [],
        "function_def": "def cells_above_threshold(matrix, threshold):\n    result = []\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            if matrix[i][j] > threshold:\n                result.append((i, j))\n    return result"
    },
    {
        "function_name": "motif_occurrences",
        "file_name": "pattern_matching.py",
        "parameters": {
            "`sequence`": "A string representing a DNA sequence.",
            "`motif`": "A string representing a DNA motif."
        },
        "objectives": [
            "Implement a function that finds all occurrences of the motif in the DNA sequence.",
            "Use a sliding window approach to efficiently find the occurrences.",
            "Return a list of coordinates of the occurrences."
        ],
        "import_lines": [],
        "function_def": "def motif_occurrences(sequence, motif):\n    result = []\n    for i in range(len(sequence) - len(motif) + 1):\n        if sequence[i:i + len(motif)] == motif:\n            result.append(i)\n    return result"
    },
    {
        "function_name": "optics_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "`data`": "A 2D list representing a dataset of observations, where each row is a sample and each column is a feature",
            "`num_clusters`": "An integer representing the number of clusters to find",
            "`epsilon`": "A float representing the maximum distance between points in a cluster"
        },
        "objectives": [
            "Use the OPTICS (Ordering Points To Identify the Clustering Structure) algorithm to cluster the `data` into `num_clusters` clusters.",
            "Take into account the `epsilon` value to determine the density-based clustering.",
            "Return the clustered data as a list of lists, where each inner list represents a cluster."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def optics_clustering(data, num_clusters, epsilon):\n    # Calculate the core distances and reachability distances\n    core_distances = np.zeros(len(data))\n    reachability_distances = np.zeros(len(data))\n    \n    for i in range(len(data)):\n        # Calculate the core distance\n        neighbors = [j for j in range(len(data)) if np.linalg.norm(np.array(data[i]) - np.array(data[j])) <= epsilon]\n        core_distances[i] = len(neighbors)\n        \n        # Calculate the reachability distance\n        min_distance = np.inf\n        for j in neighbors:\n            distance = np.linalg.norm(np.array(data[i]) - np.array(data[j]))\n            if distance < min_distance:\n                min_distance = distance\n        reachability_distances[i] = min_distance\n    \n    # Perform OPTICS clustering\n    clusters = []\n    for i in range(len(data)):\n        if core_distances[i] >= num_clusters:\n            cluster = [data[i]]\n            for j in range(len(data)):\n                if reachability_distances[j] <= epsilon and core_distances[j] >= num_clusters:\n                    cluster.append(data[j])\n            clusters.append(cluster)\n    \n    return clusters"
    },
    {
        "function_name": "qr_eigenvalues",
        "file_name": "matrix_operations.py",
        "parameters": {
            "`matrix`": "A 2D list representing the input matrix",
            "`num_eigenvalues`": "An integer representing the number of eigenvalues to find"
        },
        "objectives": [
            "Use the QR algorithm to find the `num_eigenvalues` eigenvalues of the `matrix`.",
            "Return the eigenvalues as a list of floats."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def qr_eigenvalues(matrix, num_eigenvalues):\n    # Perform QR decomposition\n    Q, R = np.linalg.qr(matrix)\n    \n    # Calculate the eigenvalues\n    eigenvalues = np.zeros(num_eigenvalues)\n    for i in range(num_eigenvalues):\n        eigenvalue = np.trace(R)\n        eigenvalues[i] = eigenvalue\n        R = R - eigenvalue * np.eye(len(R))\n    \n    return eigenvalues"
    },
    {
        "function_name": "approximate_match",
        "file_name": "string_matching.py",
        "parameters": {
            "`text`": "A string representing the text to be searched",
            "`pattern`": "A string representing the pattern to be searched for",
            "`k`": "An integer representing the maximum number of errors allowed in the search"
        },
        "objectives": [
            "Use dynamic programming to build a 2D table representing the edit distances between the text and the pattern.",
            "Use the table to find all occurrences of the pattern in the text with up to k errors.",
            "Return the list of occurrences."
        ],
        "import_lines": [],
        "function_def": "def approximate_match(text, pattern, k):\n    # Initialize 2D table for edit distances\n    m = len(pattern)\n    n = len(text)\n    table = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n    \n    # Initialize first row and column of table\n    for i in range(m + 1):\n        table[i][0] = i\n    for j in range(n + 1):\n        table[0][j] = j\n    \n    # Fill in table using dynamic programming\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if pattern[i - 1] == text[j - 1]:\n                table[i][j] = table[i - 1][j - 1]\n            else:\n                table[i][j] = 1 + min(table[i - 1][j - 1], table[i - 1][j], table[i][j - 1])\n    \n    # Find all occurrences of pattern in text with up to k errors\n    occurrences = []\n    for j in range(n + 1):\n        if table[m][j] <= k:\n            occurrences.append((j - m, j))\n    \n    return occurrences"
    },
    {
        "function_name": "max_combination_sum",
        "file_name": "combination_processor.py",
        "parameters": {
            "`numbers`": "A list of integers representing a sequence of numbers",
            "`k`": "An integer representing the number of elements to be picked"
        },
        "objectives": [
            "Find all combinations of `k` numbers that have the maximum sum.",
            "The function should return a list of combinations with the maximum sum."
        ],
        "import_lines": [
            "import itertools"
        ],
        "function_def": "def max_combination_sum(numbers, k):\n    max_sum = float('-inf')\n    max_combinations = []\n    \n    for combination in itertools.combinations(numbers, k):\n        if sum(combination) > max_sum:\n            max_sum = sum(combination)\n            max_combinations = [combination]\n        elif sum(combination) == max_sum:\n            max_combinations.append(combination)\n                \n    return max_combinations"
    },
    {
        "function_name": "kmp_search",
        "file_name": "string_searching_algorithms.py",
        "parameters": {
            "`sequence`": "A string representing a DNA sequence",
            "`pattern`": "A string representing the pattern to find"
        },
        "objectives": [
            "Implement the Knuth-Morris-Pratt string searching algorithm to find all occurrences of the pattern in the sequence.",
            "Use a lookup table to efficiently compute the longest proper prefix that is also a proper suffix.",
            "Return the list of all occurrences of the pattern in the sequence."
        ],
        "import_lines": [],
        "function_def": "def kmp_search(sequence, pattern):\n    n = len(sequence)\n    m = len(pattern)\n    lookup_table = [0] * m\n    j = 0\n    \n    for i in range(1, m):\n        while j > 0 and pattern[j] != pattern[i]:\n            j = lookup_table[j - 1]\n        if pattern[j] == pattern[i]:\n            j += 1\n        lookup_table[i] = j\n    \n    occurrences = []\n    j = 0\n    for i in range(n):\n        while j > 0 and sequence[i] != pattern[j]:\n            j = lookup_table[j - 1]\n        if sequence[i] == pattern[j]:\n            j += 1\n        if j == m:\n            occurrences.append(i - m + 1)\n            j = lookup_table[j - 1]\n    \n    return occurrences"
    },
    {
        "function_name": "bfs_select_nodes",
        "file_name": "graph_algorithms.py",
        "parameters": {
            "`graph`": "A dictionary representing a graph where each key is a node and its value is a list of its neighbors.",
            "`root`": "A node representing the root of the graph.",
            "`k`": "An integer representing the number of nodes to select."
        },
        "objectives": [
            "Perform a Breadth-First Search (BFS) traversal of the graph starting from the root node.",
            "Select the k nodes with the minimum depth in the graph.",
            "Return the selected nodes and their depths."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def bfs_select_nodes(graph, root, k):\n    if root not in graph:\n        return [], {}\n    \n    visited = set()\n    queue = deque([(root, 0)])\n    visited.add(root)\n    nodes = []\n    depths = {}\n    \n    while queue:\n        node, depth = queue.popleft()\n        nodes.append(node)\n        depths[node] = depth\n        \n        if len(nodes) == k:\n            break\n        \n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                queue.append((neighbor, depth + 1))\n                visited.add(neighbor)\n    \n    return nodes, depths"
    },
    {
        "function_name": "knn_search",
        "file_name": "spatial_algorithms.py",
        "parameters": {
            "`points`": "A list of tuples representing points on a plane.",
            "`k`": "An integer representing the number of nearest neighbors to find."
        },
        "objectives": [
            "Use the Ball Tree data structure to find the k nearest neighbors for each point in the set.",
            "Return the indices of the k nearest neighbors for each point.",
            "Handle the case where there are multiple points with the same distance."
        ],
        "import_lines": [
            "from sklearn.neighbors import BallTree"
        ],
        "function_def": "def knn_search(points, k):\n    tree = BallTree(points)\n    distances, indices = tree.query(points, k)\n    return indices"
    },
    {
        "function_name": "inverted_index",
        "file_name": "information_retrieval.py",
        "parameters": {
            "`documents`": "A list of strings representing the documents to be indexed.",
            "`terms`": "A list of strings representing the terms to be searched."
        },
        "objectives": [
            "Implement the Inverted Index algorithm to create an index of the documents.",
            "Calculate the TF-IDF score for each term in each document.",
            "Return a dictionary where the keys are the terms and the values are lists of tuples containing the document ID and the TF-IDF score."
        ],
        "import_lines": [
            "import math",
            "from collections import defaultdict"
        ],
        "function_def": "def inverted_index(documents, terms):\n    index = defaultdict(list)\n    for i, document in enumerate(documents):\n        words = document.split()\n        document_length = len(words)\n        doc_freq = defaultdict(int)\n        \n        for word in words:\n            doc_freq[word] += 1\n        \n        for term in terms:\n            tf = doc_freq[term] / document_length\n            idf = math.log(len(documents) / sum(1 for doc in documents if term in doc))\n            tf_idf = tf * idf\n            \n            if tf_idf > 0:\n                index[term].append((i, tf_idf))\n    \n    return index"
    },
    {
        "function_name": "longest_subsequence",
        "file_name": "dynamic_programming_algorithms.py",
        "parameters": {
            "`text`": "A string representing the input text",
            "`k`": "An integer representing the size of the subsequence",
            "`alphabet`": "A string representing the allowed characters in the subsequence"
        },
        "objectives": [
            "Use dynamic programming to find the longest subsequence of the text that only contains characters from the given alphabet.",
            "The subsequence should be of size k.",
            "Handle cases where the text does not contain enough characters from the alphabet to form a subsequence of size k.",
            "Return the longest subsequence."
        ],
        "import_lines": [],
        "function_def": "def longest_subsequence(text, k, alphabet):\n    m = len(text)\n    n = len(alphabet)\n    dp = [[0] * (k + 1) for _ in range(m + 1)]\n    subsequences = [[\"\" for _ in range(k + 1)] for _ in range(m + 1)]\n    for i in range(1, m + 1):\n        for j in range(1, min(i, k) + 1):\n            char = text[i - 1]\n            if char in alphabet:\n                if j == 1:\n                    dp[i][j] = 1\n                    subsequences[i][j] = char\n                else:\n                    dp[i][j] = dp[i - 1][j]\n                    subsequences[i][j] = subsequences[i - 1][j]\n                    if dp[i - 1][j - 1] + 1 > dp[i][j]:\n                        dp[i][j] = dp[i - 1][j - 1] + 1\n                        subsequences[i][j] = subsequences[i - 1][j - 1] + char\n    return subsequences[-1][-1]"
    },
    {
        "function_name": "max_in_window",
        "file_name": "stream_processing.py",
        "parameters": {
            "`stream`": "A stream of integers.",
            "`window_size`": "The size of the sliding window."
        },
        "objectives": [
            "Use a queue data structure to calculate the maximum value in the sliding window.",
            "Return the maximum values."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def max_in_window(stream, window_size):\n    queue = deque()\n    max_values = []\n    \n    for num in stream:\n        while queue and queue[0] < num:\n            queue.popleft()\n        queue.append(num)\n        \n        if len(queue) > window_size:\n            queue.popleft()\n        \n        if len(queue) == window_size:\n            max_values.append(queue[0])\n    \n    return max_values"
    },
    {
        "function_name": "shortest_substring",
        "file_name": "text_analysis.py",
        "parameters": {
            "`text`": "A string",
            "`keywords`": "A list of strings"
        },
        "objectives": [
            "Given a text and a list of keywords, find the shortest substring that contains all the keywords.",
            "Use a sliding window approach to optimize the search process.",
            "Return the shortest substring and its length."
        ],
        "import_lines": [],
        "function_def": "def shortest_substring(text, keywords):\n    keyword_set = set(keywords)\n    min_length = float('inf')\n    shortest_substring = \"\"\n    \n    for i in range(len(text)):\n        keyword_count = 0\n        window = set()\n        for j in range(i, len(text)):\n            if text[j] in keyword_set and text[j] not in window:\n                keyword_count += 1\n                window.add(text[j])\n            if keyword_count == len(keywords):\n                if j - i + 1 < min_length:\n                    min_length = j - i + 1\n                    shortest_substring = text[i:j + 1]\n                break\n    \n    return shortest_substring, min_length"
    },
    {
        "function_name": "kth_smallest",
        "file_name": "statistical_analysis.py",
        "parameters": {
            "`numbers`": "A list of integers",
            "`k`": "An integer"
        },
        "objectives": [
            "Given a list of numbers and an integer k, find the k-th smallest number in the list.",
            "Use a heap data structure to optimize the search process.",
            "Return the k-th smallest number."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def kth_smallest(numbers, k):\n    heap = []\n    for num in numbers:\n        if len(heap) < k:\n            heapq.heappush(heap, -num)\n        else:\n            if num < -heap[0]:\n                heapq.heappop(heap)\n                heapq.heappush(heap, -num)\n    \n    return -heap[0]"
    },
    {
        "function_name": "database_query",
        "file_name": "database_operations.py",
        "parameters": {
            "`database`": "A dictionary representing a database where each key is an attribute and each value is a list of integers.",
            "`query`": "A dictionary representing a query where each key is an attribute and each value is an integer."
        },
        "objectives": [
            "Implement a database query function that retrieves all records from the database that match the given query.",
            "The function should return the retrieved records as a list of dictionaries."
        ],
        "import_lines": [],
        "function_def": "def database_query(database, query):\n    records = []\n    for attribute, value in query.items():\n        if attribute in database:\n            matching_records = [{attr: database[attr][i] for attr in database} for i, attr_value in enumerate(database[attribute]) if attr_value == value]\n            records.extend(matching_records)\n    return records"
    },
    {
        "function_name": "log_parser",
        "file_name": "log_analysis.py",
        "parameters": {
            "`logs`": "A list of strings representing the logs from the system.",
            "`pattern`": "A string representing the pattern to extract from the logs."
        },
        "objectives": [
            "Perform a log parsing operation to extract relevant information from the logs.",
            "Use regular expressions to extract the pattern from the logs.",
            "Group the extracted information by the pattern and calculate the frequency of each group.",
            "Return a dictionary with the pattern as the key and the frequency as the value."
        ],
        "import_lines": [
            "import re",
            "from collections import defaultdict"
        ],
        "function_def": "def log_parser(logs, pattern):\n    frequency_dict = defaultdict(int)\n    for log in logs:\n        match = re.search(pattern, log)\n        if match:\n            frequency_dict[match.group(0)] += 1\n    return dict(frequency_dict)"
    },
    {
        "function_name": "tokenize_document",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`document`": "A string representing the document to be tokenized.",
            "`stop_words`": "A list of strings representing the stop words to be removed."
        },
        "objectives": [
            "Implement a text preprocessing algorithm to tokenize the document.",
            "Remove special characters, punctuation, and stop words from the document.",
            "Split the document into individual words and convert them to lower case.",
            "Return the tokenized document as a list of strings."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def tokenize_document(document, stop_words):\n    document = re.sub(r'[^a-zA-Z\\s]', '', document)\n    words = document.lower().split()\n    tokenized_document = [word for word in words if word not in stop_words]\n    return tokenized_document"
    },
    {
        "function_name": "circle_points",
        "file_name": "geometry.py",
        "parameters": {
            "`points`": "A list of 2D points.",
            "`center`": "The center of a circle.",
            "`radius`": "The radius of the circle."
        },
        "objectives": [
            "Find all the points that lie within or on the circle.",
            "Calculate the distance of each point from the center of the circle.",
            "Sort the points based on their distances from the center.",
            "Return the sorted points and their distances."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def circle_points(points, center, radius):\n    points_in_circle = []\n    distances = []\n    \n    for point in points:\n        distance = math.sqrt((point[0] - center[0])**2 + (point[1] - center[1])**2)\n        if distance <= radius:\n            points_in_circle.append(point)\n            distances.append(distance)\n    \n    sorted_points = [point for _, point in sorted(zip(distances, points_in_circle))]\n    return sorted_points, sorted(distances)"
    },
    {
        "function_name": "max_unique_substrings",
        "file_name": "string_algorithms.py",
        "parameters": {
            "`s`": "The string to be processed.",
            "`k`": "The maximum length of a substring."
        },
        "objectives": [
            "Find all substrings of length k that have the maximum number of unique characters.",
            "Return the substrings with the maximum number of unique characters."
        ],
        "import_lines": [],
        "function_def": "def max_unique_substrings(s, k):\n    max_unique = 0\n    substrings = []\n    \n    for i in range(len(s) - k + 1):\n        substring = s[i:i+k]\n        unique_chars = len(set(substring))\n        if unique_chars > max_unique:\n            max_unique = unique_chars\n            substrings = [substring]\n        elif unique_chars == max_unique:\n            substrings.append(substring)\n    \n    return substrings"
    },
    {
        "function_name": "max_subarrays",
        "file_name": "array_search.py",
        "parameters": {
            "`array`": "A list of integers",
            "`window_size`": "An integer representing the size of the window"
        },
        "objectives": [
            "Use a sliding window approach to find all maximum subarrays of the given size",
            "For each subarray, calculate the sum of its elements",
            "Return the subarrays with their corresponding sums"
        ],
        "import_lines": [],
        "function_def": "def max_subarrays(array, window_size):\n    if len(array) < window_size:\n        return []\n    result = []\n    max_sum = float('-inf')\n    for i in range(len(array) - window_size + 1):\n        subarray = array[i:i + window_size]\n        subarray_sum = sum(subarray)\n        if subarray_sum > max_sum:\n            max_sum = subarray_sum\n            result = [subarray]\n        elif subarray_sum == max_sum:\n            result.append(subarray)\n    return [(subarray, sum(subarray)) for subarray in result]"
    },
    {
        "function_name": "satisfiability",
        "file_name": "satisfiability.py",
        "parameters": {
            "`num_variables`": "An integer representing the number of variables.",
            "`clauses`": "A list of tuples representing the clauses in the CNF formula.",
            "`max_attempts`": "An integer representing the maximum number of attempts."
        },
        "objectives": [
            "Use a stochastic local search algorithm to solve the CNF satisfiability problem.",
            "Calculate the number of satisfied clauses.",
            "Return the assignment of variables that satisfies the maximum number of clauses."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def satisfiability(num_variables, clauses, max_attempts):\n    assignment = [random.choice([0, 1]) for _ in range(num_variables)]\n    max_satisfied = 0\n    \n    for _ in range(max_attempts):\n        new_assignment = assignment[:]\n        variable = random.randint(0, num_variables - 1)\n        new_assignment[variable] = 1 - new_assignment[variable]\n        satisfied = 0\n        for clause in clauses:\n            if any(new_assignment[i] == x for i, x in clause):\n                satisfied += 1\n        if satisfied > max_satisfied:\n            max_satisfied = satisfied\n            assignment = new_assignment\n    \n    return assignment"
    },
    {
        "function_name": "motif_searcher",
        "file_name": "bioinformatics.py",
        "parameters": {
            "`sequences`": "A list of strings representing DNA sequences.",
            "`motif`": "A string representing a motif to search for.",
            "`threshold`": "An integer representing the minimum number of occurrences."
        },
        "objectives": [
            "Use dynamic programming to calculate the score of the motif in each sequence.",
            "Calculate the p-value of the motif in each sequence using the binomial distribution.",
            "Return the sequences with a p-value less than or equal to the threshold and their corresponding scores."
        ],
        "import_lines": [
            "from scipy.stats import binom",
            "import numpy as np"
        ],
        "function_def": "def motif_searcher(sequences, motif, threshold):\n    results = []\n    for sequence in sequences:\n        score = 0\n        for i in range(len(sequence) - len(motif) + 1):\n            if sequence[i:i+len(motif)] == motif:\n                score += 1\n        p_value = binom.sf(score - 1, len(sequence) - len(motif) + 1, 0.25 ** len(motif))\n        if p_value <= threshold:\n            results.append((sequence, score, p_value))\n    return results"
    },
    {
        "function_name": "pattern_searcher",
        "file_name": "string_algorithms.py",
        "parameters": {
            "`text`": "A string representing the text to search.",
            "`pattern`": "A string representing the pattern to search for."
        },
        "objectives": [
            "Use the Rabin-Karp algorithm to search for the pattern in the text.",
            "Use a rolling hash to avoid rehashing the entire pattern.",
            "Return the starting indices of all occurrences of the pattern."
        ],
        "import_lines": [],
        "function_def": "def pattern_searcher(text, pattern):\n    results = []\n    h = 1\n    for _ in range(len(pattern) - 1):\n        h = (h * 256) % 101\n    p = 0\n    t = 0\n    for i in range(len(pattern)):\n        p = (p * 256 + ord(pattern[i])) % 101\n        t = (t * 256 + ord(text[i])) % 101\n    for i in range(len(text) - len(pattern) + 1):\n        if p == t:\n            match = True\n            for j in range(len(pattern)):\n                if text[i + j] != pattern[j]:\n                    match = False\n                    break\n            if match:\n                results.append(i)\n        if i < len(text) - len(pattern):\n            t = (256 * (t - ord(text[i]) * h) + ord(text[i + len(pattern)])) % 101\n            if t < 0:\n                t += 101\n    return results"
    },
    {
        "function_name": "eigen_decomposition",
        "file_name": "matrix_analysis.py",
        "parameters": {
            "`matrix`": "A 2D list representing a matrix",
            "`k`": "An integer representing the number of eigenvalues and eigenvectors to be computed"
        },
        "objectives": [
            "Use the power iteration method to compute the k largest eigenvalues and their corresponding eigenvectors.",
            "Return a list of tuples, where each tuple contains an eigenvalue and its corresponding eigenvector."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def eigen_decomposition(matrix, k):\n    eigenvalues = []\n    eigenvectors = []\n    for _ in range(k):\n        v = np.random.rand(len(matrix))\n        v = v / np.linalg.norm(v)\n        eigenvalue = 0\n        for _ in range(100):\n            v_next = np.dot(matrix, v)\n            eigenvalue = np.dot(v, v_next)\n            v = v_next / np.linalg.norm(v_next)\n        eigenvalues.append(eigenvalue)\n        eigenvectors.append(v)\n    return list(zip(eigenvalues, eigenvectors))"
    },
    {
        "function_name": "q_learning",
        "file_name": "reinforcement_learning.py",
        "parameters": {
            "`rewards`": "A 2D list representing the rewards for each state-action pair.",
            "`transitions`": "A 2D list representing the transitions between states.",
            "`discount_factor`": "A float representing the discount factor for the rewards."
        },
        "objectives": [
            "Use the Q-learning algorithm to find the optimal policy for the given Markov Decision Process (MDP).",
            "Use a dictionary to efficiently store and retrieve the Q-values.",
            "Return the optimal policy along with the corresponding Q-values."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def q_learning(rewards, transitions, discount_factor):\n    num_states = len(rewards)\n    num_actions = len(rewards[0])\n    q_values = {}\n    \n    for state in range(num_states):\n        for action in range(num_actions):\n            q_values[(state, action)] = 0\n    \n    for _ in range(10000):\n        state = random.randint(0, num_states - 1)\n        action = random.randint(0, num_actions - 1)\n        reward = rewards[state][action]\n        next_state = transitions[state][action]\n        next_action = random.randint(0, num_actions - 1)\n        q_value = q_values.get((state, action), 0)\n        next_q_value = q_values.get((next_state, next_action), 0)\n        q_values[(state, action)] = q_value + 0.1 * (reward + discount_factor * next_q_value - q_value)\n    \n    policy = []\n    for state in range(num_states):\n        max_q_value = float('-inf')\n        max_action = -1\n        for action in range(num_actions):\n            q_value = q_values.get((state, action), 0)\n            if q_value > max_q_value:\n                max_q_value = q_value\n                max_action = action\n        policy.append(max_action)\n    \n    return policy, q_values"
    },
    {
        "function_name": "vector_operations",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`vector1`": "A list of integers representing the first vector.",
            "`vector2`": "A list of integers representing the second vector.",
            "`vector3`": "A list of integers representing the third vector."
        },
        "objectives": [
            "Calculate the dot product of the first two vectors.",
            "Use the cross product formula to calculate the cross product of the first two vectors.",
            "Calculate the magnitude of the resulting vector from the cross product."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def vector_operations(vector1, vector2, vector3):\n    dot_product = sum(a * b for a, b in zip(vector1, vector2))\n    cross_product = [vector1[1]*vector2[2] - vector1[2]*vector2[1],\n                     vector1[2]*vector2[0] - vector1[0]*vector2[2],\n                     vector1[0]*vector2[1] - vector1[1]*vector2[0]]\n    magnitude = math.sqrt(sum(a**2 for a in cross_product))\n    \n    return dot_product, cross_product, magnitude"
    },
    {
        "function_name": "equal_width_binning",
        "file_name": "discretization.py",
        "parameters": {
            "`data`": "A 2D list representing the input data where each sublist is a record.",
            "`num_bins`": "An integer representing the number of bins to be used for discretization."
        },
        "objectives": [
            "Implement the equal-width binning method to discretize the data into num_bins equal-width bins.",
            "Handle cases where the data contains missing values.",
            "Return a 2D list representing the discretized data."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def equal_width_binning(data, num_bins):\n    discretized_data = []\n    \n    for i in range(len(data[0])):\n        column = [row[i] for row in data]\n        min_val = min(column)\n        max_val = max(column)\n        bin_width = (max_val - min_val) / num_bins\n        \n        discretized_column = []\n        for val in column:\n            if math.isnan(val):\n                discretized_column.append(None)\n            else:\n                bin_num = math.floor((val - min_val) / bin_width)\n                if bin_num == num_bins:\n                    bin_num -= 1\n                discretized_column.append(bin_num)\n        \n        if not discretized_data:\n            discretized_data = [[val] for val in discretized_column]\n        else:\n            for j, val in enumerate(discretized_column):\n                discretized_data[j].append(val)\n    \n    return discretized_data"
    },
    {
        "function_name": "topological_sorting",
        "file_name": "scheduling.py",
        "parameters": {
            "`schedule`": "A dictionary representing a schedule where each key is a time slot and its corresponding value is a list of tasks to be performed at that time slot.",
            "`dependencies`": "A dictionary representing the dependencies between tasks where each key is a task and its corresponding value is a list of tasks that it depends on."
        },
        "objectives": [
            "Implement the topological sorting algorithm to order the tasks in the schedule based on their dependencies.",
            "Handle cases where there are circular dependencies between tasks.",
            "Return a list representing the ordered tasks."
        ],
        "import_lines": [],
        "function_def": "def topological_sorting(schedule, dependencies):\n    graph = {}\n    in_degree = {}\n    \n    # Build the graph\n    for time_slot, tasks in schedule.items():\n        for task in tasks:\n            if task not in graph:\n                graph[task] = []\n            if task not in in_degree:\n                in_degree[task] = 0\n    \n    for task, deps in dependencies.items():\n        for dep in deps:\n            if dep not in graph:\n                graph[dep] = []\n            graph[dep].append(task)\n            in_degree[task] += 1\n    \n    # Perform topological sorting\n    queue = [task for task in in_degree if in_degree[task] == 0]\n    ordered_tasks = []\n    \n    while queue:\n        task = queue.pop(0)\n        ordered_tasks.append(task)\n        \n        for neighbor in graph[task]:\n            in_degree[neighbor] -= 1\n            if in_degree[neighbor] == 0:\n                queue.append(neighbor)\n    \n    # Check for circular dependencies\n    if len(ordered_tasks) != len(graph):\n        raise ValueError(\"Circular dependencies detected\")\n    \n    return ordered_tasks"
    },
    {
        "function_name": "textrank_algorithm",
        "file_name": "text_summarization.py",
        "parameters": {
            "`text`": "A string representing the text to be analyzed.",
            "`n`": "An integer representing the number of sentences to extract."
        },
        "objectives": [
            "Implement a text ranking algorithm to extract the top n sentences from the text based on their relevance.",
            "Use a graph-based approach to represent the text as a graph of sentences and their relationships.",
            "Calculate the sentence scores using a variant of the PageRank algorithm.",
            "Return the top n sentences with their scores."
        ],
        "import_lines": [
            "import networkx as nx",
            "import numpy as np"
        ],
        "function_def": "def textrank_algorithm(text, n):\n    # Split the text into sentences\n    sentences = text.split(\". \")\n    \n    # Create a graph of sentences and their relationships\n    graph = nx.Graph()\n    graph.add_nodes_from(range(len(sentences)))\n    \n    # Add edges between sentences based on their similarity\n    for i in range(len(sentences)):\n        for j in range(i + 1, len(sentences)):\n            if len(set(sentences[i].split()) & set(sentences[j].split())) > 0:\n                graph.add_edge(i, j)\n    \n    # Calculate the sentence scores using PageRank\n    sentence_scores = nx.pagerank(graph)\n    \n    # Sort the sentences by their scores and return the top n sentences\n    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\n    return [(sentences[i], sentence_scores[i]) for i in top_sentences]"
    },
    {
        "function_name": "path_finder",
        "file_name": "grid_navigation.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a grid of cells",
            "`start_position`": "A tuple representing the starting position (x, y)",
            "`target_position`": "A tuple representing the target position (x, y)"
        },
        "objectives": [
            "Use a depth-first search algorithm to find a path from the start position to the target position in the grid.",
            "The path should avoid any obstacles (represented by zeros) in the grid.",
            "If multiple paths exist, return the shortest one."
        ],
        "import_lines": [
            "import collections"
        ],
        "function_def": "def path_finder(matrix, start_position, target_position):\n    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n    visited = set()\n    stack = collections.deque([(start_position, [start_position])])\n    shortest_path = None\n    \n    while stack:\n        (x, y), path = stack.pop()\n        if (x, y) == target_position and (shortest_path is None or len(path) < len(shortest_path)):\n            shortest_path = path\n        if (x, y) not in visited:\n            visited.add((x, y))\n            for dx, dy in directions:\n                nx, ny = x + dx, y + dy\n                if (0 <= nx < len(matrix) and 0 <= ny < len(matrix[nx]) and\n                        matrix[nx][ny] != 0 and (nx, ny) not in visited):\n                    stack.append(((nx, ny), path + [(nx, ny)]))\n    \n    return shortest_path"
    },
    {
        "function_name": "game_of_life",
        "file_name": "cellular_automata.py",
        "parameters": {
            "`grid`": "A 2D list of integers representing a grid of cells",
            "`num_iterations`": "An integer representing the number of iterations for the simulation"
        },
        "objectives": [
            "Use the Game of Life simulation to evolve the grid over the given number of iterations.",
            "Apply the rules of the Game of Life to each cell in the grid.",
            "Return the final state of the grid after the simulation."
        ],
        "import_lines": [],
        "function_def": "def game_of_life(grid, num_iterations):\n    directions = [(0, 1), (0, -1), (1, 0), (-1, 0), (1, 1), (-1, -1), (1, -1), (-1, 1)]\n    for _ in range(num_iterations):\n        new_grid = [[cell for cell in row] for row in grid]\n        for i in range(len(grid)):\n            for j in range(len(grid[i])):\n                live_neighbors = sum(1 for dx, dy in directions\n                                     if 0 <= i + dx < len(grid) and 0 <= j + dy < len(grid[i])\n                                     and grid[i + dx][j + dy] == 1)\n                if grid[i][j] == 1 and (live_neighbors < 2 or live_neighbors > 3):\n                    new_grid[i][j] = 0\n                elif grid[i][j] == 0 and live_neighbors == 3:\n                    new_grid[i][j] = 1\n        grid = new_grid\n    return grid"
    },
    {
        "function_name": "prioritized_breadth_first_search",
        "file_name": "social_network_analysis.py",
        "parameters": {
            "`social_network`": "A 2D list representing the adjacency matrix of a social network",
            "`initial_node`": "An integer representing the node to start the traversal from",
            "`hop_limit`": "An integer representing the maximum number of hops to traverse"
        },
        "objectives": [
            "Perform a modified breadth-first traversal of the social network starting from the initial node.",
            "Use a priority queue to prioritize nodes based on their degree centrality.",
            "Identify the top 5 nodes with the highest degree centrality within the hop limit and return their degrees."
        ],
        "import_lines": [
            "import networkx as nx",
            "from collections import deque",
            "import heapq"
        ],
        "function_def": "def prioritized_breadth_first_search(social_network, initial_node, hop_limit):\n    graph = nx.from_numpy_array(np.array(social_network))\n    queue = [(graph.degree(initial_node), initial_node, 0)]\n    visited = set()\n    top_nodes = []\n    \n    while queue:\n        degree, node, hops = heapq.heappop(queue)\n        if node not in visited and hops <= hop_limit:\n            visited.add(node)\n            top_nodes.append((node, degree))\n            for neighbor in graph[node]:\n                if neighbor not in visited:\n                    heapq.heappush(queue, (graph.degree(neighbor), neighbor, hops + 1))\n    \n    return sorted(top_nodes, key=lambda x: x[1], reverse=True)[:5]"
    },
    {
        "function_name": "bollinger_bands",
        "file_name": "bollinger_bands.py",
        "parameters": {
            "`stock_prices`": "A list of floats representing the historical stock prices",
            "`window_size`": "An integer representing the window size for the moving average",
            "`std_dev_window_size`": "An integer representing the window size for the standard deviation"
        },
        "objectives": [
            "Calculate the moving average and standard deviation of the stock prices using the given window sizes.",
            "Use the Bollinger Bands indicator to identify when the stock price is overbought or oversold.",
            "Return the list of overbought and oversold signals."
        ],
        "import_lines": [
            "import numpy as np",
            "from collections import deque"
        ],
        "function_def": "def bollinger_bands(stock_prices, window_size, std_dev_window_size):\n    queue = deque(maxlen=window_size)\n    moving_averages = []\n    standard_deviations = []\n    \n    for price in stock_prices:\n        queue.append(price)\n        if len(queue) == window_size:\n            moving_average = np.mean(queue)\n            moving_averages.append(moving_average)\n            standard_deviation = np.std(queue)\n            standard_deviations.append(standard_deviation)\n    \n    bollinger_bands = zip(moving_averages, standard_deviations)\n    overbought_signals = []\n    oversold_signals = []\n    \n    for i, (moving_average, standard_deviation) in enumerate(bollinger_bands):\n        if stock_prices[i] > moving_average + 2 * standard_deviation:\n            overbought_signals.append(stock_prices[i])\n        elif stock_prices[i] < moving_average - 2 * standard_deviation:\n            oversold_signals.append(stock_prices[i])\n    \n    return overbought_signals, oversold_signals"
    },
    {
        "function_name": "radix_sort",
        "file_name": "sorting_algorithms.py",
        "parameters": {
            "`data`": "A list of integers representing the input data."
        },
        "objectives": [
            "Implement a function that performs a radix sort on a given list of integers.",
            "Use a counting sort as a subroutine to sort the elements based on significant bits.",
            "Handle the case where the input data is empty or contains negative numbers.",
            "Return the sorted list of integers."
        ],
        "import_lines": [],
        "function_def": "def radix_sort(data):\n    RADIX = 10\n    placement = 1\n    \n    max_digit = max(data)\n    while placement < max_digit:\n        buckets = [list() for _ in range(RADIX)]\n        for i in data:\n            tmp = int((i / placement) % RADIX)\n            buckets[tmp].append(i)\n        a = 0\n        for b in range(RADIX):\n            buck = buckets[b]\n            for i in buck:\n                data[a] = i\n                a += 1\n        placement *= RADIX\n    return data"
    },
    {
        "function_name": "extract_features",
        "file_name": "text_analysis.py",
        "parameters": {
            "`text_documents`": "A list of text documents, where each document is a string.",
            "`vocabulary`": "A set of words to include in the analysis.",
            "`max_features`": "An integer, representing the maximum number of features to extract."
        },
        "objectives": [
            "Implement a TF-IDF algorithm to extract features from the text documents.",
            "Use a max heap to keep track of the top features.",
            "Return the top features."
        ],
        "import_lines": [
            "from collections import Counter",
            "import heapq"
        ],
        "function_def": "def extract_features(text_documents, vocabulary, max_features):\n    # Calculate the TF-IDF scores\n    tfidf_scores = []\n    for document in text_documents:\n        words = document.split()\n        tf = Counter(words)\n        idf = Counter()\n        for word in vocabulary:\n            idf[word] = sum(1 for doc in text_documents if word in doc.split())\n        tfidf = {word: tf[word] * np.log(len(text_documents) / idf[word]) for word in vocabulary if word in tf}\n        tfidf_scores.append(tfidf)\n    # Use a max heap to keep track of the top features\n    max_heap = []\n    for scores in tfidf_scores:\n        for word, score in scores.items():\n            if len(max_heap) < max_features:\n                heapq.heappush(max_heap, (score, word))\n            else:\n                heapq.heappushpop(max_heap, (score, word))\n    # Return the top features\n    return [word for score, word in max_heap]"
    },
    {
        "function_name": "eigenvectors_by_power_iteration",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix",
            "`k`": "An integer representing the number of eigenvectors to find"
        },
        "objectives": [
            "Implement a function that calculates the eigenvectors of the matrix using the power iteration method.",
            "Use the Gram-Schmidt process to orthogonalize the eigenvectors.",
            "Find the k eigenvectors corresponding to the largest eigenvalues.",
            "Return the eigenvectors and their corresponding eigenvalues."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def eigenvectors_by_power_iteration(matrix, k):\n    eigenvalues = np.linalg.eigvals(matrix)\n    eigenvectors = []\n    \n    for _ in range(k):\n        v = np.random.rand(len(matrix))\n        for _ in range(len(matrix)):\n            v = np.dot(matrix, v)\n            v = v / np.linalg.norm(v)\n        \n        # Gram-Schmidt process\n        for i in range(len(eigenvectors)):\n            v = v - np.dot(v, eigenvectors[i]) * eigenvectors[i]\n        v = v / np.linalg.norm(v)\n        \n        eigenvalue = np.dot(v.T, np.dot(matrix, v)) / np.dot(v.T, v)\n        eigenvectors.append((v, eigenvalue))\n    \n    return eigenvectors"
    },
    {
        "function_name": "eigenvector_selector",
        "file_name": "matrix_algorithms.py",
        "parameters": {
            "`array`": "A 2D array of integers representing a matrix",
            "`k`": "An integer representing the number of eigenvectors to select"
        },
        "objectives": [
            "Implement the power iteration method to find the top k eigenvectors of the matrix",
            "Use the QR algorithm to orthogonalize the eigenvectors",
            "Ensure that the eigenvectors are orthonormal and have the largest eigenvalues",
            "Return the eigenvectors and their corresponding eigenvalues"
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.linalg import qr"
        ],
        "function_def": "def eigenvector_selector(array, k):\n    num_rows, num_cols = len(array), len(array[0])\n    eigenvalues = np.linalg.eigvals(array)\n    eigenvectors = np.linalg.eig(array)[1]\n    indices = np.argsort(-eigenvalues)\n    selected_eigenvectors = eigenvectors[:, indices[:k]]\n    # Orthogonalize the eigenvectors using QR algorithm\n    q, r = qr(selected_eigenvectors)\n    return q, eigenvalues[indices[:k]]"
    },
    {
        "function_name": "random_number_generator",
        "file_name": "randomness.py",
        "parameters": {
            "`seeds`": "A list of integers representing the initial values for the random number generators",
            "`n`": "An integer representing the number of random numbers to generate",
            "`distribution`": "A string representing the distribution of the random numbers (e.g. \"uniform\", \"normal\", \"poisson\")"
        },
        "objectives": [
            "Use the Linear Congruential Generator (LCG) algorithm to generate a sequence of random numbers.",
            "Use the given distribution to transform the generated random numbers into the desired distribution.",
            "Return the sequence of random numbers."
        ],
        "import_lines": [
            "import numpy as np",
            "import scipy.stats as stats"
        ],
        "function_def": "def random_number_generator(seeds, n, distribution):\n    random_numbers = []\n    for seed in seeds:\n        a = 1664525\n        c = 1013904223\n        m = 2**32\n        x = seed\n        for _ in range(n):\n            x = (a * x + c) % m\n            u = x / m\n            if distribution == \"uniform\":\n                random_numbers.append(u)\n            elif distribution == \"normal\":\n                random_numbers.append(stats.norm.ppf(u, loc=0, scale=1))\n            elif distribution == \"poisson\":\n                random_numbers.append(stats.poisson.ppf(u, mu=1))\n    return random_numbers"
    },
    {
        "function_name": "dna_aligner",
        "file_name": "bioinformatics.py",
        "parameters": {
            "`genes`": "A list of strings representing the DNA sequences",
            "`threshold`": "A float representing the minimum similarity score"
        },
        "objectives": [
            "Use the Needleman-Wunsch algorithm to align the DNA sequences.",
            "Calculate the similarity score for each alignment.",
            "Filter the alignments that have a similarity score greater than or equal to the threshold.",
            "Return the aligned sequences and their similarity scores."
        ],
        "import_lines": [],
        "function_def": "def dna_aligner(genes, threshold):\n    aligned_sequences = []\n    for i in range(len(genes)):\n        for j in range(i+1, len(genes)):\n            sequence1 = genes[i]\n            sequence2 = genes[j]\n            gap_penalty = -1\n            match_score = 1\n            mismatch_penalty = -1\n            score_matrix = np.zeros((len(sequence1)+1, len(sequence2)+1))\n            for k in range(len(sequence1)+1):\n                score_matrix[k][0] = gap_penalty * k\n            for l in range(len(sequence2)+1):\n                score_matrix[0][l] = gap_penalty * l\n            for k in range(1, len(sequence1)+1):\n                for l in range(1, len(sequence2)+1):\n                    match = score_matrix[k-1][l-1] + (match_score if sequence1[k-1] == sequence2[l-1] else mismatch_penalty)\n                    delete = score_matrix[k-1][l] + gap_penalty\n                    insert = score_matrix[k][l-1] + gap_penalty\n                    score_matrix[k][l] = max(match, delete, insert)\n            k = len(sequence1)\n            l = len(sequence2)\n            aligned_sequence1 = \"\"\n            aligned_sequence2 = \"\"\n            while k > 0 or l > 0:\n                if k > 0 and l > 0 and sequence1[k-1] == sequence2[l-1]:\n                    aligned_sequence1 = sequence1[k-1] + aligned_sequence1\n                    aligned_sequence2 = sequence2[l-1] + aligned_sequence2\n                    k -= 1\n                    l -= 1\n                elif k > 0 and score_matrix[k][l] == score_matrix[k-1][l] + gap_penalty:\n                    aligned_sequence1 = sequence1[k-1] + aligned_sequence1\n                    aligned_sequence2 = \"-\" + aligned_sequence2\n                    k -= 1\n                elif l > 0 and score_matrix[k][l] == score_matrix[k][l-1] + gap_penalty:\n                    aligned_sequence1 = \"-\" + aligned_sequence1\n                    aligned_sequence2 = sequence2[l-1] + aligned_sequence2\n                    l -= 1\n                else:\n                    aligned_sequence1 = sequence1[k-1] + aligned_sequence1\n                    aligned_sequence2 = sequence2[l-1] + aligned_sequence2\n                    k -= 1\n                    l -= 1\n            similarity_score = score_matrix[len(sequence1)][len(sequence2)] / max(len(sequence1), len(sequence2))\n            if similarity_score >= threshold:\n                aligned_sequences.append((aligned_sequence1, aligned_sequence2, similarity_score))\n    return aligned_sequences"
    },
    {
        "function_name": "convolution",
        "file_name": "image_processing.py",
        "parameters": {
            "`image`": "A 2D list of integers representing an image",
            "`kernel`": "A 2D list of integers representing a convolutional kernel"
        },
        "objectives": [
            "Perform a convolution of the image with the `kernel`.",
            "Calculate the sum of the products of corresponding elements in the image and kernel.",
            "Return the convolved image."
        ],
        "import_lines": [],
        "function_def": "def convolution(image, kernel):\n    num_rows, num_cols = len(image), len(image[0])\n    kernel_rows, kernel_cols = len(kernel), len(kernel[0])\n    convolved_image = [[0 for _ in range(num_cols - kernel_cols + 1)] for _ in range(num_rows - kernel_rows + 1)]\n    \n    for i in range(num_rows - kernel_rows + 1):\n        for j in range(num_cols - kernel_cols + 1):\n            sum_products = 0\n            for k in range(kernel_rows):\n                for l in range(kernel_cols):\n                    sum_products += image[i + k][j + l] * kernel[k][l]\n            convolved_image[i][j] = sum_products\n    \n    return convolved_image"
    },
    {
        "function_name": "topological_sort",
        "file_name": "project_plan.py",
        "parameters": {
            "`plan`": "A list of integers representing a project plan",
            "`dependencies`": "A 2D list of integers representing dependencies between tasks"
        },
        "objectives": [
            "Perform a topological sort of the project plan based on the `dependencies`.",
            "Identify any cycles in the dependencies.",
            "Return the sorted plan."
        ],
        "import_lines": [
            "from collections import defaultdict, deque"
        ],
        "function_def": "def topological_sort(plan, dependencies):\n    graph = defaultdict(list)\n    in_degree = {task: 0 for task in plan}\n    \n    for dependency in dependencies:\n        graph[dependency[0]].append(dependency[1])\n        in_degree[dependency[1]] += 1\n    \n    queue = deque([task for task in plan if in_degree[task] == 0])\n    \n    sorted_plan = []\n    while queue:\n        task = queue.popleft()\n        sorted_plan.append(task)\n        \n        for neighbor in graph[task]:\n            in_degree[neighbor] -= 1\n            if in_degree[neighbor] == 0:\n                queue.append(neighbor)\n    \n    if any(in_degree[task] != 0 for task in plan):\n        raise ValueError(\"Cycle detected in dependencies\")\n    \n    return sorted_plan"
    },
    {
        "function_name": "regex_search",
        "file_name": "text_analysis.py",
        "parameters": {
            "`strings`": "A list of strings",
            "`pattern`": "A string representing a regular expression"
        },
        "objectives": [
            "Compile the `pattern` into a regular expression object.",
            "Search for matches of the `pattern` in each string.",
            "Return a dictionary with the matched strings and their corresponding matches."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def regex_search(strings, pattern):\n    regex = re.compile(pattern)\n    matches = {}\n    \n    for string in strings:\n        match = regex.search(string)\n        if match:\n            matches[string] = match.group()\n    \n    return matches"
    },
    {
        "function_name": "function_applier",
        "file_name": "function_application.py",
        "parameters": {
            "`functions`": "A list of functions",
            "`inputs`": "A list of inputs"
        },
        "objectives": [
            "Apply each function to each input and calculate the output.",
            "Calculate the maximum output for each function across all inputs.",
            "Return a dictionary mapping each function to its maximum output."
        ],
        "import_lines": [],
        "function_def": "def function_applier(functions, inputs):\n    max_outputs = {}\n    for func in functions:\n        max_output = float('-inf')\n        for input_value in inputs:\n            output = func(input_value)\n            if output > max_output:\n                max_output = output\n        max_outputs[func] = max_output\n    return max_outputs"
    },
    {
        "function_name": "query_answerer",
        "file_name": "table_querying.py",
        "parameters": {
            "`tables`": "A list of lists of integers representing tables",
            "`queries`": "A list of tuples representing queries"
        },
        "objectives": [
            "Answer each query by finding the sum of all values in the tables that match the query condition.",
            "The query condition is a tuple of (operator, value), where operator can be 'eq', 'neq', 'lt', 'gt', 'lte', 'gte'.",
            "Return a list of query results."
        ],
        "import_lines": [],
        "function_def": "def query_answerer(tables, queries):\n    query_results = []\n    for query in queries:\n        operator, value = query\n        result = 0\n        for table in tables:\n            for row in table:\n                if operator == 'eq' and row == value:\n                    result += row\n                elif operator == 'neq' and row != value:\n                    result += row\n                elif operator == 'lt' and row < value:\n                    result += row\n                elif operator == 'gt' and row > value:\n                    result += row\n                elif operator == 'lte' and row <= value:\n                    result += row\n                elif operator == 'gte' and row >= value:\n                    result += row\n        query_results.append(result)\n    return query_results"
    },
    {
        "function_name": "ngram_extractor",
        "file_name": "text_processing.py",
        "parameters": {
            "`text`": "A string representing the input text",
            "`ngram_size`": "An integer representing the size of the n-grams to extract",
            "`min_frequency`": "An integer representing the minimum frequency of n-grams to keep"
        },
        "objectives": [
            "Implement an n-gram extraction algorithm to extract n-grams from the input text",
            "Use a hash map to count the frequency of each n-gram",
            "Filter out n-grams that have a frequency less than the minimum frequency",
            "Return the extracted n-grams along with their frequencies"
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def ngram_extractor(text, ngram_size, min_frequency):\n    ngrams = defaultdict(int)\n    words = text.split()\n    for i in range(len(words) - ngram_size + 1):\n        ngram = ' '.join(words[i:i + ngram_size])\n        ngrams[ngram] += 1\n        \n    filtered_ngrams = {ngram: freq for ngram, freq in ngrams.items() if freq >= min_frequency}\n    \n    return filtered_ngrams"
    },
    {
        "function_name": "page_rank",
        "file_name": "web_analysis.py",
        "parameters": {
            "`web_pages`": "A dictionary representing the web pages where each key is a page and its value is a list of its neighbors.",
            "`damping_factor`": "A float representing the damping factor for the PageRank algorithm."
        },
        "objectives": [
            "Use the PageRank algorithm to calculate the importance of each web page.",
            "Ensure that the damping factor is between 0 and 1.",
            "Return the importance of each web page."
        ],
        "import_lines": [],
        "function_def": "def page_rank(web_pages, damping_factor):\n    N = len(web_pages)\n    ranks = {page: 1.0 / N for page in web_pages}\n    for _ in range(100):  # Perform 100 iterations\n        new_ranks = {}\n        for page in web_pages:\n            rank = (1 - damping_factor) / N\n            for neighbor in web_pages[page]:\n                rank += damping_factor * ranks[neighbor] / len(web_pages[neighbor])\n            new_ranks[page] = rank\n        ranks = new_ranks\n    return ranks"
    },
    {
        "function_name": "word_segmentation",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`text`": "A string representing the text",
            "`dictionary`": "A list of strings representing the dictionary"
        },
        "objectives": [
            "Implement the dynamic programming approach to solve the word segmentation problem.",
            "Use a table to efficiently store and retrieve the maximum probability of each prefix.",
            "Handle the case where the text is not in the dictionary."
        ],
        "import_lines": [],
        "function_def": "def word_segmentation(text, dictionary):\n    max_prob = [0] * (len(text) + 1)\n    max_prob[0] = 1\n    segmentation = [[] for _ in range(len(text) + 1)]\n    \n    for i in range(1, len(text) + 1):\n        for j in range(i):\n            if text[j:i] in dictionary:\n                if max_prob[i] < max_prob[j] * (len(text) - i):\n                    max_prob[i] = max_prob[j] * (len(text) - i)\n                    segmentation[i] = segmentation[j] + [text[j:i]]\n    \n    return segmentation[-1]"
    },
    {
        "function_name": "numerical_differentiation",
        "file_name": "function_analysis.py",
        "parameters": {
            "`functs`": "A list of functions representing mathematical functions",
            "`x_values`": "A list of floats representing the x-values to evaluate the functions at"
        },
        "objectives": [
            "Evaluate each function at each x-value using numerical differentiation.",
            "Use the definition of a derivative to approximate the derivative of each function at each x-value.",
            "Return a list of dictionaries where each dictionary contains the function, x-value, and approximate derivative."
        ],
        "import_lines": [],
        "function_def": "def numerical_differentiation(functs, x_values):\n    results = []\n    \n    for funct in functs:\n        for x in x_values:\n            h = 1e-7\n            derivative = (funct(x + h) - funct(x - h)) / (2 * h)\n            results.append({\"function\": funct.__name__, \"x\": x, \"derivative\": derivative})\n    \n    return results"
    },
    {
        "function_name": "max_profit",
        "file_name": "stock_trading.py",
        "parameters": {
            "`prices`": "A list of integers representing the stock prices",
            "`fee`": "An integer representing the transaction fee",
            "`k`": "An integer representing the maximum number of transactions"
        },
        "objectives": [
            "Find the maximum profit that can be achieved with at most k transactions.",
            "Use dynamic programming to calculate the maximum profit for each subproblem.",
            "Return the maximum profit."
        ],
        "import_lines": [],
        "function_def": "def max_profit(prices, fee, k):\n    n = len(prices)\n    buy = [float('-inf')] * (k + 1)\n    sell = [0] * (k + 1)\n    \n    for price in prices:\n        for i in range(1, k + 1):\n            buy[i] = max(buy[i], sell[i - 1] - price)\n            sell[i] = max(sell[i], buy[i] + price - fee)\n    \n    return sell[k]"
    },
    {
        "function_name": "identify_large_senders",
        "file_name": "transaction_analysis.py",
        "parameters": {
            "`transactions`": "A list of tuples representing transactions, each containing a sender, recipient, and amount",
            "`min_amount`": "An integer representing the minimum amount to consider"
        },
        "objectives": [
            "Identify the users who have sent transactions with amounts above the minimum threshold.",
            "Calculate the total amount sent by each user.",
            "Return a list of tuples containing the users and their total amounts."
        ],
        "import_lines": [],
        "function_def": "def identify_large_senders(transactions, min_amount):\n    senders = {}\n    \n    for sender, _, amount in transactions:\n        if amount >= min_amount:\n            if sender in senders:\n                senders[sender] += amount\n            else:\n                senders[sender] = amount\n    \n    return list(senders.items())"
    },
    {
        "function_name": "projection",
        "file_name": "projection.py",
        "parameters": {
            "`points`": "A list of tuples representing the points in 2D space.",
            "`line`": "A list of two tuples representing the line to project onto."
        },
        "objectives": [
            "Implement the projection algorithm to project points onto a line.",
            "Use the formula for projection to calculate the projected coordinates of each point.",
            "Return the projected points."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def projection(points, line):\n    # Calculate the vector of the line\n    vector = (line[1][0] - line[0][0], line[1][1] - line[0][1])\n    length = math.sqrt(vector[0]**2 + vector[1]**2)\n    unit_vector = (vector[0] / length, vector[1] / length)\n    \n    # Project each point onto the line\n    projected_points = []\n    for point in points:\n        # Calculate the vector from the first point on the line to the current point\n        current_vector = (point[0] - line[0][0], point[1] - line[0][1])\n        \n        # Calculate the projection of the current point\n        dot_product = current_vector[0] * unit_vector[0] + current_vector[1] * unit_vector[1]\n        projected_point = (line[0][0] + dot_product * unit_vector[0], line[0][1] + dot_product * unit_vector[1])\n        projected_points.append(projected_point)\n    \n    return projected_points"
    },
    {
        "function_name": "pattern_matching",
        "file_name": "convolution.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix.",
            "`pattern`": "A 2D list of integers representing a pattern to search for."
        },
        "objectives": [
            "Use a 2D convolution algorithm to find occurrences of the pattern in the matrix.",
            "Pad the matrix with zeros to ensure that the pattern can be matched at the edges.",
            "Slide the pattern over the matrix and calculate the sum of products at each position.",
            "Compare the sum of products to a threshold value and return the positions of matches."
        ],
        "import_lines": [],
        "function_def": "def pattern_matching(matrix, pattern):\n    padded_matrix = [[0] * (len(matrix[0]) + len(pattern[0]) - 1) for _ in range(len(matrix) + len(pattern) - 1)]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            padded_matrix[i][j] = matrix[i][j]\n    threshold = sum(sum(row) for row in pattern) / 2\n    matches = []\n    for i in range(len(padded_matrix) - len(pattern) + 1):\n        for j in range(len(padded_matrix[0]) - len(pattern[0]) + 1):\n            sum_of_products = sum(padded_matrix[x][y] * pattern[x - i][y - j] for x in range(len(pattern)) for y in range(len(pattern[0])))\n            if sum_of_products > threshold:\n                matches.append((i, j))\n    return matches"
    },
    {
        "function_name": "transitive_closure",
        "file_name": "relation_algorithms.py",
        "parameters": {
            "`variables`": "A list of variables.",
            "`relations`": "A list of binary relations between variables."
        },
        "objectives": [
            "Implement Warshall's algorithm to find the transitive closure of a relation.",
            "Calculate the transitive closure of the given relation.",
            "Return the transitive closure matrix."
        ],
        "import_lines": [],
        "function_def": "def transitive_closure(variables, relations):\n    n = len(variables)\n    closure = [[0]*n for _ in range(n)]\n    \n    for i in range(n):\n        for j in range(n):\n            if (variables[i], variables[j]) in relations:\n                closure[i][j] = 1\n                \n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                if closure[i][k] == 1 and closure[k][j] == 1:\n                    closure[i][j] = 1\n                    \n    return closure"
    },
    {
        "function_name": "quantile_analysis",
        "file_name": "quantile_analysis.py",
        "parameters": {
            "`sequence`": "A list of integers representing a sequence of numbers",
            "`window_size`": "An integer representing the size of the window for the calculation",
            "`num_quantiles`": "An integer representing the number of quantiles to calculate"
        },
        "objectives": [
            "Calculate the moving average of the sequence using the given window size",
            "Calculate the quantiles of the sequence within the given window",
            "Return the moving average and quantiles as a tuple"
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def quantile_analysis(sequence, window_size, num_quantiles):\n    moving_average = np.convolve(sequence, np.ones(window_size) / window_size, mode='same')\n    quantiles = np.zeros((len(sequence), num_quantiles))\n    for i in range(len(sequence) - window_size + 1):\n        window = sequence[i:i + window_size]\n        quantiles[i + window_size // 2] = np.quantile(window, np.linspace(0, 1, num_quantiles))\n    return moving_average, quantiles"
    },
    {
        "function_name": "stock_forecaster",
        "file_name": "finance.py",
        "parameters": {
            "`stock_data`": "A dictionary where each key is a stock symbol and each value is a list of daily stock prices.",
            "`forecast_days`": "An integer representing the number of days to forecast into the future.",
            "`window_size`": "An integer representing the size of the sliding window to use for the forecast."
        },
        "objectives": [
            "Use a moving average strategy to forecast future stock prices.",
            "Calculate the daily return for each stock and select the top 3 stocks with the highest average returns.",
            "For the selected stocks, use a linear regression model to forecast the stock prices for the specified number of days.",
            "Return a dictionary where the keys are the selected stock symbols and the values are lists of forecasted prices."
        ],
        "import_lines": [
            "import numpy as np",
            "from sklearn.linear_model import LinearRegression"
        ],
        "function_def": "def stock_forecaster(stock_data, forecast_days, window_size):\n    # Calculate daily returns and select top 3 stocks\n    stock_returns = {}\n    for stock, prices in stock_data.items():\n        daily_returns = np.diff(prices) / prices[:-1]\n        avg_return = np.mean(daily_returns)\n        stock_returns[stock] = avg_return\n    \n    selected_stocks = sorted(stock_returns, key=stock_returns.get, reverse=True)[:3]\n    \n    # Forecast prices using linear regression\n    forecasted_prices = {}\n    for stock in selected_stocks:\n        prices = stock_data[stock]\n        x = np.arange(len(prices))\n        model = LinearRegression()\n        model.fit(x.reshape(-1, 1), np.array(prices))\n        forecasted_x = np.arange(len(prices), len(prices) + forecast_days)\n        forecasted_prices[stock] = model.predict(forecasted_x.reshape(-1, 1)).tolist()\n    \n    return forecasted_prices"
    },
    {
        "function_name": "rotate_image_stack",
        "file_name": "image_processing.py",
        "parameters": {
            "`image_stack`": "A 3D array representing a stack of images",
            "`rotation_angle`": "An integer representing the angle of rotation in degrees"
        },
        "objectives": [
            "Implement a 3D rotation algorithm to rotate the image stack around a specified axis.",
            "Use the Euler's rotation formula to calculate the rotation matrix.",
            "Handle the case where the rotation angle is greater than 360 degrees."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def rotate_image_stack(image_stack, rotation_angle):\n    angle_rad = np.deg2rad(rotation_angle % 360)\n    rotation_matrix = np.array([\n        [np.cos(angle_rad), -np.sin(angle_rad), 0],\n        [np.sin(angle_rad), np.cos(angle_rad), 0],\n        [0, 0, 1]\n    ])\n    \n    rotated_stack = np.zeros(image_stack.shape)\n    for i in range(image_stack.shape[0]):\n        for j in range(image_stack.shape[1]):\n            for k in range(image_stack.shape[2]):\n                rotated_point = np.dot(rotation_matrix, np.array([i, j, k]))\n                rotated_stack[int(rotated_point[0]), int(rotated_point[1]), int(rotated_point[2])] = image_stack[i, j, k]\n    \n    return rotated_stack"
    },
    {
        "function_name": "city_selection",
        "file_name": "urban_planning.py",
        "parameters": {
            "`populations`": "A list of integers representing the populations of cities",
            "`budget`": "An integer representing the budget for construction",
            "`cost_per_unit`": "An integer representing the cost per unit of construction"
        },
        "objectives": [
            "Implement the greedy algorithm to select the cities with the largest population for construction based on the budget and cost per unit.",
            "Calculate the total population served by the selected cities.",
            "Return a tuple containing the selected cities and the total population served."
        ],
        "import_lines": [],
        "function_def": "def city_selection(populations, budget, cost_per_unit):\n    selected_cities = []\n    total_population_served = 0\n    for i, population in enumerate(populations):\n        cost = cost_per_unit * population\n        if cost <= budget:\n            selected_cities.append(i)\n            total_population_served += population\n            budget -= cost\n    \n    return selected_cities, total_population_served"
    },
    {
        "function_name": "project_polyhedron",
        "file_name": "geometry.py",
        "parameters": {
            "`vertices`": "A list of tuples representing the 3D coordinates of vertices of a polyhedron",
            "`faces`": "A list of lists of integers representing the vertices that form each face of the polyhedron",
            "`axis`": "An integer representing the axis to project the polyhedron onto"
        },
        "objectives": [
            "Calculate the projection of the polyhedron onto the specified axis.",
            "Use the separating axis theorem to find the projected interval of the polyhedron.",
            "Return the projected interval."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def project_polyhedron(vertices, faces, axis):\n    if axis == 0:\n        projected_vertices = [v[1:] for v in vertices]\n    elif axis == 1:\n        projected_vertices = [v[0::2] for v in vertices]\n    else:\n        projected_vertices = [v[:2] for v in vertices]\n    \n    min_val = float('inf')\n    max_val = float('-inf')\n    \n    for face in faces:\n        face_vertices = [projected_vertices[i] for i in face]\n        min_face_val = min([np.linalg.norm(np.cross(v1 - v0, v2 - v0)) / np.linalg.norm(v2 - v0) for v0, v1, v2 in zip(face_vertices, face_vertices[1:] + face_vertices[:1], face_vertices[2:] + face_vertices[:2])])\n        max_face_val = max([np.linalg.norm(np.cross(v1 - v0, v2 - v0)) / np.linalg.norm(v2 - v0) for v0, v1, v2 in zip(face_vertices, face_vertices[1:] + face_vertices[:1], face_vertices[2:] + face_vertices[:2])])\n        \n        if min_face_val < min_val:\n            min_val = min_face_val\n        if max_face_val > max_val:\n            max_val = max_face_val\n    \n    return (min_val, max_val)"
    },
    {
        "function_name": "correlation_matrix",
        "file_name": "correlation_analysis.py",
        "parameters": {
            "`matrix`": "A 2D list representing the input matrix",
            "`threshold`": "A float representing the minimum required correlation"
        },
        "objectives": [
            "Implement a function to calculate the correlation matrix from the given input matrix.",
            "Use a technique to ignore highly correlated features based on the threshold.",
            "Return the filtered correlation matrix."
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.stats import pearsonr"
        ],
        "function_def": "def correlation_matrix(matrix, threshold):\n    correlation = np.corrcoef(matrix.T)\n    for i in range(correlation.shape[0]):\n        for j in range(correlation.shape[1]):\n            if i != j and abs(correlation[i][j]) > threshold:\n                correlation[j][i] = 0\n                correlation[i][j] = 0\n    np.fill_diagonal(correlation, 1)\n    return correlation"
    },
    {
        "function_name": "k_max_values",
        "file_name": "matrix_search.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix",
            "`k`": "An integer representing the number of maximum values to find"
        },
        "objectives": [
            "Implement a heap-based algorithm to find the k maximum values in the matrix.",
            "Return a list of the k maximum values in descending order."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def k_max_values(matrix, k):\n    max_heap = []\n    for row in matrix:\n        for val in row:\n            if len(max_heap) < k:\n                heapq.heappush(max_heap, val)\n            else:\n                heapq.heappushpop(max_heap, val)\n    \n    max_values = []\n    while max_heap:\n        max_values.append(heapq.heappop(max_heap))\n    \n    return sorted(max_values, reverse=True)"
    },
    {
        "function_name": "evaluate_expression",
        "file_name": "math_expression_evaluator.py",
        "parameters": {
            "`expression`": "A string representing the mathematical expression to be evaluated.",
            "`variables`": "A dictionary of variables and their corresponding values.",
            "`precision`": "An integer representing the decimal precision of the result."
        },
        "objectives": [
            "Evaluate the mathematical expression using the given variables.",
            "Perform floating-point arithmetic to handle decimals.",
            "Round the result to the specified precision.",
            "Return the final result."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def evaluate_expression(expression, variables, precision):\n    for var, value in variables.items():\n        expression = expression.replace(var, str(value))\n        \n    result = eval(expression)\n    result = round(result, precision)\n    \n    return result"
    },
    {
        "function_name": "stack_simulator",
        "file_name": "stack_simulator.py",
        "parameters": {
            "`sequence`": "A list of integers representing the sequence of operations.",
            "`stack_size`": "An integer representing the maximum size of the stack."
        },
        "objectives": [
            "Parse the sequence of operations and simulate a stack.",
            "Handle push and pop operations.",
            "If the stack size exceeds the maximum size, raise an error.",
            "Return the final state of the stack."
        ],
        "import_lines": [],
        "function_def": "def stack_simulator(sequence, stack_size):\n    stack = []\n    for op in sequence:\n        if op == 0:\n            if len(stack) > 0:\n                stack.pop()\n        else:\n            if len(stack) < stack_size:\n                stack.append(op)\n            else:\n                raise Exception(\"Stack Overflow\")\n    return stack"
    },
    {
        "function_name": "moving_average_filter",
        "file_name": "signal_processing.py",
        "parameters": {
            "`signal`": "A list of integers representing a signal",
            "`filter_size`": "An integer representing the size of the moving average filter"
        },
        "objectives": [
            "Implement a moving average filter to smooth out the signal.",
            "Calculate the average of the last `filter_size` values of the signal at each position.",
            "Return the filtered signal as a list of integers."
        ],
        "import_lines": [],
        "function_def": "def moving_average_filter(signal, filter_size):\n    # Initialize the filtered signal\n    filtered_signal = []\n    \n    # Iterate over the signal\n    for i in range(len(signal)):\n        # Calculate the start index of the filter\n        start = max(0, i - filter_size + 1)\n        \n        # Calculate the average of the filter values\n        average = sum(signal[start:i+1]) / (i - start + 1)\n        \n        # Append the average to the filtered signal\n        filtered_signal.append(int(average))\n        \n    return filtered_signal"
    },
    {
        "function_name": "thresholding",
        "file_name": "image_processing.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing a matrix",
            "`threshold`": "An integer representing the threshold value"
        },
        "objectives": [
            "Implement a thresholding algorithm to binarize the matrix.",
            "Compare each element of the matrix to the threshold value and set it to 1 if greater, otherwise set it to 0.",
            "Return the binarized matrix as a 2D list of integers."
        ],
        "import_lines": [],
        "function_def": "def thresholding(matrix, threshold):\n    # Initialize the binarized matrix\n    binarized_matrix = []\n    \n    # Iterate over the rows of the matrix\n    for row in matrix:\n        # Initialize the binarized row\n        binarized_row = []\n        \n        # Iterate over the elements of the row\n        for element in row:\n            # Compare the element to the threshold value and set it accordingly\n            if element > threshold:\n                binarized_row.append(1)\n            else:\n                binarized_row.append(0)\n        \n        # Append the binarized row to the binarized matrix\n        binarized_matrix.append(binarized_row)\n    \n    return binarized_matrix"
    },
    {
        "function_name": "sobel_filter",
        "file_name": "image_processing.py",
        "parameters": {
            "`image`": "A 2D list representing the pixel values of an image.",
            "`filter_size`": "An integer representing the size of the filter."
        },
        "objectives": [
            "Implement a function that applies a Sobel filter to an image to detect edges.",
            "Use a 3x3 filter to compute the horizontal and vertical gradients.",
            "Combine the gradients to produce the edge map."
        ],
        "import_lines": [],
        "function_def": "def sobel_filter(image, filter_size):\n    if filter_size != 3:\n        raise ValueError(\"Filter size must be 3\")\n    horizontal_filter = [\n        [-1, 0, 1],\n        [-2, 0, 2],\n        [-1, 0, 1]\n    ]\n    vertical_filter = [\n        [-1, -2, -1],\n        [0, 0, 0],\n        [1, 2, 1]\n    ]\n    edge_map = [[0 for _ in range(len(image[0]))] for _ in range(len(image))]\n    for i in range(1, len(image) - 1):\n        for j in range(1, len(image[0]) - 1):\n            horizontal_grad = 0\n            vertical_grad = 0\n            for x in range(-1, 2):\n                for y in range(-1, 2):\n                    horizontal_grad += image[i + x][j + y] * horizontal_filter[x + 1][y + 1]\n                    vertical_grad += image[i + x][j + y] * vertical_filter[x + 1][y + 1]\n            edge_map[i][j] = (horizontal_grad ** 2 + vertical_grad ** 2) ** 0.5\n    return edge_map"
    },
    {
        "function_name": "train_neural_network",
        "file_name": "neural_networks.py",
        "parameters": {
            "`network`": "A list of lists representing the connections between nodes in a neural network.",
            "`inputs`": "A list of floats representing the input values to the network.",
            "`learning_rate`": "A float representing the learning rate for backpropagation."
        },
        "objectives": [
            "Implement a function that trains a neural network using backpropagation.",
            "Use a sigmoid activation function for the hidden and output layers.",
            "Update the weights and biases using stochastic gradient descent."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def train_neural_network(network, inputs, learning_rate):\n    outputs = []\n    for i in range(len(network)):\n        for j in range(len(network[i])):\n            output = 0\n            for k in range(len(inputs)):\n                output += inputs[k] * network[i][j][k]\n            output = 1 / (1 + math.exp(-output))\n            outputs.append(output)\n    for i in range(len(network)):\n        for j in range(len(network[i])):\n            for k in range(len(inputs)):\n                error = 0\n                for l in range(len(outputs)):\n                    error += outputs[l] * (1 - outputs[l]) * network[i][j][k]\n                network[i][j][k] -= learning_rate * error\n    return network"
    },
    {
        "function_name": "lis",
        "file_name": "sequence_algorithms.py",
        "parameters": {
            "`sequence`": "A list of numbers representing the sequence",
            "`subsequence_length`": "The length of the subsequence to find"
        },
        "objectives": [
            "Implement the Longest Increasing Subsequence (LIS) algorithm to find the longest increasing subsequence in the sequence.",
            "Use dynamic programming to build up a table of the longest increasing subsequences ending at each position.",
            "Use this table to construct the actual longest increasing subsequence.",
            "Return the longest increasing subsequence."
        ],
        "import_lines": [],
        "function_def": "def lis(sequence, subsequence_length):\n    lis_table = [1] * len(sequence)\n    prev_table = [None] * len(sequence)\n    \n    for i in range(1, len(sequence)):\n        for j in range(i):\n            if sequence[i] > sequence[j] and lis_table[i] < lis_table[j] + 1:\n                lis_table[i] = lis_table[j] + 1\n                prev_table[i] = j\n    \n    max_length = max(lis_table)\n    if max_length < subsequence_length:\n        return []\n    \n    index = lis_table.index(max_length)\n    subsequence = []\n    \n    while index is not None:\n        subsequence.append(sequence[index])\n        index = prev_table[index]\n    \n    return subsequence[::-1]"
    },
    {
        "function_name": "lda_topics",
        "file_name": "topic_modeling.py",
        "parameters": {
            "`text`": "A string representing the text to analyze.",
            "`num_topics`": "An integer representing the number of topics to extract."
        },
        "objectives": [
            "Implement the Latent Dirichlet Allocation (LDA) algorithm to extract topics from the text.",
            "Use a dictionary to keep track of words and their corresponding topic assignments.",
            "Return a dictionary where each key is a topic and its corresponding value is a list of words that are most likely to belong to that topic."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def lda_topics(text, num_topics):\n    words = text.split()\n    word_counts = {}\n    for word in words:\n        if word not in word_counts:\n            word_counts[word] = 0\n        word_counts[word] += 1\n    \n    topics = {}\n    for i in range(num_topics):\n        topics[i] = {}\n    \n    for word in word_counts:\n        topic_assignments = np.random.dirichlet(np.ones(num_topics), size=1)[0]\n        topic = np.argmax(topic_assignments)\n        topics[topic][word] = word_counts[word]\n    \n    for topic in topics:\n        words = list(topics[topic].keys())\n        counts = list(topics[topic].values())\n        topics[topic] = list(zip(words, counts))\n    \n    return topics"
    },
    {
        "function_name": "most_frequent_prefixes",
        "file_name": "string_processing.py",
        "parameters": {
            "`string`": "A string representing the input text",
            "`n`": "An integer representing the length of the prefix to be extracted",
            "`k`": "An integer representing the number of most frequent prefixes to return"
        },
        "objectives": [
            "Implement a Trie data structure to store the prefixes of the input string.",
            "Use a breadth-first search (BFS) algorithm to extract all prefixes of length n from the Trie.",
            "Return the k most frequent prefixes of length n."
        ],
        "import_lines": [
            "import collections"
        ],
        "function_def": "def most_frequent_prefixes(string, n, k):\n    # Create a Trie\n    trie = {}\n    for i in range(len(string)):\n        node = trie\n        for j in range(i, len(string)):\n            if string[j] not in node:\n                node[string[j]] = {}\n            node = node[string[j]]\n            if '#' not in node:\n                node['#'] = 0\n            node['#'] += 1\n    \n    # Extract all prefixes of length n\n    prefixes = []\n    queue = [(trie, \"\")]\n    while queue:\n        node, prefix = queue.pop(0)\n        if len(prefix) == n:\n            prefixes.append(prefix)\n        else:\n            for char, child_node in node.items():\n                if char != '#':\n                    queue.append((child_node, prefix + char))\n    \n    # Return the k most frequent prefixes\n    frequency = collections.Counter(prefixes)\n    return [prefix for prefix, _ in frequency.most_common(k)]"
    },
    {
        "function_name": "merge_stacks",
        "file_name": "stack_processor.py",
        "parameters": {
            "`n`": "An integer representing the number of stacks.",
            "`stacks`": "A list of lists representing the stacks.",
            "`k`": "An integer representing the number of stacks to merge."
        },
        "objectives": [
            "Merge the top k stacks into one stack.",
            "Use a priority queue to keep track of the stacks with the smallest top element.",
            "Return the merged stack as a list of integers."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def merge_stacks(n, stacks, k):\n    # Create a priority queue to keep track of the stacks with the smallest top element\n    queue = []\n    for i in range(n):\n        if stacks[i]:\n            heapq.heappush(queue, (stacks[i][-1], i))\n    \n    merged_stack = []\n    while queue and len(merged_stack) < k:\n        # Get the stack with the smallest top element\n        _, stack_index = heapq.heappop(queue)\n        merged_stack.extend(stacks[stack_index])\n        stacks[stack_index] = []\n        \n        # Add the next element from the stack to the priority queue\n        if stack_index < n and stacks[stack_index]:\n            heapq.heappush(queue, (stacks[stack_index][-1], stack_index))\n    \n    return merged_stack"
    },
    {
        "function_name": "community_detection",
        "file_name": "community_detection.py",
        "parameters": {
            "`graph`": "A dictionary representing the graph, where each key is a node and its value is a list of its neighbors.",
            "`seed_node`": "The node from which the community detection starts.",
            "`num_communities`": "The expected number of communities in the graph."
        },
        "objectives": [
            "Perform a random walk on the graph to detect communities.",
            "Use the PageRank algorithm to calculate the probability of each node belonging to each community.",
            "Return a dictionary where each key is a node and its value is the community it belongs to."
        ],
        "import_lines": [
            "import random",
            "from collections import defaultdict"
        ],
        "function_def": "def community_detection(graph, seed_node, num_communities):\n    communities = defaultdict(list)\n    node_probabilities = defaultdict(lambda: [0.0] * num_communities)\n    \n    # Perform a random walk on the graph to detect communities\n    for _ in range(1000):\n        current_node = seed_node\n        for _ in range(100):\n            neighbors = graph[current_node]\n            next_node = random.choice(neighbors)\n            node_probabilities[current_node][random.randint(0, num_communities - 1)] += 1\n            current_node = next_node\n    \n    # Use the PageRank algorithm to calculate the probability of each node belonging to each community\n    for _ in range(100):\n        for node in graph:\n            probabilities = node_probabilities[node]\n            for i in range(num_communities):\n                probability = 0\n                for neighbor in graph[node]:\n                    probability += node_probabilities[neighbor][i]\n                probabilities[i] = 0.15 + 0.85 * probability / len(graph[node])\n        for node in graph:\n            node_probabilities[node] = [probability / sum(node_probabilities[node]) for probability in node_probabilities[node]]\n    \n    # Assign each node to a community based on its probabilities\n    for node in graph:\n        community = node_probabilities[node].index(max(node_probabilities[node]))\n        communities[community].append(node)\n    \n    return communities"
    },
    {
        "function_name": "recommender_system",
        "file_name": "recommender_system.py",
        "parameters": {
            "`users`": "A list of integers representing the users.",
            "`items`": "A list of integers representing the items.",
            "`ratings`": "A dictionary where the keys are tuples of user and item and the values are the ratings.",
            "`num_recommends`": "The number of recommendations to generate for each user."
        },
        "objectives": [
            "Apply the Collaborative Filtering (CF) algorithm to generate recommendations.",
            "Use the matrix factorization approach to optimize the calculation.",
            "Return a dictionary where each key is a user and the value is a list of recommended items."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def recommender_system(users, items, ratings, num_recommends):\n    # Initialize the user and item matrices\n    user_matrix = np.random.rand(len(users), 10)\n    item_matrix = np.random.rand(len(items), 10)\n    \n    # Apply the CF algorithm to generate recommendations\n    for _ in range(1000):\n        for user in users:\n            for item in items:\n                if (user, item) in ratings:\n                    rating = ratings[(user, item)]\n                    error = rating - np.dot(user_matrix[user], item_matrix[item])\n                    user_matrix[user] += 0.1 * error * item_matrix[item]\n                    item_matrix[item] += 0.1 * error * user_matrix[user]\n    \n    # Generate the recommendations for each user\n    recommends = {}\n    for user in users:\n        scores = [np.dot(user_matrix[user], item_matrix[item]) for item in items]\n        recommends[user] = [items[i] for i in sorted(range(len(scores)), key=lambda x: scores[x], reverse=True)[:num_recommends]]\n    \n    return recommends"
    },
    {
        "function_name": "top_ranked_pages",
        "file_name": "pagerank.py",
        "parameters": {
            "`link_graph`": "A dictionary representing the link graph of a website, where each key is a page and its corresponding value is a list of pages it links to.",
            "`seed_page`": "A string representing the seed page to start the ranking from.",
            "`damping_factor`": "A float representing the damping factor for the PageRank algorithm."
        },
        "objectives": [
            "Implement the PageRank algorithm to calculate the ranking of each page in the link graph.",
            "Calculate the PageRank score for each page based on the number of links pointing to it and the damping factor.",
            "Normalize the PageRank scores to ensure they sum up to 1.",
            "Return a dictionary mapping each page to its PageRank score."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def top_ranked_pages(link_graph, seed_page, damping_factor):\n    pages = list(link_graph.keys())\n    num_pages = len(pages)\n    page_ranks = {page: 1 / num_pages for page in pages}\n    iteration = 0\n    \n    while iteration < 100:\n        new_page_ranks = {}\n        for page in pages:\n            rank = (1 - damping_factor) / num_pages\n            for other_page in pages:\n                if page in link_graph[other_page]:\n                    rank += damping_factor * page_ranks[other_page] / len(link_graph[other_page])\n            new_page_ranks[page] = rank\n        \n        page_ranks = new_page_ranks\n        iteration += 1\n    \n    page_ranks = {page: rank / sum(page_ranks.values()) for page, rank in page_ranks.items()}\n    \n    return page_ranks"
    },
    {
        "function_name": "clustering_coefficient",
        "file_name": "graph_theory.py",
        "parameters": {
            "`graph`": "A dictionary representing the adjacency list of a graph, where each key is a node and its corresponding value is a list of neighboring nodes.",
            "`node`": "A string representing the node to calculate the clustering coefficient for."
        },
        "objectives": [
            "Calculate the clustering coefficient of the given node in the graph.",
            "Count the number of triangles that include the given node.",
            "Calculate the maximum possible number of triangles that can include the given node based on its degree.",
            "Return the clustering coefficient as a float value between 0 and 1."
        ],
        "import_lines": [],
        "function_def": "def clustering_coefficient(graph, node):\n    neighbors = graph[node]\n    num_neighbors = len(neighbors)\n    triangles = 0\n    \n    for i in range(num_neighbors):\n        for j in range(i + 1, num_neighbors):\n            if neighbors[j] in graph[neighbors[i]]:\n                triangles += 1\n    \n    max_triangles = num_neighbors * (num_neighbors - 1) / 2\n    clustering_coeff = triangles / max_triangles\n    \n    return clustering_coeff"
    },
    {
        "function_name": "smooth_trajectory",
        "file_name": "signal_processing.py",
        "parameters": {
            "`trajectory_points`": "A list of tuples representing the points in the trajectory, where each tuple contains the x and y coordinates.",
            "`smoothing_factor`": "A float representing the smoothing factor for the trajectory."
        },
        "objectives": [
            "Apply a moving average filter to smooth the trajectory points.",
            "Calculate the smoothed x and y coordinates for each point in the trajectory.",
            "Return a list of tuples representing the smoothed trajectory points."
        ],
        "import_lines": [],
        "function_def": "def smooth_trajectory(trajectory_points, smoothing_factor):\n    num_points = len(trajectory_points)\n    smoothed_points = []\n    \n    for i in range(num_points):\n        window_size = int(smoothing_factor * num_points)\n        window_start = max(0, i - window_size // 2)\n        window_end = min(num_points, i + window_size // 2 + 1)\n        window_points = trajectory_points[window_start:window_end]\n        smoothed_x = sum(x for x, y in window_points) / len(window_points)\n        smoothed_y = sum(y for x, y in window_points) / len(window_points)\n        smoothed_points.append((smoothed_x, smoothed_y))\n    \n    return smoothed_points"
    },
    {
        "function_name": "information_diffusion",
        "file_name": "epidemiology.py",
        "parameters": {
            "`network`": "A dictionary representing the social network",
            "`seed_nodes`": "A list of nodes representing the seed nodes",
            "` diffusion_rate`": "A float representing the diffusion rate of the information",
            "`threshold`": "A float representing the threshold for adopting the information"
        },
        "objectives": [
            "Implement the linear threshold model to simulate the diffusion of information in a social network",
            "Use a threshold function to determine whether a node adopts the information based on its neighbors",
            "Return the final set of nodes that adopted the information"
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def information_diffusion(network, seed_nodes, diffusion_rate, threshold):\n    adopted = set(seed_nodes)\n    for node in network:\n        if node not in adopted and sum([1 for neighbor in network[node] if neighbor in adopted]) / len(network[node]) > threshold:\n            adopted.add(node)\n    \n    for _ in range(1000):\n        for node in network:\n            if node not in adopted and random.random() < diffusion_rate * sum([1 for neighbor in network[node] if neighbor in adopted]) / len(network[node]):\n                adopted.add(node)\n    \n    return adopted"
    },
    {
        "function_name": "paragraph_clusterer",
        "file_name": "nlp_operations.py",
        "parameters": {
            "`paragraphs`": "A list of strings representing the paragraphs.",
            "`headings`": "A list of strings representing the headings.",
            "`threshold`": "A float representing the minimum similarity score."
        },
        "objectives": [
            "Create a graph where each paragraph is a node and the edges represent the similarity between the paragraphs.",
            "Use a clustering algorithm to group the paragraphs into clusters based on their similarity.",
            "Assign a heading to each cluster based on the maximum similarity score between the cluster paragraphs and the headings.",
            "Return a dictionary where the keys are the headings and the values are the corresponding paragraphs."
        ],
        "import_lines": [
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.metrics.pairwise import cosine_similarity",
            "from sklearn.cluster import KMeans"
        ],
        "function_def": "def paragraph_clusterer(paragraphs, headings, threshold):\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform(paragraphs)\n    \n    # Calculate the similarity matrix\n    similarity_matrix = cosine_similarity(tfidf, tfidf)\n    \n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=len(headings))\n    clusters = kmeans.fit_predict(similarity_matrix)\n    \n    # Assign a heading to each cluster\n    cluster_headings = {}\n    for cluster in range(len(headings)):\n        cluster_paragraphs = [paragraphs[i] for i in range(len(paragraphs)) if clusters[i] == cluster]\n        heading_similarities = []\n        for heading in headings:\n            heading_tfidf = vectorizer.transform([heading])\n            heading_similarity = cosine_similarity(tfidf, heading_tfidf)\n            heading_similarities.append((heading, max(heading_similarity)))\n        cluster_headings[heading_similarities[0][0]] = cluster_paragraphs\n    \n    return cluster_headings"
    },
    {
        "function_name": "statistical_process_control",
        "file_name": "signal_processing.py",
        "parameters": {
            "`signal`": "A list of time-series data representing a signal",
            "`threshold`": "A threshold value to determine if the signal is active or not",
            "`window_size`": "The size of the sliding window to use for the calculation"
        },
        "objectives": [
            "Implement a statistical process control algorithm to determine if the signal is in control or not.",
            "Use a sliding window approach to calculate the moving average and standard deviation of the signal.",
            "Calculate the upper and lower control limits.",
            "Return a list indicating whether the signal is in control or not at each point in time."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def statistical_process_control(signal, threshold, window_size):\n    in_control = []\n    moving_averages = []\n    moving_stddevs = []\n    \n    for i in range(len(signal)):\n        window = signal[max(0, i - window_size + 1):i + 1]\n        if len(window) >= window_size:\n            moving_average = np.mean(window)\n            moving_stddev = np.std(window)\n            moving_averages.append(moving_average)\n            moving_stddevs.append(moving_stddev)\n            upper_control_limit = moving_average + 3 * moving_stddev\n            lower_control_limit = moving_average - 3 * moving_stddev\n            if signal[i] > upper_control_limit or signal[i] < lower_control_limit:\n                in_control.append(False)\n            else:\n                in_control.append(True)\n        else:\n            in_control.append(None)\n    \n    return in_control"
    },
    {
        "function_name": "find_meeting_time",
        "file_name": "scheduling.py",
        "parameters": {
            "`schedule`": "A list of tuples representing a schedule of tasks, where each tuple contains a start time, end time, and a boolean indicating whether the task is a break or not",
            "`meeting_duration`": "An integer representing the maximum duration of a meeting"
        },
        "objectives": [
            "Find the first available time slot in the schedule that can accommodate a meeting of the given duration",
            "Ensure that the time slot does not overlap with any breaks",
            "If multiple time slots are available, choose the one that is closest to the start of the schedule",
            "Return the start time of the available time slot"
        ],
        "import_lines": [],
        "function_def": "def find_meeting_time(schedule, meeting_duration):\n    schedule.sort(key=lambda x: x[0])\n    for i in range(len(schedule)):\n        if not schedule[i][2]:  # Check if the task is not a break\n            start_time = schedule[i][0]\n            end_time = schedule[i][1]\n            if i == 0 or start_time - schedule[i-1][1] >= meeting_duration:\n                return schedule[i-1][1] if i != 0 else 0\n    return -1  # Return -1 if no available time slot is found"
    },
    {
        "function_name": "find_keywords",
        "file_name": "text_processing.py",
        "parameters": {
            "`text`": "A string of text",
            "`keywords`": "A list of keywords to search for",
            "`max_distance`": "An integer representing the maximum distance between keywords"
        },
        "objectives": [
            "Implement a function that finds all occurrences of the keywords in the text",
            "Ensure that the distance between any two keywords is less than or equal to the maximum distance",
            "Return the occurrences as a list of tuples containing the keyword, start index, and end index"
        ],
        "import_lines": [],
        "function_def": "def find_keywords(text, keywords, max_distance):\n    occurrences = []\n    for keyword in keywords:\n        start = 0\n        while start < len(text):\n            index = text.find(keyword, start)\n            if index != -1:\n                occurrences.append((keyword, index, index+len(keyword)))\n                start = index + len(keyword) + max_distance\n            else:\n                break\n    return occurrences"
    },
    {
        "function_name": "customer_segmentation",
        "file_name": "marketing.py",
        "parameters": {
            "`data`": "A list of dictionaries, where each dictionary represents a customer with 'age', 'income', and 'purchase_history' keys.",
            "`num_centers`": "An integer representing the number of centers."
        },
        "objectives": [
            "Use the k-means algorithm to cluster the customers into segments.",
            "Calculate the mean and standard deviation of each segment.",
            "Return the assignment of customers to segments."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def customer_segmentation(data, num_centers):\n    # Convert data to numerical arrays\n    ages = np.array([customer['age'] for customer in data])\n    incomes = np.array([customer['income'] for customer in data])\n    purchase_histories = np.array([customer['purchase_history'] for customer in data])\n    \n    # Initialize k-means\n    centers = np.random.rand(num_centers, 3)\n    assignments = np.zeros(len(data))\n    \n    while True:\n        # Assign customers to centers\n        for i, customer in enumerate(data):\n            distances = np.linalg.norm(centers - np.array([ages[i], incomes[i], purchase_histories[i]]), axis=1)\n            assignments[i] = np.argmin(distances)\n        \n        # Update centers\n        new_centers = np.copy(centers)\n        for k in range(num_centers):\n            assigned_customers = np.where(assignments == k)[0]\n            if len(assigned_customers) > 0:\n                new_centers[k] = np.mean(np.array([ages[assigned_customers], incomes[assigned_customers], purchase_histories[assigned_customers]]), axis=1)\n        \n        # Check convergence\n        if np.all(centers == new_centers):\n            break\n        \n        centers = new_centers\n    \n    segments = {}\n    for k in range(num_centers):\n        segments[k] = []\n    \n    for i, assignment in enumerate(assignments):\n        segments[assignment].append(data[i])\n    \n    return segments"
    },
    {
        "function_name": "similar_strings",
        "file_name": "string_similarity.py",
        "parameters": {
            "`strings`": "A list of strings",
            "`threshold`": "An integer"
        },
        "objectives": [
            "Use the Levenshtein distance algorithm to find the strings that are within a certain threshold of edits from each other.",
            "Return the list of strings that are within the threshold of edits."
        ],
        "import_lines": [],
        "function_def": "def levenshtein_distance(s1, s2):\n    m, n = len(s1), len(s2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n    return dp[m][n]"
    },
    {
        "function_name": "template_matching",
        "file_name": "image_processing.py",
        "parameters": {
            "`images`": "A list of 2D lists representing the input images",
            "`template`": "A 2D list representing the template image"
        },
        "objectives": [
            "Use the template matching algorithm to find the location of the template image within each input image.",
            "Calculate the similarity score between the template and each subregion of the input images using the normalized cross-correlation metric.",
            "Return the location of the template image within each input image."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def template_matching(images, template):\n    template_height, template_width = len(template), len(template[0])\n    locations = []\n    \n    for image in images:\n        image_height, image_width = len(image), len(image[0])\n        correlation_scores = np.zeros((image_height - template_height + 1, image_width - template_width + 1))\n        \n        for i in range(correlation_scores.shape[0]):\n            for j in range(correlation_scores.shape[1]):\n                subregion = image[i:i + template_height, j:j + template_width]\n                correlation_score = np.mean((subregion - np.mean(subregion)) * (template - np.mean(template)))\n                correlation_scores[i, j] = correlation_score / (np.std(subregion) * np.std(template))\n        \n        max_score = np.max(correlation_scores)\n        max_score_indices = np.argmax(correlation_scores)\n        x, y = max_score_indices // correlation_scores.shape[1], max_score_indices % correlation_scores.shape[1]\n        locations.append((x, y))\n    \n    return locations"
    },
    {
        "function_name": "grade_calculator",
        "file_name": "student_grades.py",
        "parameters": {
            "`grades`": "A list of lists of student grades (floats between 0 and 100)",
            "`weightages`": "A list of weightages for each assignment (floats between 0 and 1)"
        },
        "objectives": [
            "Calculate the weighted average grade for each student.",
            "Calculate the grade point average (GPA) for each student.",
            "Return the weighted average grades and GPAs."
        ],
        "import_lines": [],
        "function_def": "def grade_calculator(grades, weightages):\n    weighted_average_grades = []\n    gpas = []\n    \n    for student_grades in grades:\n        weighted_sum = 0\n        for i in range(len(student_grades)):\n            weighted_sum += student_grades[i] * weightages[i]\n        weighted_average_grades.append(weighted_sum / sum(weightages))\n        \n        # Calculate the GPA\n        grade_points = 0\n        for grade in student_grades:\n            if grade >= 90:\n                grade_points += 4\n            elif grade >= 80:\n                grade_points += 3\n            elif grade >= 70:\n                grade_points += 2\n            elif grade >= 60:\n                grade_points += 1\n        gpa = grade_points / len(student_grades)\n        gpas.append(gpa)\n    \n    return weighted_average_grades, gpas"
    },
    {
        "function_name": "traffic_analysis",
        "file_name": "web_traffic.py",
        "parameters": {
            "`website_data`": "A list of lists containing website traffic data (integers representing the number of visitors per hour)",
            "`threshold`": "An integer representing the threshold for traffic"
        },
        "objectives": [
            "Identify the peak traffic hours for each website.",
            "Calculate the maximum traffic for each website.",
            "Return the peak traffic hours and maximum traffic for each website."
        ],
        "import_lines": [],
        "function_def": "def traffic_analysis(website_data, threshold):\n    peak_traffic_hours = []\n    maximum_traffic = []\n    \n    for data in website_data:\n        max_traffic_hour = None\n        max_traffic = float('-inf')\n        \n        # Identify the peak traffic hour and maximum traffic\n        for i in range(len(data)):\n            if data[i] > max_traffic and data[i] >= threshold:\n                max_traffic = data[i]\n                max_traffic_hour = i\n        \n        peak_traffic_hours.append(max_traffic_hour)\n        maximum_traffic.append(max_traffic)\n    \n    return peak_traffic_hours, maximum_traffic"
    },
    {
        "function_name": "chemical_analysis",
        "file_name": "chemistry.py",
        "parameters": {
            "`chemical_formulas`": "A list of chemical formulas (strings consisting of elements and their counts)",
            "`elements`": "A list of elements to find in the formulas"
        },
        "objectives": [
            "Parse each chemical formula to extract the elements and their counts.",
            "Calculate the total count of each element across all formulas.",
            "Return the total count of each element."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def chemical_analysis(chemical_formulas, elements):\n    element_counts = {}\n    \n    # Parse each chemical formula to extract the elements and their counts\n    for formula in chemical_formulas:\n        matches = re.findall(r'([A-Z][a-z]*)(\\d*)', formula)\n        for match in matches:\n            element = match[0]\n            count = int(match[1]) if match[1] else 1\n            \n            # Update the element count\n            if element in element_counts:\n                element_counts[element] += count\n            else:\n                element_counts[element] = count\n    \n    # Calculate the total count of each element across all formulas\n    total_counts = []\n    for element in elements:\n        if element in element_counts:\n            total_counts.append(element_counts[element])\n        else:\n            total_counts.append(0)\n    \n    return total_counts"
    },
    {
        "function_name": "solve_linear_system",
        "file_name": "linear_algebra.py",
        "parameters": {
            "`equations`": "A list of strings representing a system of linear equations.",
            "`variables`": "A list of strings representing the variables in the system."
        },
        "objectives": [
            "Parse the equations and variables to construct an augmented matrix.",
            "Perform Gaussian elimination to solve the system of equations.",
            "Return the solution as a dictionary where each key is a variable and its corresponding value is the solution."
        ],
        "import_lines": [
            "import re",
            "from sympy import Matrix"
        ],
        "function_def": "def solve_linear_system(equations, variables):\n    matrix = []\n    \n    for equation in equations:\n        row = [0] * (len(variables) + 1)\n        \n        for i, variable in enumerate(variables):\n            coefficient = re.search(r'([-+]?\\d*)\\*?' + variable, equation)\n            if coefficient:\n                row[i] = int(coefficient.group(1) or 1)\n        \n        constant = re.search(r'=\\s*(-?\\d+)', equation)\n        if constant:\n            row[-1] = int(constant.group(1))\n        \n        matrix.append(row)\n    \n    matrix = Matrix(matrix)\n    reduced_matrix = matrix.rref()[0]\n    solutions = {}\n    \n    for i, variable in enumerate(variables):\n        solutions[variable] = reduced_matrix.col(-1)[i]\n    \n    return solutions"
    },
    {
        "function_name": "gaussian_elimination",
        "file_name": "matrix_transformations.py",
        "parameters": {
            "matrix": "A 2D list representing a matrix.",
            "target_row": "An integer representing the row to transform.",
            "target_col": "An integer representing the column to transform."
        },
        "objectives": [
            "Implement the Gaussian elimination algorithm to transform the matrix into row echelon form.",
            "Use the target row and column to determine the pivot element.",
            "Return the transformed matrix."
        ],
        "import_lines": [],
        "function_def": "def gaussian_elimination(matrix, target_row, target_col):\n    n = len(matrix)\n    for i in range(target_row, n):\n        if matrix[i][target_col] != 0:\n            matrix[i], matrix[target_row] = matrix[target_row], matrix[i]\n            break\n    pivot = matrix[target_row][target_col]\n    matrix[target_row] = [element / pivot for element in matrix[target_row]]\n    for i in range(n):\n        if i != target_row:\n            factor = matrix[i][target_col]\n            matrix[i] = [matrix[i][j] - factor * matrix[target_row][j] for j in range(len(matrix[i]))]\n    return matrix"
    },
    {
        "function_name": "bucket_sort",
        "file_name": "distribution_sort.py",
        "parameters": {
            "numbers": "A list of integers representing the numbers to sort.",
            "bucket_size": "An integer representing the size of each bucket."
        },
        "objectives": [
            "Implement the bucket sort algorithm to sort the input numbers.",
            "Use a list of lists to represent the buckets, where each sublist contains numbers that fall within a certain range.",
            "Return the sorted list of numbers."
        ],
        "import_lines": [],
        "function_def": "def bucket_sort(numbers, bucket_size):\n    min_value = min(numbers)\n    max_value = max(numbers)\n    bucket_count = (max_value - min_value) // bucket_size + 1\n    buckets = [[] for _ in range(bucket_count)]\n    \n    for num in numbers:\n        index = (num - min_value) // bucket_size\n        buckets[index].append(num)\n    \n    for bucket in buckets:\n        bucket.sort()\n    \n    sorted_numbers = []\n    for bucket in buckets:\n        sorted_numbers.extend(bucket)\n    \n    return sorted_numbers"
    },
    {
        "function_name": "connected_component_labeling",
        "file_name": "image_algorithms.py",
        "parameters": {
            "`image`": "A 2D list of integers representing the image",
            "`threshold`": "An integer representing the threshold value"
        },
        "objectives": [
            "Implement the Connected Component Labeling algorithm to label all connected components",
            "Use a dictionary to keep track of the labels",
            "Return a new image where each pixel is labeled with its corresponding component label"
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def connected_component_labeling(image, threshold):\n    labels = [[0 for _ in range(len(image[0]))] for _ in range(len(image))]\n    component_labels = {}\n    component = 1\n    \n    directions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n    \n    for i in range(len(image)):\n        for j in range(len(image[0])):\n            if image[i][j] > threshold and labels[i][j] == 0:\n                queue = deque([(i, j)])\n                labels[i][j] = component\n                \n                while queue:\n                    x, y = queue.popleft()\n                    \n                    for dx, dy in directions:\n                        nx, ny = x + dx, y + dy\n                        if 0 <= nx < len(image) and 0 <= ny < len(image[0]) and image[nx][ny] > threshold and labels[nx][ny] == 0:\n                            labels[nx][ny] = component\n                            queue.append((nx, ny))\n                \n                component += 1\n    \n    return labels"
    },
    {
        "function_name": "apply_mask",
        "file_name": "matrix_multiply.py",
        "parameters": {
            "`arr`": "A 2D list representing the matrix",
            "`mask`": "A 2D list representing the mask"
        },
        "objectives": [
            "Apply the mask to the matrix using matrix multiplication",
            "Perform element-wise multiplication for each row of the matrix",
            "Return the resulting matrix"
        ],
        "import_lines": [],
        "function_def": "def apply_mask(arr, mask):\n    result = []\n    for row in arr:\n        new_row = []\n        for i in range(len(mask[0])):\n            sum = 0\n            for j in range(len(mask)):\n                sum += row[j] * mask[j][i]\n            new_row.append(sum)\n        result.append(new_row)\n    return result"
    },
    {
        "function_name": "market_basket_analysis",
        "file_name": "market_analysis.py",
        "parameters": {
            "`customers`": "A list of (customer_id, order_id, date) tuples representing the customer orders",
            "`products`": "A list of (product_id, category) tuples representing the product categories",
            "`threshold`": "An integer representing the minimum number of orders required for a product to be considered popular"
        },
        "objectives": [
            "Perform a market basket analysis task using the Apriori algorithm.",
            "Use a dictionary to efficiently store the product categories and their corresponding order frequencies.",
            "Identify the popular products that meet the threshold and return their categories."
        ],
        "import_lines": [
            "from collections import defaultdict",
            "import itertools"
        ],
        "function_def": "def market_basket_analysis(customers, products, threshold):\n    product_orders = defaultdict(int)\n    for customer_id, order_id, date in customers:\n        for product_id, category in products:\n            if order_id == product_id:\n                product_orders[product_id] += 1\n    \n    popular_products = {product_id: category for product_id, category in products if product_orders[product_id] >= threshold}\n    \n    return popular_products"
    },
    {
        "function_name": "most_diverse_molecule",
        "file_name": "molecular_analysis.py",
        "parameters": {
            "`molecules`": "A list of strings representing molecular formulas",
            "`atoms`": "A list of strings representing atom types"
        },
        "objectives": [
            "Parse each molecular formula to count the occurrences of each atom type.",
            "For each molecule, calculate the diversity of its atoms.",
            "Return the molecule with the highest diversity."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def most_diverse_molecule(molecules, atoms):\n    max_diversity = float('-inf')\n    best_molecule = None\n    for molecule in molecules:\n        atom_counts = Counter(re.findall('|'.join(atoms), molecule))\n        diversity = len(atom_counts)\n        if diversity > max_diversity:\n            max_diversity = diversity\n            best_molecule = molecule\n    return best_molecule"
    },
    {
        "function_name": "web_crawler",
        "file_name": "web_crawler.py",
        "parameters": {
            "`web_graph`": "A dictionary representing a web graph, where each key is a webpage URL and each value is a list of webpage URLs that the page links to.",
            "`seed_urls`": "A list of webpage URLs representing the initial seed pages for the search engine.",
            "`max_depth`": "An integer representing the maximum depth of the search."
        },
        "objectives": [
            "Implement a web crawler to traverse the web graph and collect URLs up to a certain depth.",
            "Use a breadth-first search algorithm to traverse the graph efficiently.",
            "Return a set of unique URLs that were reached during the search."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def web_crawler(web_graph, seed_urls, max_depth):\n    visited = set()\n    queue = deque([(url, 0) for url in seed_urls])\n    \n    while queue:\n        url, depth = queue.popleft()\n        if url not in visited and depth <= max_depth:\n            visited.add(url)\n            for neighbor in web_graph.get(url, []):\n                queue.append((neighbor, depth + 1))\n    \n    return visited"
    },
    {
        "function_name": "gaussian_blur",
        "file_name": "gaussian_blur.py",
        "parameters": {
            "`image_data`": "A 2D list of integers representing image pixel values.",
            "`kernel_size`": "A tuple representing the size of the kernel, where each element is an integer.",
            "`sigma`": "A float representing the standard deviation of the Gaussian distribution."
        },
        "objectives": [
            "Implement a Gaussian blur filter to smooth the input image.",
            "Use a convolutional approach to efficiently apply the filter.",
            "Return the filtered image."
        ],
        "import_lines": [
            "from scipy.ndimage import gaussian_filter"
        ],
        "function_def": "def gaussian_blur(image_data, kernel_size, sigma):\n    kernel = [[(1 / (2 * 3.14159 * sigma ** 2)) * (2.71828 ** ((i - kernel_size[0] // 2) ** 2 / (2 * sigma ** 2)))\n               * (2.71828 ** ((j - kernel_size[1] // 2) ** 2 / (2 * sigma ** 2))) for j in range(kernel_size[1])] for i in range(kernel_size[0])]\n    return gaussian_filter(image_data, sigma)"
    },
    {
        "function_name": "regex_matcher",
        "file_name": "string_algorithms.py",
        "parameters": {
            "`regex`": "A regular expression string.",
            "`strings`": "A list of strings."
        },
        "objectives": [
            "Implement a recursive descent parser for regular expressions.",
            "Match each string against the regular expression and identify the matches.",
            "Return the list of matched strings."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def regex_matcher(regex, strings):\n    pattern = re.compile(regex)\n    matches = []\n    \n    for string in strings:\n        match = pattern.fullmatch(string)\n        if match:\n            matches.append(string)\n    \n    return matches"
    },
    {
        "function_name": "matrix_multiplier",
        "file_name": "numeric_algorithms.py",
        "parameters": {
            "`A`": "A 2D matrix.",
            "`B`": "A 2D matrix."
        },
        "objectives": [
            "Implement Strassen's algorithm to efficiently multiply two matrices.",
            "Divide the matrices into sub-matrices for efficient multiplication.",
            "Return the product of the two matrices."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def matrix_multiplier(A, B):\n    num_rows_A, num_cols_A = A.shape\n    num_rows_B, num_cols_B = B.shape\n    \n    if num_cols_A != num_rows_B:\n        raise ValueError(\"Matrix multiplication is not possible\")\n    \n    if num_rows_A == 1:\n        return A * B\n    \n    mid_A = num_rows_A // 2\n    mid_B = num_cols_B // 2\n    \n    A11, A12, A21, A22 = A[:mid_A, :mid_B], A[:mid_A, mid_B:], A[mid_A:, :mid_B], A[mid_A:, mid_B:]\n    B11, B12, B21, B22 = B[:mid_A, :mid_B], B[:mid_A, mid_B:], B[mid_A:, :mid_B], B[mid_A:, mid_B:]\n    \n    M1 = matrix_multiplier(A11 + A22, B11 + B22)\n    M2 = matrix_multiplier(A21 + A22, B11)\n    M3 = matrix_multiplier(A11, B12 - B22)\n    M4 = matrix_multiplier(A22, B21 - B11)\n    M5 = matrix_multiplier(A11 + A12, B22)\n    M6 = matrix_multiplier(A21 - A11, B11 + B12)\n    M7 = matrix_multiplier(A12 - A22, B21 + B22)\n    \n    C11 = M1 + M4 - M5 + M7\n    C12 = M3 + M5\n    C21 = M2 + M4\n    C22 = M1 - M2 + M3 + M6\n    \n    return np.vstack((np.hstack((C11, C12)), np.hstack((C21, C22))))"
    },
    {
        "function_name": "named_entity_recognition",
        "file_name": "named_entity_recognition.py",
        "parameters": {
            "`sentence`": "A string representing a sentence.",
            "`entities`": "A list of strings representing the entities to extract."
        },
        "objectives": [
            "Apply the Named Entity Recognition (NER) algorithm to extract entities from the sentence.",
            "Use a dictionary-based approach to identify entities in the sentence.",
            "Return the extracted entities."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def named_entity_recognition(sentence, entities):\n    entity_dict = {entity.lower(): entity for entity in entities}\n    sentence_lower = sentence.lower()\n    \n    extracted_entities = []\n    \n    for entity, full_entity in entity_dict.items():\n        if entity in sentence_lower:\n            extracted_entities.append(full_entity)\n    \n    return extracted_entities"
    },
    {
        "function_name": "kmer_frequencies",
        "file_name": "genomics.py",
        "parameters": {
            "`sequences`": "A list of strings representing the sequences",
            "`k`": "An integer representing the length of the k-mer",
            "`threshold`": "An integer representing the minimum frequency of the k-mer"
        },
        "objectives": [
            "Use the K-mer algorithm to find the most frequent k-mers in the `sequences`.",
            "Calculate the frequency of each k-mer in the `sequences`.",
            "Return the top k-mers with a frequency greater than or equal to `threshold`."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def kmer_frequencies(sequences, k, threshold):\n    kmer_freq = defaultdict(int)\n    for sequence in sequences:\n        for i in range(len(sequence) - k + 1):\n            kmer = sequence[i:i+k]\n            kmer_freq[kmer] += 1\n    top_kmers = sorted(kmer_freq.items(), key=lambda x: x[1], reverse=True)\n    top_kmers = [kmer for kmer, freq in top_kmers if freq >= threshold]\n    return top_kmers"
    },
    {
        "function_name": "page_rank",
        "file_name": "network_analysis.py",
        "parameters": {
            "`pages`": "A list of strings representing the web pages",
            "`links`": "A dictionary representing the links between the web pages"
        },
        "objectives": [
            "Implement a page ranking algorithm using Markov chains",
            "Use the power iteration method to compute the page ranks",
            "Return a dictionary representing the page ranks"
        ],
        "import_lines": [],
        "function_def": "def page_rank(pages, links):\n    # Compute the transition matrix\n    transition_matrix = {}\n    for page in pages:\n        transition_matrix[page] = {}\n        for link in links[page]:\n            transition_matrix[page][link] = 1 / len(links[page])\n    \n    # Compute the page ranks using the power iteration method\n    page_ranks = {page: 1 / len(pages) for page in pages}\n    for _ in range(100):\n        new_page_ranks = {}\n        for page in pages:\n            new_page_ranks[page] = 0\n            for link in links[page]:\n                new_page_ranks[page] += page_ranks[link] * transition_matrix[link][page]\n        page_ranks = new_page_ranks\n    \n    return page_ranks"
    },
    {
        "function_name": "latent_dirichlet_allocation",
        "file_name": "topic_modeling_algorithms.py",
        "parameters": {
            "`corpus`": "A list of strings representing the documents in the corpus",
            "`num_topics`": "An integer representing the number of topics to extract",
            "`num_iterations`": "An integer representing the number of iterations for the algorithm"
        },
        "objectives": [
            "Implement the Latent Dirichlet Allocation (LDA) algorithm to extract topics from the given corpus.",
            "Use a generative model to represent the corpus and infer the topic assignments.",
            "Return the topic assignments and the word distributions for each topic."
        ],
        "import_lines": [
            "import numpy as np",
            "import random"
        ],
        "function_def": "def latent_dirichlet_allocation(corpus, num_topics, num_iterations):\n    word_counts = {}\n    for document in corpus:\n        for word in document.split():\n            if word not in word_counts:\n                word_counts[word] = 0\n            word_counts[word] += 1\n    word_list = list(word_counts.keys())\n    topic_assignments = []\n    word_distributions = []\n    for _ in range(num_topics):\n        word_distribution = [random.random() for _ in range(len(word_list))]\n        word_distributions.append(word_distribution)\n    for _ in range(num_iterations):\n        for document in corpus:\n            words = document.split()\n            for word in words:\n                topic = random.randint(0, num_topics - 1)\n                topic_assignments.append(topic)\n                word_distributions[topic][word_list.index(word)] += 1\n        for topic in range(num_topics):\n            topic_word_count = sum(word_distributions[topic])\n            for word in range(len(word_list)):\n                word_distributions[topic][word] /= topic_word_count\n    return topic_assignments, word_distributions"
    },
    {
        "function_name": "kmers_frequency",
        "file_name": "bioinformatics_algorithms.py",
        "parameters": {
            "`sequences`": "A list of strings representing the DNA sequences",
            "`k`": "An integer representing the length of the k-mer"
        },
        "objectives": [
            "Find all unique k-mers in the DNA sequences.",
            "Calculate the frequency of each k-mer.",
            "Identify the most frequent k-mer."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def kmers_frequency(sequences, k):\n    kmers = set()\n    frequency = defaultdict(int)\n    for sequence in sequences:\n        for i in range(len(sequence) - k + 1):\n            kmer = sequence[i:i+k]\n            kmers.add(kmer)\n            frequency[kmer] += 1\n    \n    max_frequency = max(frequency.values())\n    most_frequent_kmer = [kmer for kmer, freq in frequency.items() if freq == max_frequency]\n    \n    return kmers, frequency, most_frequent_kmer"
    },
    {
        "function_name": "convolve_image",
        "file_name": "cnn.py",
        "parameters": {
            "`image`": "A 2D list representing the image, where each pixel is a tuple of RGB values.",
            "`kernel`": "A 2D list representing the kernel, where each element is a weight.",
            "`stride`": "The stride of the convolution."
        },
        "objectives": [
            "Implement a convolutional neural network (CNN) layer to apply the kernel to the image.",
            "Use a sliding window approach to apply the kernel to each region of the image.",
            "Return the feature map obtained by convolving the image with the kernel."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def convolve_image(image, kernel, stride):\n    image_height, image_width = len(image), len(image[0])\n    kernel_height, kernel_width = len(kernel), len(kernel[0])\n    feature_map = np.zeros((image_height - kernel_height + 1, image_width - kernel_width + 1))\n    for i in range(0, image_height - kernel_height + 1, stride):\n        for j in range(0, image_width - kernel_width + 1, stride):\n            region = [row[j:j + kernel_width] for row in image[i:i + kernel_height]]\n            feature_map[i // stride, j // stride] = np.sum([np.sum(region[k] * kernel[k]) for k in range(kernel_height)])\n    return feature_map"
    },
    {
        "function_name": "are_anagrams",
        "file_name": "string_algorithms.py",
        "parameters": {
            "`str1`": "A string representing the first string.",
            "`str2`": "A string representing the second string."
        },
        "objectives": [
            "Implement a function that checks if two strings are anagrams of each other.",
            "Use a sorting approach to sort the characters in both strings and compare them.",
            "Return True if the strings are anagrams, False otherwise."
        ],
        "import_lines": [],
        "function_def": "def are_anagrams(str1, str2):\n    return sorted(str1) == sorted(str2)"
    },
    {
        "function_name": "activity_selection",
        "file_name": "greedy_algorithms.py",
        "parameters": {
            "`activities`": "A list of tuples representing activities with start and end times ((start, end)).",
            "`n`": "An integer representing the number of resources available."
        },
        "objectives": [
            "Activity Selection Problem: select the maximum number of non-overlapping activities that can be performed.",
            "Use a greedy approach to select the activities.",
            "Return the list of selected activities."
        ],
        "import_lines": [],
        "function_def": "def activity_selection(activities, n):\n    activities.sort(key=lambda x: x[1])\n    selected_activities = [activities[0]]\n    \n    for i in range(1, len(activities)):\n        if activities[i][0] >= selected_activities[-1][1] and len(selected_activities) < n:\n            selected_activities.append(activities[i])\n    \n    return selected_activities"
    },
    {
        "function_name": "query_retrieval",
        "file_name": "vector_search.py",
        "parameters": {
            "`num_queries`": "An integer representing the number of queries",
            "`num_documents`": "An integer representing the number of documents",
            "`query_similarity_threshold`": "A float representing the similarity threshold for queries",
            "`document_vectors`": "A 2D list of floats representing the document vectors",
            "`query_vectors`": "A 2D list of floats representing the query vectors"
        },
        "objectives": [
            "Calculate the cosine similarity between each query vector and document vector",
            "Identify the top k documents for each query based on the similarity score",
            "Filter out documents with a similarity score below the threshold",
            "Return the filtered documents for each query"
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy import spatial"
        ],
        "function_def": "def query_retrieval(num_queries, num_documents, query_similarity_threshold, document_vectors, query_vectors):\n    query_results = []\n    for query_vector in query_vectors:\n        similarities = []\n        for document_vector in document_vectors:\n            similarity = 1 - spatial.distance.cosine(query_vector, document_vector)\n            similarities.append((similarity, document_vector))\n        similarities.sort(reverse=True)\n        filtered_results = [document for similarity, document in similarities if similarity >= query_similarity_threshold]\n        query_results.append(filtered_results[:min(len(filtered_results), num_documents)])\n    return query_results"
    },
    {
        "function_name": "turing_machine",
        "file_name": "turing_machine.py",
        "parameters": {
            "`machine`": "A dictionary representing a Turing machine with its states, transitions, and tape.",
            "`input_string`": "A string representing the input to the Turing machine."
        },
        "objectives": [
            "Implement the Turing machine algorithm to process the input string.",
            "Use a state machine to keep track of the current state and transition to the next state based on the input symbol and the current state.",
            "Return the final output of the Turing machine."
        ],
        "import_lines": [],
        "function_def": "def turing_machine(machine, input_string):\n    current_state = machine['initial_state']\n    tape = ['_'] + list(input_string) + ['_']\n    head_position = 1\n    while True:\n        current_symbol = tape[head_position]\n        transition = machine['transitions'].get((current_state, current_symbol))\n        if transition is None:\n            break\n        next_state, next_symbol, direction = transition\n        tape[head_position] = next_symbol\n        current_state = next_state\n        if direction == 'R':\n            head_position += 1\n        elif direction == 'L':\n            head_position -= 1\n        elif direction == 'S':\n            break\n    return ''.join(tape[1:-1])"
    },
    {
        "function_name": "resource_allocation",
        "file_name": "resource_allocation.py",
        "parameters": {
            "`schedule`": "A list of tuples representing a schedule, where each tuple contains a task name and its duration.",
            "`resources`": "A list of tuples representing resources, where each tuple contains a resource name and its availability."
        },
        "objectives": [
            "Implement a greedy algorithm to assign tasks to resources based on their availability.",
            "Prioritize tasks with longer durations first.",
            "Return the assigned tasks for each resource."
        ],
        "import_lines": [],
        "function_def": "def resource_allocation(schedule, resources):\n    schedule.sort(key=lambda x: x[1], reverse=True)\n    resource_allocation = {}\n    \n    for task, duration in schedule:\n        best_resource = None\n        best_availability = 0\n        \n        for resource, availability in resources:\n            if availability >= duration and availability > best_availability:\n                best_resource = resource\n                best_availability = availability\n        \n        if best_resource is not None:\n            resources[resources.index((best_resource, best_availability))] = (best_resource, best_availability - duration)\n            resource_allocation.setdefault(best_resource, []).append(task)\n    \n    return resource_allocation"
    },
    {
        "function_name": "word_finder",
        "file_name": "text_processor.py",
        "parameters": {
            "`s`": "A string representing the text to be searched",
            "`words`": "A list of strings representing the words to search for"
        },
        "objectives": [
            "Find all occurrences of each word in the given text.",
            "For each word, return the indices of its occurrences."
        ],
        "import_lines": [],
        "function_def": "def word_finder(s, words):\n    word_indices = {word: [] for word in words}\n    for word in words:\n        index = s.find(word)\n        while index != -1:\n            word_indices[word].append(index)\n            index = s.find(word, index + 1)\n    return word_indices"
    },
    {
        "function_name": "suspicious_transaction_detector",
        "file_name": "banking_processor.py",
        "parameters": {
            "`transactions`": "A list of tuples representing the transactions in a banking system",
            "`threshold`": "An integer representing the minimum amount for a transaction to be considered suspicious",
            "`n`": "An integer representing the number of accounts"
        },
        "objectives": [
            "Identify suspicious transactions based on the threshold.",
            "For each account, calculate the total amount of suspicious transactions.",
            "Return the account with the maximum total amount of suspicious transactions."
        ],
        "import_lines": [],
        "function_def": "def suspicious_transaction_detector(transactions, threshold, n):\n    account_amounts = [0] * n\n    for sender, receiver, amount in transactions:\n        if amount > threshold:\n            account_amounts[sender] += amount\n            account_amounts[receiver] += amount\n    max_amount = max(account_amounts)\n    return account_amounts.index(max_amount), max_amount"
    },
    {
        "function_name": "estimate_distribution_expected_values",
        "file_name": "monte_carlo_simulation.py",
        "parameters": {
            "`distributions`": "List of probability distributions (e.g., Gaussian, Poisson)",
            "`samples`": "Integer representing the number of samples to draw from each distribution"
        },
        "objectives": [
            "Implement a Monte Carlo simulation to estimate the expected value of each distribution.",
            "Use the samples drawn from each distribution to estimate the expected value.",
            "Return a dictionary where each key is the distribution name and the value is the estimated expected value."
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy import stats"
        ],
        "function_def": "def estimate_distribution_expected_values(distributions, samples):\n    result = {}\n    \n    for distribution in distributions:\n        name = distribution['name']\n        params = distribution['params']\n        \n        if name == 'Gaussian':\n            dist = stats.norm(loc=params['loc'], scale=params['scale'])\n        elif name == 'Poisson':\n            dist = stats.poisson(mu=params['mu'])\n        \n        samples_drawn = dist.rvs(size=samples)\n        expected_value = np.mean(samples_drawn)\n        \n        result[name] = expected_value\n    \n    return result"
    },
    {
        "function_name": "anomaly_detection",
        "file_name": "time_series_analysis.py",
        "parameters": {
            "`sequence`": "A list of integers representing the input sequence",
            "`window_size`": "An integer representing the size of the sliding window",
            "`threshold`": "A float representing the threshold for determining anomalies"
        },
        "objectives": [
            "Implement a modified sliding window algorithm to detect anomalies in the input sequence.",
            "Use a statistical method (e.g., mean and standard deviation) to model the normal behavior of the sequence within the sliding window.",
            "Compute the distance between each data point and the modeled normal behavior.",
            "Mark data points with a distance greater than the `threshold` as anomalies.",
            "Return the detected anomalies and their corresponding indices."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def anomaly_detection(sequence, window_size, threshold):\n    anomalies = []\n    \n    for i in range(len(sequence)):\n        start_index = max(0, i - window_size)\n        end_index = i + 1\n        window = sequence[start_index:end_index]\n        mean = np.mean(window)\n        std_dev = np.std(window)\n        \n        if not window:\n            continue\n        \n        distance = abs(sequence[i] - mean)\n        if distance > threshold * std_dev:\n            anomalies.append((sequence[i], i))\n    \n    return anomalies"
    },
    {
        "function_name": "community_detection",
        "file_name": "community_detection.py",
        "parameters": {
            "`network`": "An adjacency matrix representing a social network",
            "`seed`": "A node representing the seed node for the community detection algorithm",
            "`epsilon`": "A float representing the resolution parameter for the community detection algorithm"
        },
        "objectives": [
            "Implement the Louvain community detection algorithm to find communities in the social network.",
            "Use a hierarchical approach to iteratively merge communities based on the modularity optimization criterion.",
            "Return a list of community assignments for each node."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def community_detection(network, seed, epsilon):\n    # Initialize the community assignments\n    communities = {node: node for node in range(len(network))}\n    \n    # Initialize the modularity matrix\n    modularity_matrix = np.copy(network)\n    \n    # Define the Louvain algorithm\n    while True:\n        # Initialize the list of communities to merge\n        communities_to_merge = []\n        \n        # Iterate over the communities\n        for community in set(communities.values()):\n            # Get the nodes in the community\n            nodes_in_community = [node for node, community_assignment in communities.items() if community_assignment == community]\n            \n            # Calculate the modularity gain for each possible merge\n            modularity_gains = {}\n            for other_community in set(communities.values()):\n                if community != other_community:\n                    modularity_gain = 0\n                    for node1 in nodes_in_community:\n                        for node2 in [node for node, community_assignment in communities.items() if community_assignment == other_community]:\n                            modularity_gain += modularity_matrix[node1, node2]\n                    modularity_gains[other_community] = modularity_gain\n            \n            # Select the community to merge with\n            if modularity_gains:\n                community_to_merge = max(modularity_gains, key=modularity_gains.get)\n                communities_to_merge.append((community, community_to_merge))\n        \n        # Merge the communities\n        for community1, community2 in communities_to_merge:\n            for node in [node for node, community_assignment in communities.items() if community_assignment == community1]:\n                communities[node] = community2\n        \n        # Update the modularity matrix\n        modularity_matrix = np.zeros((len(network), len(network)))\n        for community in set(communities.values()):\n            nodes_in_community = [node for node, community_assignment in communities.items() if community_assignment == community]\n            for node1 in nodes_in_community:\n                for node2 in nodes_in_community:\n                    modularity_matrix[node1, node2] = network[node1, node2]\n        \n        # Check for convergence\n        if not communities_to_merge:\n            break\n    \n    return list(communities.values())"
    },
    {
        "function_name": "monotonic_stack",
        "file_name": "stack_algorithms.py",
        "parameters": {
            "`sequence`": "A list of integers representing the sequence.",
            "`k`": "An integer representing the size of the sliding window."
        },
        "objectives": [
            "Implement the monotonic stack algorithm to find the maximum value in each sliding window of size k.",
            "Use a stack to keep track of the indices of the elements in the sliding window.",
            "Return a list of the maximum values in each sliding window."
        ],
        "import_lines": [],
        "function_def": "def monotonic_stack(sequence, k):\n    max_values = []\n    stack = []\n    \n    for i, num in enumerate(sequence):\n        while stack and sequence[stack[-1]] < num:\n            stack.pop()\n        stack.append(i)\n        \n        if i >= k and stack[0] <= i - k:\n            stack.pop(0)\n        \n        if i >= k - 1:\n            max_values.append(sequence[stack[0]])\n    \n    return max_values"
    },
    {
        "function_name": "gauss_jordan_elimination",
        "file_name": "linear_algebra_algorithms.py",
        "parameters": {
            "`A`": "A 2D list representing a matrix.",
            "`B`": "A 2D list representing another matrix."
        },
        "objectives": [
            "Implement the Gauss-Jordan elimination algorithm to transform the matrix A into reduced row echelon form (RREF).",
            "Use elementary row operations to transform the matrix.",
            "Return the RREF of matrix A."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def gauss_jordan_elimination(A):\n    n = A.shape[0]\n    m = A.shape[1]\n    \n    current_row = 0\n    for j in range(m):\n        if current_row >= n:\n            break\n        \n        pivot_row = current_row\n        while pivot_row < n and A[pivot_row, j] == 0:\n            pivot_row += 1\n        \n        if pivot_row == n:\n            continue\n        \n        A[[current_row, pivot_row]] = A[[pivot_row, current_row]]\n        \n        pivot = A[current_row, j]\n        A[current_row] = A[current_row] / pivot\n        \n        for i in range(n):\n            if i != current_row:\n                A[i] = A[i] - A[i, j] * A[current_row]\n        \n        current_row += 1\n    \n    return A"
    },
    {
        "function_name": "consensus_sequence",
        "file_name": "bioinformatics_algorithms.py",
        "parameters": {
            "`sequences`": "A list of strings representing DNA sequences.",
            "`length`": "An integer representing the length of the consensus sequence."
        },
        "objectives": [
            "Use the Needleman-Wunsch algorithm to find the optimal alignment between each pair of sequences.",
            "For each pair of sequences, calculate the similarity score based on the alignment.",
            "Return the consensus sequence, the average similarity score, and the standard deviation of the similarity scores."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def consensus_sequence(sequences, length):\n    match = 1\n    mismatch = -1\n    gap = -1\n    consensus = []\n    similarities = []\n    \n    for i in range(len(sequences)):\n        for j in range(i + 1, len(sequences)):\n            seq1, seq2 = sequences[i], sequences[j]\n            m, n = len(seq1), len(seq2)\n            dp = np.zeros((m + 1, n + 1))\n            \n            for k in range(m + 1):\n                dp[k, 0] = gap * k\n            for k in range(n + 1):\n                dp[0, k] = gap * k\n            \n            for k in range(1, m + 1):\n                for l in range(1, n + 1):\n                    if seq1[k - 1] == seq2[l - 1]:\n                        score = match\n                    else:\n                        score = mismatch\n                    dp[k, l] = max(dp[k - 1, l] + gap, dp[k, l - 1] + gap, dp[k - 1, l - 1] + score)\n            \n            alignment = []\n            k, l = m, n\n            while k > 0 or l > 0:\n                if k > 0 and l > 0 and seq1[k - 1] == seq2[l - 1]:\n                    alignment.append(seq1[k - 1])\n                    k -= 1\n                    l -= 1\n                elif k > 0 and dp[k, l] == dp[k - 1, l] + gap:\n                    alignment.append(seq1[k - 1])\n                    k -= 1\n                else:\n                    alignment.append(seq2[l - 1])\n                    l -= 1\n            \n            alignment.reverse()\n            consensus.append(alignment)\n            similarity = dp[m, n] / (m + n)\n            similarities.append(similarity)\n    \n    avg_similarity = np.mean(similarities)\n    std_dev_similarity = np.std(similarities)\n    \n    consensus = [seq[:length] for seq in consensus]\n    \n    return consensus, avg_similarity, std_dev_similarity"
    },
    {
        "function_name": "find_paths",
        "file_name": "path_search.py",
        "parameters": {
            "`graph`": "A dictionary representing an adjacency list of a graph",
            "`start`": "A node representing the starting point of the path",
            "`end`": "A node representing the end point of the path",
            "`path_length`": "An integer representing the target length of the path"
        },
        "objectives": [
            "Use a breadth-first search (BFS) algorithm to find all paths of length `path_length` from the `start` node to the `end` node",
            "Ensure that each path visits each node at most once",
            "Return the paths in the order they were found"
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def find_paths(graph, start, end, path_length):\n    queue = deque([(start, [start])])\n    result = []\n    while queue:\n        node, path = queue.popleft()\n        if len(path) == path_length + 1 and node == end:\n            result.append(path)\n        elif len(path) < path_length + 1:\n            for neighbor in graph[node]:\n                if neighbor not in path:\n                    queue.append((neighbor, path + [neighbor]))\n    return result"
    },
    {
        "function_name": "most_similar_graphs",
        "file_name": "graph_similarity.py",
        "parameters": {
            "`graphs`": "A list of dictionaries representing adjacency lists of graphs",
            "`threshold`": "A float representing the minimum similarity threshold"
        },
        "objectives": [
            "Use the Jaccard similarity coefficient to compare the graphs.",
            "Find the most similar pair of graphs with a similarity greater than or equal to the threshold.",
            "Return the indices of the most similar pair of graphs and their similarity."
        ],
        "import_lines": [],
        "function_def": "def most_similar_graphs(graphs, threshold):\n    n = len(graphs)\n    max_similarity = 0\n    most_similar_pair = (0, 0)\n    \n    for i in range(n):\n        for j in range(i + 1, n):\n            graph_i = set(graphs[i])\n            graph_j = set(graphs[j])\n            intersection = graph_i.intersection(graph_j)\n            union = graph_i.union(graph_j)\n            similarity = len(intersection) / len(union)\n            if similarity >= threshold and similarity > max_similarity:\n                max_similarity = similarity\n                most_similar_pair = (i, j)\n    \n    return most_similar_pair, max_similarity"
    },
    {
        "function_name": "common_k_mers",
        "file_name": "sequence_analysis.py",
        "parameters": {
            "`sequences`": "A list of strings representing DNA sequences",
            "`k`": "An integer representing the length of the k-mers"
        },
        "objectives": [
            "Find all k-mers that appear in all sequences.",
            "Use a hash set to store the k-mers.",
            "Return a list of the common k-mers."
        ],
        "import_lines": [],
        "function_def": "def common_k_mers(sequences, k):\n    k_mers = set()\n    for sequence in sequences:\n        sequence_k_mers = set()\n        for i in range(len(sequence) - k + 1):\n            k_mer = sequence[i:i + k]\n            sequence_k_mers.add(k_mer)\n        if not k_mers:\n            k_mers = sequence_k_mers\n        else:\n            k_mers = k_mers.intersection(sequence_k_mers)\n    \n    return list(k_mers)"
    },
    {
        "function_name": "optimal_restriction_sites",
        "file_name": "genomics.py",
        "parameters": {
            "`enzyme`": "A string representing the enzyme",
            "`dna_sequence`": "A string representing the DNA sequence",
            "`restriction_sites`": "A list of tuples representing the restriction sites and their distances"
        },
        "objectives": [
            "Find the optimal restriction sites in the DNA sequence that maximizes the number of fragments.",
            "Use dynamic programming to store the maximum number of fragments for each prefix of the sequence.",
            "Return the maximum number of fragments and the corresponding restriction sites."
        ],
        "import_lines": [],
        "function_def": "def optimal_restriction_sites(enzyme, dna_sequence, restriction_sites):\n    n = len(dna_sequence)\n    max_fragments = [1] + [0] * (n - 1)\n    sites = [(-1, -1)] * n\n    \n    for i in range(1, n):\n        for j in range(i):\n            if dna_sequence[j:i + 1] in restriction_sites:\n                distance = restriction_sites[dna_sequence[j:i + 1]]\n                if max_fragments[j] + 1 > max_fragments[i]:\n                    max_fragments[i] = max_fragments[j] + 1\n                    sites[i] = (j, distance)\n    \n    max_fragments_count = max_fragments[-1]\n    optimal_sites = []\n    i = n - 1\n    while i > 0:\n        if sites[i][0] != -1:\n            optimal_sites.append((sites[i][0], sites[i][1]))\n            i = sites[i][0]\n        else:\n            break\n    \n    optimal_sites.reverse()\n    return max_fragments_count, optimal_sites"
    },
    {
        "function_name": "cellular_automaton",
        "file_name": "cellular_automaton.py",
        "parameters": {
            "rules": "A list of strings representing the rules for the cellular automaton",
            "initial_state": "A string representing the initial state of the cellular automaton",
            "generations": "An integer representing the number of generations to simulate"
        },
        "objectives": [
            "Implement the Game of Life cellular automaton",
            "Use a 2D grid to keep track of the state of each cell",
            "Apply the rules to update the state of each cell in each generation"
        ],
        "import_lines": [],
        "function_def": "def cellular_automaton(rules, initial_state, generations):\n    grid = [[cell == '1' for cell in initial_state]]\n    for _ in range(generations):\n        new_grid = [[False for _ in range(len(grid[0]))]]\n        for i in range(len(grid)):\n            for j in range(len(grid[0])):\n                alive_neighbors = 0\n                for x in range(-1, 2):\n                    for y in range(-1, 2):\n                        if 0 <= i + x < len(grid) and 0 <= j + y < len(grid[0]):\n                            alive_neighbors += grid[i + x][j + y]\n                alive_neighbors -= grid[i][j]\n                new_grid[i][j] = grid[i][j] and alive_neighbors in [2, 3] or not grid[i][j] and alive_neighbors == 3\n        grid = new_grid\n    return ''.join('1' if cell else '0' for row in grid for cell in row)"
    },
    {
        "function_name": "network_assignment",
        "file_name": "network_optimization.py",
        "parameters": {
            "`rates`": "A list of integers representing the data transfer rates of different networks",
            "`capacities`": "A list of integers representing the capacities of different networks",
            "`demands`": "A list of integers representing the demands of different applications"
        },
        "objectives": [
            "Use the Hungarian algorithm to assign each application to a network such that the total cost is minimized",
            "The cost of assigning an application to a network is the ratio of the demand to the minimum of the rate and capacity",
            "Return the assignment and the total cost"
        ],
        "import_lines": [
            "from scipy.optimize import linear_sum_assignment"
        ],
        "function_def": "def network_assignment(rates, capacities, demands):\n    # Calculate the cost matrix\n    cost_matrix = [[max(demand / min(rate, capacity), 1) for capacity in capacities] for demand, rate in zip(demands, rates)]\n    \n    # Use the Hungarian algorithm to find the assignment\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    \n    # Calculate the total cost\n    total_cost = sum(cost_matrix[row_ind[i]][col_ind[i]] for i in range(len(row_ind)))\n    \n    return {demands[i]: capacities[col_ind[i]] for i in range(len(demands))}, total_cost"
    },
    {
        "function_name": "scheduling_algorithm",
        "file_name": "task_scheduling.py",
        "parameters": {
            "`schedule`": "A list of tuples representing the start and end times of tasks",
            "`resources`": "A list of integers representing the available resources"
        },
        "objectives": [
            "Use the Scheduling Algorithm to schedule tasks based on the Earliest Deadline First (EDF) strategy.",
            "For each task, allocate the required resources from the available resources.",
            "Return a schedule where tasks are ordered by their end times."
        ],
        "import_lines": [],
        "function_def": "def scheduling_algorithm(schedule, resources):\n    # Sort tasks by end time\n    schedule.sort(key=lambda x: x[1])\n    \n    # Allocate resources\n    allocated_resources = [0] * len(resources)\n    for i, (start, end) in enumerate(schedule):\n        for j in range(len(resources)):\n            if allocated_resources[j] <= start:\n                allocated_resources[j] = end\n                break\n    \n    # Create a schedule\n    scheduled_tasks = []\n    for i, (start, end) in enumerate(schedule):\n        scheduled_tasks.append((end, i))\n    \n    return scheduled_tasks"
    },
    {
        "function_name": "threshold_based_clustering",
        "file_name": "clustering.py",
        "parameters": {
            "`matrix`": "A 2D list representing a matrix of integers",
            "`threshold`": "An integer representing the threshold for the threshold-based clustering algorithm"
        },
        "objectives": [
            "Implement the threshold-based clustering algorithm to cluster the rows of the matrix based on their similarities.",
            "Modify the algorithm to consider the threshold constraint and return the clusters with at least two rows.",
            "Return a list of clusters, where each cluster is a list of row indices."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def compute_similarity(matrix, row1, row2):\n    return np.dot(matrix[row1], matrix[row2]) / (np.linalg.norm(matrix[row1]) * np.linalg.norm(matrix[row2]))"
    },
    {
        "function_name": "delivery_router",
        "file_name": "delivery_system.py",
        "parameters": {
            "`graph`": "A dictionary representing a weighted graph, where each key is a node and its value is a list of tuples containing the neighboring nodes and the edge weights",
            "`start`": "A string representing the starting node",
            "`end`": "A string representing the ending node",
            "`time_limit`": "An integer representing the time limit for the delivery"
        },
        "objectives": [
            "Find the shortest path in the graph from the start node to the end node using Dijkstra's algorithm.",
            "Ensure that the total time spent on the path does not exceed the time limit.",
            "If there are multiple paths with the same minimum time, return the path with the minimum cost."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def delivery_router(graph, start, end, time_limit):\n    distances = {node: float('inf') for node in graph}\n    times = {node: float('inf') for node in graph}\n    costs = {node: float('inf') for node in graph}\n    previous = {node: None for node in graph}\n    \n    distances[start] = 0\n    times[start] = 0\n    costs[start] = 0\n    \n    queue = [(0, 0, 0, start)]\n    \n    while queue:\n        current_distance, current_time, current_cost, current_node = heapq.heappop(queue)\n        \n        if current_node == end:\n            break\n        \n        for neighbor, edge_weight, edge_time, edge_cost in graph[current_node]:\n            new_distance = current_distance + edge_weight\n            new_time = current_time + edge_time\n            new_cost = current_cost + edge_cost\n            \n            if new_time <= time_limit and new_distance < distances[neighbor]:\n                distances[neighbor] = new_distance\n                times[neighbor] = new_time\n                costs[neighbor] = new_cost\n                previous[neighbor] = current_node\n                heapq.heappush(queue, (new_distance, new_time, new_cost, neighbor))\n    \n    path = []\n    current_node = end\n    while current_node != start:\n        path.append(current_node)\n        current_node = previous[current_node]\n    path.append(start)\n    path.reverse()\n    \n    return path, distances[end], times[end], costs[end]"
    },
    {
        "function_name": "count_submatrices_with_sum_greater_than_threshold",
        "file_name": "matrix_operations.py",
        "parameters": {
            "`matrix`": "A 2D list of integers representing the matrix",
            "`threshold`": "An integer representing the threshold value"
        },
        "objectives": [
            "Find the number of submatrices in the given matrix that have a sum greater than the threshold value.",
            "Use a prefix sum array to efficiently calculate the sum of each submatrix.",
            "Return the number of submatrices with a sum greater than the threshold value."
        ],
        "import_lines": [],
        "function_def": "def count_submatrices_with_sum_greater_than_threshold(matrix, threshold):\n    count = 0\n    rows, cols = len(matrix), len(matrix[0])\n    prefix_sum = [[0] * (cols + 1) for _ in range(rows + 1)]\n    \n    for i in range(1, rows + 1):\n        for j in range(1, cols + 1):\n            prefix_sum[i][j] = prefix_sum[i-1][j] + prefix_sum[i][j-1] - prefix_sum[i-1][j-1] + matrix[i-1][j-1]\n    \n    for i in range(rows):\n        for j in range(cols):\n            for k in range(i, rows):\n                for col in range(j, cols):\n                    submatrix_sum = prefix_sum[k+1][col+1] - prefix_sum[k+1][j] - prefix_sum[i][col+1] + prefix_sum[i][j]\n                    if submatrix_sum > threshold:\n                        count += 1\n    \n    return count"
    },
    {
        "function_name": "find_non_conflicting_time_slots",
        "file_name": "schedule_operations.py",
        "parameters": {
            "`schedule`": "A list of tuples representing the schedule, where each tuple contains a start time and an end time",
            "`target_time`": "An integer representing the target time"
        },
        "objectives": [
            "Find all time slots in the given schedule that are not conflicting with the target time.",
            "Use a hash table to efficiently store and retrieve time slots.",
            "Return a list of non-conflicting time slots."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def find_non_conflicting_time_slots(schedule, target_time):\n    time_slots = []\n    \n    for start, end in schedule:\n        if start < target_time < end:\n            continue\n        time_slots.append((start, end))\n    \n    return time_slots"
    },
    {
        "function_name": "epsilon_net",
        "file_name": "epsilon_net.py",
        "parameters": {
            "`points`": "A list of 2D points representing different shapes.",
            "`epsilon`": "A float representing the maximum distance between points."
        },
        "objectives": [
            "Use a epsilon-net algorithm to select a subset of points that approximate the shape of the original points.",
            "Calculate the distance between each point in the original set and its closest point in the epsilon-net.",
            "Return the epsilon-net and the distances."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def epsilon_net(points, epsilon):\n    # Initialize the epsilon-net\n    net = []\n    \n    # Select the first point\n    net.append(points[0])\n    \n    # Epsilon-net algorithm\n    for point in points[1:]:\n        if np.all(np.linalg.norm(np.array(point) - np.array(net), axis=1) > epsilon):\n            net.append(point)\n    \n    # Calculate the distances\n    distances = []\n    for point in points:\n        distances.append(np.min(np.linalg.norm(np.array(point) - np.array(net), axis=1)))\n    \n    return net, distances"
    },
    {
        "function_name": "suffix_tree",
        "file_name": "suffix_tree.py",
        "parameters": {
            "`sequence`": "A string representing a DNA sequence.",
            "`k`": "An integer representing the length of the substrings."
        },
        "objectives": [
            "Use a suffix tree algorithm to find all substrings of length k that appear at least twice in the sequence.",
            "Calculate the longest common substring between each pair of substrings.",
            "Return the substrings and the longest common substrings."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def suffix_tree(sequence, k):\n    # Initialize the suffix tree\n    tree = {}\n    \n    # Build the suffix tree\n    for i in range(len(sequence)):\n        substring = sequence[i:i+k]\n        if substring not in tree:\n            tree[substring] = []\n        tree[substring].append(i)\n    \n    # Find the substrings that appear at least twice\n    substrings = []\n    for substring, indices in tree.items():\n        if len(indices) > 1:\n            substrings.append(substring)\n    \n    # Calculate the longest common substring between each pair of substrings\n    longest_common_substrings = []\n    for i in range(len(substrings)):\n        for j in range(i+1, len(substrings)):\n            substring1 = substrings[i]\n            substring2 = substrings[j]\n            longest_common_substring = np.empty(0, dtype=int)\n            for k in range(min(len(substring1), len(substring2)), 0, -1):\n                if substring1[:k] == substring2[:k]:\n                    longest_common_substring = substring1[:k]\n                    break\n            longest_common_substrings.append(longest_common_substring)\n    \n    return substrings, longest_common_substrings"
    },
    {
        "function_name": "sequential_pattern_miner",
        "file_name": "data_mining.py",
        "parameters": {
            "`sequences`": "A list of lists of integers representing sequences of events",
            "`threshold`": "An integer representing the minimum support for a pattern",
            "`max_length`": "An integer representing the maximum length of a pattern"
        },
        "objectives": [
            "Use the PrefixSpan algorithm to find sequential patterns from the given sequences.",
            "Filter patterns based on their support and length.",
            "Return a list of patterns that satisfy the conditions."
        ],
        "import_lines": [
            "import itertools",
            "from collections import defaultdict"
        ],
        "function_def": "def sequential_pattern_miner(sequences, threshold, max_length):\n    patterns = []\n    support_counts = defaultdict(int)\n    \n    for sequence in sequences:\n        for length in range(1, max_length + 1):\n            for pattern in itertools.combinations(sequence, length):\n                support_counts[pattern] += 1\n    \n    for pattern, count in support_counts.items():\n        if count >= threshold:\n            patterns.append(pattern)\n    \n    return patterns"
    },
    {
        "function_name": "projected_triangle_areas",
        "file_name": "computer_vision.py",
        "parameters": {
            "`mesh`": "A 3D list of floats representing the 3D mesh of an object",
            "`axis`": "An integer representing the axis to calculate the surface area along (0 for x-axis, 1 for y-axis, 2 for z-axis)"
        },
        "objectives": [
            "Calculate the surface area of each individual triangle in the mesh.",
            "Project each triangle onto the specified axis and calculate the area of the projected triangle.",
            "Return a list of tuples, where each tuple contains the index of the triangle and its projected area."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def projected_triangle_areas(mesh, axis):\n    projected_areas = []\n    for i, triangle in enumerate(mesh):\n        # Calculate the surface area of the triangle\n        v1, v2, v3 = triangle\n        surface_area = 0.5 * np.linalg.norm(np.cross(v2 - v1, v3 - v1))\n        \n        # Project the triangle onto the specified axis\n        if axis == 0:\n            projected_triangle = [(v[1], v[2]) for v in triangle]\n        elif axis == 1:\n            projected_triangle = [(v[0], v[2]) for v in triangle]\n        elif axis == 2:\n            projected_triangle = [(v[0], v[1]) for v in triangle]\n        \n        # Calculate the area of the projected triangle\n        v1, v2, v3 = projected_triangle\n        projected_area = 0.5 * np.abs((v2[0] - v1[0]) * (v3[1] - v1[1]) - (v2[1] - v1[1]) * (v3[0] - v1[0]))\n        \n        projected_areas.append((i, projected_area))\n    \n    return projected_areas"
    },
    {
        "function_name": "autocorrelation_with_fourier_transform",
        "file_name": "signal_processing.py",
        "parameters": {
            "`time_series`": "A list of floats representing a time series signal",
            "`window_size`": "An integer representing the size of the window to use for the calculation"
        },
        "objectives": [
            "Calculate the Fourier transform of the time series signal.",
            "Use the Fourier transform to calculate the power spectral density (PSD) of the signal.",
            "Use the PSD to calculate the autocorrelation of the signal at each lag within the specified window size."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def autocorrelation_with_fourier_transform(time_series, window_size):\n    # Calculate the Fourier transform of the time series signal\n    fourier_transform = np.fft.fft(time_series)\n    \n    # Calculate the power spectral density (PSD) of the signal\n    psd = np.abs(fourier_transform) ** 2\n    \n    # Calculate the autocorrelation of the signal at each lag within the specified window size\n    autocorrelation = []\n    for lag in range(window_size):\n        autocorrelation.append(np.real(np.fft.ifft(psd))[-lag-1])\n    \n    return autocorrelation"
    },
    {
        "function_name": "shortest_paths_in_merged_graph",
        "file_name": "graph_analysis.py",
        "parameters": {
            "`graphs`": "A list of dictionaries representing the adjacency lists of multiple graphs",
            "`nodes`": "A list of nodes that must be present in the merged graph"
        },
        "objectives": [
            "Create a new graph that is the union of all the input graphs.",
            "Ensure that the merged graph contains all the specified nodes.",
            "Use the merged graph to calculate the shortest paths between all pairs of nodes."
        ],
        "import_lines": [
            "import networkx as nx",
            "import numpy as np"
        ],
        "function_def": "def shortest_paths_in_merged_graph(graphs, nodes):\n    # Create a new graph that is the union of all the input graphs\n    merged_graph = nx.Graph()\n    for graph in graphs:\n        merged_graph.add_edges_from(graph.edges())\n    \n    # Ensure that the merged graph contains all the specified nodes\n    for node in nodes:\n        if node not in merged_graph:\n            merged_graph.add_node(node)\n    \n    # Use the merged graph to calculate the shortest paths between all pairs of nodes\n    shortest_paths = {}\n    for node1 in merged_graph.nodes():\n        for node2 in merged_graph.nodes():\n            if node1 != node2:\n                try:\n                    shortest_path = nx.shortest_path(merged_graph, node1, node2)\n                    shortest_paths[(node1, node2)] = shortest_path\n                except nx.NetworkXNoPath:\n                    pass\n    \n    return shortest_paths"
    },
    {
        "function_name": "sentiment_classifier_with_tf_idf",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`reviews`": "A list of strings representing product reviews",
            "`sentiment_labels`": "A list of integers representing the sentiment labels of the reviews (1 for positive, 0 for negative)"
        },
        "objectives": [
            "Tokenize each review into individual words and remove stopwords.",
            "Use the tokenized reviews to calculate the term frequency-inverse document frequency (TF-IDF) of each word.",
            "Use the TF-IDF and sentiment labels to train a sentiment classifier."
        ],
        "import_lines": [
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.linear_model import LogisticRegression",
            "import re"
        ],
        "function_def": "def sentiment_classifier_with_tf_idf(reviews, sentiment_labels):\n    # Tokenize each review into individual words and remove stopwords\n    vectorizer = TfidfVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(reviews)\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, sentiment_labels, test_size=0.2, random_state=42)\n    \n    # Train a sentiment classifier using the TF-IDF and sentiment labels\n    classifier = LogisticRegression()\n    classifier.fit(X_train, y_train)\n    \n    return classifier"
    },
    {
        "function_name": "template_location_uncertainty",
        "file_name": "computer_vision.py",
        "parameters": {
            "`images`": "A list of 3D numpy arrays representing images",
            "`templates`": "A list of 3D numpy arrays representing template images"
        },
        "objectives": [
            "Use normalized cross-correlation to find the location of each template in each image.",
            "Use the locations to calculate the covariance matrix of the template locations.",
            "Use the covariance matrix to determine the uncertainty of the template locations."
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.signal import fftconvolve"
        ],
        "function_def": "def template_location_uncertainty(images, templates):\n    # Use normalized cross-correlation to find the location of each template in each image\n    locations = []\n    for image in images:\n        for template in templates:\n            correlation = fftconvolve(image, template[::-1, ::-1, ::-1], mode='same')\n            correlation /= np.linalg.norm(correlation)\n            location = np.unravel_index(np.argmax(correlation), correlation.shape)\n            locations.append(location)\n    \n    # Calculate the covariance matrix of the template locations\n    covariance_matrix = np.cov(locations)\n    \n    return covariance_matrix"
    },
    {
        "function_name": "influence_propagation",
        "file_name": "social_network.py",
        "parameters": {
            "`network`": "A dictionary representing the social network, where each key is a person and its corresponding value is a list of friends.",
            "`seed`": "A string representing the person to start the influence propagation from.",
            "`influence_probability`": "A float representing the probability of influencing a friend."
        },
        "objectives": [
            "Implement a probabilistic influence propagation algorithm to simulate the spread of influence in the social network.",
            "Use a queue data structure to keep track of people to influence next.",
            "Return the final set of influenced people."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def influence_propagation(network, seed, influence_probability):\n    influenced = set()\n    queue = deque([seed])\n    influenced.add(seed)\n    \n    while queue:\n        person = queue.popleft()\n        for friend in network[person]:\n            if friend not in influenced and np.random.rand() < influence_probability:\n                influenced.add(friend)\n                queue.append(friend)\n    \n    return influenced"
    },
    {
        "function_name": "mesh_surface_detection",
        "file_name": "computer_graphics.py",
        "parameters": {
            "`vertices`": "A numpy array representing the vertices of the mesh.",
            "`faces`": "A numpy array representing the faces of the mesh.",
            "`threshold`": "A float representing the minimum distance between the mesh and the surface to consider it close enough."
        },
        "objectives": [
            "Use a technique such as ray casting to determine whether the mesh is close enough to the surface.",
            "Use a technique such as spatial indexing to efficiently find the closest faces to each vertex.",
            "Return the mesh vertices that are close enough to the surface, along with the corresponding surface normals."
        ],
        "import_lines": [
            "import numpy as np",
            "from scipy.spatial import KDTree"
        ],
        "function_def": "def mesh_surface_detection(vertices, faces, threshold):\n    # Use ray casting to determine whether the mesh is close enough to the surface\n    ray_direction = np.array([0, 0, 1])\n    ray_origins = vertices\n    face_normals = np.cross(vertices[faces[:, 1]] - vertices[faces[:, 0]], vertices[faces[:, 2]] - vertices[faces[:, 0]])\n    \n    # Use spatial indexing to efficiently find the closest faces to each vertex\n    face_tree = KDTree(vertices[faces[:, 0]])\n    face_indices = face_tree.query(ray_origins, k=1)[1]\n    \n    close_vertices = []\n    surface_normals = []\n    for vertex, face_index in zip(ray_origins, face_indices):\n        if np.dot(ray_direction, face_normals[face_index]) < threshold:\n            close_vertices.append(vertex)\n            surface_normals.append(face_normals[face_index])\n    \n    return np.array(close_vertices), np.array(surface_normals)"
    },
    {
        "function_name": "find_pairs",
        "file_name": "pair_finder.py",
        "parameters": {
            "nums": "A list of integers",
            "k": "The number of pairs to find",
            "threshold": "The minimum sum of a pair to be considered"
        },
        "objectives": [
            "Find k pairs of numbers in the list that have a sum greater than or equal to the threshold.",
            "Use a two-pointer technique to optimize the search process.",
            "Return a list of the k pairs found."
        ],
        "import_lines": [],
        "function_def": "def find_pairs(nums, k, threshold):\n    nums.sort()\n    pairs = []\n    left, right = 0, len(nums) - 1\n    while left < right and len(pairs) < k:\n        if nums[left] + nums[right] >= threshold:\n            pairs.append((nums[left], nums[right]))\n            left += 1\n            right -= 1\n        else:\n            left += 1\n    return pairs"
    },
    {
        "function_name": "find_target_in_submatrices",
        "file_name": "matrix_search.py",
        "parameters": {
            "matrix": "A 2D list of integers",
            "target": "The target value to be found in the matrix",
            "num_rows": "The number of rows in the matrix to search in"
        },
        "objectives": [
            "Divide the matrix into sub-matrices of size num_rows x num_rows.",
            "For each sub-matrix, find all the occurrences of the target value.",
            "Return a list of the coordinates of the target value in each sub-matrix."
        ],
        "import_lines": [],
        "function_def": "def find_target_in_submatrices(matrix, target, num_rows):\n    occurrences = []\n    for i in range(0, len(matrix), num_rows):\n        for j in range(0, len(matrix[0]), num_rows):\n            sub_matrix = [row[j:j + num_rows] for row in matrix[i:i + num_rows]]\n            for x in range(num_rows):\n                for y in range(num_rows):\n                    if sub_matrix[x][y] == target:\n                        occurrences.append(((i+x, j+y), (i, j)))\n    return occurrences"
    },
    {
        "function_name": "generate_sequence",
        "file_name": "sequence_generator.py",
        "parameters": {
            "seq": "A list of integers representing a sequence",
            "length": "The length of the sequence to be generated",
            "seed": "The initial number to generate the sequence from"
        },
        "objectives": [
            "Generate a sequence of the given length using a simple linear congruential generator (LCG) algorithm.",
            "Use the seed value to start generating the sequence.",
            "Return the generated sequence."
        ],
        "import_lines": [],
        "function_def": "def generate_sequence(seq, length, seed):\n    a, c, m = 1664525, 1013904223, 2**32\n    x = seed\n    sequence = []\n    for _ in range(length):\n        x = (a * x + c) % m\n        sequence.append(x % (max(seq) - min(seq) + 1) + min(seq))\n    return sequence"
    },
    {
        "function_name": "rotate_array",
        "file_name": "array_rotator.py",
        "parameters": {
            "arr": "A list of integers",
            "k": "The number of rotations to perform"
        },
        "objectives": [
            "Perform k left rotations on the array.",
            "Use a queue data structure to optimize the rotation process.",
            "Return the rotated array."
        ],
        "import_lines": [
            "from collections import deque"
        ],
        "function_def": "def rotate_array(arr, k):\n    dq = deque(arr)\n    for _ in range(k):\n        dq.append(dq.popleft())\n    return list(dq)"
    },
    {
        "function_name": "k_largest_elements",
        "file_name": "heap_sort.py",
        "parameters": {
            "array": "A list of integers representing the array to sort",
            "k": "An integer representing the number of largest elements to find"
        },
        "objectives": [
            "Implement a heap-based approach to find the k largest elements in the array",
            "Use a min heap to store the k largest elements",
            "Return the k largest elements in sorted order"
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def k_largest_elements(array, k):\n    min_heap = []\n    for num in array:\n        if len(min_heap) < k:\n            heapq.heappush(min_heap, num)\n        elif num > min_heap[0]:\n            heapq.heappop(min_heap)\n            heapq.heappush(min_heap, num)\n    return sorted(min_heap, reverse=True)"
    },
    {
        "function_name": "score_analysis",
        "file_name": "statistics.py",
        "parameters": {
            "`scores`": "A list of tuples representing the input scores, where each tuple contains the student ID, assignment ID, and score",
            "`num_assignments`": "An integer representing the total number of assignments"
        },
        "objectives": [
            "Calculate the average score for each student across all assignments.",
            "Calculate the standard deviation of scores for each assignment.",
            "Return the student IDs with their average scores and the assignment IDs with their standard deviations."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def score_analysis(scores, num_assignments):\n    student_scores = {}\n    assignment_scores = {}\n    \n    for student_id, assignment_id, score in scores:\n        if student_id not in student_scores:\n            student_scores[student_id] = []\n        student_scores[student_id].append(score)\n        \n        if assignment_id not in assignment_scores:\n            assignment_scores[assignment_id] = []\n        assignment_scores[assignment_id].append(score)\n    \n    student_average_scores = {}\n    assignment_std_devs = {}\n    \n    for student_id, scores in student_scores.items():\n        student_average_scores[student_id] = np.mean(scores)\n    \n    for assignment_id, scores in assignment_scores.items():\n        assignment_std_devs[assignment_id] = np.std(scores)\n    \n    return student_average_scores, assignment_std_devs"
    },
    {
        "function_name": "iht_algorithm",
        "file_name": "matrix_equations.py",
        "parameters": {
            "`matrix`": "A 2D list representing the input matrix",
            "`alpha`": "A float representing the regularization parameter"
        },
        "objectives": [
            "Implement the Iterative Hard Thresholding (IHT) algorithm to find the sparse solution to the matrix equation.",
            "Return the sparse solution."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def iht_algorithm(matrix, alpha, tol=1e-6, max_iter=1000):\n    m, n = len(matrix), len(matrix[0])\n    x = np.zeros(n)\n    \n    for _ in range(max_iter):\n        x_new = x - np.dot(np.transpose(matrix), np.dot(matrix, x) - np.dot(np.transpose(matrix), np.dot(matrix, x)))\n        x_new[np.abs(x_new) < alpha] = 0\n        \n        if np.linalg.norm(x_new - x) < tol:\n            break\n        \n        x = x_new\n    \n    return x"
    },
    {
        "function_name": "highlight_keywords",
        "file_name": "text_utilities.py",
        "parameters": {
            "`text`": "A string representing the input text.",
            "`keywords`": "A list of strings representing the keywords to be highlighted."
        },
        "objectives": [
            "Implement a text highlighting algorithm to highlight the occurrences of the given keywords within the input text.",
            "Use regular expressions to search for the keywords and surround them with HTML tags to highlight them.",
            "Return the highlighted text with the keywords surrounded by HTML tags."
        ],
        "import_lines": [
            "import re"
        ],
        "function_def": "def highlight_keywords(text, keywords):\n    for keyword in keywords:\n        text = re.sub(r'\\b' + re.escape(keyword) + r'\\b', lambda match: f'<span style=\"background-color: yellow;\">{match.group(0)}</span>', text, flags=re.IGNORECASE)\n    return text"
    },
    {
        "function_name": "detect_outliers",
        "file_name": "data_analysis.py",
        "parameters": {
            "`data`": "A list of integers representing the input data.",
            "`threshold`": "An integer representing the threshold for the outlier detection."
        },
        "objectives": [
            "Implement the Z-score method for outlier detection to identify the data points that are beyond a certain threshold.",
            "Use the mean and standard deviation of the data to calculate the Z-scores.",
            "Return a list of the data points that are identified as outliers."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def detect_outliers(data, threshold):\n    mean = np.mean(data)\n    std_dev = np.std(data)\n    \n    z_scores = [(x - mean) / std_dev for x in data]\n    \n    outliers = [x for x, z in zip(data, z_scores) if abs(z) > threshold]\n    \n    return outliers"
    },
    {
        "function_name": "polygon_triangulation",
        "file_name": "geometry.py",
        "parameters": {
            "`vertices`": "A list of tuples representing the vertices of the polygon",
            "`edges`": "A list of tuples representing the edges of the polygon"
        },
        "objectives": [
            "Perform a polygon triangulation using the Ear Clipping algorithm",
            "Use a stack to keep track of the vertices to visit",
            "Return a list of triangles representing the triangulated polygon"
        ],
        "import_lines": [],
        "function_def": "def polygon_triangulation(vertices, edges):\n    # Initialize the stack\n    stack = []\n    for vertex in vertices:\n        stack.append(vertex)\n    \n    # Initialize the triangulated polygon\n    triangulated_polygon = []\n    \n    # Triangulate the polygon\n    while stack:\n        vertex = stack.pop()\n        for edge in edges:\n            if vertex in edge:\n                other_vertex = edge[0] if vertex == edge[1] else edge[1]\n                if other_vertex in stack:\n                    triangle = (vertex, other_vertex, stack[-1])\n                    triangulated_polygon.append(triangle)\n                    stack.remove(other_vertex)\n                    break\n    \n    return triangulated_polygon"
    },
    {
        "function_name": "stock_signals",
        "file_name": "stock_market.py",
        "parameters": {
            "`data`": "A list of integers representing daily stock prices.",
            "`window_size`": "An integer representing the size of the moving average window.",
            "`threshold`": "A float representing the buy/sell threshold."
        },
        "objectives": [
            "Calculate the moving average of the stock prices over the given window size.",
            "Identify the days when the price crossed the moving average and exceeded the threshold.",
            "Return a list of buy/sell signals (1 for buy, -1 for sell, 0 for no signal)."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def stock_signals(data, window_size, threshold):\n    moving_avg = np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n    signals = np.zeros(len(data))\n    for i in range(window_size, len(data)):\n        if data[i] > moving_avg[i - window_size] * (1 + threshold):\n            signals[i] = 1\n        elif data[i] < moving_avg[i - window_size] * (1 - threshold):\n            signals[i] = -1\n    return signals.tolist()"
    },
    {
        "function_name": "keyword_context",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`text`": "A string representing a piece of text.",
            "`keywords`": "A list of strings representing keywords to extract.",
            "`context_size`": "An integer representing the size of the context window."
        },
        "objectives": [
            "Tokenize the text and extract the keywords.",
            "For each keyword, extract the surrounding context of a given size.",
            "Return a dictionary where each key is a keyword and the value is a list of contexts."
        ],
        "import_lines": [
            "import re",
            "from collections import defaultdict"
        ],
        "function_def": "def keyword_context(text, keywords, context_size):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    keyword_contexts = defaultdict(list)\n    for i in range(len(tokens)):\n        if tokens[i] in keywords:\n            start = max(0, i - context_size)\n            end = min(len(tokens), i + context_size + 1)\n            context = ' '.join(tokens[start:i] + tokens[i+1:end])\n            keyword_contexts[tokens[i]].append(context)\n    return dict(keyword_contexts)"
    },
    {
        "function_name": "kmer_finder",
        "file_name": "bioinformatics.py",
        "parameters": {
            "`sequence`": "A string representing a DNA sequence.",
            "`k`": "An integer representing the length of the k-mer.",
            "`threshold`": "A float representing the minimum frequency threshold."
        },
        "objectives": [
            "Calculate the frequency of each k-mer in the sequence.",
            "Return the k-mers that appear more frequently than the given threshold.",
            "For each k-mer, calculate the reverse complement and return it as well."
        ],
        "import_lines": [
            "from collections import defaultdict"
        ],
        "function_def": "def kmer_finder(sequence, k, threshold):\n    kmers = defaultdict(int)\n    for i in range(len(sequence) - k + 1):\n        kmer = sequence[i:i+k]\n        kmers[kmer] += 1\n    frequent_kmers = []\n    for kmer, freq in kmers.items():\n        if freq > threshold * len(sequence):\n            reverse_complement = ''.join({'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}[base] for base in kmer[::-1])\n            frequent_kmers.append((kmer, reverse_complement))\n    return frequent_kmers"
    },
    {
        "function_name": "vehicle_routing",
        "file_name": "vehicle_routing.py",
        "parameters": {
            "`tour`": "A list of 2D points representing the cities",
            "`num_vehicles`": "The number of vehicles in the fleet"
        },
        "objectives": [
            "Calculate the distance between each pair of cities.",
            "Use the nearest neighbor algorithm to find a good initial solution for the vehicle routing problem.",
            "Split the tour into sub-tours based on the number of vehicles and return the sub-tours."
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def vehicle_routing(tour, num_vehicles):\n    num_cities = len(tour)\n    distances = np.zeros((num_cities, num_cities))\n    for i in range(num_cities):\n        for j in range(i + 1, num_cities):\n            distance = np.linalg.norm(np.array(tour[i]) - np.array(tour[j]))\n            distances[i][j] = distance\n            distances[j][i] = distance\n    \n    initial_solution = [0]\n    current_city = 0\n    unvisited_cities = list(range(1, num_cities))\n    while unvisited_cities:\n        next_city = None\n        min_distance = float('inf')\n        for city in unvisited_cities:\n            if distances[current_city][city] < min_distance:\n                next_city = city\n                min_distance = distances[current_city][city]\n        initial_solution.append(next_city)\n        unvisited_cities.remove(next_city)\n        current_city = next_city\n    \n    sub_tours = []\n    for i in range(num_vehicles):\n        sub_tour = []\n        for j in range(i, num_cities, num_vehicles):\n            sub_tour.append(tour[initial_solution[j]])\n        sub_tours.append(sub_tour)\n    \n    return sub_tours"
    },
    {
        "function_name": "haversine_distance",
        "file_name": "geometric_algorithms.py",
        "parameters": {
            "`lat1`, `lon1`, `lat2`, `lon2`": "Floats representing the latitude and longitude coordinates of two points on the surface of the Earth.",
            "`radius`": "Float representing the radius of the Earth in kilometers."
        },
        "objectives": [
            "Calculate the distance between two points on the surface of the Earth using the Haversine formula.",
            "Take into account the Earth's radius to convert the latitude and longitude coordinates to radians.",
            "Return the distance in kilometers."
        ],
        "import_lines": [
            "import math"
        ],
        "function_def": "def haversine_distance(lat1, lon1, lat2, lon2, radius):\n    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return radius * c"
    },
    {
        "function_name": "largest_numbers",
        "file_name": "heap_algorithms.py",
        "parameters": {
            "`numbers`": "A list of integers",
            "`k`": "An integer representing the number of largest numbers to find"
        },
        "objectives": [
            "Implement a heap-based algorithm to find the k largest numbers in the list.",
            "Use a min heap to efficiently select the k largest numbers.",
            "Return the k largest numbers in the list."
        ],
        "import_lines": [
            "import heapq"
        ],
        "function_def": "def largest_numbers(numbers, k):\n    min_heap = []\n    for num in numbers:\n        if len(min_heap) < k:\n            heapq.heappush(min_heap, num)\n        else:\n            heapq.heappushpop(min_heap, num)\n    \n    return sorted(min_heap, reverse=True)"
    },
    {
        "function_name": "min_window",
        "file_name": "algorithmic.py",
        "parameters": {
            "`Binary_string`": "A string of binary digits (0s and 1s).",
            "`window_size`": "An integer representing the window size."
        },
        "objectives": [
            "Implement a method to find the minimum window that contains all the binary digits.",
            "Use a sliding window approach to find the minimum window.",
            "Calculate the length of the minimum window.",
            "Return the minimum window and its length."
        ],
        "import_lines": [],
        "function_def": "def min_window(binary_string, window_size):\n    left = 0\n    min_length = np.inf\n    min_window = \"\"\n    \n    for right in range(len(binary_string)):\n        window = binary_string[left:right + 1]\n        if '0' in window and '1' in window and len(window) < min_length:\n            min_length = len(window)\n            min_window = window\n        if len(window) == window_size:\n            left += 1\n    \n    return min_window, min_length"
    },
    {
        "function_name": "dfs_pathfinding",
        "file_name": "pathfinding_algorithms.py",
        "parameters": {
            "`grid`": "A 2D list of integers representing a grid",
            "`obstacles`": "A list of tuples representing coordinates of obstacles in the grid",
            "`start_point`": "A tuple representing the starting point in the grid",
            "`end_point`": "A tuple representing the ending point in the grid"
        },
        "objectives": [
            "Implement a depth-first search (DFS) algorithm to find a path between the start point and the end point while avoiding obstacles.",
            "Use a stack to keep track of nodes to visit.",
            "Return the path as a list of coordinates."
        ],
        "import_lines": [],
        "function_def": "def dfs_pathfinding(grid, obstacles, start_point, end_point):\n    stack = [(start_point, [start_point])]\n    while stack:\n        (x, y), path = stack.pop()\n        if (x, y) == end_point:\n            return path\n        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            nx, ny = x + dx, y + dy\n            if (0 <= nx < len(grid) and 0 <= ny < len(grid[0]) and\n                    (nx, ny) not in obstacles and (nx, ny) not in path):\n                stack.append(((nx, ny), path + [(nx, ny)]))\n    return None"
    },
    {
        "function_name": "matrix_rank_reduction",
        "file_name": "matrix_decomposition.py",
        "parameters": {
            "`matrix`": "A 2D list of numbers representing the matrix",
            "`target_rank`": "An integer representing the target rank of the matrix"
        },
        "objectives": [
            "Use a singular value decomposition (SVD) algorithm to decompose the input matrix",
            "Perform a rank reduction on the matrix to achieve the target rank",
            "Use the rank-reduced matrix to calculate the reconstruction error"
        ],
        "import_lines": [
            "import numpy as np"
        ],
        "function_def": "def matrix_rank_reduction(matrix, target_rank):\n    # Perform SVD decomposition\n    U, sigma, Vt = np.linalg.svd(matrix)\n    \n    # Reduce the rank of the matrix\n    rank_reduced_matrix = np.dot(U[:, :target_rank], np.dot(np.diag(sigma[:target_rank]), Vt[:target_rank, :]))\n    \n    # Calculate the reconstruction error\n    reconstruction_error = np.linalg.norm(matrix - rank_reduced_matrix)\n    \n    return rank_reduced_matrix, reconstruction_error"
    },
    {
        "function_name": "burrows_wheeler",
        "file_name": "bioinformatics.py",
        "parameters": {
            "`sequence`": "A string representing a DNA sequence.",
            "`k`": "An integer representing the length of the k-mers."
        },
        "objectives": [
            "Use the Burrows-Wheeler Transform (BWT) to transform the DNA sequence into a matrix.",
            "Sort the k-mers in the matrix to produce the BWT matrix.",
            "Return the BWT matrix and the original sequence."
        ],
        "import_lines": [],
        "function_def": "def burrows_wheeler(sequence, k):\n    # Create the matrix of k-mers\n    matrix = []\n    \n    for i in range(len(sequence)):\n        kmer = sequence[i:i+k]\n        matrix.append(kmer + sequence[:i] + sequence[i+k:])\n    \n    # Sort the k-mers in the matrix\n    sorted_matrix = sorted(matrix)\n    \n    # Create the BWT matrix\n    bwt_matrix = [row[-1] for row in sorted_matrix]\n    \n    return bwt_matrix, sequence"
    },
    {
        "function_name": "textrank_segmentation",
        "file_name": "natural_language_processing.py",
        "parameters": {
            "`text`": "A string representing the text to be segmented",
            "`num_segments`": "An integer representing the number of segments",
            "`window_size`": "An integer representing the window size"
        },
        "objectives": [
            "Use the TextRank algorithm to segment the text into `num_segments` segments.",
            "Initialize the graph with sentences as nodes and edges between sentences with a shared topic.",
            "Calculate the scores of each sentence based on the graph structure and the sentence length.",
            "Select the `num_segments` sentences with the highest scores.",
            "Segment the text into `window_size`-sized windows and assign each sentence to its corresponding window.",
            "Return the segmented text."
        ],
        "import_lines": [
            "import networkx as nx"
        ],
        "function_def": "def textrank_segmentation(text, num_segments, window_size):\n    # Tokenize the text into sentences\n    sentences = text.split('. ')\n    \n    # Initialize the graph\n    graph = nx.Graph()\n    \n    # Add nodes and edges to the graph\n    for i, sentence in enumerate(sentences):\n        graph.add_node(i)\n        for j in range(i + 1, len(sentences)):\n            if len(set(sentence.split()) & set(sentences[j].split())) > 0:\n                graph.add_edge(i, j)\n    \n    # Calculate the scores of each sentence\n    scores = []\n    for i in range(len(sentences)):\n        score = 0\n        for j in range(len(sentences)):\n            if graph.has_edge(i, j):\n                score += 1\n        scores.append((i, score / len(sentence.split())))\n    \n    # Select the num_segments sentences with the highest scores\n    selected_sentences = sorted(scores, key=lambda x: x[1], reverse=True)[:num_segments]\n    \n    # Segment the text into window_size-sized windows\n    windows = []\n    for sentence in selected_sentences:\n        window = text[max(0, sentence[0] - window_size): sentence[0] + window_size]\n        windows.append(window)\n    \n    return windows"
    },
    {
        "function_name": "generate_sentences",
        "file_name": "natural_language_generation.py",
        "parameters": {
            "`text`": "a string representing the input text",
            "`num_sentences`": "an integer representing the number of sentences to generate",
            "`max_sentence_length`": "an integer representing the maximum length of a sentence"
        },
        "objectives": [
            "Use a Markov chain to generate a sentence based on the frequency of word pairs in the input text.",
            "Ensure the generated sentence is within the maximum length limit.",
            "Use a loop to generate the specified number of sentences.",
            "Return a list of generated sentences."
        ],
        "import_lines": [
            "import random"
        ],
        "function_def": "def generate_sentences(text, num_sentences, max_sentence_length):\n    # Tokenize the text into words\n    words = text.split()\n    \n    # Create a dictionary to store word pairs and their frequencies\n    word_pairs = {}\n    for i in range(len(words) - 1):\n        pair = (words[i], words[i + 1])\n        if pair in word_pairs:\n            word_pairs[pair] += 1\n        else:\n            word_pairs[pair] = 1\n    \n    generated_sentences = []\n    for _ in range(num_sentences):\n        sentence = []\n        current_word = random.choice(words)\n        sentence.append(current_word)\n        for _ in range(max_sentence_length - 1):\n            if (current_word, ) in word_pairs:\n                next_word = random.choices([pair[1] for pair in word_pairs if pair[0] == current_word], weights=[word_pairs[pair] for pair in word_pairs if pair[0] == current_word])[0]\n                sentence.append(next_word)\n                current_word = next_word\n            else:\n                break\n        generated_sentences.append(' '.join(sentence))\n    \n    return generated_sentences"
    },
    {
        "function_name": "has_path_sum",
        "file_name": "binary_tree.py",
        "parameters": {
            "`tree`": "A binary tree, where each node is a dictionary with keys 'value', 'left', and 'right'.",
            "`target`": "An integer representing the target sum."
        },
        "objectives": [
            "Implement a function to check if a path in a binary tree sums up to a target value.",
            "Use recursion to traverse the tree and check the sum of each path.",
            "Return True if a path sums up to the target value, False otherwise."
        ],
        "import_lines": [],
        "function_def": "def has_path_sum(tree, target):\n    if tree is None:\n        return False\n    if tree['value'] == target and tree['left'] is None and tree['right'] is None:\n        return True\n    return has_path_sum(tree['left'], target - tree['value']) or has_path_sum(tree['right'], target - tree['value'])"
    },
    {
        "function_name": "max_tasks",
        "file_name": "scheduling.py",
        "parameters": {
            "`schedule`": "A list of tuples, where each tuple represents a task with its start and end times."
        },
        "objectives": [
            "Implement a function to find the maximum number of tasks that can be scheduled without conflicts.",
            "Use dynamic programming to optimize the scheduling.",
            "Return the maximum number of tasks that can be scheduled."
        ],
        "import_lines": [],
        "function_def": "def max_tasks(schedule):\n    schedule.sort(key=lambda x: x[1])\n    tasks = [schedule[0]]\n    for task in schedule[1:]:\n        if task[0] >= tasks[-1][1]:\n            tasks.append(task)\n    return len(tasks)"
    },
    {
        "function_name": "ngram_frequencies",
        "file_name": "ngram_frequencies.py",
        "parameters": {
            "text": "A string representing the input text",
            "ngrams": "An integer representing the number of words in each n-gram",
            "min_frequency": "An integer representing the minimum frequency of n-grams to consider"
        },
        "objectives": [
            "Tokenize the input text into words and remove punctuation and stop words.",
            "Use a hash table to efficiently count the frequency of each n-gram in the text.",
            "Return a dictionary where the keys are the n-grams and the values are their corresponding frequencies."
        ],
        "import_lines": [
            "import re",
            "from collections import Counter"
        ],
        "function_def": "def ngram_frequencies(text, ngrams, min_frequency):\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    stop_words = set(['is', 'and', 'the', 'a', 'an', 'in', 'on', 'at', 'by', 'with'])\n    words = [word for word in words if word not in stop_words]\n    ngrams_freq = Counter()\n    for i in range(len(words) - ngrams + 1):\n        ngram = tuple(words[i:i+ngrams])\n        ngrams_freq[ngram] += 1\n    return {ngram: freq for ngram, freq in ngrams_freq.items() if freq >= min_frequency}"
    }
]